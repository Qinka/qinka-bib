@inproceedings{Zhong2017b,
abstract = {Positron emission tomography – computed tomography (PET-CT) has been widely used in modern cancer imaging. Accurate tumor delineation from PET and CT plays an important role in radiation therapy. The PET-CT co-segmentation technique, which makes use of advantages of both modalities, has achieved impressive performance for tumor delineation. In this work, we propose a novel 3D image matting based semi-automated co-segmentation method for tumor delineation on dual PET-CT scans. The “matte” values generated by 3D image matting are employed to compute the region costs for the graph based co-segmentation. Compared to previous PET-CT co-segmentation methods, our method is completely data-driven in the design of cost functions, thus using much less hyper-parameters in our segmentation model. Comparative experiments on 54 PET-CT scans of lung cancer patients demonstrated the effectiveness of our method.},
author = {Zhong, Zisha and Kim, Yusung and Buatti, John and Wu, Xiaodong},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-67564-0_4},
isbn = {9783319675633},
issn = {16113349},
keywords = {Co-segmentation,Image matting,Image segmentation,Interactive segmentation,Lung tumor segmentation},
pages = {31--42},
title = {{3D alpha matting based co-segmentation of tumors on PET-CT images}},
volume = {10555 LNCS},
year = {2017}
}
@article{Khvostikov2018,
abstract = {Computer-aided early diagnosis of Alzheimers Disease (AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the subject of extensive research in recent years. Some recent studies have shown promising results in the AD and MCI determination using structural and functional Magnetic Resonance Imaging (sMRI, fMRI), Positron Emission Tomography (PET) and Diffusion Tensor Imaging (DTI) modalities. Furthermore, fusion of imaging modalities in a supervised machine learning framework has shown promising direction of research. In this paper we first review major trends in automatic classification methods such as feature extraction based methods as well as deep learning approaches in medical image analysis applied to the field of Alzheimer's Disease diagnostics. Then we propose our own algorithm for Alzheimer's Disease diagnostics based on a convolutional neural network and sMRI and DTI modalities fusion on hippocampal ROI using data from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (http://adni.loni.usc.edu). Comparison with a single modality approach shows promising results. We also propose our own method of data augmentation for balancing classes of different size and analyze the impact of the ROI size on the classification results as well.},
annote = {{\_}eprint: 1801.05968},
archivePrefix = {arXiv},
arxivId = {1801.05968},
author = {Khvostikov, Alexander and Aderghal, Karim and Benois-Pineau, Jenny and Krylov, Andrey and Catheline, Gwenaelle},
eprint = {1801.05968},
title = {{3D CNN-based classification using sMRI and MD-DTI images for Alzheimer disease studies}},
url = {http://arxiv.org/abs/1801.05968},
year = {2018}
}
@inproceedings{Dou2016,
abstract = {Automatic liver segmentation from CT volumes is a crucial prerequisite yet challenging task for computer-aided hepatic disease diagnosis and treatment. In this paper,we present a novel 3D deeply supervised network (3D DSN) to address this challenging task. The proposed 3D DSN takes advantage of a fully convolutional architecture which performs efficient end-to-end learning and inference. More importantly,we introduce a deep supervision mechanism during the learning process to combat potential optimization difficulties,and thus the model can acquire a much faster convergence rate and more powerful discrimination capability. On top of the high-quality score map produced by the 3D DSN,a conditional random field model is further employed to obtain refined segmentation results. We evaluated our framework on the public MICCAI-SLiver07 dataset. Extensive experiments demonstrated that our method achieves competitive segmentation results to state-of-the-art approaches with a much faster processing speed.},
address = {Cham},
archivePrefix = {arXiv},
arxivId = {1607.00582},
author = {Dou, Qi and Chen, Hao and Jin, Yueming and Yu, Lequan and Qin, Jing and Heng, Pheng Ann},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46723-8_18},
editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R and Unal, Gozde and Wells, William},
eprint = {1607.00582},
isbn = {9783319467221},
issn = {16113349},
pages = {149--157},
publisher = {Springer International Publishing},
title = {{3D deeply supervised network for automatic liver segmentation from CT volumes}},
volume = {9901 LNCS},
year = {2016}
}
@inproceedings{Zhong2018,
annote = {ISSN: 1945-8452},
author = {Zhong, Z and Kim, Y and Zhou, L and Plichta, K and Allen, B and Buatti, J and Wu, X},
booktitle = {2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)},
doi = {10.1109/ISBI.2018.8363561},
keywords = {3D fully convolutional networks,Biomedical imaging,Computed tomography,Image segmentation,Lung,PET-CT images,PET-CT scans,Three-dimensional displays,Tumors,automated accurate tumor delineation,biomedical MRI,cancer,cancer diagnosis,co-segmentation,co-segmentation model,computed tomography,computerised tomography,critical diagnostic information,deep learning,dual-modality imaging,final tumor segmentation results,fully convolutional networks,graph cut,image classification,image segmentation,learning (artificial intelligence),lung,lung cancer patients,lung tumor segmentation,medical image processing,positron emission tomography,probability maps,semantic segmentation framework,tumor reading,tumours},
month = {apr},
pages = {228--231},
title = {{3D fully convolutional networks for co-segmentation of tumors on PET-CT images}},
year = {2018}
}
@inproceedings{Kim2013,
abstract = {Scene understanding is an important yet very challenging problem in computer vision. In the past few years, researchers have taken advantage of the recent diffusion of depth-RGB (RGB-D) cameras to help simplify the problem of inferring scene semantics. However, while the added 3D geometry is certainly useful to segment out objects with different depth values, it also adds complications in that the 3D geometry is often incorrect because of noisy depth measurements and the actual 3D extent of the objects is usually unknown because of occlusions. In this paper we propose a new method that allows us to jointly refine the 3D reconstruction of the scene (raw depth values) while accurately segmenting out the objects or scene elements from the 3D reconstruction. This is achieved by introducing a new model which we called Voxel-CRF. The Voxel-CRF model is based on the idea of constructing a conditional random field over a 3D volume of interest which captures the semantic and 3D geometric relationships among different elements (voxels) of the scene. Such model allows to jointly estimate (1) a dense voxel-based 3D reconstruction and (2) the semantic labels associated with each voxel even in presence of partial occlusions using an approximate yet efficient inference strategy. We evaluated our method on the challenging NYU Depth dataset (Version 1 and 2). Experimental results show that our method achieves competitive accuracy in inferring scene semantics and visually appealing results in improving the quality of the 3D reconstruction. We also demonstrate an interesting application of object removal and scene completion from RGB-D images. {\textcopyright} 2013 IEEE.},
author = {Kim, Byung Soo and Kohli, Pushmeet and Savarese, Silvio},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2013.180},
isbn = {9781479928392},
keywords = {3D reconstruction,RGB-D,Scene understanding},
pages = {1425--1432},
title = {{3D scene understanding by voxel-CRF}},
url = {https://doi.org/10.1109/ICCV.2013.180},
year = {2013}
}
@inproceedings{Wu2014,
abstract = {3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet - a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.},
annote = {{\_}eprint: 1406.5670},
archivePrefix = {arXiv},
arxivId = {1406.5670},
author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298801},
eprint = {1406.5670},
isbn = {9781467369640},
issn = {10636919},
keywords = {Computer Science - Computer Vision and Pattern Rec},
pages = {1912--1920},
title = {{3D ShapeNets: A deep representation for volumetric shapes}},
url = {http://arxiv.org/abs/1406.5670},
volume = {07-12-June},
year = {2015}
}
@inproceedings{Jurio2010,
abstract = {In this work we carry out a comparison study between different color spaces in clustering-based image segmentation. We use two similar clustering algorithms, one based on the entropy and the other on the ignorance. The study involves four color spaces and, in all cases, each pixel is represented by the values of the color channels in that space. Our purpose is to identify the best color representation, if there is any, when using this kind of clustering algorithms. {\textcopyright} Springer-Verlag Berlin Heidelberg 2010.},
address = {Berlin, Heidelberg},
author = {Jurio, Aranzazu and Pagola, Miguel and Galar, Mikel and Lopez-Molina, Carlos and Paternain, Daniel},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-642-14058-7_55},
editor = {H{\"{u}}llermeier, Eyke and Kruse, Rudolf and Hoffmann, Frank},
isbn = {9783642140570},
issn = {18650929},
keywords = {CMY,Clustering,Color space,HSV,Image segmentation,RGB,YUV},
pages = {532--541},
publisher = {Springer Berlin Heidelberg},
title = {{A comparison study of different color spaces in clustering based image segmentation}},
volume = {81 PART 2},
year = {2010}
}
@misc{Kalsotra2019,
abstract = {Background subtraction is an effective method of choice when it comes to detection of moving objects in videos and has been recognized as a breakthrough for the wide range of applications of intelligent video analytics (IVA). In recent years, a number of video datasets intended for background subtraction have been created to address the problem of large realistic datasets with accurate ground truth. The use of these datasets enables qualitative as well as quantitative comparisons and allows benchmarking of different algorithms. Finding the appropriate dataset is generally a cumbersome task for an exhaustive evaluation of algorithms. Therefore, we systematically survey standard video datasets and list their applicability for different applications. This paper presents a comprehensive account of public video datasets for background subtraction and attempts to cover the lack of a detailed description of each dataset. The video datasets are presented in chronological order of their appearance. Current trends of deep learning in background subtraction along with top-ranked background subtraction methods are also discussed in this paper. The survey introduced in this paper will assist researchers of the computer vision community in the selection of appropriate video dataset to evaluate their algorithms on the basis of challenging scenarios that exist in both indoor and outdoor environments.},
author = {Kalsotra, Rudrika and Arora, Sakshi},
booktitle = {IEEE Access},
doi = {10.1109/ACCESS.2019.2914961},
issn = {21693536},
keywords = {Background model,background subtraction,challenges,datasets,deep neural networks,foreground,intelligent video analytics (IVA),video frames},
pages = {59143--59171},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A Comprehensive Survey of Video Datasets for Background Subtraction}},
volume = {7},
year = {2019}
}
@article{Zhang2017,
abstract = {Automated cardiac image interpretation has the potential to transform clinical practice in multiple ways including enabling low-cost serial assessment of cardiac function in the primary care and rural setting. We hypothesized that advances in computer vision could enable building a fully automated, scalable analysis pipeline for echocardiogram (echo) interpretation. Our approach entailed: 1) preprocessing; 2) convolutional neural networks (CNN) for view identification, image segmentation, and phasing of the cardiac cycle; 3) quantification of chamber volumes and left ventricular mass; 4) particle tracking to compute longitudinal strain; and 5) targeted disease detection. CNNs accurately identified views (e.g. 99{\%} for apical 4-chamber) and segmented individual cardiac chambers. Cardiac structure measurements agreed with study report values (e.g. mean absolute deviations (MAD) of 7.7 mL/kg/m2 for left ventricular diastolic volume index, 2918 studies). We computed automated ejection fraction and longitudinal strain measurements (within 2 cohorts), which agreed with commercial software-derived values [for ejection fraction, MAD=5.3{\%}, N=3101 studies; for strain, MAD=1.5{\%} (n=197) and 1.6{\%} (n=110)], and demonstrated applicability to serial monitoring of breast cancer patients for trastuzumab cardiotoxicity. Overall, we found that, compared to manual measurements, automated measurements had superior performance across seven internal consistency metrics with an average increase in the Spearman correlation coefficient of 0.05 (p=0.02). Finally, we developed disease detection algorithms for hypertrophic cardiomyopathy and cardiac amyloidosis, with C-statistics of 0.93 and 0.84, respectively. Our pipeline lays the groundwork for using automated interpretation to support point-of-care handheld cardiac ultrasound and large-scale analysis of the millions of echos archived within healthcare systems.},
annote = {{\_}eprint: 1706.07342},
archivePrefix = {arXiv},
arxivId = {1706.07342},
author = {Zhang, Jeffrey and Gajjala, Sravani and Agrawal, Pulkit and Tison, Geoffrey H and Hallock, Laura A and Beussink-Nelson, Lauren and Fan, Eugene and Aras, Mandar A and Jordan, ChaRandle and Fleischmann, Kirsten E and Melisko, Michelle and Qasim, Atif and Efros, Alexei and Shah, Sanjiv J and Bajcsy, Ruzena and Deo, Rahul C},
eprint = {1706.07342},
title = {{A Computer Vision Pipeline for Automated Determination of Cardiac Structure and Function and Detection of Disease by Two-Dimensional Echocardiography}},
url = {http://arxiv.org/abs/1706.07342},
year = {2017}
}
@article{Kumar2017,
abstract = {Nuclear segmentation in digital microscopic tissue images can enable extraction of high-quality features for nuclear morphometrics and other analysis in computational pathology. Conventional image processing techniques, such as Otsu thresholding and watershed segmentation, do not work effectively on challenging cases, such as chromatin-sparse and crowded nuclei. In contrast, machine learning-based segmentation can generalize across various nuclear appearances. However, training machine learning algorithms requires data sets of images, in which a vast number of nuclei have been annotated. Publicly accessible and annotated data sets, along with widely agreed upon metrics to compare techniques, have catalyzed tremendous innovation and progress on other image classification problems, particularly in object recognition. Inspired by their success, we introduce a large publicly accessible data set of hematoxylin and eosin (HE)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries, whose quality was validated by a medical doctor. Because our data set is taken from multiple hospitals and includes a diversity of nuclear appearances from several patients, disease states, and organs, techniques trained on it are likely to generalize well and work right out-of-the-box on other HE-stained images. We also propose a new metric to evaluate nuclear segmentation results that penalizes object- and pixel-level errors in a unified manner, unlike previous metrics that penalize only one type of error. We also propose a segmentation technique based on deep learning that lays a special emphasis on identifying the nuclear boundaries, including those between the touching or overlapping nuclei, and works well on a diverse set of test images.},
author = {Kumar, Neeraj and Verma, Ruchika and Sharma, Sanuj and Bhargava, Surabhi and Vahadane, Abhishek and Sethi, Amit},
doi = {10.1109/TMI.2017.2677499},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Annotation,boundaries,dataset,deep learning,nuclear segmentation,nuclei},
month = {jul},
number = {7},
pages = {1550--1560},
title = {{A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology}},
volume = {36},
year = {2017}
}
@article{Zhao2018,
abstract = {Accurate and reliable brain tumor segmentation is a critical component in cancer diagnosis, treatment planning, and treatment outcome evaluation. Build upon successful deep learning techniques, a novel brain tumor segmentation method is developed by integrating fully convolutional neural networks (FCNNs) and Conditional Random Fields (CRFs) in a unified framework to obtain segmentation results with appearance and spatial consistency. We train a deep learning based segmentation model using 2D image patches and image slices in following steps: 1) training FCNNs using image patches; 2) training CRFs as Recurrent Neural Networks (CRF-RNN) using image slices with parameters of FCNNs fixed; and 3) fine-tuning the FCNNs and the CRF-RNN using image slices. Particularly, we train 3 segmentation models using 2D image patches and slices obtained in axial, coronal and sagittal views respectively, and combine them to segment brain tumors using a voting based fusion strategy. Our method could segment brain images slice-by-slice, much faster than those based on image patches. We have evaluated our method based on imaging data provided by the Multimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015 and BRATS 2016. The experimental results have demonstrated that our method could build a segmentation model with Flair, T1c, and T2 scans and achieve competitive performance as those built with Flair, T1, T1c, and T2 scans.},
author = {Zhao, Xiaomei and Wu, Yihong and Song, Guidong and Li, Zhenye and Zhang, Yazhuo and Fan, Yong},
doi = {https://doi.org/10.1016/j.media.2017.10.002},
issn = {1361-8415},
journal = {Medical Image Analysis},
keywords = {Brain tumor segmentation,Conditional random fields,Deep learning,Fully convolutional neural networks},
pages = {98 -- 111},
title = {{A deep learning model integrating FCNNs and CRFs for brain tumor segmentation}},
url = {http://www.sciencedirect.com/science/article/pii/S136184151730141X},
volume = {43},
year = {2018}
}
@inproceedings{Zhou2017,
abstract = {Deep neural networks have been widely adopted for automatic organ segmentation from abdominal CT scans. However, the segmentation accuracy of some small organs (e.g., the pancreas) is sometimes below satisfaction, arguably because deep networks are easily disrupted by the complex and variable background regions which occupies a large fraction of the input volume. In this paper, we formulate this problem into a fixed-point model which uses a predicted segmentation mask to shrink the input region. This is motivated by the fact that a smaller input region often leads to more accurate segmentation. In the training process, we use the ground-truth annotation to generate accurate input regions and optimize network weights. On the testing stage, we fix the network parameters and update the segmentation results in an iterative manner. We evaluate our approach on the NIH pancreas segmentation dataset, and outperform the state-of-the-art by more than {\$}{\$}4$\backslash$backslash{\%}{\$}{\$}, measured by the average Dice-S{\o}rensen Coefficient (DSC). In addition, we report {\$}{\$}62.43$\backslash$backslash{\%}{\$}{\$}DSC in the worst case, which guarantees the reliability of our approach in clinical applications.},
address = {Cham},
author = {Zhou, Yuyin and Xie, Lingxi and Shen, Wei and Wang, Yan and Fishman, Elliot K and Yuille, Alan L},
booktitle = {Medical Image Computing and Computer Assisted Intervention − MICCAI 2017},
editor = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D Louis and Duchesne, Simon},
isbn = {978-3-319-66182-7},
pages = {693--701},
publisher = {Springer International Publishing},
title = {{A Fixed-Point Model for Pancreas Segmentation in Abdominal CT Scans}},
year = {2017}
}
@article{Jimenez-Carretero2019,
abstract = {Lung vessel segmentation has been widely explored by the biomedical image processing community; however, the differentiation of arterial from venous irrigation is still a challenge. Pulmonary artery–vein (AV) segmentation using computed tomography (CT) is growing in importance owing to its undeniable utility in multiple cardiopulmonary pathological states, especially those implying vascular remodelling, allowing the study of both flow systems separately. We present a new framework to approach the separation of tree-like structures using local information and a specifically designed graph-cut methodology that ensures connectivity as well as the spatial and directional consistency of the derived subtrees. This framework has been applied to the pulmonary AV classification using a random forest (RF) pre-classifier to exploit the local anatomical differences of arteries and veins. The evaluation of the system was performed using 192 bronchopulmonary segment phantoms, 48 anthropomorphic pulmonary CT phantoms, and 26 lungs from noncontrast CT images with precise voxel-based reference standards obtained by manually labelling the vessel trees. The experiments reveal a relevant improvement in the accuracy (∼ 20{\%}) of the vessel particle classification with the proposed framework with respect to using only the pre-classification based on local information applied to the whole area of the lung under study. The results demonstrated the accurate differentiation between arteries and veins in both clinical and synthetic cases, specifically when the image quality can guarantee a good airway segmentation, which opens a huge range of possibilities in the clinical study of cardiopulmonary diseases.},
author = {Jimenez-Carretero, Daniel and Bermejo-Pel{\'{a}}ez, David and Nardelli, Pietro and Fraga, Patricia and Fraile, Eduardo and {San Jos{\'{e}} Est{\'{e}}par}, Ra{\'{u}}l and Ledesma-Carbayo, Maria J},
doi = {10.1016/j.media.2018.11.011},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Arteries,Artery-vein segmentation,Graph-cuts,Lung,Noncontrast CT,Phantoms,Random forest,Veins},
pages = {144--159},
title = {{A graph-cut approach for pulmonary artery-vein segmentation in noncontrast CT images}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841518308740},
volume = {52},
year = {2019}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed. {\textcopyright} 1943 The University of Chicago Press.},
author = {McCulloch, Warren S and Pitts, Walter},
doi = {10.1007/BF02478259},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
number = {4},
pages = {115--133},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {https://doi.org/10.1007/BF02478259},
volume = {5},
year = {1943}
}
@article{Avants2011,
abstract = {The United States National Institutes of Health (NIH) commit significant support to open-source data and software resources in order to foment reproducibility in the biomedical imaging sciences. Here, we report and evaluate a recent product of this commitment: Advanced Neuroimaging Tools (ANTs), which is approaching its 2.0 release. The ANTs open source software library consists of a suite of state-of-the-art image registration, segmentation and template building tools for quantitative morphometric analysis. In this work, we use ANTs to quantify, for the first time, the impact of similarity metrics on the affine and deformable components of a template-based normalization study. We detail the ANTs implementation of three similarity metrics: squared intensity difference, a new and faster cross-correlation, and voxel-wise mutual information. We then use two-fold cross-validation to compare their performance on openly available, manually labeled, T1-weighted MRI brain image data of 40 subjects (UCLA's LPBA40 dataset). We report evaluation results on cortical and whole brain labels for both the affine and deformable components of the registration. Results indicate that the best ANTs methods are competitive with existing brain extraction results (Jaccard = 0.958) and cortical labeling approaches. Mutual information affine mapping combined with cross-correlation diffeomorphic mapping gave the best cortical labeling results (Jaccard = 0.669. ±. 0.022). Furthermore, our two-fold cross-validation allows us to quantify the similarity of templates derived from different subgroups. Our open code, data and evaluation scripts set performance benchmark parameters for this state-of-the-art toolkit. This is the first study to use a consistent transformation framework to provide a reproducible evaluation of the isolated effect of the similarity metric on optimal template construction and brain labeling. {\textcopyright} 2010 Elsevier Inc.},
author = {Avants, Brian B and Tustison, Nicholas J and Song, Gang and Cook, Philip A and Klein, Arno and Gee, James C},
doi = {10.1016/j.neuroimage.2010.09.025},
issn = {10538119},
journal = {NeuroImage},
number = {3},
pages = {2033--2044},
pmid = {20851191},
title = {{A reproducible evaluation of ANTs similarity metric performance in brain image registration}},
url = {http://www.sciencedirect.com/science/article/pii/S1053811910012061},
volume = {54},
year = {2011}
}
@article{Hsu2016,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Fowler, Bridget},
doi = {10.1177/02632760022050997},
eprint = {0-387-31073-8},
institution = {Department of Computer Science National Taiwan University, Taipei 106, Taiwan},
isbn = {013805326X},
issn = {02632764},
journal = {Theory, Culture and Society},
month = {may},
number = {1},
pages = {39--61},
pmid = {18190633},
title = {{A sociological analysis of the satanic verses affair}},
url = {http://www.csie.ntu.edu.tw/{\%}7B{~}{\%}7Dcjlin/papers/guide/guide.pdf},
volume = {17},
year = {2000}
}
@article{Jeyavathana2016,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 305502844 A : Analysis -processing Segmentation Article CITATIONS 0 READS 171 3 , including : Balasubramanian Manonmaniam 17 SEE Anbarasa Manonmaniam 12 SEE All . The . Abstract : Pre - Processing and Segmentation Techniques are used in the application of medical images . Image segmentation is a tediousprocess due to restrictions on Image acquisitions . The most important goal of medical image segmentation is to perform operations on images to detect patterns and to retrieve information from it . In this paper , first medical image processing is discussed . Then we have been proposed approaches to segment CT and CXR images . The comparative study of various image processing techniques has been given in tabular form . This survey provides details of automated segmentation methods , specifically discussed in the context of CT images . The motive is to discuss the problems encountered in the segmentation of CT images , and the relative merits and limitations of methods currently available for segmentation of medical images .},
author = {{Beaulah Jeyavathana}, R and Balasubramanian, R and Pandian, A Anbarasa},
journal = {International Journal of Research and Scientific Innovation},
keywords = {CT,CXR,Pre - processing,Segmentation},
number = {June},
pages = {2321--2705},
title = {{A Survey : Analysis on Pre - processing and Segmentation Techniques for Medical Images}},
volume = {III},
year = {2016}
}
@inproceedings{Lee2015,
abstract = {A precise analysis of medical image is an important stage in the contouring phase throughout radiotherapy preparation. Medical images are mostly used as radiographic techniques in diagnosis, clinical studies and treatment planning Medical image processing tool are also similarly as important. With a medical image processing tool, it is possible to speed up and enhance the operation of the analysis of the medical image. This paper describes medical image processing software tool which attempts to secure the same kind of programmability advantage for exploring applications of the pipelined processors. These tools simulate complete systems consisting of several of the proposed processing components, in a configuration described by a graphical schematic diagram. In this paper, fifteen different medical image processing tools will be compared in several aspects. The main objective of the comparison is to gather and analysis on the tool in order to recommend users of different operating systems on what type of medical image tools to be used when analysing different types of imaging. A result table was attached and discussed in the paper.},
author = {Lee, Lay Khoon and Liew, Siau Chuin},
booktitle = {2015 4th International Conference on Software Engineering and Computer Systems, ICSECS 2015: Virtuous Software Solutions for Big Data},
doi = {10.1109/ICSECS.2015.7333105},
isbn = {9781467367226},
keywords = {computer vision,image processing,tools component},
pages = {171--176},
title = {{A survey of medical image processing tools}},
year = {2015}
}
@article{Yong2012,
abstract = {More than 30 students from university campus participated in the Development of Biomedical Image Processing Software Package for New Learners Survey investigating the use of software package for processing and editing image. The survey was available online for six months. Facts and opinions were sought to learn the general information, interactive image processing tool, non-interactive (automatic) tool, current status and future of image processing package tool. Composed of 19 questions, the survey built a comprehensive picture of the software package, programming language, workflow of the tool and captured the attitudes of the respondents. Result shows that MATLAB was difficult to use but it was viewed in high regard however. The result of this study is expected to be beneficial and able to assist users on effective image processing and analysis in a newly developed software package.},
author = {Yong, Ching Yee and Chew, Kim Mey and Mahmood, Nasrul Humaimi and Ariffin, Ismail},
doi = {10.1016/j.sbspro.2012.09.654},
issn = {18770428},
journal = {Procedia - Social and Behavioral Sciences},
keywords = {Image editting,Image processing,Medical imaging,Software package,Visualisation tools},
pages = {265--271},
title = {{A Survey of Visualization Tools in Medical Imaging}},
url = {http://www.sciencedirect.com/science/article/pii/S187704281204116X},
volume = {56},
year = {2012}
}
@misc{Litjens2017,
abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
archivePrefix = {arXiv},
arxivId = {1702.05747},
author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A.W.M. and van Ginneken, Bram and S{\'{a}}nchez, Clara I},
booktitle = {Medical Image Analysis},
doi = {10.1016/j.media.2017.07.005},
eprint = {1702.05747},
issn = {13618423},
keywords = {Convolutional neural networks,Deep learning,Medical imaging,Survey},
pages = {60--88},
pmid = {28778026},
title = {{A survey on deep learning in medical image analysis}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841517301135},
volume = {42},
year = {2017}
}
@article{Shorten2019,
abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
author = {Shorten, Connor and Khoshgoftaar, Taghi M},
doi = {10.1186/s40537-019-0197-0},
issn = {21961115},
journal = {Journal of Big Data},
keywords = {Big data,Data Augmentation,Deep Learning,GANs,Image data},
number = {1},
pages = {60},
title = {{A survey on Image Data Augmentation for Deep Learning}},
url = {https://doi.org/10.1186/s40537-019-0197-0},
volume = {6},
year = {2019}
}
@article{Yao2017,
abstract = {Pre-processing is an important step in digital image matting, which aims to classify more accurate foreground and background pixels from the unknown region of the input three-region mask (Trimap). This step has no relation with the well-known matting equation and only compares color differences between the current unknown pixel and those known pixels. These newly classified pure pixels are then fed to the matting process as samples to improve the quality of the final matte. However, in the research field of image matting, the importance of pre-processing step is still blurry. Moreover, there are no corresponding review articles for this step, and the quantitative comparison of Trimap and alpha mattes after this step still remains unsolved. In this paper, the necessity and the importance of pre-processing step in image matting are firstly discussed in details. Next, current pre-processing methods are introduced by using the following two categories: static thresholding methods and dynamic thresholding methods. Analyses and experimental results show that static thresholding methods, especially the most popular iterative method, can make accurate pixel classifications in those general Trimaps with relatively fewer unknown pixels. However, in a much larger Trimap, there methods are limited by the conservative color and spatial thresholds. In contrast, dynamic thresholding methods can make much aggressive classifications on much difficult cases, but still strongly suffer from noises and false classifications. In addition, the sharp boundary detector is further discussed as a prior of pure pixels. Finally, summaries and a more effective approach are presented for pre-processing compared with the existing methods.},
author = {Yao, Gui Lin},
doi = {10.1007/s11390-017-1709-z},
issn = {10009000},
journal = {Journal of Computer Science and Technology},
keywords = {Trimap expansion,image matting,pixel classification,pre-processing},
number = {1},
pages = {122--138},
title = {{A Survey on Pre-Processing in Image Matting}},
url = {https://doi.org/10.1007/s11390-017-1709-z},
volume = {32},
year = {2017}
}
@article{Hindy2018,
abstract = {With the world moving towards being increasingly dependent on computers and automation, one of the main challenges in the current decade has been to build secure applications, systems and networks. Alongside these challenges, the number of threats is rising exponentially due to the attack surface increasing through numerous interfaces offered for each service. To alleviate the impact of these threats, researchers have proposed numerous solutions; however, current tools often fail to adapt to ever-changing architectures, associated threats and 0-days. This manuscript aims to provide researchers with a taxonomy and survey of current dataset composition and current Intrusion Detection Systems (IDS) capabilities and assets. These taxonomies and surveys aim to improve both the efficiency of IDS and the creation of datasets to build the next generation IDS as well as to reflect networks threats more accurately in future datasets. To this end, this manuscript also provides a taxonomy and survey or network threats and associated tools. The manuscript highlights that current IDS only cover 25{\%} of our threat taxonomy, while current datasets demonstrate clear lack of real-network threats and attack representation, but rather include a large number of deprecated threats, hence limiting the accuracy of current machine learning IDS. Moreover, the taxonomies are open-sourced to allow public contributions through a Github repository.},
archivePrefix = {arXiv},
arxivId = {1806.03517},
author = {Hindy, Hanan and Brosset, David and Bayne, Ethan and Seeam, Amar and Tachtatzis, Christos and Atkinson, Robert and Bellekens, Xavier},
eprint = {1806.03517},
month = {jun},
title = {{A Taxonomy and Survey of Intrusion Detection System Design Techniques, Network Threats and Datasets}},
url = {http://arxiv.org/abs/1806.03517},
year = {2018}
}
@inproceedings{Song2017,
abstract = {We propose a method to reconstruct, complete and semantically label a 3D scene from a single input depth image. We improve the accuracy of the regressed semantic 3D maps by a novel architecture based on adversarial learning. In particular, we suggest using multiple adversarial loss terms that not only enforce realistic outputs with respect to the ground truth, but also an effective embedding of the internal features. This is done by correlating the latent features of the encoder working on partial 2.5D data with the latent features extracted from a variational 3D auto-encoder trained to reconstruct the complete semantic scene. In addition, differently from other approaches that operate entirely through 3D convolutions, at test time we retain the original 2.5D structure of the input during downsampling to improve the effectiveness of the internal representation of our model. We test our approach on the main benchmark datasets for semantic scene completion to qualitatively and quantitatively assess the effectiveness of our proposal.},
archivePrefix = {arXiv},
arxivId = {1810.10901},
author = {Wang, Yida and Tan, David Joseph and Navab, Nassir and Tombari, Federico},
booktitle = {Proceedings - 2018 International Conference on 3D Vision, 3DV 2018},
doi = {10.1109/3DV.2018.00056},
eprint = {1810.10901},
isbn = {9781538684252},
keywords = {Adversarial training,Depth image,Latent space,Scene completion},
pages = {426--434},
title = {{Adversarial semantic scene completion from a single depth image}},
year = {2018}
}
@inproceedings{Khagi2019,
abstract = {Various Convolutional Neural Network (CNN) architecture has been proposed for image classification and Object recognition. For the image based classification, it is a complex task for CNN to deal with hundreds of MRI Image slices, each of almost identical nature in a single patient. So, classifying a number of patients as an AD, MCI or NC based on 3D MRI becomes vague technique using 2D CNN architecture. Hence, to address this issue, we have simplified the idea of classifying patients on basis of 3D MRI but acknowledging the 2D features generated from the CNN framework. We present our idea regarding how to obtain 2D features from MRI and transform it to be applicable to classify using machine learning algorithm. Our experiment shows the result of classifying 3 class subjects patients. We employed scratched trained CNN or pretrained Alexnet CNN as generic feature extractor of 2D image which dimensions were reduced using PCA+TSNE, and finally classifying using simple Machine learning algorithm like KNN, Navies Bayes Classifier. Although the result is not so impressive but it definitely shows that this can be better than scratch trained CNN softmax classification based on probability score. The generated feature can be well manipulated and refined for better accuracy, sensitivity, and specificity.},
author = {Khagi, Bijen and Lee, Chung Ghiu and Kwon, Goo Rak},
booktitle = {BMEiCON 2018 - 11th Biomedical Engineering International Conference},
doi = {10.1109/BMEiCON.2018.8609974},
isbn = {9781538657249},
keywords = {CNN,Classifier,Generic feature,MRI,PCA,TSNE},
title = {{Alzheimer's disease Classification from Brain MRI based on transfer learning from CNN}},
year = {2019}
}
@article{Chen2018,
abstract = {Fully-connected Conditional Random Field (CRF) is often used as post-processing to refine voxel classification results by encouraging spatial coherence. In this paper, we propose a new end-to-end training method called Posterior-CRF. In contrast with previous approaches which use the original image intensity in the CRF, our approach applies 3D, fully connected CRF to the posterior probabilities from a CNN and optimizes both CNN and CRF together. The experiments on white matter hyperintensities segmentation demonstrate that our method outperforms CNN, post-processing CRF and different end-to-end training CRF approaches.},
annote = {{\_}eprint: 1811.03549},
archivePrefix = {arXiv},
arxivId = {1811.03549},
author = {Chen, Shuai and de Bruijne, Marleen},
eprint = {1811.03549},
title = {{An End-to-end Approach to Semantic Segmentation with 3D CNN and Posterior-CRF in Medical Images}},
url = {http://arxiv.org/abs/1811.03549},
year = {2018}
}
@article{Ruder2016,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
annote = {{\_}eprint: 1609.04747},
archivePrefix = {arXiv},
arxivId = {1609.04747},
author = {Ruder, Sebastian},
eprint = {1609.04747},
journal = {CoRR},
title = {{An overview of gradient descent optimization algorithms}},
url = {http://arxiv.org/abs/1609.04747},
volume = {abs/1609.0},
year = {2016}
}
@article{Thomas2014,
abstract = {Tractography based on diffusion-weighted MRI (DWI) is widely used for mapping the structural connections of the human brain. Its accuracy is known to be limited by technical factors affecting in vivo data acquisition, such as noise, artifacts, and data undersampling resulting from scan time constraints. It generally is assumed that improvements in data quality and implementation of sophisticated tractography methods will lead to increasingly accurate maps of human anatomical connections. However, assessing the anatomical accuracy of DWI tractography is difficult because of the lack of independent knowledge of the true anatomical connections in humans. Here we investigate the future prospects of DWI-based connectional imaging by applying advanced tractography methods to an ex vivo DWI dataset of the macaque brain. The results of different tractography methods were compared with maps of known axonal projections from previous tracer studies in the macaque. Despite the exceptional quality of the DWI data, none of the methods demonstrated high anatomical accuracy. The methods that showed the highest sensitivity showed the lowest specificity, and vice versa. Additionally, anatomical accuracy was highly dependent upon parameters of the tractography algorithm, with different optimal values for mapping different pathways. These results suggest that there is an inherent limitation in determining long-range anatomical projections based on voxelaveraged estimates of local fiber orientation obtained from DWI data that is unlikely to be overcome by improvements in data acquisition and analysis alone.},
author = {Thomas, Cibu and Ye, Frank Q. and Irfanoglu, M. Okan and Modi, Pooja and Saleem, Kadharbatcha S. and Leopold, David A. and Pierpaoli, Carlo},
doi = {10.1073/pnas.1405672111},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = {nov},
number = {46},
pages = {16574--16579},
title = {{Anatomical accuracy of brain connections derived from diffusion MRI tractography is inherently limited}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1405672111},
volume = {111},
year = {2014}
}
@inproceedings{Jin2018,
abstract = {Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet [51], PNAS [29], usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Extensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The code and documentation are available at https://autokeras.com. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.},
archivePrefix = {arXiv},
arxivId = {1806.10282},
author = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/3292500.3330648},
eprint = {1806.10282},
isbn = {9781450362016},
keywords = {AutoML,Automated Machine Learning,Bayesian Optimization,Network Morphism,Neural Architecture Search},
pages = {1946--1956},
title = {{Auto-keras: An efficient neural architecture search system}},
year = {2019}
}
@article{Liu2019,
abstract = {Accurate diagnosis of thyroid nodules using ultrasonography is a valuable but tough task even for experienced radiologists, considering both benign and malignant nodules have heterogeneous appearances. Computer-aided diagnosis (CAD) methods could potentially provide objective suggestions to assist radiologists. However, the performance of existing learning-based approaches is still limited, for direct application of general learning models often ignores critical domain knowledge related to the specific nodule diagnosis. In this study, we propose a novel deep-learning-based CAD system, guided by task-specific prior knowledge, for automated nodule detection and classification in ultrasound images. Our proposed CAD system consists of two stages. First, a multi-scale region-based detection network is designed to learn pyramidal features for detecting nodules at different feature scales. The region proposals are constrained by the prior knowledge about size and shape distributions of real nodules. Then, a multi-branch classification network is proposed to integrate multi-view diagnosis-oriented features, in which each network branch captures and enhances one specific group of characteristics that were generally used by radiologists. We evaluated and compared our method with the state-of-the-art CAD methods and experienced radiologists on two datasets, i.e. Dataset I and Dataset II. The detection and diagnostic accuracy on Dataset I were 97.5{\%} and 97.1{\%}, respectively. Besides, our CAD system also achieved better performance than experienced radiologists on Dataset II, with improvements of accuracy for 8{\%}. The experimental results demonstrate that our proposed method is effective in the discrimination of thyroid nodules.},
author = {Liu, Tianjiao and Guo, Qianqian and Lian, Chunfeng and Ren, Xuhua and Liang, Shujun and Yu, Jing and Niu, Lijuan and Sun, Weidong and Shen, Dinggang},
doi = {10.1016/j.media.2019.101555},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Clinical knowledge,Convolutional neural networks,Thyroid nodule,Ultrasound image},
pages = {101555},
pmid = {31520984},
title = {{Automated detection and classification of thyroid nodules in ultrasound images using clinical-knowledge-guided convolutional neural networks}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841519300970},
volume = {58},
year = {2019}
}
@article{Lu2017,
abstract = {Purpose: Segmentation of the liver from abdominal computed tomography (CT) images is an essential step in some computer-assisted clinical interventions, such as surgery planning for living donor liver transplant, radiotherapy and volume measurement. In this work, we develop a deep learning algorithm with graph cut refinement to automatically segment the liver in CT scans. Methods: The proposed method consists of two main steps: (i) simultaneously liver detection and probabilistic segmentation using 3D convolutional neural network; (ii) accuracy refinement of the initial segmentation with graph cut and the previously learned probability map. Results: The proposed approach was validated on forty CT volumes taken from two public databases MICCAI-Sliver07 and 3Dircadb1. For the MICCAI-Sliver07 test dataset, the calculated mean ratios of volumetric overlap error (VOE), relative volume difference (RVD), average symmetric surface distance (ASD), root-mean-square symmetric surface distance (RMSD) and maximum symmetric surface distance (MSD) are 5.9, 2.7 {\%}, 0.91, 1.88 and 18.94 mm, respectively. For the 3Dircadb1 dataset, the calculated mean ratios of VOE, RVD, ASD, RMSD and MSD are 9.36, 0.97 {\%}, 1.89, 4.15 and 33.14 mm, respectively. Conclusions: The proposed method is fully automatic without any user interaction. Quantitative results reveal that the proposed approach is efficient and accurate for hepatic volume estimation in a clinical setup. The high correlation between the automatic and manual references shows that the proposed method can be good enough to replace the time-consuming and nonreproducible manual segmentation method.},
archivePrefix = {arXiv},
arxivId = {1605.03012},
author = {Lu, Fang and Wu, Fa and Hu, Peijun and Peng, Zhiyi and Kong, Dexing},
doi = {10.1007/s11548-016-1467-3},
eprint = {1605.03012},
issn = {18616429},
journal = {International Journal of Computer Assisted Radiology and Surgery},
keywords = {3D convolution neural network,CT images,Graph cut,Liver segmentation},
number = {2},
pages = {171--182},
title = {{Automatic 3D liver location and segmentation via convolutional neural network and graph cut}},
url = {https://doi.org/10.1007/s11548-016-1467-3},
volume = {12},
year = {2017}
}
@inproceedings{Paszke2017,
abstract = {In this article, we describe an automatic differentiation module of PyTorch-a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and Facebook, Zachary Devito and Research, A I and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Srl, Orobix and Lerer, Adam},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
title = {{Automatic differentiation in PyTorch}},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
year = {2019}
}
@article{He2019,
abstract = {Deep-learning techniques have penetrated all aspects of our lives and brought us great convenience. However, the process of building a high-quality deep-learning system for a specific task is time-consuming, requires extensive resources and relies on human expertise, hindering the further development of deep learning applications in both industry and academia. To alleviate this problem, a growing number of research projects focus on automated machine learning (AutoML). In this paper, we provide a comprehensive and up-to-date study on the state-of-the-art (SOTA) in AutoML. First, we introduce the AutoML techniques in detail, in relation to the machine-learning pipeline. We then summarize existing research on neural architecture search (NAS), as this is one of the most popular topics in the field of AutoML. We also compare the performance of models generated by NAS algorithms with that of human-designed models. Finally, we present several open problems for future research.},
annote = {{\_}eprint: 1908.00709},
archivePrefix = {arXiv},
arxivId = {1908.00709},
author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
eprint = {1908.00709},
title = {{AutoML: A Survey of the State-of-the-Art}},
url = {http://arxiv.org/abs/1908.00709},
year = {2019}
}
@inproceedings{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82{\%} top-5 test error, exceeding the accuracy of human raters.},
annote = {{\_}eprint: 1502.03167},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1502.03167},
isbn = {9781510810587},
pages = {448--456},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
url = {http://arxiv.org/abs/1502.03167},
volume = {1},
year = {2015}
}
@article{Wang2019,
abstract = {Accurate segmentation of infant brain magnetic resonance (MR) images into white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) is an indispensable foundation for early studying of brain growth patterns and morphological changes in neurodevelopmental disorders. Nevertheless, in the isointense phase (approximately 6-9 months of age), due to inherent myelination and maturation process, WM and GM exhibit similar levels of intensity in both T1-weighted (T1w) and T2-weighted (T2w) MR images, making tissue segmentation very challenging. Despite many efforts were devoted to brain segmentation, only few studies have focused on the segmentation of 6-month infant brain images. With the idea of boosting methodological development in the community, iSeg-2017 challenge (http://iseg2017.web.unc.edu) provides a set of 6-month infant subjects with manual labels for training and testing the participating methods. Among the 21 automatic segmentation methods participating in iSeg-2017, we review the 8 top-ranked teams, in terms of Dice ratio, modified Hausdorff distance and average surface distance, and introduce their pipelines, implementations, as well as source codes. We further discuss limitations and possible future directions. We hope the dataset in iSeg-2017 and this review article could provide insights into methodological development for the community.},
author = {Wang, Li and Nie, Dong and Li, Guannan and Puybareau, Elodie and Dolz, Jose and Zhang, Qian and Wang, Fan and Xia, Jing and Wu, Zhengwang and Chen, Jia-Wei and Thung, Kim-Han and Bui, Toan Duc and Shin, Jitae and Zeng, Guodong and Zheng, Guoyan and Fonov, Vladimir S. and Doyle, Andrew and Xu, Yongchao and Moeskops, Pim and Pluim, Josien P. W. and Desrosiers, Christian and Ayed, Ismail Ben and Sanroma, Gerard and Benkarim, Oualid M. and Casamitjana, Adria and Vilaplana, Veronica and Lin, Weili and Li, Gang and Shen, Dinggang},
doi = {10.1109/tmi.2019.2901712},
issn = {0278-0062},
journal = {IEEE Transactions on Medical Imaging},
month = {sep},
number = {9},
pages = {2219--2230},
title = {{Benchmark on Automatic Six-Month-Old Infant Brain Segmentation Algorithms: The iSeg-2017 Challenge}},
url = {https://ieeexplore.ieee.org/document/8654000/},
volume = {38},
year = {2019}
}
@article{Devlin2018,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5{\%} (7.7{\%} point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
annote = {{\_}eprint: 1810.04805},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
eprint = {1810.04805},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805},
year = {2018}
}
@article{Werbos1974,
author = {Thesis, Science and Appl, Ph D and Harvard, Math},
number = {January 1974},
title = {{Beyond Regression : New Tools for Prediction and Analysis in the Behavioral}},
year = {2018}
}
@inproceedings{Wei2016,
abstract = {It is obvious that big data can bring us new opportunities to discover valuable information. Apparently, corresponding big datasets are powerful tools for scholars, which connect theoretical studies to reality. They can help scholars to evaluate their achievements and find new problems. In recent years, there has been a significant growth in research data repositories and registries. However, these infrastructures are fragmented across institutions, countries and research domains. As such, finding research datasets is not a trivial task for many researchers. Thus we investigated 195 papers regarding big data on some notable international conferences in recent 3 years, and also gathered 285 datasets mentioned in them. In this paper, we present and analyze our survey results in terms of the status quo of big data research and datasets from different aspects. In particular, we propose two different taxonomies of big datasets and classify our surveyed datasets into them. In addition, we also give a brief introduction about 7 widely accepted data collections online. Finally, some basic principles for scholars in choosing and using big datasets are given.},
author = {Wei, Yi and Liu, Shijun and Sun, Jiao and Cui, Lizhen and Pan, Li and Wu, Lei},
booktitle = {Proceedings - 2016 IEEE International Congress on Big Data, BigData Congress 2016},
doi = {10.1109/BigDataCongress.2016.62},
isbn = {9781509026227},
keywords = {Big data,Datasets,Survey},
month = {oct},
pages = {394--401},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Big datasets for research: A survey on flagship conferences}},
year = {2016}
}
@article{Zhangb,
author = {Zhang, Liang and Zhang, Jiaming},
doi = {10.1109/TMI.2020.2975347},
title = {{Block Level Skip Connections across Cascaded V-Net for Multi-organ Segmentation}},
url = {https://ieeexplore.ieee.org/document/9006924}
}
@article{Pereira2016,
abstract = {Among brain tumors, gliomas are the most common and aggressive, leading to a very short life expectancy in their highest grade. Thus, treatment planning is a key stage to improve the quality of life of oncological patients. Magnetic resonance imaging (MRI) is a widely used imaging technique to assess these tumors, but the large amount of data produced by MRI prevents manual segmentation in a reasonable time, limiting the use of precise quantitative measurements in the clinical practice. So, automatic and reliable segmentation methods are required; however, the large spatial and structural variability among brain tumors make automatic segmentation a challenging problem. In this paper, we propose an automatic segmentation method based on Convolutional Neural Networks (CNN), exploring small 3 x 3 kernels. The use of small kernels allows designing a deeper architecture, besides having a positive effect against overfitting, given the fewer number of weights in the network. We also investigated the use of intensity normalization as a pre-processing step, which though not common in CNN-based segmentation methods, proved together with data augmentation to be very effective for brain tumor segmentation in MRI images. Our proposal was validated in the Brain Tumor Segmentation Challenge 2013 database (BRATS 2013), obtaining simultaneously the first position for the complete, core, and enhancing regions in Dice Similarity Coefficient metric (0.88, 0.83, 0.77) for the Challenge data set. Also, it obtained the overall first position by the online evaluation platform. We also participated in the on-site BRATS 2015 Challenge using the same model, obtaining the second place, with Dice Similarity Coefficient metric of 0.78, 0.65, and 0.75 for the complete, core, and enhancing regions, respectively.},
author = {Pereira, Sergio and Pinto, Adriano and Alves, Victor and Silva, Carlos A},
doi = {10.1109/TMI.2016.2538465},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Brain tumor,brain tumor segmentation,convolutional neural networks,deep learning,glioma,magnetic resonance imaging},
number = {5},
pages = {1240--1251},
title = {{Brain Tumor Segmentation Using Convolutional Neural Networks in MRI Images}},
volume = {35},
year = {2016}
}
@misc{Schmainda2018,
abstract = {This collection includes datasets from 20 subjects with primary newly diagnosed glioblastoma who were treated with surgery and standard concomitant chemo-radiation therapy (CRT) followed by adjuvant chemotherapy. Two MRI exams are included for each patient: within 90 days following CRT completion and at progression (determined clinically, and based on a combination of clinical performance and/or imaging findings, and punctuated by a change in treatment or intervention). All image sets are in DICOM format and contain T1w (pre and post-contrast agent), FLAIR, T2w, ADC, normalized cerebral blood flow, normalized relative cerebral blood volume, standardized relative cerebral blood volume, and binary tumor masks (generated using T1w images). The perfusion images were generated from dynamic susceptibility contrast (GRE-EPI DSC) imaging following a preload of contrast agent. All of the series are co-registered with the T1+C images. The intent of this dataset is for assessing deep learning algorithm performance to predict tumor progression.},
author = {Schmainda, K.M. and Prah, M.},
booktitle = {Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2018.15quzvnb},
title = {{Brain-Tumor-Progression}},
url = {http://doi.org/10.7937/K9/TCIA.2018.15quzvnb},
year = {2018}
}
@inproceedings{Jia2014,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying generalpurpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (≈ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
booktitle = {MM 2014 - Proceedings of the 2014 ACM Conference on Multimedia},
doi = {10.1145/2647868.2654889},
eprint = {1408.5093},
isbn = {9781450330633},
keywords = {Computer vision,Machine learning,Neural networks,Open source,Parallel computation},
pages = {675--678},
title = {{Caffe: Convolutional architecture for fast feature embedding}},
year = {2014}
}
@article{Bontempi2019,
abstract = {Many functional and structural neuroimaging studies call for accurate morphometric segmentation of different brain structures starting from image intensity values of MRI scans. Current automatic (multi-) atlas-based segmentation strategies often lack accuracy on difficult-to-segment brain structures and, since these methods rely on atlas-to-scan alignment, they may take long processing times. Recently, methods deploying solutions based on Convolutional Neural Networks (CNNs) are making the direct analysis of out-of-the-scanner data feasible. However, current CNN-based solutions partition the test volume into 2D or 3D patches, which are processed independently. This entails a loss of global contextual information thereby negatively impacting the segmentation accuracy. In this work, we design and test an optimised end-to-end CNN architecture that makes the exploitation of global spatial information computationally tractable, allowing to process a whole MRI volume at once. We adopt a weakly supervised learning strategy by exploiting a large dataset composed by 947 out-of-the-scanner (3 Tesla T1-weighted 1mm isotropic MP-RAGE 3D sequences) MR Images. The resulting model is able to produce accurate multi-structure segmentation results in only few seconds. Different quantitative measures demonstrate an improved accuracy of our solution when compared to state-of-the-art techniques. Moreover, through a randomised survey involving expert neuroscientists, we show that subjective judgements clearly prefer our solution with respect to the widely adopted atlas-based FreeSurfer software.},
archivePrefix = {arXiv},
arxivId = {1909.05085},
author = {Bontempi, Dennis and Benini, Sergio and Signoroni, Alberto and Svanera, Michele and Muckli, Lars},
eprint = {1909.05085},
month = {sep},
title = {{CEREBRUM: a fast and fully-volumetric Convolutional Encoder-decodeR for weakly-supervised sEgmentation of BRain strUctures from out-of-the-scanner MRI}},
url = {http://arxiv.org/abs/1909.05085},
year = {2019}
}
@article{Goodfellow2015,
abstract = {The ICML 2013 Workshop on Challenges in Representation Learning. 11http://deeplearning.net/icml2013-workshop-competition. focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions.},
archivePrefix = {arXiv},
arxivId = {1307.0414},
author = {Goodfellow, Ian J and Erhan, Dumitru and {Luc Carrier}, Pierre and Courville, Aaron and Mirza, Mehdi and Hamner, Ben and Cukierski, Will and Tang, Yichuan and Thaler, David and Lee, Dong Hyun and Zhou, Yingbo and Ramaiah, Chetan and Feng, Fangxiang and Li, Ruifan and Wang, Xiaojie and Athanasakis, Dimitris and Shawe-Taylor, John and Milakov, Maxim and Park, John and Ionescu, Radu and Popescu, Marius and Grozea, Cristian and Bergstra, James and Xie, Jingjing and Romaszko, Lukasz and Xu, Bing and Chuang, Zhang and Bengio, Yoshua},
doi = {10.1016/j.neunet.2014.09.005},
eprint = {1307.0414},
issn = {18792782},
journal = {Neural Networks},
keywords = {Competition,Dataset,Representation learning},
pages = {59--63},
title = {{Challenges in representation learning: A report on three machine learning contests}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608014002159},
volume = {64},
year = {2015}
}
@inproceedings{Madani2018,
abstract = {{\textcopyright} COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only. Medical imaging datasets are limited in size due to privacy issues and the high cost of obtaining annotations. Augmentation is a widely used practice in deep learning to enrich the data in data-limited scenarios and to avoid overfitting. However, standard augmentation methods that produce new examples of data by varying lighting, field of view, and spatial rigid transformations do not capture the biological variance of medical imaging data and could result in unrealistic images. Generative adversarial networks (GANs) provide an avenue to understand the underlying structure of image data which can then be utilized to generate new realistic samples. In this work, we investigate the use of GANs for producing chest X-ray images to augment a dataset. This dataset is then used to train a convolutional neural network to classify images for cardiovascular abnormalities. We compare our augmentation strategy with traditional data augmentation and show higher accuracy for normal vs abnormal classification in chest X-rays.},
annote = {Backup Publisher: International Society for Optics and Photonics},
author = {Moradi, Mehdi and Madani, Ali and Karargyris, Alexandros and Syeda-Mahmood, Tanveer F.},
booktitle = {Medical Imaging 2018: Image Processing},
doi = {10.1117/12.2293971},
editor = {Angelini, Elsa D and Landman, Bennett A},
isbn = {9781510616370},
issn = {16057422},
keywords = {Convolutional networks,Generative adversarial networks,data augmentation},
pages = {57},
publisher = {SPIE},
title = {{Chest x-ray generation and data augmentation for cardiovascular abnormality classification}},
url = {https://doi.org/10.1117/12.2293971},
volume = {10574},
year = {2018}
}
@inproceedings{Khvostikov2017,
abstract = {Computer-aided early diagnosis of Alzheimer's Disease (AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the subject of extensive research in recent years. Some recent studies have shown promising results in the AD and MCI determination using structural and functional Magnetic Resonance Imaging (sMRI, fMRI), Positron Emission Tomography (PET) and Diffusion Tensor Imaging (DTI) modalities. This paper reviews the major trends in automatic classification methods such as feature extraction based methods as well as deep learning approaches in medical image analysis applied to the field of Alzheimer's Disease diagnostics. Different fusion methodologies to combine heterogeneous image modalities to improve classification scores are also considered.},
author = {Khvostikov, Alexander and Benois-Pineau, Jenny and Krylov, Andrey and Catheline, Gwenaelle},
booktitle = {GraphiCon 2017 - 27th International Conference on Computer Graphics and Vision},
isbn = {9785794429633},
keywords = {Alzheimer's Disease,Convolutional neural networks,Deep learning,Fusion,Machine Learning,Medical imaging,Mild cognitive impairment,Review},
pages = {237--242},
title = {{Classification methods on different brain imaging modalities for Alzheimer disease studies}},
year = {2017}
}
@article{Kumar2018,
abstract = {The analysis of multi-modality positron emission tomography and computed tomography (PET-CT) images for computer-aided diagnosis applications (e.g., detection and segmentation) requires combining the sensitivity of PET to detect abnormal regions with anatomical localization from CT. Current methods for PET-CT image analysis either process the modalities separately or fuse information from each modality based on knowledge about the image analysis task. These methods generally do not consider the spatially varying visual characteristics that encode different information across different modalities, which have different priorities at different locations. For example, a high abnormal PET uptake in the lungs is more meaningful for tumor detection than physiological PET uptake in the heart. Our aim is to improve the fusion of the complementary information in multi-modality PET-CT with a new supervised convolutional neural network (CNN) that learns to fuse complementary information for multi-modality medical image analysis. Our CNN first encodes modality-specific features and then uses them to derive a spatially varying fusion map that quantifies the relative importance of each modality's feature across different spatial locations. These fusion maps are then multiplied with the modality-specific feature maps to obtain a representation of the complementary multi-modality information at different locations, which can then be used for image analysis. We evaluated the ability of our CNN to detect and segment multiple regions (lungs, mediastinum, and tumors) with different fusion requirements using a dataset of PET-CT images of lung cancer. We compared our method to baseline techniques for multi-modality image fusion (fused inputs (FSs), multi-branch (MB) techniques, and multi-channel (MC) techniques) and segmentation. Our findings show that our CNN had a significantly higher foreground detection accuracy (99.29{\%}, p {\textless} {\{}0.05) than the fusion baselines (FS: 99.00{\%}, MB: 99.08{\%}, and TC: 98.92{\%}) and a significantly higher Dice score (63.85{\%}) than the recent PET-CT tumor segmentation methods.}},
annote = {{\_}eprint: 1810.02492},
archivePrefix = {arXiv},
arxivId = {1810.02492},
author = {Kumar, Ashnil and Fulham, Michael and Feng, Dagan and Kim, Jinman},
doi = {10.1109/TMI.2019.2923601},
eprint = {1810.02492},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Multi-modality imaging,PET-CT,deep learning,fusion learning},
number = {1},
pages = {204--217},
title = {{Co-Learning Feature Fusion Maps from PET-CT Images of Lung Cancer}},
volume = {39},
year = {2020}
}
@article{Reinhard2001,
author = {Reinhard, Erik and Ashikhmin, Michael and Gooch, Bruce and Shirley, Peter},
doi = {10.1109/38.946629},
issn = {02721716},
journal = {IEEE Computer Graphics and Applications},
number = {5},
pages = {34--41},
title = {{Color transfer between images}},
volume = {21},
year = {2001}
}
@article{Magee2009,
abstract = {Abstract. Colour consistency in light microscopy based histology is an increasingly important problem with the advent of Gigapixel digital slide scanners and automatic image analysis. This paper presents an evaluation of two novel colour normalisation approaches against the previously utilised method of linear normalisation in l$\alpha$$\beta$ colourspace. These approaches map the colour distribution of an over/under stained image to that of a well stained target image. The first novel approach presented is a multi-modal extension to linear normalisation in l$\alpha$$\beta$ colourspace using an automatic image segmentation method and defining separate transforms for each class. The second approach normalises in a representation space obtained using stain specific colour deconvolution. Additionally, we present a method for estimation of the required colour deconvolution vectors directly from the image data. Our evaluation demonstrates the inherent variability in the original data, the known theoretical problems with linear normalisation in l$\alpha$$\beta$ colourspace, and that a multi-modal colour deconvolution based approach overcomes these problems. The segmentation based approach, while producing good results on the majority of images, is less successful than the colour deconvolution method for a significant minority of images as robust segmentation is required to avoid introducing artifacts.},
author = {{Magee D., Treanor D., Crellin D., Shires M., Smith K., Mohee K.}, Quirke P},
journal = {Optical Tissue Image analysis in Microscopy, Histopathology and Endoscopy (MICCAI Workshop)},
pages = {100--111},
title = {{Colour Normalisation in Digital Histopathology Images}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.157.5405},
year = {2009}
}
@inproceedings{Mou2014,
abstract = {Programming language processing (similar to natural language processing) is a hot research topic in the field of software engineering; it has also aroused growing interest in the artificial intelligence community. However, different from a natural language sentence, a program contains rich, explicit, and complicated structural information. Hence, traditional NLP models may be inappropriate for programs. In this paper, we propose a novel tree-based convolutional neural network (TBCNN) for programming language processing, in which a convolution kernel is designed over programs' abstract syntax trees to capture structural information. TBCNN is a generic architecture for programming language processing; our experiments show its effectiveness in two different program analysis tasks: classifying programs according to functionality, and detecting code snippets of certain patterns. TBCNN outperforms baseline methods, including several neural models for NLP.},
annote = {{\_}eprint: 1409.5718},
archivePrefix = {arXiv},
arxivId = {1409.5718},
author = {Mou, Lili and Li, Ge and Zhang, Lu and Wang, Tao and Jin, Zhi},
booktitle = {30th AAAI Conference on Artificial Intelligence, AAAI 2016},
eprint = {1409.5718},
isbn = {9781577357605},
pages = {1287--1293},
title = {{Convolutional neural networks over tree structures for programming language processing}},
url = {http://arxiv.org/abs/1409.5718},
volume = {abs/1409.5},
year = {2016}
}
@article{Rusu2017,
abstract = {Objective: To develop an approach for radiology-pathology fusion of ex vivo histology of surgically excised pulmonary nodules with pre-operative CT, to radiologically map spatial extent of the invasive adenocarcinomatous component of the nodule. Methods: Six subjects (age: 75 ± 11 years) with pre-operative CT and surgically excised ground-glass nodules (size: 22.5 ± 5.1 mm) with a significant invasive adenocarcinomatous component ({\textgreater}5 mm) were included. The pathologist outlined disease extent on digitized histology specimens; two radiologists and a pulmonary critical care physician delineated the entire nodule on CT (in-plane resolution: {\textless}0.8 mm, inter-slice distance: 1–5 mm). We introduced a novel reconstruction approach to localize histology slices in 3D relative to each other while using CT scan as spatial constraint. This enabled the spatial mapping of the extent of tumour invasion from histology onto CT. Results: Good overlap of the 3D reconstructed histology and the nodule outlined on CT was observed (65.9 ± 5.2{\%}). Reduction in 3D misalignment of corresponding anatomical landmarks on histology and CT was observed (1.97 ± 0.42 mm). Moreover, the CT attenuation (HU) distributions were different when comparing invasive and in situ regions. Conclusion: This proof-of-concept study suggests that our fusion method can enable the spatial mapping of the invasive adenocarcinomatous component from 2D histology slices onto in vivo CT. Key Points: • 3D reconstructions are generated from 2D histology specimens of ground glass nodules. • The reconstruction methodology used pre-operative in vivo CT as 3D spatial constraint. • The methodology maps adenocarcinoma extent from digitized histology onto in vivo CT. • The methodology potentially facilitates the discovery of CT signature of invasive adenocarcinoma.},
author = {Rusu, Mirabela and Rajiah, Prabhakar and Gilkeson, Robert and Yang, Michael and Donatelli, Christopher and Thawani, Rajat and Jacono, Frank J and Linden, Philip and Madabhushi, Anant},
doi = {10.1007/s00330-017-4813-0},
issn = {14321084},
journal = {European Radiology},
keywords = {Computed tomography,Computer-assisted image processing,Lung adenocarcinoma,Multimodal imaging,Pathology},
number = {10},
pages = {4209--4217},
title = {{Co-registration of pre-operative CT with ex vivo surgically excised ground glass nodules to define spatial extent of invasive adenocarcinoma on in vivo imaging: a proof-of-concept study}},
url = {https://doi.org/10.1007/s00330-017-4813-0},
volume = {27},
year = {2017}
}
@article{Rister2018,
abstract = {Fully-convolutional neural networks have achieved superior performance in a variety of image segmentation tasks. However, their training requires laborious manual annotation of large datasets, as well as acceleration by parallel processors with high-bandwidth memory, such as GPUs. We show that simple models can achieve competitive accuracy for organ segmentation on CT images when trained with extensive data augmentation, which leverages existing graphics hardware to quickly apply geometric and photometric transformations to 3D image data. On 3 mm{\^{}}3 CT volumes, our GPU implementation is 2.6-8X faster than a widely-used CPU version, including communication overhead. We also show how to automatically generate training labels using rudimentary morphological operations, which are efficiently computed by 3D Fourier transforms. We combined fully-automatic labels for the lungs and bone with semi-automatic ones for the liver, kidneys and bladder, to create a dataset of 130 labeled CT scans. To achieve the best results from data augmentation, our model uses the intersection-over-union (IOU) loss function, a close relative of the Dice loss. We discuss its mathematical properties and explain why it outperforms the usual weighted cross-entropy loss for unbalanced segmentation tasks. We conclude that there is no unique IOU loss function, as the naive one belongs to a broad family of functions with the same essential properties. When combining data augmentation with the IOU loss, our model achieves a Dice score of 78-92{\%} for each organ. The trained model, code and dataset will be made publicly available, to further medical imaging research.},
annote = {{\_}eprint: 1811.11226},
archivePrefix = {arXiv},
arxivId = {1811.11226},
author = {Rister, Blaine and Yi, Darvin and Shivakumar, Kaushik and Nobashi, Tomomi and Rubin, Daniel L},
eprint = {1811.11226},
journal = {CoRR},
title = {{CT organ segmentation using GPU data augmentation, unsupervised labels and IOU loss}},
url = {http://arxiv.org/abs/1811.11226},
volume = {abs/1811.1},
year = {2018}
}
@article{Chetlur2014,
abstract = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36{\%} on a standard model while also reducing memory consumption.},
archivePrefix = {arXiv},
arxivId = {1410.0759},
author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
eprint = {1410.0759},
journal = {arXiv: Neural and Evolutionary Computing},
title = {{cuDNN: Efficient Primitives for Deep Learning}},
url = {http://arxiv.org/abs/1410.0759},
year = {2014}
}
@inproceedings{Liu2018,
abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
archivePrefix = {arXiv},
arxivId = {1806.09055},
author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
booktitle = {7th International Conference on Learning Representations, ICLR 2019},
eprint = {1806.09055},
title = {{DARTS: Differentiable architecture search}},
volume = {abs/1806.0},
year = {2019}
}
@article{Takahashi2019,
abstract = {Deep convolutional neural networks (CNNs) have achieved remarkable results in image processing tasks. However, their high expression ability risks overfitting. Consequently, data augmentation techniques have been proposed to prevent overfitting while enriching datasets. Recent CNN architectures with more parameters are rendering traditional data augmentation techniques insufficient. In this study, we propose a new data augmentation technique called random image cropping and patching (RICAP) which randomly crops four images and patches them to create a new training image. Moreover, RICAP mixes the class labels of the four images, resulting in an advantage similar to label smoothing. We evaluated RICAP with current state-of-the-art CNNs (e.g., the shake-shake regularization model) by comparison with competitive data augmentation techniques such as cutout and mixup. RICAP achieves a new state-of-the-art test error of {\$}2.19\backslash{\%}{\$} on CIFAR-10. We also confirmed that deep CNNs with RICAP achieve better results on classification tasks using CIFAR-100 and ImageNet and an image-caption retrieval task using Microsoft COCO.},
annote = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
archivePrefix = {arXiv},
arxivId = {1811.09030},
author = {Takahashi, Ryo and Matsubara, Takashi and Uehara, Kuniaki},
doi = {10.1109/tcsvt.2019.2935128},
eprint = {1811.09030},
issn = {1051-8215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
pages = {1--1},
title = {{Data Augmentation using Random Image Cropping and Patching for Deep CNNs}},
url = {http://dx.doi.org/10.1109/TCSVT.2019.2935128},
year = {2019}
}
@misc{ArmatoIIISamuelG.2015,
abstract = {The Lung Image Database Consortium image collection (LIDC-IDRI) consists of diagnostic and lung cancer screening thoracic computed tomography (CT) scans with marked-up annotated lesions. It is a web-accessible international resource for development, training, and evaluation of computer-assisted diagnostic (CAD) methods for lung cancer detection and diagnosis. Initiated by the National Cancer Institute (NCI), further advanced by the Foundation for the National Institutes of Health (FNIH), and accompanied by the Food and Drug Administration (FDA) through active participation, this public-private partnership demonstrates the success of a consortium founded on a consensus-based process. Seven academic centers and eight medical imaging companies collaborated to create this data set which contains 1018 cases. Each subject includes images from a clinical thoracic CT scan and an associated XML file that records the results of a two-phase image annotation process performed by four experienced thoracic radiologists. In the initial blinded-read phase, each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories ("nodule {\textgreater} or =3 mm," "nodule {\textless}3 mm," and "non-nodule {\textgreater} or =3 mm"). In the subsequent unblinded-read phase, each radiologist independently reviewed their own marks along with the anonymized marks of the three other radiologists to render a final opinion. The goal of this process was to identify as completely as possible all lung nodules in each CT scan without requiring forced consensus.},
author = {III, Armato and G., Samuel and McLennan and Geoffrey and Bidaut and Luc and McNitt-Gray and Michael, F. and R., Meyer and Charles and Reeves},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2015.LO9QL9SX},
publisher = {The Cancer Imaging Archive},
title = {{Data From LIDC-IDRI}},
url = {https://wiki.cancerimagingarchive.net/x/rgAe},
year = {2015}
}
@article{Chapman2019,
abstract = {Generating value from data requires the ability to find, access and make sense of datasets. There are many efforts underway to encourage data sharing and reuse, from scientific publishers asking authors to submit data alongside manuscripts to data marketplaces, open data portals and data communities. Google recently beta released a search service for datasets, which allows users to discover data stored in various online repositories via keyword queries. These developments foreshadow an emerging research field around dataset search or retrieval that broadly encompasses frameworks, methods and tools that help match a user data need against a collection of datasets. Here, we survey the state of the art of research and commercial systems in dataset retrieval. We identify what makes dataset search a research field in its own right, with unique challenges and methods and highlight open problems. We look at approaches and implementations from related areas dataset search is drawing upon, including information retrieval, databases, entity-centric and tabular search in order to identify possible paths to resolve these open problems as well as immediate next steps that will take the field forward.},
archivePrefix = {arXiv},
arxivId = {1901.00735},
author = {Chapman, Adriane and Simperl, Elena and Koesten, Laura and Konstantinidis, George and Ib{\'{a}}{\~{n}}ez-Gonzalez, Luis-Daniel and Kacprzak, Emilia and Groth, Paul},
eprint = {1901.00735},
month = {jan},
title = {{Dataset search: a survey}},
url = {http://arxiv.org/abs/1901.00735},
year = {2019}
}
@article{Aerts2014,
abstract = {Human cancers exhibit strong phenotypic differences that can be visualized noninvasively by medical imaging. Radiomics refers to the comprehensive quantification of tumour phenotypes by applying a large number of quantitative image features. Here we present a radiomic analysis of 440 features quantifying tumour image intensity, shape and texture, which are extracted from computed tomography data of 1,019 patients with lung or head-and-neck cancer. We find that a large number of radiomic features have prognostic power in independent data sets of lung and head-and-neck cancer patients, many of which were not identified as significant before. Radiogenomics analysis reveals that a prognostic radiomic signature, capturing intratumour heterogeneity, is associated with underlying gene-expression patterns. These data suggest that radiomics identifies a general prognostic phenotype existing in both lung and head-and-neck cancer. This may have a clinical impact as imaging is routinely used in clinical practice, providing an unprecedented opportunity to improve decision-support in cancer treatment at low cost.},
author = {Aerts, Hugo J W L and Velazquez, Emmanuel Rios and Leijenaar, Ralph T H and Parmar, Chintan and Grossmann, Patrick and Carvalho, Sara and Bussink, Johan and Monshouwer, Ren{\'{e}} and Haibe-Kains, Benjamin and Rietveld, Derek and Hoebers, Frank and Rietbergen, Michelle M and Leemans, C Ren{\'{e}} and Dekker, Andre and Quackenbush, John and Gillies, Robert J and Lambin, Philippe},
doi = {10.1038/ncomms5006},
issn = {2041-1723},
journal = {Nature Communications},
number = {1},
pages = {4006},
title = {{Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach}},
url = {https://doi.org/10.1038/ncomms5006},
volume = {5},
year = {2014}
}
@inproceedings{Tran2016,
abstract = {Over the last few years deep learning methods have emerged as one of the most prominent approaches for video analysis. However, so far their most successful applications have been in the area of video classification and detection, i.e., problems involving the prediction of a single class label or a handful of output variables per video. Furthermore, while deep networks are commonly recognized as the best models to use in these domains, there is a widespread perception that in order to yield successful results they often require time-consuming architecture search, manual tweaking of parameters and computationally intensive preprocessing or post-processing methods. In this paper we challenge these views by presenting a deep 3D convolutional architecture trained end to end to perform voxel-level prediction, i.e., to output a variable at every voxel of the video. Most importantly, we show that the same exact architecture can be used to achieve competitive results on three widely different voxel-prediction tasks: video semantic segmentation, optical flow estimation, and video coloring. The three networks learned on these problems are trained from raw video without any form of preprocessing and their outputs do not require post-processing to achieve outstanding performance. Thus, they offer an efficient alternative to traditional and much more computationally expensive methods in these video domains.},
address = {United States},
archivePrefix = {arXiv},
arxivId = {1511.06681},
author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2016.57},
eprint = {1511.06681},
isbn = {9781467388504},
issn = {21607516},
pages = {402--409},
publisher = {IEEE Computer Society},
title = {{Deep End2End Voxel2Voxel Prediction}},
year = {2016}
}
@book{Goodfellow2016DeepLearning,
author = {Goodfellow, Ian},
isbn = {0262035618},
publisher = {The MIT Press},
title = {{Deep Learning (Adaptive Computation and Machine Learning series)}},
url = {https://www.xarg.org/ref/a/0262035618/},
year = {2016}
}
@article{Litjens2016,
author = {Litjens, Geert and S{\'{a}}nchez, Clara I. and Timofeeva, Nadya and Hermsen, Meyke and Nagtegaal, Iris and Kovacs, Iringo and {Hulsbergen - van de Kaa}, Christina and Bult, Peter and van Ginneken, Bram and van der Laak, Jeroen},
doi = {10.1038/srep26286},
issn = {2045-2322},
journal = {Scientific Reports},
month = {sep},
number = {1},
pages = {26286},
title = {{Deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis}},
url = {http://www.nature.com/articles/srep26286},
volume = {6},
year = {2016}
}
@article{Hosny2018,
abstract = {Background: Non-small-cell lung cancer (NSCLC) patients often demonstrate varying clinical courses and outcomes, even within the same tumor stage. This study explores deep learning applications in medical imaging allowing for the automated quantification of radiographic characteristics and potentially improving patient stratification. Methods and findings: We performed an integrative analysis on 7 independent datasets across 5 institutions totaling 1,194 NSCLC patients (age median = 68.3 years [range 32.5–93.3], survival median = 1.7 years [range 0.0–11.7]). Using external validation in computed tomography (CT) data, we identified prognostic signatures using a 3D convolutional neural network (CNN) for patients treated with radiotherapy (n = 771, age median = 68.0 years [range 32.5–93.3], survival median = 1.3 years [range 0.0–11.7]). We then employed a transfer learning approach to achieve the same for surgery patients (n = 391, age median = 69.1 years [range 37.2–88.0], survival median = 3.1 years [range 0.0–8.8]). We found that the CNN predictions were significantly associated with 2-year overall survival from the start of respective treatment for radiotherapy (area under the receiver operating characteristic curve [AUC] = 0.70 [95{\%} CI 0.63–0.78], p {\textless} 0.001) and surgery (AUC = 0.71 [95{\%} CI 0.60–0.82], p {\textless} 0.001) patients. The CNN was also able to significantly stratify patients into low and high mortality risk groups in both the radiotherapy (p {\textless} 0.001) and surgery (p = 0.03) datasets. Additionally, the CNN was found to significantly outperform random forest models built on clinical parameters—including age, sex, and tumor node metastasis stage—as well as demonstrate high robustness against test–retest (intraclass correlation coefficient = 0.91) and inter-reader (Spearman's rank-order correlation = 0.88) variations. To gain a better understanding of the characteristics captured by the CNN, we identified regions with the most contribution towards predictions and highlighted the importance of tumor-surrounding tissue in patient stratification. We also present preliminary findings on the biological basis of the captured phenotypes as being linked to cell cycle and transcriptional processes. Limitations include the retrospective nature of this study as well as the opaque black box nature of deep learning networks. Conclusions: Our results provide evidence that deep learning networks may be used for mortality risk stratification based on standard-of-care CT images from NSCLC patients. This evidence motivates future research into better deciphering the clinical and biological basis of deep learning networks as well as validation in prospective data.},
annote = {From Duplicate 2 (Deep learning for lung cancer prognostication: A retrospective multi-cohort radiomics study - Hosny, Ahmed; Parmar, Chintan; Coroller, Thibaud P; Grossmann, Patrick; Zeleznik, Roman; Kumar, Avnish; Bussink, Johan; Gillies, Robert J; Mak, Raymond H; Aerts, Hugo J.W.L.)

Publisher: Public Library of Science},
author = {Hosny, Ahmed and Parmar, Chintan and Coroller, Thibaud P. and Grossmann, Patrick and Zeleznik, Roman and Kumar, Avnish and Bussink, Johan and Gillies, Robert J. and Mak, Raymond H. and Aerts, Hugo J.W.L.},
doi = {10.1371/journal.pmed.1002711},
issn = {15491676},
journal = {PLoS Medicine},
number = {11},
pmid = {27212078},
title = {{Deep learning for lung cancer prognostication: A retrospective multi-cohort radiomics study}},
url = {https://doi.org/10.1371/journal.pmed.1002711},
volume = {15},
year = {2018}
}
@inproceedings{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
isbn = {9781467388504},
issn = {10636919},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
url = {http://arxiv.org/abs/1512.03385},
volume = {2016-Decem},
year = {2016}
}
@inproceedings{Xie2016,
abstract = {As 3D movie viewing becomes mainstream and the Virtual Reality (VR) market emerges, the demand for 3D contents is growing rapidly. Producing 3D videos, however, remains challenging. In this paper we propose to use deep neural networks to automatically convert 2Dvideos and images to a stereoscopic 3D format. In contrast to previous automatic 2D-to-3D conversion algorithms, which have separate stages and need ground truth depth map as supervision, our approach is trained endto-end directly on stereo pairs extracted from existing 3D movies. This novel training scheme makes it possible to exploit orders of magnitude more data and significantly increases performance. Indeed, Deep3D outperforms baselines in both quantitative and human subject evaluations.},
annote = {From Duplicate 2 (Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks - Xie, Junyuan; Girshick, Ross; Farhadi, Ali)

{\_}eprint: 1604.03650},
archivePrefix = {arXiv},
arxivId = {1604.03650},
author = {Xie, Junyuan and Girshick, Ross and Farhadi, Ali},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46493-0_51},
eprint = {1604.03650},
isbn = {9783319464923},
issn = {16113349},
keywords = {Deep convolutional neural networks,Monocular stereo reconstruction},
pages = {842--857},
title = {{Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks}},
url = {http://arxiv.org/abs/1604.03650},
volume = {9908 LNCS},
year = {2016}
}
@inproceedings{Zhu2018,
abstract = {In this work, we present a fully automated lung computed tomography (CT) cancer diagnosis system, DeepLung. DeepLung consists of two components, nodule detection (identifying the locations of candidate nodules) and classification (classifying candidate nodules into benign or malignant). Considering the 3D nature of lung CT data and the compactness of dual path networks (DPN), two deep 3D DPN are designed for nodule detection and classification respectively. Specifically, a 3D Faster Regions with Convolutional Neural Net (R-CNN) is designed for nodule detection with 3D dual path blocks and a U-net-like encoder-decoder structure to effectively learn nodule features. For nodule classification, gradient boosting machine (GBM) with 3D dual path network features is proposed. The nodule classification subnetwork was validated on a public dataset from LIDC-IDRI, on which it achieved better performance than state-of-the-art approaches and surpassed the performance of experienced doctors based on image modality. Within the DeepLung system, candidate nodules are detected first by the nodule detection subnetwork, and nodule diagnosis is conducted by the classification subnetwork. Extensive experimental results demonstrate that DeepLung has performance comparable to experienced doctors both for the nodule-level and patient-level diagnosis on the LIDC-IDRI dataset.},
annote = {{\_}eprint: 1801.09555},
archivePrefix = {arXiv},
arxivId = {1801.09555},
author = {Zhu, Wentao and Liu, Chaochun and Fan, Wei and Xie, Xiaohui},
booktitle = {Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018},
doi = {10.1109/WACV.2018.00079},
eprint = {1801.09555},
isbn = {9781538648865},
pages = {673--681},
title = {{DeepLung: Deep 3D dual path nets for automated pulmonary nodule detection and classification}},
volume = {2018-Janua},
year = {2018}
}
@article{Beers2018,
abstract = {Translating neural networks from theory to clinical practice has unique challenges, specifically in the field of neuroimaging. In this paper, we present DeepNeuro, a deep learning framework that is best-suited to putting deep learning algorithms for neuroimaging in practical usage with a minimum of friction. We show how this framework can be used to both design and train neural network architectures, as well as modify state-of-the-art architectures in a flexible and intuitive way. We display the pre- and postprocessing functions common in the medical imaging community that DeepNeuro offers to ensure consistent performance of networks across variable users, institutions, and scanners. And we show how pipelines created in DeepNeuro can be concisely packaged into shareable Docker containers and command-line interfaces using DeepNeuro's pipeline resources.},
annote = {{\_}eprint: 1808.04589},
archivePrefix = {arXiv},
arxivId = {1808.04589},
author = {Beers, Andrew and Brown, James and Chang, Ken and Hoebel, Katharina and Gerstner, Elizabeth and Rosen, Bruce and Kalpathy-Cramer, Jayashree},
eprint = {1808.04589},
title = {{DeepNeuro: an open-source deep learning toolbox for neuroimaging}},
url = {http://arxiv.org/abs/1808.04589},
year = {2018}
}
@article{Flynn2015,
annote = {From Duplicate 2 (DeepStereo: Learning to Predict New Views from the World's Imagery - Flynn, John; Neulander, Ivan; Philbin, James; Snavely, Noah)

{\_}eprint: 1506.06825},
archivePrefix = {arXiv},
arxivId = {1506.06825},
author = {Flynn, John and Neulander, Ivan and Philbin, James and Snavely, Noah},
eprint = {1506.06825},
journal = {CoRR},
title = {{DeepStereo: Learning to Predict New Views from the World's Imagery}},
url = {http://arxiv.org/abs/1506.06825},
volume = {abs/1506.0},
year = {2015}
}
@inproceedings{He2015a,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94{\%} top-5 test error on the ImageNet 2012 classification dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%} [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1{\%}, [26]) on this dataset.},
annote = {{\_}eprint: 1502.01852},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.123},
eprint = {1502.01852},
isbn = {9781467383912},
issn = {15505499},
pages = {1026--1034},
title = {{Delving deep into rectifiers: Surpassing human-level performance on imagenet classification}},
volume = {2015 Inter},
year = {2015}
}
@inproceedings{Huang2016,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L2+1) direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
annote = {{\_}eprint: 1608.06993},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.243},
eprint = {1608.06993},
isbn = {9781538604571},
pages = {2261--2269},
title = {{Densely connected convolutional networks}},
url = {http://arxiv.org/abs/1608.06993},
volume = {2017-Janua},
year = {2017}
}
@article{Lafferty2001,
author = {Lafferty, John and Mccallum, Andrew and Pereira, Fernando C N},
journal = {Machine Learning},
number = {Icml},
pages = {282--289},
title = {{Departmental Papers ( CIS ) Conditional Random Fields : Probabilistic Models for Segmenting and Labeling Sequence Data Conditional Random Fields : Probabilistic Models}},
volume = {2001},
year = {2001}
}
@inproceedings{Baker2016,
abstract = {At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Qlearning with an -greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.},
archivePrefix = {arXiv},
arxivId = {1611.02167},
author = {Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1611.02167},
title = {{Designing neural network architectures using reinforcement learning}},
volume = {abs/1611.0},
year = {2019}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets. {\textcopyright} 2014 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Deep learning,Model combination,Neural networks,Regularization},
pages = {1929--1958},
title = {{Dropout: A simple way to prevent neural networks from overfitting}},
url = {http://jmlr.org/papers/v15/srivastava14a.html},
volume = {15},
year = {2014}
}
@inproceedings{Chen2017,
abstract = {In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64 × 4d) with 26{\%} smaller model size, 25{\%} less computational cost and 8{\%} lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.},
archivePrefix = {arXiv},
arxivId = {1707.01629},
author = {Chen, Yunpeng and Li, Jianan and Xiao, Huaxin and Jin, Xiaojie and Yan, Shuicheng and Feng, Jiashi},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1707.01629},
issn = {10495258},
pages = {4468--4476},
title = {{Dual path networks}},
volume = {2017-Decem},
year = {2017}
}
@inproceedings{Simonovsky2017,
abstract = {A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches.},
annote = {{\_}eprint: 1704.02901},
archivePrefix = {arXiv},
arxivId = {1704.02901},
author = {Simonovsky, Martin and Komodakis, Nikos},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.11},
eprint = {1704.02901},
isbn = {9781538604571},
pages = {29--38},
title = {{Dynamic edge-conditioned filters in convolutional neural networks on graphs}},
url = {http://arxiv.org/abs/1704.02901},
volume = {2017-Janua},
year = {2017}
}
@article{Wang2018,
abstract = {Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information, so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds, including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks, including ModelNet40, ShapeNetPart, and S3DIS.},
annote = {{\_}eprint: 1801.07829},
archivePrefix = {arXiv},
arxivId = {1801.07829},
author = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E and Bronstein, Michael M and Solomon, Justin M},
doi = {10.1145/3326362},
eprint = {1801.07829},
issn = {15577368},
journal = {ACM Transactions on Graphics},
keywords = {Classification,Point cloud,Segmentation},
number = {5},
title = {{Dynamic graph Cnn for learning on point clouds}},
url = {http://arxiv.org/abs/1801.07829},
volume = {38},
year = {2019}
}
@inproceedings{Cai2017,
abstract = {Techniques for automatically designing deep neural network architectures such as reinforcement learning based approaches have recently shown promising results. However, their success is based on vast computational resources (e.g. hundreds of GPUs), making them difficult to be widely used. A noticeable limitation is that they still design and train each network from scratch during the exploration of the architecture space, which is highly inefficient. In this paper, we propose a new framework toward efficient architecture search by exploring the architecture space based on the current network and reusing its weights. We employ a reinforcement learning agent as the meta-controller, whose action is to grow the network depth or layer width with function-preserving transformations. As such, the previously validated networks can be reused for further exploration, thus saves a large amount of computational cost. We apply our method to explore the architecture space of the plain convolutional neural networks (no skip-connections, branching etc.) on image benchmark datasets (CIFAR-10, SVHN) with restricted computational resources (5 GPUs). Our method can design highly competitive networks that outperform existing networks using the same design scheme. On CIFAR-10, our model without skip-connections achieves 4.23{\%} test error rate, exceeding a vast majority of modern architectures and approaching DenseNet. Furthermore, by applying our method to explore the DenseNet architecture space, we are able to achieve more accurate networks with fewer parameters.},
archivePrefix = {arXiv},
arxivId = {1707.04873},
author = {Cai, Han and Chen, Tianyao and Zhang, Weinan and Yu, Yong and Wang, Jun},
booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
eprint = {1707.04873},
isbn = {9781577358008},
pages = {2787--2794},
title = {{Efficient architecture search by network transformation}},
year = {2018}
}
@article{Kamnitsas2016,
abstract = {We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumours, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.},
annote = {{\_}eprint: 1603.05959},
archivePrefix = {arXiv},
arxivId = {1603.05959},
author = {Kamnitsas, Konstantinos and Ledig, Christian and Newcombe, Virginia F.J. and Simpson, Joanna P and Kane, Andrew D and Menon, David K and Rueckert, Daniel and Glocker, Ben},
doi = {10.1016/j.media.2016.10.004},
eprint = {1603.05959},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {3D convolutional neural network,Brain lesions,Deep learning,Fully connected CRF,Segmentation},
pages = {61--78},
title = {{Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation}},
url = {http://arxiv.org/abs/1603.05959},
volume = {36},
year = {2017}
}
@inproceedings{Pham2018,
abstract = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. ENAS constructs a large computational graph, where each subgraph represents a neural network architecture, hence forcing all architectures to share their parameters. A controller is trained with policy gradient to search for a subgraph that maximizes the expected reward on a validation set. Meanwhile a model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Sharing parameters among child models allows ENAS to deliver strong empirical performances, whilst using much fewer GPU-hours than existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On Penn Treebank, ENAS discovers a novel architecture that achieves a test perplexity of 56.3, on par with the existing state-of-the-art among all methods without post-training processing. On CIFAR-10, ENAS finds a novel architecture that achieves 2.89{\%} test error, which is on par with the 2.65{\%} test error of NASNet (Zoph et al., 2018).},
archivePrefix = {arXiv},
arxivId = {1802.03268},
author = {Pham, Hieu and Guan, Melody Y and Zoph, Barret and Le, Quoc V and Dean, Jeff},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1802.03268},
isbn = {9781510867963},
pages = {6522--6531},
title = {{Efficient Neural Architecture Search via parameter Sharing}},
volume = {9},
year = {2018}
}
@inproceedings{Klokov2017,
abstract = {We present a new deep learning architecture (called Kdnetwork) that is designed for 3D model recognition tasks and works with unstructured point clouds. The new architecture performs multiplicative transformations and shares parameters of these transformations according to the subdivisions of the point clouds imposed onto them by kdtrees. Unlike the currently dominant convolutional architectures that usually require rasterization on uniform twodimensional or three-dimensional grids, Kd-networks do not rely on such grids in any way and therefore avoid poor scaling behavior. In a series of experiments with popular shape recognition benchmarks, Kd-networks demonstrate competitive performance in a number of shape recognition tasks such as shape classification, shape retrieval and shape part segmentation.},
annote = {{\_}eprint: 1704.01222},
archivePrefix = {arXiv},
arxivId = {1704.01222},
author = {Klokov, Roman and Lempitsky, Victor},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.99},
eprint = {1704.01222},
isbn = {9781538610329},
issn = {15505499},
pages = {863--872},
title = {{Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models}},
url = {http://arxiv.org/abs/1704.01222},
volume = {2017-Octob},
year = {2017}
}
@article{Stanley2001,
abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
author = {Stanley, Kenneth O and Miikkulainen, Risto},
doi = {10.1162/106365602320169811},
issn = {10636560},
journal = {Evolutionary Computation},
keywords = {Competing conventions,Genetic algorithms,Network topologies,Neural networks,Neuroevolution,Speciation},
number = {2},
pages = {99--127},
pmid = {12180173},
title = {{Evolving neural networks through augmenting topologies}},
volume = {10},
year = {2002}
}
@article{Qaiser2019,
abstract = {Tumor segmentation in whole-slide images of histology slides is an important step towards computer-assisted diagnosis. In this work, we propose a tumor segmentation framework based on the novel concept of persistent homology profiles (PHPs). For a given image patch, the homology profiles are derived by efficient computation of persistent homology, which is an algebraic tool from homology theory. We propose an efficient way of computing topological persistence of an image, alternative to simplicial homology. The PHPs are devised to distinguish tumor regions from their normal counterparts by modeling the atypical characteristics of tumor nuclei. We propose two variants of our method for tumor segmentation: one that targets speed without compromising accuracy and the other that targets higher accuracy. The fast version is based on a selection of exemplar image patches from a convolution neural network (CNN) and patch classification by quantifying the divergence between the PHPs of exemplars and the input image patch. Detailed comparative evaluation shows that the proposed algorithm is significantly faster than competing algorithms while achieving comparable results. The accurate version combines the PHPs and high-level CNN features and employs a multi-stage ensemble strategy for image patch labeling. Experimental results demonstrate that the combination of PHPs and CNN features outperform competing algorithms. This study is performed on two independently collected colorectal datasets containing adenoma, adenocarcinoma, signet, and healthy cases. Collectively, the accurate tumor segmentation produces the highest average patch-level F1-score, as compared with competing algorithms, on malignant and healthy cases from both the datasets. Overall the proposed framework highlights the utility of persistent homology for histopathology image analysis.},
archivePrefix = {arXiv},
arxivId = {1805.03699},
author = {Qaiser, Talha and Tsang, Yee Wah and Taniyama, Daiki and Sakamoto, Naoya and Nakane, Kazuaki and Epstein, David and Rajpoot, Nasir},
doi = {10.1016/j.media.2019.03.014},
eprint = {1805.03699},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Colorectal (colon) cancer,Computational pathology,Deep learning,Histology image analysis,Persistent homology,Tumor segmentation},
pages = {1--14},
title = {{Fast and accurate tumor segmentation of histology images using persistent homology and deep convolutional features}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841518302688},
volume = {55},
year = {2019}
}
@inproceedings{Klein2016,
abstract = {Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for large datasets, training and validating a single configuration often takes hours, days, or even weeks, which limits the achievable performance. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed FABOLAS, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that FABOLAS often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods or the recently proposed bandit strategy Hyperband.},
archivePrefix = {arXiv},
arxivId = {1605.07079},
author = {Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017},
eprint = {1605.07079},
title = {{Fast Bayesian optimization of machine learning hyperparameters on large datasets}},
volume = {abs/1605.0},
year = {2017}
}
@article{Ren2015,
annote = {From Duplicate 1 (Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks - Ren, Shaoqing; He, Kaiming; Girshick, Ross B; Sun, Jian)

{\_}eprint: 1506.01497},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross B and Sun, Jian},
eprint = {1506.01497},
journal = {CoRR},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
url = {http://arxiv.org/abs/1506.01497},
volume = {abs/1506.0},
year = {2015}
}
@article{Zbontar2018,
abstract = {Accelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements has the potential to reduce medical costs, minimize stress to patients and make MRI possible in applications where it is currently prohibitively slow or expensive. We introduce the fastMRI dataset, a large-scale collection of both raw MR measurements and clinical MR images, that can be used for training and evaluation of machine-learning approaches to MR image reconstruction. By introducing standardized evaluation criteria and a freely-accessible dataset, our goal is to help the community make rapid advances in the state of the art for MR image reconstruction. We also provide a self-contained introduction to MRI for machine learning researchers with no medical imaging background.},
archivePrefix = {arXiv},
arxivId = {1811.08839},
author = {Zbontar, Jure and Knoll, Florian and Sriram, Anuroop and Murrell, Tullie and Huang, Zhengnan and Muckley, Matthew J. and Defazio, Aaron and Stern, Ruben and Johnson, Patricia and Bruno, Mary and Parente, Marc and Geras, Krzysztof J. and Katsnelson, Joe and Chandarana, Hersh and Zhang, Zizhao and Drozdzal, Michal and Romero, Adriana and Rabbat, Michael and Vincent, Pascal and Yakubova, Nafissa and Pinkerton, James and Wang, Duo and Owens, Erich and Zitnick, C. Lawrence and Recht, Michael P. and Sodickson, Daniel K. and Lui, Yvonne W.},
eprint = {1811.08839},
month = {nov},
title = {{fastMRI: An Open Dataset and Benchmarks for Accelerated MRI}},
url = {http://arxiv.org/abs/1811.08839},
year = {2018}
}
@article{Henschel2019,
abstract = {Traditional neuroimage analysis pipelines involve computationally intensive, time-consuming optimization steps, and thus, do not scale well to large cohort studies with thousands or tens of thousands of individuals. In this work we propose a fast and accurate deep learning based neuroimaging pipeline for the automated processing of structural human brain MRI scans, including surface reconstruction and cortical parcellation. To this end, we introduce an advanced deep learning architecture capable of whole brain segmentation into 95 classes in under 1 minute, mimicking FreeSurfer's anatomical segmentation and cortical parcellation. The network architecture incorporates local and global competition via competitive dense blocks and competitive skip pathways, as well as multi-slice information aggregation that specifically tailor network performance towards accurate segmentation of both cortical and sub-cortical structures. Further, we perform fast cortical surface reconstruction and thickness analysis by introducing a spectral spherical embedding and by directly mapping the cortical labels from the image to the surface. This approach provides a full FreeSurfer alternative for volumetric analysis (within 1 minute) and surface-based thickness analysis (within only around 1h run time). For sustainability of this approach we perform extensive validation: we assert high segmentation accuracy on several unseen datasets, measure generalizability and demonstrate increased test-retest reliability, and increased sensitivity to disease effects relative to traditional FreeSurfer.},
annote = {{\_}eprint: 1910.03866},
archivePrefix = {arXiv},
arxivId = {1910.03866},
author = {Henschel, Leonie and Conjeti, Sailesh and Estrada, Santiago and Diers, Kersten and Fischl, Bruce and Reuter, Martin},
eprint = {1910.03866},
title = {{FastSurfer -- A fast and accurate deep learning based neuroimaging pipeline}},
url = {http://arxiv.org/abs/1910.03866},
year = {2019}
}
@article{Neher2014,
abstract = {Purpose: Phantom-based validation of diffusion-weighted image processing techniques is an important key to innovation in the field and is widely used. Openly available and user friendly tools for the flexible generation of tailor-made datasets for the specific tasks at hand can greatly facilitate the work of researchers around the world.
Methods: We present an open-source framework, Fiberfox, that enables (1) the intuitive definition of arbitrary artificial white matter fiber tracts, (2) signal generation from those fibers by means of the most recent multi-compartment modeling techniques, and (3) simulation of the actual MR acquisition that allows for the introduction of realistic MRI-related effects into the final image.
Results: We show that real acquisitions can be closely approximated by simulating the acquisition of the well-known FiberCup phantom. We further demonstrate the advantages of our framework by evaluating the effects of imaging artifacts and acquisition settings on the outcome of 12 tractography algorithms.
Conclusion: Our findings suggest that experiments on a realistic software phantom might change the conclusions drawn from earlier hardware phantom experiments. Fiberfox may find application in validating and further developing methods such as tractography, super-resolution, diffusion modeling or artifact correction.},
author = {Neher, Peter F. and Laun, Frederik B. and Stieltjes, Bram and Maier-Hein, Klaus H.},
doi = {10.1002/mrm.25045},
issn = {15222594},
journal = {Magnetic Resonance in Medicine},
keywords = {Artifact simulation,Diffusion-weighted imaging,Open-source software,S synthetic white matter fibers,Software phantoms},
month = {nov},
number = {5},
pages = {1460--1470},
pmid = {24323973},
title = {{Fiberfox: Facilitating the creation of realistic white matter software phantoms}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24323973},
volume = {72},
year = {2014}
}
@misc{Fischl2012,
abstract = {FreeSurfer is a suite of tools for the analysis of neuroimaging data that provides an array of algorithms to quantify the functional, connectional and structural properties of the human brain. It has evolved from a package primarily aimed at generating surface representations of the cerebral cortex into one that automatically creates models of most macroscopically visible structures in the human brain given any reasonable T1-weighted input image. It is freely available, runs on a wide variety of hardware and software platforms, and is open source. {\textcopyright} 2012 Elsevier Inc.},
author = {Fischl, Bruce},
booktitle = {NeuroImage},
doi = {10.1016/j.neuroimage.2012.01.021},
issn = {10538119},
keywords = {MRI,Morphometry,Registration,Segmentation},
number = {2},
pages = {774--781},
pmid = {22248573},
title = {{FreeSurfer}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3685476/},
volume = {62},
year = {2012}
}
@article{Radul2001,
abstract = {We introduce a class of Lawson monads and show that these monads have a functional representation. A characterisation of such a representation for the inclusion hyperspace monad is given.},
author = {Radul, Taras},
doi = {10.1023/A:1012052928198},
issn = {09272852},
journal = {Applied Categorical Structures},
keywords = {Functional representations,Lawson monad},
number = {5},
pages = {457--463},
title = {{Functional representations of Lawson monads}},
volume = {9},
year = {2001}
}
@incollection{Lecun1989,
author = {Lecun, Yann},
booktitle = {Connectionism in perspective},
editor = {Pfeifer, R and Schreter, Z and Fogelman, F and Steels, L},
publisher = {Elsevier},
title = {{Generalization and network design strategies}},
year = {1989}
}
@article{Yi2019,
abstract = {Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.},
author = {Yi, Xin and Walia, Ekta and Babyn, Paul},
doi = {https://doi.org/10.1016/j.media.2019.101552},
issn = {1361-8415},
journal = {Medical Image Analysis},
keywords = {Deep learning,Generative adversarial network,Generative model,Medical imaging,Review},
pages = {101552},
title = {{Generative adversarial network in medical imaging: A review}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841518308430},
volume = {58},
year = {2019}
}
@article{Brock2016,
abstract = {When working with three-dimensional data, choice of representation is key. We explore voxel-based models, and present evidence for the viability of voxellated representations in applications including shape modeling and object classification. Our key contributions are methods for training voxel-based variational autoencoders, a user interface for exploring the latent space learned by the autoencoder, and a deep convolutional neural network architecture for object classification. We address challenges unique to voxel-based representations, and empirically evaluate our models on the ModelNet benchmark, where we demonstrate a 51.5{\%} relative improvement in the state of the art for object classification.},
annote = {{\_}eprint: 1608.04236},
archivePrefix = {arXiv},
arxivId = {1608.04236},
author = {Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
eprint = {1608.04236},
journal = {CoRR},
title = {{Generative and Discriminative Voxel Modeling with Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1608.04236},
volume = {abs/1608.0},
year = {2016}
}
@article{Zamir2017,
annote = {From Duplicate 1 (Generic 3D Representation via Pose Estimation and Matching - Zamir, Amir Roshan; Wekel, Tilman; Agrawal, Pulkit; Wei, Colin; Malik, Jitendra; Savarese, Silvio)

{\_}eprint: 1710.08247},
archivePrefix = {arXiv},
arxivId = {1710.08247},
author = {Zamir, Amir Roshan and Wekel, Tilman and Agrawal, Pulkit and Wei, Colin and Malik, Jitendra and Savarese, Silvio},
eprint = {1710.08247},
journal = {CoRR},
title = {{Generic 3D Representation via Pose Estimation and Matching}},
url = {http://arxiv.org/abs/1710.08247},
volume = {abs/1710.0},
year = {2017}
}
@misc{Bronstein2016,
abstract = {Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.},
annote = {{\_}eprint: 1611.08097},
archivePrefix = {arXiv},
arxivId = {1611.08097},
author = {Bronstein, Michael M and Bruna, Joan and Lecun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
booktitle = {IEEE Signal Processing Magazine},
doi = {10.1109/MSP.2017.2693418},
eprint = {1611.08097},
issn = {10535888},
number = {4},
pages = {18--42},
title = {{Geometric Deep Learning: Going beyond Euclidean data}},
url = {http://arxiv.org/abs/1611.08097},
volume = {34},
year = {2017}
}
@article{Meagher1982,
abstract = {A geometric modeling technique called Octree Encoding is presented. Arbitrary 3-D objects can be represented to any specified resolution in a hierarchical 8-ary tree structure or "octree" Objects may be concave or convex, have holes (including interior holes), consist of disjoint parts, and possess sculptured (i.e., "free-form") surfaces. The memory required for representation and manipulation is on the order of the surface area of the object. A complexity metric is proposed based on the number of nodes in an object's tree representation. Efficient (linear time) algorithms have been developed for the Boolean operations (union, intersection and difference), geometric operations (translation, scaling and rotation), N-dimensional interference detection, and display from any point in space with hidden surfaces removed. The algorithms require neither floating-point operations, integer multiplications, nor integer divisions. In addition, many independent sets of very simple calculations are typically generated, allowing implementation over many inexpensive high-bandwidth processors operating in parallel. Real time analysis and manipulation of highly complex situations thus becomes possible. {\textcopyright} 1982.},
author = {Meagher, Donald},
doi = {10.1016/0146-664X(82)90104-6},
issn = {0146664X},
journal = {Computer Graphics and Image Processing},
number = {2},
pages = {129--147},
title = {{Geometric modeling using octree encoding}},
url = {https://www.sciencedirect.com/science/article/pii/0146664X82901046{\%}0Ahttp://fab.cba.mit.edu/classes/S62.12/docs/Meagher{\_}octree.pdf},
volume = {19},
year = {1982}
}
@inproceedings{Han2011,
abstract = {Tumor segmentation in PET and CT images is notoriously challenging due to the low spatial resolution in PET and low contrast in CT images. In this paper, we have proposed a general framework to use both PET and CT images simultaneously for tumor segmentation. Our method utilizes the strength of each imaging modality: the superior contrast of PET and the superior spatial resolution of CT. We formulate this problem as a Markov Random Field (MRF) based segmentation of the image pair with a regularized term that penalizes the segmentation difference between PET and CT. Our method simulates the clinical practice of delineating tumor simultaneously using both PET and CT, and is able to concurrently segment tumor from both modalities, achieving globally optimal solutions in low-order polynomial time by a single maximum flow computation. The method was evaluated on clinically relevant tumor segmentation problems. The results showed that our method can effectively make use of both PET and CT image information, yielding segmentation accuracy of 0.85 in Dice similarity coefficient and the average median hausdorff distance (HD) of 6.4 mm, which is 10 {\%} (resp., 16 {\%}) improvement compared to the graph cuts method solely using the PET (resp., CT) images. {\textcopyright} 2011 Springer-Verlag.},
author = {Han, Dongfeng and Bayouth, John and Song, Qi and Taurani, Aakant and Sonka, Milan and Buatti, John and Wu, Xiaodong},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-22092-0_21},
isbn = {9783642220913},
issn = {03029743},
pages = {245--256},
title = {{Globally optimal tumor segmentation in PET-CT images: A graph-based co-segmentation method}},
volume = {6801 LNCS},
year = {2011}
}
@article{LeCun1998a,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Marlow2010,
abstract = {Haskell is a general purpose, purely functional programming language incorporating many recent innovations in programming language design. Haskell provides higher-order functions, non-strict semantics, static poly- morphic typing, user-defined algebraic datatypes, pattern-matching, list comprehensions, a module system, a monadic I/O system, and a rich set of primitive datatypes, including lists, arrays, arbitrary and fixed precision integers, and floating-point numbers. Haskell is both the culmination and solidification of many years of research on non-strict functional languages.},
author = {Marlow, Simon},
journal = {Language},
pages = {329},
title = {{Haskell 2010 Language Report}},
url = {http://haskell.org/definition/haskell2010.pdf},
year = {2010}
}
@article{Shao2018,
abstract = {We present a novel spatial hashing based data structure to facilitate 3D shape analysis using convolutional neural networks (CNNs). Our method builds hierarchical hash tables for an input model under different resolutions that leverage the sparse occupancy of 3D shape boundary. Based on this data structure, we design two efficient GPU algorithms namely hash2col and col2hash so that the CNN operations like convolution and pooling can be efficiently parallelized. The perfect spatial hashing is employed as our spatial hashing scheme, which is not only free of hash collision but also nearly minimal so that our data structure is almost of the same size as the raw input. Compared with existing 3D CNN methods, our data structure significantly reduces the memory footprint during the CNN training. As the input geometry features are more compactly packed, CNN operations also run faster with our data structure. The experiment shows that, under the same network structure, our method yields comparable or better benchmark results compared with the state-of-the-art while it has only one-third memory consumption when under high resolutions (i.e. 256 3).},
annote = {{\_}eprint: 1803.11385},
archivePrefix = {arXiv},
arxivId = {1803.11385},
author = {Shao, Tianjia and Yang, Yin and Weng, Yanlin and Hou, Qiming and Zhou, Kun},
doi = {10.1109/TVCG.2018.2887262},
eprint = {1803.11385},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Computational modeling,Convolution,Data structures,Shape,Solid modeling,Three-dimensional displays,Two dimensional displays,convolutional neural network,perfect hashing,shape classification,shape retrieval,shape segmentation},
title = {{H-CNN: Spatial Hashing Based CNN for 3D Shape Analysis}},
year = {2018}
}
@inproceedings{Liu2017,
abstract = {We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6{\%} on CIFAR-10 and 20.3{\%} when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3{\%} less top-1 accuracy on CIFAR-10 and 0.1{\%} less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.},
archivePrefix = {arXiv},
arxivId = {1711.00436},
author = {Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
booktitle = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
eprint = {1711.00436},
title = {{Hierarchical representations for efficient architecture search}},
volume = {abs/1711.0},
year = {2018}
}
@article{Dolz2018,
abstract = {Recently, dense connections have attracted substantial attention in computer vision because they facilitate gradient flow and implicit deep supervision during training. Particularly, DenseNet that connects each layer to every other layer in a feed-forward fashion and has shown impressive performances in natural image classification tasks. We propose HyperDenseNet, a 3-D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems. Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path but also between those across different paths. This contrasts with the existing multi-modal CNN approaches, in which modeling several modalities relies entirely on a single joint layer (or level of abstraction) for fusion, typically either at the input or at the output of the network. Therefore, the proposed network has total freedom to learn more complex combinations between the modalities, within and in-between all the levels of abstraction, which increases significantly the learning representation. We report extensive evaluations over two different and highly competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing on six month infant data and the latter on adult images. HyperDenseNet yielded significant improvements over many state-of-the-art segmentation networks, ranking at the top on both benchmarks. We further provide a comprehensive experimental analysis of features re-use, which confirms the importance of hyper-dense connections in multi-modal representation learning. Our code is publicly available.},
annote = {{\_}eprint: 1804.02967},
archivePrefix = {arXiv},
arxivId = {1804.02967},
author = {Dolz, Jose and Gopinath, Karthik and Yuan, Jing and Lombaert, Herve and Desrosiers, Christian and {Ben Ayed}, Ismail},
doi = {10.1109/TMI.2018.2878669},
eprint = {1804.02967},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {3-D CNN,Deep learning,brain MRI,multi-modal imaging,segmentation},
number = {5},
pages = {1116--1126},
title = {{HyperDense-Net: A Hyper-Densely Connected CNN for Multi-Modal Image Segmentation}},
url = {http://arxiv.org/abs/1804.02967},
volume = {38},
year = {2019}
}
@article{Singanamalli2016,
abstract = {Background To identify computer extracted in vivo dynamic contrast enhanced (DCE) MRI markers associated with quantitative histomorphometric (QH) characteristics of microvessels and Gleason scores (GS) in prostate cancer. Methods This study considered retrospective data from 23 biopsy confirmed prostate cancer patients who underwent 3 Tesla multiparametric MRI before radical prostatectomy (RP). Representative slices from RP specimens were stained with vascular marker CD31. Tumor extent was mapped from RP sections onto DCE MRI using nonlinear registration methods. Seventy-seven microvessel QH features and 18 DCE MRI kinetic features were extracted and evaluated for their ability to distinguish low from intermediate and high GS. The effect of temporal sampling on kinetic features was assessed and correlations between those robust to temporal resolution and microvessel features discriminative of GS were examined. Results A total of 12 microvessel architectural features were discriminative of low and intermediate/high grade tumors with area under the receiver operating characteristic curve (AUC)  0.7. These features were most highly correlated with mean washout gradient (WG) (max rho = −0.62). Independent analysis revealed WG to be moderately robust to temporal resolution (intraclass correlation coefficient [ICC] = 0.63) and WG variance, which was poorly correlated with microvessel features, to be predictive of low grade tumors (AUC = 0.77). Enhancement ratio was the most robust (ICC = 0.96) and discriminative (AUC = 0.78) kinetic feature but was moderately correlated with microvessel features (max rho = −0.52). Conclusion Computer extracted features of prostate DCE MRI appear to be correlated with microvessel architecture and may be discriminative of low versus intermediate and high GS. J. MAGN. RESON. IMAGING 2016;43:149–158.},
annote = {{\_}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jmri.24975},
author = {Singanamalli, Asha and Rusu, Mirabela and Sparks, Rachel E and Shih, Natalie N C and Ziober, Amy and Wang, Li-Ping and Tomaszewski, John and Rosen, Mark and Feldman, Michael and Madabhushi, Anant},
doi = {10.1002/jmri.24975},
journal = {Journal of Magnetic Resonance Imaging},
keywords = {DCE MRI,Gleason grades,imaging biomarkers,microvessel architecture,prostate cancer,quantitative histomorphometry},
number = {1},
pages = {149--158},
title = {{Identifying in vivo DCE MRI markers associated with microvessel architecture and gleason grades of prostate cancer}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jmri.24975},
volume = {43},
year = {2016}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {10.1145/3065386},
editor = {Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q},
issn = {15577317},
journal = {Communications of the ACM},
number = {6},
pages = {84--90},
publisher = {Curran Associates, Inc.},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
volume = {60},
year = {2017}
}
@book{Summers2018,
annote = {{\_}eprint: 1805.11272},
author = {Summers, Cecilia and Dinneen, Michael J},
title = {{Improved Mixed-Example Data Augmentation}},
year = {2018}
}
@inproceedings{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and nonresidual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08{\%} top-5 error on the test set of the ImageNet classification (CLS) challenge.},
annote = {{\_}eprint: 1602.07261},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
booktitle = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
eprint = {1602.07261},
pages = {4278--4284},
title = {{Inception-v4, inception-ResNet and the impact of residual connections on learning}},
year = {2017}
}
@inproceedings{Silberman2012,
abstract = {We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation. {\textcopyright} 2012 Springer-Verlag.},
address = {Berlin, Heidelberg},
annote = {event-place: Florence, Italy},
author = {Silberman, Nathan and Hoiem, Derek and Kohli, Pushmeet and Fergus, Rob},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33715-4_54},
isbn = {9783642337147},
issn = {03029743},
number = {PART 5},
pages = {746--760},
publisher = {Springer-Verlag},
series = {ECCV'12},
title = {{Indoor segmentation and support inference from RGBD images}},
url = {http://dx.doi.org/10.1007/978-3-642-33715-4{\_}54},
volume = {7576 LNCS},
year = {2012}
}
@article{Crankshaw2018,
abstract = {The dominant cost in production machine learning workloads is not training individual models but serving predictions from increasingly complex prediction pipelines spanning multiple models, machine learning frameworks, and parallel hardware accelerators. Due to the complex interaction between model configurations and parallel hardware, prediction pipelines are challenging to provision and costly to execute when serving interactive latency-sensitive applications. This challenge is exacerbated by the unpredictable dynamics of bursty workloads. In this paper we introduce InferLine, a system which efficiently provisions and executes ML inference pipelines subject to end-to-end latency constraints by proactively optimizing and reactively controlling per-model configuration in a fine-grained fashion. Unpredictable changes in the serving workload are dynamically and cost-optimally accommodated with minimal service level degradation. InferLine introduces (1) automated model profiling and pipeline lineage extraction, (2) a fine-grain, cost-minimizing pipeline configuration planner, and (3) a fine-grain reactive controller. InferLine is able to configure and deploy prediction pipelines across a wide range of workload patterns and latency goals. It outperforms coarse-grained configuration alternatives by up 7.6x in cost while achieving up to 32x lower SLO miss rate on real workloads and generalizes across state-of-the-art model serving frameworks.},
annote = {{\_}eprint: 1812.01776},
archivePrefix = {arXiv},
arxivId = {1812.01776},
author = {Crankshaw, Daniel and Sela, Gur-Eyal and Zumar, Corey and Mo, Xiangxi and Gonzalez, Joseph E and Stoica, Ion and Tumanov, Alexey},
eprint = {1812.01776},
title = {{InferLine: ML Inference Pipeline Composition Framework}},
url = {http://arxiv.org/abs/1812.01776},
year = {2018}
}
@article{Armeni2017,
abstract = {We present a dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. The dataset covers over 6,000m2 and contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360{\{}$\backslash$deg{\}} equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces. The dataset is available here: http://3Dsemantics.stanford.edu/},
annote = {{\_}eprint: 1702.01105},
archivePrefix = {arXiv},
arxivId = {1702.01105},
author = {Armeni, Iro and Sax, Sasha and Zamir, Amir Roshan and Savarese, Silvio},
eprint = {1702.01105},
journal = {CoRR},
keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Robotics},
title = {{Joint 2D-3D-Semantic Data for Indoor Scene Understanding}},
url = {http://arxiv.org/abs/1702.01105},
volume = {abs/1702.0},
year = {2017}
}
@article{Ghiasi2016,
annote = {From Duplicate 1 (Laplacian Reconstruction and Refinement for Semantic Segmentation - Ghiasi, Golnaz; Fowlkes, Charless C)

{\_}eprint: 1605.02264},
archivePrefix = {arXiv},
arxivId = {1605.02264},
author = {Ghiasi, Golnaz and Fowlkes, Charless C},
eprint = {1605.02264},
journal = {CoRR},
title = {{Laplacian Reconstruction and Refinement for Semantic Segmentation}},
url = {http://arxiv.org/abs/1605.02264},
volume = {abs/1605.0},
year = {2016}
}
@inproceedings{Real2017,
abstract = {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6{\%} (95.6{\%} for ensemble) and 77.0{\%}, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.},
archivePrefix = {arXiv},
arxivId = {1703.01041},
author = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V and Kurakin, Alexey},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
eprint = {1703.01041},
isbn = {9781510855144},
pages = {4429--4446},
title = {{Large-scale evolution of image classifiers}},
volume = {6},
year = {2017}
}
@inproceedings{Vahadane2016,
abstract = {For better perception and analysis of images, good quality and high resolution (HR) are always preferred over degraded and low resolution (LR) images. Getting HR images can be cost and time prohibitive. Super resolution (SR) techniques can be an affordable alternative for small zoom factors. In medical imaging, specifically in the case of histological images, estimating an HR image from an LR one requires preservation of complex textures and edges defining various biological features (nuclei, cytoplasm etc.). This challenge is further aggravated by the scale variance of histological images that are taken of a flat biopsy slide instead of a 3D world. We propose an algorithm for SR of histological images that learns a mapping from zero-phase component analysis (ZCA)-whitened LR patches to ZCA-whitened HR patches at the desired scale. ZCA-whitening exploits the redundancy in data and enhances the texture and edges energies to better learn the desired LR to HR mapping, which we learn using a neural network. The qualitative and quantitative validation shows that improvements in HR estimation by proposed algorithm are statistically significant over benchmark learning-based SR algorithms.},
author = {Vahadane, Abhishek and Kumar, Neeraj and Sethi, Amit},
booktitle = {Proceedings - International Symposium on Biomedical Imaging},
doi = {10.1109/ISBI.2016.7493391},
isbn = {9781479923502},
issn = {19458452},
keywords = {Image super-resolution,histological image,neural network},
month = {apr},
pages = {816--819},
title = {{Learning based super-resolution of histological images}},
volume = {2016-June},
year = {2016}
}
@inproceedings{Esteves2017,
abstract = {We address the problem of 3D rotation equivariance in convolutional neural networks. 3D rotations have been a challenging nuisance in 3D classification tasks requiring higher capacity and extended data augmentation in order to tackle it. We model 3D data with multi-valued spherical functions and we propose a novel spherical convolutional network that implements exact convolutions on the sphere by realizing them in the spherical harmonic domain. Resulting filters have local symmetry and are localized by enforcing smooth spectra. We apply a novel pooling on the spectral domain and our operations are independent of the underlying spherical resolution throughout the network. We show that networks with much lower capacity and without requiring data augmentation can exhibit performance comparable to the state of the art in standard retrieval and classification benchmarks.},
annote = {{\_}eprint: 1711.06721},
archivePrefix = {arXiv},
arxivId = {1711.06721},
author = {Esteves, Carlos and Allen-Blanchette, Christine and Makadia, Ameesh and Daniilidis, Kostas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01261-8_4},
eprint = {1711.06721},
isbn = {9783030012601},
issn = {16113349},
number = {November},
pages = {54--70},
title = {{Learning SO(3) Equivariant Representations with Spherical CNNs}},
url = {http://arxiv.org/abs/1711.06721},
volume = {11217 LNCS},
year = {2018}
}
@inproceedings{Tran2014,
abstract = {We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets, 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets, and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8{\%} accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.},
annote = {{\_}eprint: 1412.0767},
archivePrefix = {arXiv},
arxivId = {1412.0767},
author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.510},
eprint = {1412.0767},
isbn = {9781467383912},
issn = {15505499},
keywords = {Computer Science - Computer Vision and Pattern Rec},
pages = {4489--4497},
title = {{Learning spatiotemporal features with 3D convolutional networks}},
volume = {2015 Inter},
year = {2015}
}
@article{Swiderska-Chadaj2019,
abstract = {The immune system is of critical importance in the development of cancer. The evasion of destruction by the immune system is one of the emerging hallmarks of cancer. We have built a dataset of 171,166 manually annotated CD3+ and CD8+ cells, which we used to train deep learning algorithms for automatic detection of lymphocytes in histopathology images to better quantify immune response. Moreover, we investigate the effectiveness of four deep learning based methods when different subcompartments of the whole-slide image are considered: normal tissue areas, areas with immune cell clusters, and areas containing artifacts. We have compared the proposed methods in breast, colon and prostate cancer tissue slides collected from nine different medical centers. Finally, we report the results of an observer study on lymphocyte quantification, which involved four pathologists from different medical centers, and compare their performance with the automatic detection. The results give insights on the applicability of the proposed methods for clinical use. U-Net obtained the highest performance with an F1-score of 0.78 and the highest agreement with manual evaluation ($\kappa$=0.72), whereas the average pathologists agreement with reference standard was $\kappa$=0.64. The test set and the automatic evaluation procedure are publicly available at lyon19.grand-challenge.org.},
author = {Swiderska-Chadaj, Zaneta and Pinckaers, Hans and van Rijthoven, Mart and Balkenhol, Maschenka and Melnikova, Margarita and Geessink, Oscar and Manson, Quirine and Sherman, Mark and Polonia, Antonio and Parry, Jeremy and Abubakar, Mustapha and Litjens, Geert and van der Laak, Jeroen and Ciompi, Francesco},
doi = {10.1016/j.media.2019.101547},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Computational pathology,Deep learning,Immune cell detection,Immunohistochemistry},
pages = {101547},
title = {{Learning to detect lymphocytes in immunohistochemistry with deep learning}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841519300829},
volume = {58},
year = {2019}
}
@inproceedings{Zoph2017,
abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the 'NASNet search space') which enables transferability. In our experiments, we search for the best convolutional layer (or 'cell') on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a 'NASNet architecture'. We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4{\%} error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7{\%} top-1 and 96.2{\%} top-5 on ImageNet. Our model is 1.2{\%} better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28{\%} in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74{\%} top-1 accuracy, which is 3.1{\%} better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0{\%} achieving 43.1{\%} mAP on the COCO dataset.},
archivePrefix = {arXiv},
arxivId = {1707.07012},
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00907},
eprint = {1707.07012},
isbn = {9781538664209},
issn = {10636919},
pages = {8697--8710},
title = {{Learning Transferable Architectures for Scalable Image Recognition}},
year = {2018}
}
@article{Zhang2019,
abstract = {Reconstructing 3D geometry from satellite imagery is an important topic of research. However, disparities exist between how this 3D reconstruction problem is handled in the remote sensing context and how multi-view reconstruction pipelines have been developed in the computer vision community. In this paper, we explore whether state-of-the-art reconstruction pipelines from the vision community can be applied to the satellite imagery. Along the way, we address several challenges adapting vision-based structure from motion and multi-view stereo methods. We show that vision pipelines can offer competitive speed and accuracy in the satellite context.},
annote = {{\_}eprint: 1910.02989},
archivePrefix = {arXiv},
arxivId = {1910.02989},
author = {Zhang, Kai and Snavely, Noah and Sun, Jin},
eprint = {1910.02989},
title = {{Leveraging Vision Reconstruction Pipelines for Satellite Imagery}},
url = {http://arxiv.org/abs/1910.02989},
year = {2019}
}
@book{Mitchell1997MachineLearning,
author = {Mitchell, Tom M},
isbn = {0070428077},
publisher = {McGraw-Hill Education},
title = {{Machine Learning}},
url = {https://www.xarg.org/ref/a/0070428077/},
year = {1997}
}
@book{Flach2012,
abstract = {The emerging field of Ecosystem Informatics applies methods from computer science and mathematics to address fundamental and applied problems in the ecosystem sciences. The ecosystem sciences are in the midst of a revolution driven by a combination of emerging technologies for improved sensing and the critical need for better science to help manage global climate change. This paper describes several initiatives at Oregon State University in ecosystem informatics. At the level of sensor technologies, this paper describes two projects: (a) wireless, battery-free sensor networks for forests and (b) rapid throughput automated arthropod population counting. At the level of data preparation and data cleaning, this paper describes the application of linear gaussian dynamic Bayesian networks to automated anomaly detection in temperature data streams. Finally, the paper describes two educational activities: (a) a summer institute in ecosystem informatics and (b) an interdisciplinary Ph.D. program in Ecosystem Informatics for mathematics, computer science, and the ecosystem sciences.},
author = {Flach, Peter},
isbn = {1107422221, 9781107422223},
publisher = {Posts Telecom Press and Cambridge University Press},
title = {{Machine Learning: The Art and Science of Algorithms That Make Sense of Data}},
translator = {Duan, Fei},
year = {2012}
}
@article{Skibbe2019,
abstract = {Understanding the connectivity in the brain is an important prerequisite for understanding how the brain processes information. In the Brain/MINDS project, a connectivity study on marmoset brains uses two-photon microscopy fluorescence images of axonal projections to collect the neuron connectivity from defined brain regions at the mesoscopic scale. The processing of the images requires the detection and segmentation of the axonal tracer signal. The objective is to detect as much tracer signal as possible while not misclassifying other background structures as the signal. This can be challenging because of imaging noise, a cluttered image background, distortions or varying image contrast cause problems. We are developing MarmoNet, a pipeline that processes and analyzes tracer image data of the common marmoset brain. The pipeline incorporates state-of-the-art machine learning techniques based on artificial convolutional neural networks (CNN) and image registration techniques to extract and map all relevant information in a robust manner. The pipeline processes new images in a fully automated way. This report introduces the current state of the tracer signal analysis part of the pipeline.},
annote = {{\_}eprint: 1908.00876},
archivePrefix = {arXiv},
arxivId = {1908.00876},
author = {Skibbe, Henrik and Watakabe, Akiya and Nakae, Ken and Gutierrez, Carlos Enrique and Tsukada, Hiromichi and Hata, Junichi and Kawase, Takashi and Gong, Rui and Woodward, Alexander and Doya, Kenji and Okano, Hideyuki and Yamamori, Tetsuo and Ishii, Shin},
eprint = {1908.00876},
title = {{MarmoNet: a pipeline for automated projection mapping of the common marmoset brain from whole-brain serial two-photon tomography}},
url = {http://arxiv.org/abs/1908.00876},
year = {2019}
}
@article{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron.},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
doi = {10.1109/TPAMI.2018.2844175},
eprint = {1703.06870},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Instance segmentation,convolutional neural network,object detection,pose estimation},
number = {2},
pages = {386--397},
pmid = {29994331},
title = {{Mask R-CNN}},
volume = {42},
year = {2020}
}
@article{Dice1945,
abstract = {The coefficient of association of Forbes indicates the amount of association be- tween two given species compared to the amount of association between them expected by chance. In order to provide a simple direct measure of the amount of association of one species with another the association index is proposed. If a is the number of random samples of a given series in which species A occurs and h is the number of samples in which another species B occurs together with A, then the association index B/A = h/a. Similarly, if b is the number of samples in which species B occurs, then the associa- tion index A/B = h/b. There is also proposed a coincidence index, 2h/(a + b), whose value is intermediate between the two reciprocal association indices. As a measure of the statistical reliability of the deviation shown by the samples of a given series from the amount of associa- tion expected by chance, the chi-square test may be used.},
author = {Dice, Lee R.},
doi = {10.2307/1932409},
issn = {0012-9658},
journal = {Ecology},
number = {3},
pages = {297--302},
title = {{Measures of the Amount of Ecologic Association Between Species}},
volume = {26},
year = {1945}
}
@article{Muller2019,
annote = {{\_}eprint: 1910.09308},
archivePrefix = {arXiv},
arxivId = {1910.09308},
author = {M{\"{u}}ller, Dominik and Kramer, Frank},
eprint = {1910.09308},
title = {{MIScnn: A Framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning}},
year = {2019}
}
@article{Alkhasli2019,
abstract = {Background: The fronto-striatal network is involved in various motor, cognitive, and emotional processes, such as spatial attention, working memory, decision-making, and emotion regulation. Intermittent theta burst transcranial magnetic stimulation (iTBS) has been shown to modulate functional connectivity of brain networks. Long stimulation intervals, as well as high stimulation intensities are typically applied in transcranial magnetic stimulation (TMS) therapy for mood disorders. The role of stimulation intensity on network function and homeostasis has not been explored systematically yet. Objective: In this pilot study, we aimed to modulate fronto-striatal connectivity by applying iTBS at different intensities to the left dorso-lateral prefrontal cortex (DLPFC). We measured individual and group changes by comparing resting state functional magnetic resonance imaging (rsfMRI) both pre-iTBS and post-iTBS. Differential effects of individual sub- vs. supra-resting motor-threshold stimulation intensities were assessed. Methods: Sixteen healthy subjects underwent excitatory iTBS at two intensities [90{\%} and 120{\%} of individual resting motor threshold (rMT)] on separate days. Six-hundred pulses (2 s trains, 8 s pauses, duration of 3 min, 20 s) were applied over the left DLPFC. Directly before and 7 min after stimulation, task-free rsfMRI sessions, lasting 10 min each, were conducted. Individual seed-to-seed functional connectivity changes were calculated for 10 fronto-striatal and amygdala regions of interest with the SPM toolbox DPABI. Results: Sub-threshold-iTBS increased functional connectivity directly between the left DLPFC and the left and right caudate, respectively. Supra-threshold stimulation did not change fronto-striatal functional connectivity but increased functional connectivity between the right amygdala and the right caudate. Conclusion: A short iTBS protocol applied at sub-threshold intensities was not only sufficient, but favorable, in order to increase bilateral fronto-striatal functional connectivity, while minimizing side effects. The absence of an increase in functional connectivity after supra-threshold stimulation was possibly caused by network homeostatic effects.},
author = {Alkhasli, Isabel and Sakreida, Katrin and Mottaghy, Felix M. and Binkofski, Ferdinand},
doi = {10.3389/fnhum.2019.00190},
issn = {16625161},
journal = {Frontiers in Human Neuroscience},
keywords = {DLPFC,Fronto-striatal network,Functional connectivity,Intermittent theta burst stimulation (iTBS),Prefrontal cortex,Resting state,Striatum},
month = {jun},
title = {{Modulation of fronto-striatal functional connectivity using transcranial magnetic stimulation}},
url = {https://www.frontiersin.org/article/10.3389/fnhum.2019.00190/full},
volume = {13},
year = {2019}
}
@article{Song2019,
abstract = {It is a challenging problem to achieve generalized nuclear segmentation in digital histopathology images. Existing techniques, using either handcrafted features in learning-based models or traditional image analysis-based approaches, do not effectively tackle the challenging cases, such as crowded nuclei, chromatin-sparse, and heavy background clutter. In contrast, deep networks have achieved state-of-the-art performance in modeling various nuclear appearances. However, their success is limited due to the size of the considered networks. We solve these problems by reformulating nuclear segmentation in terms of a cascade 2-class classification problem and propose a multi-layer boosting sparse convolutional (ML-BSC) model. In the proposed ML-BSC model, discriminative probabilistic binary decision trees (PBDTs) are designed as weak learners in each layer to cope with challenging cases. A sparsity-constrained cascade structure enables the ML-BSC model to improve representation learning. Comparing to the existing techniques, our method can accurately separate individual nuclei in complex histopathology images, and it is more robust against chromatin-sparse and heavy background clutter. An evaluation carried out using three disparate datasets demonstrates the superiority of our method over the state-of-the-art supervised approaches in terms of segmentation accuracy.},
author = {Song, Jie and Xiao, Liang and Molaei, Mohsen and Lian, Zhichao},
doi = {https://doi.org/10.1016/j.knosys.2019.03.031},
issn = {0950-7051},
journal = {Knowledge-Based Systems},
keywords = {Cascade classification,Multi-layer boosting sparse convolutional model,Nucleus segmentation,Probabilistic binary decision tree,Representation learning},
pages = {40 -- 53},
title = {{Multi-layer boosting sparse convolutional model for generalized nuclear segmentation from histopathology images}},
url = {http://www.sciencedirect.com/science/article/pii/S095070511930156X},
volume = {176},
year = {2019}
}
@article{Mahmood2018,
abstract = {Humans make accurate decisions by interpreting complex data from multiple sources. Medical diagnostics, in particular, often hinge on human interpretation of multi-modal information. In order for artificial intelligence to make progress in automated, objective, and accurate diagnosis and prognosis, methods to fuse information from multiple medical imaging modalities are required. However, combining information from multiple data sources has several challenges, as current deep learning architectures lack the ability to extract useful representations from multimodal information, and often simple concatenation is used to fuse such information. In this work, we propose Multimodal DenseNet, a novel architecture for fusing multimodal data. Instead of focusing on concatenation or early and late fusion, our proposed architectures fuses information over several layers and gives the model flexibility in how it combines information from multiple sources. We apply this architecture to the challenge of polyp characterization and landmark identification in endoscopy. Features from white light images are fused with features from narrow band imaging or depth maps. This study demonstrates that Multimodal DenseNet outperforms monomodal classification as well as other multimodal fusion techniques by a significant margin on two different datasets.},
annote = {{\_}eprint: 1811.07407},
archivePrefix = {arXiv},
arxivId = {1811.07407},
author = {Mahmood, Faisal and Yang, Ziyun and Ashley, Thomas and Durr, Nicholas J},
eprint = {1811.07407},
journal = {ArXiv e-prints},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning},
title = {{Multimodal Densenet}},
url = {http://arxiv.org/abs/1811.07407},
year = {2018}
}
@inproceedings{Tatarchenko2015a,
abstract = {We present a convolutional network capable of inferring a 3D representation of a previously unseen object given a single image of this object. Concretely, the network can predict an RGB image and a depth map of the object as seen from an arbitrary view. Several of these depth maps fused together give a full point cloud of the object. The point cloud can in turn be transformed into a surface mesh. The network is trained on renderings of synthetic 3D models of cars and chairs. It successfully deals with objects on cluttered background and generates reasonable predictions for real images of cars.},
archivePrefix = {arXiv},
arxivId = {1511.06702},
author = {Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46478-7_20},
eprint = {1511.06702},
isbn = {9783319464770},
issn = {16113349},
keywords = {3D from single image,Convolutional networks,Deep learning},
pages = {322--337},
title = {{Multi-view 3D models from single images with a convolutional network}},
url = {http://arxiv.org/abs/1511.06702},
volume = {9911 LNCS},
year = {2016}
}
@article{Su2015,
annote = {From Duplicate 1 (Multi-view Convolutional Neural Networks for 3D Shape Recognition - Su, Hang; Maji, Subhransu; Kalogerakis, Evangelos; Learned-Miller, Erik G)

{\_}eprint: 1505.00880},
archivePrefix = {arXiv},
arxivId = {1505.00880},
author = {Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik G},
eprint = {1505.00880},
journal = {CoRR},
title = {{Multi-view Convolutional Neural Networks for 3D Shape Recognition}},
url = {http://arxiv.org/abs/1505.00880},
volume = {abs/1505.0},
year = {2015}
}
@article{Tustison2010,
abstract = {A variant of the popular nonparametric nonuniform intensity normalization (N3) algorithm is proposed for bias field correction. Given the superb performance of N3 and its public availability, it has been the subject of several evaluation studies. These studies have demonstrated the importance of certain parameters associated with the B-spline least-squares fitting. We propose the substitution of a recently developed fast and robust B-spline approximation routine and a modified hierarchical optimization scheme for improved bias field correction over the original N3 algorithm. Similar to the N3 algorithm, we also make the source code, testing, and technical documentation of our contribution, which we denote as N4ITK, available to the public through the Insight Toolkit of the National Institutes of Health. Performance assessment is demonstrated using simulated data from the publicly available Brainweb database, hyperpolarized 3He lung image data, and 9.4T postmortem hippocampus data. {\textcopyright} 2006 IEEE.},
author = {Tustison, Nicholas J and Avants, Brian B and Cook, Philip A and Zheng, Yuanjie and Egan, Alexander and Yushkevich, Paul A and Gee, James C},
doi = {10.1109/TMI.2010.2046908},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {B-spline approximation,Bias field,Inhomogeneity,N3},
month = {jun},
number = {6},
pages = {1310--1320},
pmid = {20378467},
title = {{N4ITK: Improved N3 bias correction}},
volume = {29},
year = {2010}
}
@inproceedings{Chen2015,
abstract = {We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.},
archivePrefix = {arXiv},
arxivId = {1511.05641},
author = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
booktitle = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
eprint = {1511.05641},
title = {{Net2Net: Accelerating learning via knowledge transfer}},
volume = {abs/1511.0},
year = {2016}
}
@inproceedings{Wei2016,
abstract = {We present a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as network morphism in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous nonlinear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.},
archivePrefix = {arXiv},
arxivId = {1603.01670},
author = {Wei, Tao and Wang, Changhu and Rui, Yong and Chen, Chang Wen},
booktitle = {33rd International Conference on Machine Learning, ICML 2016},
eprint = {1603.01670},
isbn = {9781510829008},
pages = {842--850},
title = {{Network morphism}},
volume = {2},
year = {2016}
}
@inproceedings{Zoph2016,
abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
archivePrefix = {arXiv},
arxivId = {1904.09981},
author = {Zoph, Barret and Le, Quoc V},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1904.09981},
title = {{Neural architecture search with reinforcement learning}},
volume = {abs/1611.0},
year = {2019}
}
@book{Haykin2011,
author = {{Simon Haykin}},
publisher = {China Machine Press and Pearson Education},
title = {{Neural Network and Learning Machines 3rd edition}},
translator = {Shen, Furao and Xu, Ye and Zheng, Jun and Chao, Jin},
year = {2009}
}
@article{Rumelhart1988,
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
address = {Cambridge, MA, USA},
annote = {Section: Learning Representations by Back-propagating Errors},
author = {Mingolla, Ennio and Bullock, Daniel},
doi = {10.1016/0893-6080(89)90025-7},
editor = {Anderson, James A and Rosenfeld, Edward},
isbn = {0-262-01097-6},
issn = {08936080},
journal = {Neural Networks},
number = {5},
pages = {405--409},
publisher = {MIT Press},
title = {{Neurocomputing: Foundations of Research}},
url = {http://dl.acm.org/citation.cfm?id=65669.104451},
volume = {2},
year = {1989}
}
@article{Rajchl2018,
abstract = {NeuroNet is a deep convolutional neural network mimicking multiple popular and state-of-the-art brain segmentation tools including FSL, SPM, and MALPEM. The network is trained on 5,000 T1-weighted brain MRI scans from the UK Biobank Imaging Study that have been automatically segmented into brain tissue and cortical and sub-cortical structures using the standard neuroimaging pipelines. Training a single model from these complementary and partially overlapping label maps yields a new powerful "all-in-one", multi-output segmentation tool. The processing time for a single subject is reduced by an order of magnitude compared to running each individual software package. We demonstrate very good reproducibility of the original outputs while increasing robustness to variations in the input data. We believe NeuroNet could be an important tool in large-scale population imaging studies and serve as a new standard in neuroscience by reducing the risk of introducing bias when choosing a specific software package.},
annote = {{\_}eprint: 1806.04224},
archivePrefix = {arXiv},
arxivId = {1806.04224},
author = {Rajchl, Martin and Pawlowski, Nick and Rueckert, Daniel and Matthews, Paul M and Glocker, Ben},
eprint = {1806.04224},
title = {{NeuroNet: Fast and Robust Reproduction of Multiple Brain Image Segmentation Pipelines}},
url = {http://arxiv.org/abs/1806.04224},
year = {2018}
}
@article{Gibson2018,
abstract = {Background and objectives: Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. Established deep-learning platforms are flexible but do not provide specific functionality for medical image analysis and adapting them for this domain of application requires substantial implementation effort. Consequently, there has been substantial duplication of effort and incompatible infrastructure developed across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon. Methods: The NiftyNet infrastructure provides a modular deep-learning pipeline for a range of medical imaging applications including segmentation, regression, image generation and representation learning applications. Components of the NiftyNet pipeline including data loading, data augmentation, network architectures, loss functions and evaluation metrics are tailored to, and take advantage of, the idiosyncracies of medical image analysis and computer-assisted intervention. NiftyNet is built on the TensorFlow framework and supports features such as TensorBoard visualization of 2D and 3D images and computational graphs by default. Results: We present three illustrative medical image analysis applications built using NiftyNet infrastructure: (1) segmentation of multiple abdominal organs from computed tomography; (2) image regression to predict computed tomography attenuation maps from brain magnetic resonance images; and (3) generation of simulated ultrasound images for specified anatomical poses. Conclusions: The NiftyNet infrastructure enables researchers to rapidly develop and distribute deep learning solutions for segmentation, regression, image generation and representation learning applications, or extend the platform to new applications.},
archivePrefix = {arXiv},
arxivId = {1709.03485},
author = {Gibson, Eli and Li, Wenqi and Sudre, Carole and Fidon, Lucas and Shakir, Dzhoshkun I and Wang, Guotai and Eaton-Rosen, Zach and Gray, Robert and Doel, Tom and Hu, Yipeng and Whyntie, Tom and Nachev, Parashkev and Modat, Marc and Barratt, Dean C and Ourselin, S{\'{e}}bastien and Cardoso, M Jorge and Vercauteren, Tom},
doi = {10.1016/j.cmpb.2018.01.025},
eprint = {1709.03485},
issn = {18727565},
journal = {Computer Methods and Programs in Biomedicine},
keywords = {Convolutional neural network,Deep learning,Generative adversarial network,Image regression,Medical image analysis,Segmentation},
pages = {113--122},
title = {{NiftyNet: a deep-learning platform for medical imaging}},
url = {http://www.sciencedirect.com/science/article/pii/S0169260717311823},
volume = {158},
year = {2018}
}
@article{S.Senthilraja2014,
abstract = {Image processing concept play a important role in the field of Medical to diagnosis of diseases. Noise is introduced in the medical images due to various reasons. In Medical Imaging, Noise degrades the quality of images. This degradation includes suppression of edges, blurring boundaries etc. Edge and preservation details are very important to discover a disease. Noise removal is a very challenging issue in the Medical Image Processing. Denoising can help the physicians to diagnose the diseases. Medical Images include CT, MRI scan, X-ray and ultrasound images etc. This paper we implemented a new filter called WB-Filter for Medical Image denoising. WB-Filter mainly focuses on speckle noise {\&} Gaussian Noise removal especially in the CT scan images. Experimental results are compared with other three filtering concepts. The result images quality is measured by the PSNR, RMSE and MSE. The results demonstrate that the proposed WB-Filter concept obtaining the optimum result quality of the Medical Image.},
author = {Senthilraja, S and Suresh, P and Suganthi, M},
issn = {2229-5518},
journal = {International Journal of Scientific and Engineering Research},
month = {mar},
number = {3},
pages = {243},
title = {{Noise Reduction in Computed Tomography Image Using WB–Filter}},
volume = {5},
year = {2014}
}
@inproceedings{Wang2017,
abstract = {We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D shape analysis. Built upon the octree representation of 3D shapes, our method takes the average normal vectors of a 3D model sampled in the finest leaf octants as input and performs 3D CNN operations on the octants occupied by the 3D shape surface. We design a novel octree data structure to efficiently store the octant information and CNN features into the graphics memory and execute the entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN structures and works for 3D shapes in different representations. By restraining the computations on the octants occupied by 3D surfaces, the memory and computational costs of the O-CNN grow quadratically as the depth of the octree increases, which makes the 3D CNN feasible for high-resolution 3D models. We compare the performance of the O-CNN with other existing 3D CNN solutions and demonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks, including object classification, shape retrieval, and shape segmentation.},
archivePrefix = {arXiv},
arxivId = {1712.01537},
author = {Wang, Peng Shuai and Liu, Yang and Guo, Yu Xiao and Sun, Chun Yu and Tong, Xin},
booktitle = {ACM Transactions on Graphics},
doi = {10.1145/3072959.3073608},
eprint = {1712.01537},
issn = {15577368},
keywords = {Convolutional neural network,Object classification,Octree,Shape retrieval,Shape segmentation},
number = {4},
pages = {72:1--72:11},
title = {{O-CNN: Octree-based convolutional neural networks for 3D shape analysis}},
url = {http://doi.acm.org/10.1145/3072959.3073608},
volume = {36},
year = {2017}
}
@inproceedings{Riegler2016,
abstract = {We present OctNet, a representation for deep learning with sparse 3D data. In contrast to existing models, our representation enables 3D convolutional networks which are both deep and high resolution. Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each leaf node stores a pooled feature representation. This allows to focus memory allocation and computation to the relevant dense regions and enables deeper networks without compromising resolution. We demonstrate the utility of our OctNet representation by analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling.},
annote = {{\_}eprint: 1611.05009},
archivePrefix = {arXiv},
arxivId = {1611.05009},
author = {Riegler, Gernot and Ulusoy, Ali Osman and Geiger, Andreas},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.701},
eprint = {1611.05009},
isbn = {9781538604571},
pages = {6620--6629},
title = {{OctNet: Learning deep 3D representations at high resolutions}},
url = {http://arxiv.org/abs/1611.05009},
volume = {2017-Janua},
year = {2017}
}
@article{Wilhelms1992,
abstract = {The large size of many volume data sets often prevents visualization algorithms from providing interactive rendering. The use of hierarchical data structures can ameliorate this problem by storing summary information to prevent useless exploration of regions of little or no current interest within the volume. This paper discusses research into the use of the octree hierarchical data structure when the regions of current interest can vary during the application, and are not known a priori. Octrees are well suited to the six-sided cell structure of many volumes. A new space-efficient design is introduced for octree representations of volumes whose resolutions are not conveniently a power of two; octrees following this design are called branch-on-need octrees 1992. Also, a caching method is described that essentially passes information between octree neighbors whose visitation times may be quite different, then discards it when its useful life is over. Using the application of octrees to isosurface generation as a focus, space and time comparisons for octree-based versus more traditional “marching” methods are presented. {\textcopyright} 1992, ACM. All rights reserved.},
annote = {Place: New York, NY, USA
Publisher: ACM},
author = {Wilhelms, Jane and {Van Gelder}, Allen},
doi = {10.1145/130881.130882},
issn = {15577368},
journal = {ACM Transactions on Graphics (TOG)},
keywords = {hierarchical spatial enumeration,isosurface extraction,octree,scientific visualization},
number = {3},
pages = {201--227},
title = {{Octrees for Faster Isosurface Generation}},
url = {http://doi.acm.org/10.1145/130881.130882},
volume = {11},
year = {1992}
}
@article{Goode2013,
abstract = {Although widely touted as a replacement for glass slides and microscopes in pathology, digital slides present major challenges in data storage, transmission, processing and interoperability. Since no universal data format is in widespread use for these images today, each vendor defines its own proprietary data formats, analysis tools, viewers and software libraries. This creates issues not only for pathologists, but also for interoperability. In this paper, we present the design and implementation of OpenSlide, a vendor-neutral C library for reading and manipulating digital slides of diverse vendor formats. The library is extensible and easily interfaced to various programming languages. An application written to the OpenSlide interface can transparently handle multiple vendor formats. OpenSlide is in use today by many academic and industrial organizations world-wide, including many research sites in the United States that are funded by the National Institutes of Health.},
author = {Satyanarayanan, Mahadev and Goode, Adam and Gilbert, Benjamin and Harkes, Jan and Jukic, Drazen},
doi = {10.4103/2153-3539.119005},
issn = {2153-3539},
journal = {Journal of Pathology Informatics},
number = {1},
pages = {27},
title = {{OpenSlide: A vendor-neutral software foundation for digital pathology}},
volume = {4},
year = {2013}
}
@article{Song2013,
abstract = {Positron emission tomography (PET)-computed tomography (CT) images have been widely used in clinical practice for radiotherapy treatment planning of the radiotherapy. Many existing segmentation approaches only work for a single imaging modality, which suffer from the low spatial resolution in PET or low contrast in CT. In this work, we propose a novel method for the co-segmentation of the tumor in both PET and CT images, which makes use of advantages from each modality: the functionality information from PET and the anatomical structure information from CT. The approach formulates the segmentation problem as a minimization problem of a Markov random field model, which encodes the information from both modalities. The optimization is solved using a graph-cut based method. Two sub-graphs are constructed for the segmentation of the PET and the CT images, respectively. To achieve consistent results in two modalities, an adaptive context cost is enforced by adding context arcs between the two sub-graphs. An optimal solution can be obtained by solving a single maximum flow problem, which leads to simultaneous segmentation of the tumor volumes in both modalities. The proposed algorithm was validated in robust delineation of lung tumors on 23 PET-CT datasets and two head-and-neck cancer subjects. Both qualitative and quantitative results show significant improvement compared to the graph cut methods solely using PET or CT. {\textcopyright} 2012 IEEE.},
author = {Song, Qi and Bai, Junjie and Han, Dongfeng and Bhatia, Sudershan and Sun, Wenqing and Rockey, William and Bayouth, John E and Buatti, John M and Wu, Xiaodong},
doi = {10.1109/TMI.2013.2263388},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Context information,Positron emission tomography-computed tomography (,global optimization,graph cut,image segmentation,lung tumor},
month = {sep},
number = {9},
pages = {1685--1697},
title = {{Optimal Co-segmentation of tumor in PET-CT images with context information}},
volume = {32},
year = {2013}
}
@inproceedings{Sedaghat2016,
abstract = {Recent work has shown good recognition results in 3D object recognition using 3D convolutional networks. In this paper, we show that the object orientation plays an important role in 3D recognition. More specifically, we argue that objects induce different features in the network under rotation. Thus, we approach the category-level classification task as a multi-task problem, in which the network is trained to predict the pose of the object in addition to the class label as a parallel task. We show that this yields significant improvements in the classification results. We test our suggested architecture on several datasets representing various 3D data sources: LiDAR data, CAD models, and RGB-D images. We report state-of-the-art results on classification as well as significant improvements in precision and speed over the baseline on 3D detection.},
annote = {{\_}eprint: 1604.03351},
archivePrefix = {arXiv},
arxivId = {1604.03351},
author = {Sedaghat, Nima and Zolfaghari, Mohammadreza and Amiri, Ehsan and Brox, Thomas},
booktitle = {British Machine Vision Conference 2017, BMVC 2017},
doi = {10.5244/c.31.97},
eprint = {1604.03351},
isbn = {190172560X},
title = {{Orientation-boosted Voxel nets for 3D object recognition}},
url = {http://arxiv.org/abs/1604.03351},
volume = {abs/1604.0},
year = {2017}
}
@incollection{Cai2018,
abstract = {Automatic pancreas segmentation in radiology images, e.g., computed tomography (CT), and magnetic resonance imaging (MRI), is frequently required by computer-aided screening, diagnosis, and quantitative assessment. Yet, pancreas is a challenging abdominal organ to segment due to the high inter-patient anatomical variability in both shape and volume metrics. Recently, convolutional neural networks (CNN) have demonstrated promising performance on accurate segmentation of pancreas. However, the CNN-based method often suffers from segmentation discontinuity for reasons such as noisy image quality and blurry pancreatic boundary. In this chapter, we first discuss the CNN configurations and training objectives that lead to the state-of-the-art performance on pancreas segmentation. We then present a recurrent neural network (RNN) to address the problem of segmentation spatial inconsistency across adjacent image slices. The RNN takes outputs of the CNN and refines the segmentation by improving the shape smoothness.},
annote = {{\_}eprint: 1803.11303},
archivePrefix = {arXiv},
arxivId = {1803.11303},
author = {Cai, Jinzheng and Lu, Le and Xing, Fuyong and Yang, Lin},
booktitle = {Advances in Computer Vision and Pattern Recognition},
doi = {10.1007/978-3-030-13969-8_1},
eprint = {1803.11303},
issn = {21916594},
keywords = {Computer Science - Computer Vision and Pattern Rec},
pages = {3--21},
title = {{Pancreas Segmentation in CT and MRI via Task-Specific Network Design and Recurrent Neural Contextual Learning}},
year = {2019}
}
@inproceedings{Cai2017a,
abstract = {Deep neural networks have demonstrated very promising performance on accurate segmentation of challenging organs (e.g., pancreas) in abdominal CT and MRI scans. The current deep learning approaches conduct pancreas segmentation by processing sequences of 2D image slices independently through deep, dense per-pixel masking for each image, without explicitly enforcing spatial consistency constraint on segmentation of successive slices. We propose a new convolutional/recurrent neural network architecture to address the contextual learning and segmentation consistency problem. A deep convolutional sub-network is first designed and pre-trained from scratch. The output layer of this network module is then connected to recurrent layers and can be fine-tuned for contextual learning, in an end-to-end manner. Our recurrent sub-network is a type of Long short-term memory (LSTM) network that performs segmentation on an image by integrating its neighboring slice segmentation predictions, in the form of a dependent sequence processing. Additionally, a novel segmentation-direct loss function (named Jaccard Loss) is proposed and deep networks are trained to optimize Jaccard Index (JI) directly. Extensive experiments are conducted to validate our proposed deep models, on quantitative pancreas segmentation using both CT and MRI scans. Our method outperforms the state-of-the-art work on CT [11] and MRI pancreas segmentation [1], respectively.},
address = {Cham},
author = {Cai, Jinzheng and Lu, Le and Xie, Yuanpu and Xing, Fuyong and Yang, Lin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-66179-7_77},
editor = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D Louis and Duchesne, Simon},
isbn = {9783319661780},
issn = {16113349},
pages = {674--682},
publisher = {Springer International Publishing},
title = {{Pancreas segmentation in MRI using graph-based decision fusion on convolutional neural networks}},
volume = {10435 LNCS},
year = {2017}
}
@article{Rajan2019,
abstract = {Pulmonary embolisms (PE) are known to be one of the leading causes for cardiac-related mortality. Due to inherent variabilities in how PE manifests and the cumbersome nature of manual diagnosis, there is growing interest in leveraging AI tools for detecting PE. In this paper, we build a two-stage detection pipeline that is accurate, computationally efficient, robust to variations in PE types and kernels used for CT reconstruction, and most importantly, does not require dense annotations. Given the challenges in acquiring expert annotations in large-scale datasets, our approach produces state-of-the-art results with very sparse emboli contours (at 10mm slice spacing), while using models with significantly lower number of parameters. We achieve AUC scores of 0.94 on the validation set and 0.85 on the test set of highly severe PEs. Using a large, real-world dataset characterized by complex PE types and patients from multiple hospitals, we present an elaborate empirical study and provide guidelines for designing highly generalizable pipelines.},
annote = {{\_}eprint: 1910.02175},
archivePrefix = {arXiv},
arxivId = {1910.02175},
author = {Rajan, Deepta and Beymer, David and Abedin, Shafiqul and Dehghan, Ehsan},
eprint = {1910.02175},
title = {{Pi-PE: A Pipeline for Pulmonary Embolism Detection using Sparsely Annotated 3D CT Images}},
url = {http://arxiv.org/abs/1910.02175},
year = {2019}
}
@inproceedings{Huang2016a,
abstract = {In this paper, we tackle the labeling problem for 3D point clouds. We introduce a 3D point cloud labeling scheme based on 3D Convolutional Neural Network. Our approach minimizes the prior knowledge of the labeling problem and does not require a segmentation step or hand-crafted features as most previous approaches did. Particularly, we present solutions for large data handling during the training and testing process. Experiments performed on the urban point cloud dataset containing 7 categories of objects show the robustness of our approach.},
author = {Jing, Huang and You, Suya},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2016.7900038},
isbn = {9781509048472},
issn = {10514651},
keywords = {3D convolutional neural network,3D point cloud labeling scheme,Labeling,Neural networks,Testing,Three-dimensional displays,Training,Training data,Two dimensional displays,computer vision,data handling,neural nets,object recognition,testing process,training process,urban point cloud dataset},
pages = {2670--2675},
title = {{Point cloud labeling using 3D Convolutional Neural Network}},
year = {2016}
}
@inproceedings{Li2018,
abstract = {We present a simple and general framework for feature learning from point clouds. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point clouds are irregular and unordered, thus directly convolving kernels against features associated with the points will result in desertion of shape information and variance to point ordering. To address these problems, we propose to learn an X-transformation from the input points to simultaneously promote two causes: the first is the weighting of the input features associated with the points, and the second is the permutation of the points into a latent and potentially canonical order. Element-wise product and sum operations of the typical convolution operator are subsequently applied on the X-transformed features. The proposed method is a generalization of typical CNNs to feature learning from point clouds, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.},
annote = {{\_}eprint: 1801.07791},
archivePrefix = {arXiv},
arxivId = {1801.07791},
author = {Li, Yangyan and Bu, Rui and Sun, Mingchao and Wu, Wei and Di, Xinhan and Chen, Baoquan},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1801.07791},
issn = {10495258},
pages = {820--830},
title = {{PointCNN: Convolution on X-transformed points}},
url = {http://arxiv.org/abs/1801.07791},
volume = {2018-Decem},
year = {2018}
}
@inproceedings{Qi2016,
abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
annote = {{\_}eprint: 1612.00593},
archivePrefix = {arXiv},
arxivId = {1612.00593},
author = {Qi, Charles Ruizhongtai and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.16},
eprint = {1612.00593},
isbn = {9781538604571},
pages = {77--85},
title = {{PointNet: Deep learning on point sets for 3D classification and segmentation}},
url = {http://arxiv.org/abs/1612.00593},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Qi2017,
abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
annote = {{\_}eprint: 1706.02413},
archivePrefix = {arXiv},
arxivId = {1706.02413},
author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1706.02413},
issn = {10495258},
pages = {5100--5109},
title = {{PointNet++: Deep hierarchical feature learning on point sets in a metric space}},
url = {http://arxiv.org/abs/1706.02413},
volume = {2017-Decem},
year = {2017}
}
@inproceedings{Snoek2012,
abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1206.2944},
isbn = {9781627480031},
issn = {10495258},
pages = {2951--2959},
title = {{Practical Bayesian optimization of machine learning algorithms}},
volume = {4},
year = {2012}
}
@inproceedings{Zhong2017a,
abstract = {Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained sequentially to choose component layers. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it performs competitive results in comparison to the hand-crafted state-of-the-art networks on image classification, additionally, the best network generated by BlockQNN achieves 3.54{\%} top-1 error rate on CIFAR-10 which beats all existing auto-generate networks. (2) in the meanwhile, it offers tremendous reduction of the search space in designing networks which only spends 3 days with 32 GPUs, and (3) moreover, it has strong generalizability that the network built on CIFAR also performs well on a larger-scale ImageNet dataset.},
archivePrefix = {arXiv},
arxivId = {1708.05552},
author = {Zhong, Zhao and Yan, Junjie and Wu, Wei and Shao, Jing and Liu, Cheng Lin},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00257},
eprint = {1708.05552},
isbn = {9781538664209},
issn = {10636919},
pages = {2423--2432},
title = {{Practical Block-Wise Neural Network Architecture Generation}},
year = {2018}
}
@article{Ogiela2008,
abstract = {This chapter briefly discusses the main stages of image preprocessing. The introduction to this book mentioned that the preprocessing of medical image is subject to certain restrictions and is generally more complex than the processing of other image types [26, 52]. This is why, of the many different techniques and methods for image filtering, we have decided to discuss here only selected ones, most frequently applied to medical images and which have been proven to be suitable for that purpose in numerous practical cases. Their operation will be illustrated with examples of simple procedures aimed at improving the quality of imaging and allowing significant information to be generated for its use at the stages of image interpretation. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
address = {Berlin, Heidelberg},
author = {Ogiela, Marek R and Tadeusiewicz, Ryszard},
doi = {10.1007/978-3-540-75402-2_4},
isbn = {9783540753995},
issn = {1860949X},
journal = {Studies in Computational Intelligence},
pages = {65--97},
publisher = {Springer Berlin Heidelberg},
title = {{Preprocessing medical images and their overall enhancement}},
url = {https://doi.org/10.1007/978-3-540-75402-2{\_}4},
volume = {84},
year = {2008}
}
@article{Tofighi2019,
abstract = {Cell nuclei detection is a challenging research topic because of limitations in cellular image quality and diversity of nuclear morphology, i.e., varying nuclei shapes, sizes, and overlaps between multiple cell nuclei. This has been a topic of enduring interest with promising recent success shown by deep learning methods. These methods train convolutional neural networks (CNNs) with a training set of input images and known, labeled nuclei locations. Many such methods are supplemented by spatial or morphological processing. Using a set of canonical cell nuclei shapes, prepared with the help of a domain expert, we develop a new approach that we call shape priors (SPs) with CNNs (SPs-CNN). We further extend the network to introduce an SP layer and then allowing it to become trainable (i.e., optimizable). We call this network as tunable SP-CNN (TSP-CNN). In summary, we present new network structures that can incorporate "expected behavior" of nucleus shapes via two components: learnable layers that perform the nucleus detection and a fixed processing part that guides the learning with prior information. Analytically, we formulate two new regularization terms that are targeted at: 1) learning the shapes and 2) reducing false positives while simultaneously encouraging detection inside the cell nucleus boundary. Experimental results on two challenging datasets reveal that the proposed SP-CNN and TSP-CNN can outperform the state-of-the-art alternatives.},
archivePrefix = {arXiv},
arxivId = {1901.07061},
author = {Tofighi, Mohammad and Guo, Tiantong and Vanamala, Jairam K.P. and Monga, Vishal},
doi = {10.1109/TMI.2019.2895318},
eprint = {1901.07061},
issn = {1558254X},
journal = {IEEE transactions on medical imaging},
keywords = {Biomedical imaging,Computer architecture,Deep learning,Image edge detection,Image segmentation,Microprocessors,Nucleus detection,Shape,TSP-CNN,biology computing,canonical cell nuclei shapes,cell nuclei detection,cell nucleus boundary,cell nucleus detection,cellular biophysics,cellular image quality,convolutional neural nets,convolutional neural networks,deep learning,deep learning methods,domain expert,fixed processing part,input images,labeled nuclei locations,learnable layers,learnable shapes,learning (artificial intelligence),medical image processing,morphological processing,multiple cell nuclei,network structures,nuclear morphology,nucleus shapes,regularization terms,shape priors,spatial processing,training set,tunable SP-CNN},
month = {sep},
number = {9},
pages = {2047--2058},
title = {{Prior Information Guided Regularized Deep Learning for Cell Nucleus Detection}},
volume = {38},
year = {2019}
}
@inproceedings{Liu2017a,
abstract = {We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.},
archivePrefix = {arXiv},
arxivId = {1808.00391},
author = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01246-5_2},
eprint = {1808.00391},
isbn = {9783030012458},
issn = {16113349},
pages = {19--35},
title = {{Progressive Neural Architecture Search}},
volume = {11205 LNCS},
year = {2018}
}
@article{Oliphant2007,
annote = {{\_}eprint: https://aip.scitation.org/doi/pdf/10.1109/MCSE.2007.58},
author = {Oliphant, Travis E},
doi = {10.1109/MCSE.2007.58},
journal = {Computing in Science {\&} Engineering},
number = {3},
pages = {10--20},
title = {{Python for Scientific Computing}},
url = {https://aip.scitation.org/doi/abs/10.1109/MCSE.2007.58},
volume = {9},
year = {2007}
}
@article{Steiner2019,
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
archivePrefix = {arXiv},
arxivId = {1912.01703},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"{o}}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
eprint = {1912.01703},
title = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
url = {http://arxiv.org/abs/1912.01703},
year = {2019}
}
@article{Ruifrok2001,
abstract = {OBJECTIVE: To develop a flexible method of separation and quantification of immunohistochemical staining by means of color image analysis. STUDY DESIGN: An algorithm was developed to deconvolve the color information acquired with red-green-blue (RGB) cameras and to calculate the contribution of each of the applied stains based on stain-specific RGB absorption. The algorithm was tested using different combinations of diaminobenzidine, hematoxylin and eosin at different staining levels. RESULTS: Quantification of the different stains was not significantly influenced by the combination of multiple stains in a single sample. The color deconvolution algorithm resulted in comparable quantification independent of the stain combinations as long as the histochemical procedures did not influence the amount of stain in the sample due to bleaching because of stain solubility and saturation of staining was prevented. CONCLUSION: This image analysis algorithm provides a robust and flexible method for objective immunohistochemical analysis of samples stained with up to three different stains using a laboratory microscope, standard RGB camera setup and the public domain program NIH Image.},
author = {Ruifrok, A. C. and Johnston, Dennis A},
issn = {08846812},
journal = {Analytical and Quantitative Cytology and Histology},
keywords = {Color deconvolution,Color separation,Computer-assisted,Image processing,Immunohistochemistry},
number = {4},
pages = {291--299},
pmid = {11531144},
title = {{Quantification of histochemical staining by color deconvolution}},
volume = {23},
year = {2001}
}
@article{Daducci2014,
abstract = {Validation is arguably the bottleneck in the diffusion magnetic resonance imaging (MRI) community. This paper evaluates and compares 20 algorithms for recovering the local intra-voxel fiber structure from diffusion MRI data and is based on the results of the 'HARDI reconstruction challenge' organized in the context of the 'ISBI 2012' conference. Evaluated methods encompass a mixture of classical techniques well known in the literature such as diffusion tensor, Q-Ball and diffusion spectrum imaging, algorithms inspired by the recent theory of compressed sensing and also brand new approaches proposed for the first time at this contest. To quantitatively compare the methods under controlled conditions, two datasets with known ground-truth were synthetically generated and two main criteria were used to evaluate the quality of the reconstructions in every voxel: correct assessment of the number of fiber populations and angular accuracy in their orientation. This comparative study investigates the behavior of every algorithm with varying experimental conditions and highlights strengths and weaknesses of each approach. This information can be useful not only for enhancing current algorithms and develop the next generation of reconstruction methods, but also to assist physicians in the choice of the most adequate technique for their studies. {\textcopyright} 1982-2012 IEEE.},
author = {Daducci, Alessandro and Canales-Rodriguez, Erick Jorge and Descoteaux, Maxime and Garyfallidis, Eleftherios and Gur, Yaniv and Lin, Ying Chia and Mani, Merry and Merlet, Sylvain and Paquette, Michael and Ramirez-Manzanares, Alonso and Reisert, Marco and Rodrigues, Paulo Reis and Sepehrband, Farshid and Caruyer, Emmanuel and Choupan, Jeiran and Deriche, Rachid and Jacob, Mathews and Menegaz, Gloria and Prckovska, Vesna and Rivera, Mariano and Wiaux, Yves and Thiran, Jean Philippe},
doi = {10.1109/TMI.2013.2285500},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Diffusion magnetic resonance imaging (dMRI),local reconstruction,quantitative comparison,synthetic data,validation},
month = {feb},
number = {2},
pages = {384--399},
pmid = {24132007},
title = {{Quantitative comparison of reconstruction methods for intra-voxel fiber recovery from diffusion MRI}},
url = {http://ieeexplore.ieee.org/document/6630106/},
volume = {33},
year = {2014}
}
@article{Zhong2017,
abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
annote = {{\_}eprint: 1708.04896},
archivePrefix = {arXiv},
arxivId = {1708.04896},
author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
eprint = {1708.04896},
title = {{Random Erasing Data Augmentation}},
url = {http://arxiv.org/abs/1708.04896},
year = {2017}
}
@article{Bergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms. {\textcopyright} 2012 James Bergstra and Yoshua Bengio.},
author = {Bergstra, James and Bengio, Yoshua},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Deep learning,Global optimization,Model selection,Neural networks,Response surface modeling},
number = {1},
pages = {281--305},
title = {{Random search for hyper-parameter optimization}},
volume = {13},
year = {2012}
}
@article{Ju2015,
abstract = {Accurate lung tumor delineation plays an important role in radiotherapy treatment planning. Since the lung tumor has poor boundary in positron emission tomography (PET) images and low contrast in computed tomography (CT) images, segmentation of tumor in the PET and CT images is a challenging task. In this paper, we effectively integrate the two modalities by making fully use of the superior contrast of PET images and superior spatial resolution of CT images. Random walk and graph cut method is integrated to solve the segmentation problem, in which random walk is utilized as an initialization tool to provide object seeds for graph cut segmentation on the PET and CT images. The co-segmentation problem is formulated as an energy minimization problem which is solved by max-flow/min-cut method. A graph, including two sub-graphs and a special link, is constructed, in which one sub-graph is for the PET and another is for CT, and the special link encodes a context term which penalizes the difference of the tumor segmentation on the two modalities. To fully utilize the characteristics of PET and CT images, a novel energy representation is devised. For the PET, a downhill cost and a 3D derivative cost are proposed. For the CT, a shape penalty cost is integrated into the energy function which helps to constrain the tumor region during the segmentation. We validate our algorithm on a data set which consists of 18 PET-CT images. The experimental results indicate that the proposed method is superior to the graph cut method solely using the PET or CT is more accurate compared with the random walk method, random walk co-segmentation method, and non-improved graph cut method.},
author = {Ju, Wei and Xiang, Deihui and Zhang, Bin and Wang, Lirong and Kopriva, Ivica and Chen, Xinjian},
doi = {10.1109/TIP.2015.2488902},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Computed Tomography (CT),Positron Emission Tomography (PET),graph cut,image segmentation,interactive segmentation,lung tumor,prior information,random walk},
number = {12},
pages = {5854--5867},
title = {{Random Walk and Graph Cut for Co-Segmentation of Lung Tumor on PET-CT Images}},
volume = {24},
year = {2015}
}
@article{VanRullen2018,
abstract = {While objects from different categories can be reliably decoded from fMRI brain response patterns, it has proved more difficult to distinguish visually similar inputs, such as different instances of the same category. Here, we apply a recently developed deep learning system to the reconstruction of face images from human fMRI patterns. We trained a variational auto-encoder (VAE) neural network using a GAN (Generative Adversarial Network) unsupervised training procedure over a large dataset of celebrity faces. The auto-encoder latent space provides a meaningful, topologically organized 1024-dimensional description of each image. We then presented several thousand face images to human subjects, and learned a simple linear mapping between the multi-voxel fMRI activation patterns and the 1024 latent dimensions. Finally, we applied this mapping to novel test images, turning the obtained fMRI patterns into VAE latent codes, and ultimately the codes into face reconstructions. Qualitative and quantitative evaluation of the reconstructions revealed robust pairwise decoding ({\textgreater}95{\%} correct), and a strong improvement relative to a baseline model (PCA decomposition). Furthermore, this brain decoding model can readily be recycled to probe human face perception along many dimensions of interest; for example, the technique allowed for accurate gender classification, and even to decode which face was imagined, rather than seen by the subject. We hypothesize that the latent space of modern deep learning generative models could serve as a valid approximation for human brain representations.},
archivePrefix = {arXiv},
arxivId = {1810.03856},
author = {VanRullen, Rufin and Reddy, Leila},
eprint = {1810.03856},
month = {oct},
title = {{Reconstructing Faces from fMRI Patterns using Deep Generative Neural Networks}},
url = {http://arxiv.org/abs/1810.03856},
year = {2018}
}
@article{Real2018,
abstract = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier— AmoebaNet-A—that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9{\%} top-1 / 96.6{\%} top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
archivePrefix = {arXiv},
arxivId = {1802.01548},
author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
doi = {10.1609/aaai.v33i01.33014780},
eprint = {1802.01548},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
pages = {4780--4789},
title = {{Regularized Evolution for Image Classifier Architecture Search}},
volume = {33},
year = {2019}
}
@inproceedings{Chatfield2014,
abstract = {The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.},
annote = {{\_}eprint: 1405.3531},
archivePrefix = {arXiv},
arxivId = {1405.3531},
author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
booktitle = {BMVC 2014 - Proceedings of the British Machine Vision Conference 2014},
doi = {10.5244/c.28.6},
eprint = {1405.3531},
title = {{Return of the devil in the details: Delving deep into convolutional nets}},
year = {2014}
}
@article{Jenkinson2012,
abstract = {FSL (the FMRIB Software Library) is a comprehensive library of analysis tools for functional, structural and diffusion MRI brain imaging data, written mainly by members of the Analysis Group, FMRIB, Oxford. For this NeuroImage special issue on“20years offMRI”we have beenaskedtowrite about the history, developments and current status of FSL.We also include some descriptions of parts of FSL that are not well covered in the existing literature.We hope that some of this contentmight be of interest to users of FSL, and alsomaybe to newresearch groups considering creating, releasing and supporting newsoftware packages for brain image analysis.},
author = {Jenkinson, Mark and Beckmann, Christian F and Behrens, Timothy E J and Woolrich, Mark W and Smith, Stephen M},
doi = {10.1016/j.neuroimage.2011.09.015},
issn = {1053-8119},
journal = {NeuroImage},
keywords = {FSL,Software},
number = {2},
pages = {782--790},
title = {{Review FSL}},
url = {http://www.sciencedirect.com/science/article/pii/S1053811911010603},
volume = {62},
year = {2012}
}
@misc{Minati2009,
abstract = {This comprehensive, pedagogically-oriented review is aimed at a heterogeneous audience representative of the allied disciplines involved in research and patient care. After a foreword on epidemiology, genetics, and risk factors, the amyloid cascade model is introduced and the main neuropathological hallmarks are discussed. The progression of memory, language, visual processing, executive, attentional, and praxis deficits, and of behavioral symptoms is presented. After a summary on neuropsychological assessment, emerging biomarkers from cerebrospinal fluid assays, magnetic resonance imaging, nuclear medicine, and electrophysiology are discussed. Existing treatments are briefly reviewed, followed by an introduction to emerging disease-modifying therapies such as secretase modulators, inhibitors of Abeta aggregation, immunotherapy, inhibitors of tau protein phosphorylation, and delivery of nerve growth factor. {\textcopyright} 2009 Sage Publications.},
author = {Minati, Ludovico and Edginton, Trudi and {Grazia Bruzzone}, Maria and Giaccone, Giorgio},
booktitle = {American Journal of Alzheimer's Disease and other Dementias},
doi = {10.1177/1533317508328602},
issn = {15333175},
keywords = {Alzheimer's disease,Neuroimaging,Neuropathology,Neuropsychological testing,Pharmacotherapy},
number = {2},
pages = {95--121},
title = {{Reviews: Current concepts in alzheimer's disease: A multidisciplinary review}},
volume = {24},
year = {2009}
}
@inproceedings{Tchapmi2017,
abstract = {3D semantic scene labeling is fundamental to agents operating in the real world. In particular, labeling raw 3D point sets from sensors provides fine-grained semantics. Recent works leverage the capabilities of Neural Networks(NNs), but are limited to coarse voxel predictions and do not explicitly enforce global consistency. We present SEGCloud, an end-To-end framework to obtain 3D point-level segmentation that combines the advantages of NNs, trilinear interpolation(TI) and fully connected Conditional Random Fields (FC-CRF). Coarse voxel predictions from a 3D Fully Convolutional NN are transferred back to the raw 3D points via trilinear interpolation. Then the FC-CRF enforces global consistency and provides fine-grained semantics on the points. We implement the latter as a differentiable Recurrent NN to allow joint optimization. We evaluate the framework on two indoor and two outdoor 3D datasets (NYU V2, S3DIS, KITTI, Semantic3D.net), and show performance comparable or superior to the state-of-The-Art on all datasets.},
annote = {{\_}eprint: 1710.07563},
archivePrefix = {arXiv},
arxivId = {1710.07563},
author = {Tchapmi, Lyne and Choy, Christopher and Armeni, Iro and Gwak, JunYoung and Savarese, Silvio},
booktitle = {Proceedings - 2017 International Conference on 3D Vision, 3DV 2017},
doi = {10.1109/3DV.2017.00067},
eprint = {1710.07563},
isbn = {9781538626108},
keywords = {3D-Conditional-Random-Fields,3D-Convolutional-Neural-Networks,3D-Point-Clouds,3D-Semantic-Segmentation,RGB-D},
pages = {537--547},
title = {{SEGCloud: Semantic segmentation of 3D point clouds}},
url = {http://arxiv.org/abs/1710.07563},
volume = {abs/1710.0},
year = {2018}
}
@article{Naylor2019,
abstract = {The advent of digital pathology provides us with the challenging opportunity to automatically analyze whole slides of diseased tissue in order to derive quantitative profiles that can be used for diagnosis and prognosis tasks. In particular, for the development of interpretable models, the detection and segmentation of cell nuclei is of the utmost importance. In this paper, we describe a new method to automatically segment nuclei from Haematoxylin and Eosin (HE) stained histopathology data with fully convolutional networks. In particular, we address the problem of segmenting touching nuclei by formulating the segmentation problem as a regression task of the distance map. We demonstrate superior performance of this approach as compared to other approaches using Convolutional Neural Networks.},
author = {Naylor, Peter and La{\'{e}}, Marick and Reyal, Fabien and Walter, Thomas},
doi = {10.1109/TMI.2018.2865709},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Cancer research,deep learning,digital pathology,histopathology,nuclei segmentation},
number = {2},
pages = {448--459},
title = {{Segmentation of Nuclei in Histopathology Images by Deep Regression of the Distance Map}},
volume = {38},
year = {2019}
}
@inproceedings{Trullo2017,
abstract = {Cancer is one of the leading causes of death worldwide. Radiotherapy is a standard treatment for this condition and the first step of the radiotherapy process is to identify the target volumes to be targeted and the healthy organs at risk (OAR) to be protected. Unlike previous methods for automatic segmentation of OAR that typically use local information and individually segment each OAR, in this paper, we propose a deep learning framework for the joint segmentation of OAR in CT images of the thorax, specifically the heart, esophagus, trachea and the aorta. Making use of Fully Convolutional Networks (FCN), we present several extensions that improve the performance, including a new architecture that allows to use low level features with high level information, effectively combining local and global information for improving the localization accuracy. Finally, by using Conditional Random Fields (specifically the CRF as Recurrent Neural Network model), we are able to account for relationships between the organs to further improve the segmentation results. Experiments demonstrate competitive performance on a dataset of 30 CT scans.},
author = {Trullo, R and Petitjean, Caroline and Ruan, Su and Dubray, Bernard and Nie, D and Shen, D},
booktitle = {Proceedings - International Symposium on Biomedical Imaging},
doi = {10.1109/ISBI.2017.7950685},
isbn = {9781509011711},
issn = {19458452},
keywords = {CRF,CRFasRNN,CT Segmentation,Fully Convolutional Networks (FCN)},
pages = {1003--1006},
title = {{Segmentation of Organs at Risk in thoracic CT images using a SharpMask architecture and Conditional Random Fields}},
volume = {2017},
year = {2017}
}
@inproceedings{Hackel2017,
abstract = {This paper presents a new 3D point cloud classification benchmark data set with over four billion manually labelled points, meant as input for data-hungry (deep) learning methods. We also discuss first submissions to the benchmark that use deep convolutional neural networks (CNNs) as a work horse, which already show remarkable performance improvements over state-of-the-art. CNNs have become the de-facto standard for many tasks in computer vision and machine learning like semantic segmentation or object detection in images, but have no yet led to a true breakthrough for 3D point cloud labelling tasks due to lack of training data. With the massive data set presented in this paper, we aim at closing this data gap to help unleash the full potential of deep learning methods for 3D labelling tasks. Our semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains 8 semantic classes and covers a wide range of urban outdoor scenes: churches, streets, railroad tracks, squares, villages, soccer fields and castles. We describe our labelling interface and show that our data set provides more dense and complete point clouds with much higher overall number of labelled points compared to those already available to the research community. We further provide baseline method descriptions and comparison between methods submitted to our online system. We hope semantic3D.net will pave the way for deep learning methods in 3D point cloud labelling to learn richer, more general 3D representations, and first submissions after only a few months indicate that this might indeed be the case.},
archivePrefix = {arXiv},
arxivId = {1704.03847},
author = {Hackel, Timo and Savinov, N and Ladicky, L and Wegner, Jan D and Schindler, K and Pollefeys, M},
booktitle = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
doi = {10.5194/isprs-annals-IV-1-W1-91-2017},
eprint = {1704.03847},
issn = {21949050},
number = {1W1},
pages = {91--98},
title = {{SEMANTIC3D.NET: A NEW LARGE-SCALE POINT CLOUD CLASSIFICATION BENCHMARK}},
volume = {4},
year = {2017}
}
@inproceedings{Kipf2016,
abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
annote = {{\_}eprint: 1609.02907},
archivePrefix = {arXiv},
arxivId = {1609.02907},
author = {Kipf, Thomas N and Welling, Max},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1609.02907},
title = {{Semi-supervised classification with graph convolutional networks}},
url = {http://arxiv.org/abs/1609.02907},
volume = {abs/1609.0},
year = {2019}
}
@article{Wang2020,
abstract = {The availability of a large amount of annotated data is critical for many medical image analysis applications, in particular for those relying on deep learning methods which are known to be data-hungry. However, annotated medical data, especially multimodal data, is often scarce and costly to obtain. In this paper, we address the problem of synthesizing multi-parameter magnetic resonance imaging data (i.e. mp-MRI), which typically consists of Apparent Diffusion Coefficient (ADC) and T2-weighted (T2w) images, containing clinically significant (CS) prostate cancer (PCa) via semi-supervised learning and adversarial learning. Specifically, our synthesizer generates mp-MRI data in a sequential manner: first utilizing a decoder to generate an ADC map from a 128-d latent vector, followed by translating the ADC to the T2w image via U-Net. The synthesizer is trained in a semi-supervised manner. In the supervised training process, a limited amount of paired ADC-T2w images and the corresponding ADC encodings are provided and the synthesizer learns the paired relationship by explicitly minimizing the reconstruction losses between synthetic and real images. To avoid overfitting limited ADC encodings, an unlimited amount of random latent vectors and unpaired ADC-T2w Images are utilized in the unsupervised training process for learning the marginal image distributions of real images. To improve the robustness for training the synthesizer, we decompose the difficult task of generating full-size images into several simpler tasks which generate sub-images only. A StitchLayer is then employed to seamlessly fuse sub-images together in an interlaced manner into a full-size image. In addition, to enforce the synthetic images to indeed contain distinguishable CS PCa lesions, we propose to also maximize an auxiliary distance of Jensen-Shannon divergence (JSD) between CS and nonCS images. Experimental results show that our method can effectively synthesize a large variety of mp-MRI images which contain meaningful CS PCa lesions, display a good visual quality and have the correct paired relationship between the two modalities of a pair. Compared to the state-of-the-art methods based on adversarial learning (Liu and Tuzel, 2016; Costa et al., 2017), our method achieves a significant improvement in terms of both visual quality and several popular quantitative evaluation metrics.},
author = {Wang, Zhiwei and Lin, Yi and Cheng, Kwang-Ting (Tim) and Yang, Xin},
doi = {https://doi.org/10.1016/j.media.2019.101565},
issn = {1361-8415},
journal = {Medical Image Analysis},
keywords = {Deep learning,GAN,Generative models,Multimodal image synthesis},
pages = {101565},
title = {{Semi-supervised mp-MRI data synthesis with StitchLayer and auxiliary distance maximization}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841519301057},
volume = {59},
year = {2020}
}
@inproceedings{Hutter2011,
abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach. {\textcopyright} Springer-Verlag Berlin Heidelberg 2011.},
author = {Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-25566-3_40},
isbn = {9783642255656},
issn = {03029743},
pages = {507--523},
title = {{Sequential model-based optimization for general algorithm configuration}},
volume = {6683 LNCS},
year = {2011}
}
@article{Chang2015,
abstract = {We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.},
annote = {{\_}eprint: 1512.03012},
archivePrefix = {arXiv},
arxivId = {1512.03012},
author = {Chang, Angel X and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qi-Xing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},
eprint = {1512.03012},
journal = {CoRR},
title = {{ShapeNet: An Information-Rich 3D Model Repository}},
url = {http://arxiv.org/abs/1512.03012},
volume = {abs/1512.0},
year = {2015}
}
@misc{Zheng2018,
abstract = {In the early days, content-based image retrieval (CBIR) was studied with global features. Since 2003, image retrieval based on local descriptors (de facto SIFT) has been extensively studied for over a decade due to the advantage of SIFT in dealing with image transformations. Recently, image representations based on the convolutional neural network (CNN) have attracted increasing interest in the community and demonstrated impressive performance. Given this time of rapid evolution, this article provides a comprehensive survey of instance retrieval over the last decade. Two broad categories, SIFT-based and CNN-based methods, are presented. For the former, according to the codebook size, we organize the literature into using large/medium-sized/small codebooks. For the latter, we discuss three lines of methods, i.e., using pre-trained or fine-tuned CNN models, and hybrid methods. The first two perform a single-pass of an image to the network, while the last category employs a patch-based feature extraction scheme. This survey presents milestones in modern instance retrieval, reviews a broad selection of previous works in different categories, and provides insights on the connection between SIFT and CNN-based methods. After analyzing and comparing retrieval performance of different categories on several datasets, we discuss promising directions towards generic and specialized instance retrieval.},
archivePrefix = {arXiv},
arxivId = {1608.01807},
author = {Zheng, Liang and Yang, Yi and Tian, Qi},
booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
doi = {10.1109/TPAMI.2017.2709749},
eprint = {1608.01807},
issn = {01628828},
keywords = {Instance retrieval,SIFT,convolutional neural network,literature survey},
month = {may},
number = {5},
pages = {1224--1244},
publisher = {IEEE Computer Society},
title = {{SIFT Meets CNN: A Decade Survey of Instance Retrieval}},
volume = {40},
year = {2018}
}
@article{Ferdouse2011,
abstract = {Noise problems in signals have gained huge attention due to the need of noise-free output signal in numerous communication systems. The principal of adaptive noise cancellation is to acquire an estimation of the unwanted interfering signal and subtract it from the corrupted signal. Noise cancellation operation is controlled adaptively with the target of achieving improved signal to noise ratio. This paper concentrates upon the analysis of adaptive noise canceller using Recursive Least Square (RLS), Fast Transversal Recursive Least Square (FTRLS) and Gradient Adaptive Lattice (GAL) algorithms. The performance analysis of the algorithms is done based on convergence behavior, convergence time, correlation coefficients and signal to noise ratio. After comparing all the simulated results we observed that GAL performs the best in noise cancellation in terms of Correlation Coefficient, SNR and Convergence Time. RLS, FTRLS and GAL were never evaluated and compared before on their performance in noise cancellation in terms of the criteria we considered here.},
annote = {{\_}eprint: 1104.1962},
author = {Ferdouse, Lilatul and Akhter, Nasrin and Nipa, Tamanna Haque and Jaigirdar, Fariha Tasmin},
journal = {CoRR},
keywords = {Adaptive Filter,Computer Science - Other Computer Science,Convergence,DOAJ:Computer Science,DOAJ:Technology and Engineering,Electronic computers. Computer science,FTRLS,GAL,IJCSI,Instruments and machines,Mathematics,Mean Square Error,Noise,Q,QA1-939,QA71-90,QA75.5-76.95,RLS,Science},
title = {{Simulation and Performance Analysis of Adaptive Filtering Algorithms in Noise Cancellation}},
volume = {abs/1104.1},
year = {2011}
}
@article{Wu2016,
annote = {From Duplicate 2 (Single Image 3D Interpreter Network - Wu, Jiajun; Xue, Tianfan; Lim, Joseph J; Tian, Yuandong; Tenenbaum, Joshua B; Torralba, Antonio; Freeman, William T)

{\_}eprint: 1604.08685},
archivePrefix = {arXiv},
arxivId = {1604.08685},
author = {Wu, Jiajun and Xue, Tianfan and Lim, Joseph J and Tian, Yuandong and Tenenbaum, Joshua B and Torralba, Antonio and Freeman, William T},
eprint = {1604.08685},
journal = {CoRR},
title = {{Single Image 3D Interpreter Network}},
url = {http://arxiv.org/abs/1604.08685},
volume = {abs/1604.0},
year = {2016}
}
@article{Tatarchenko2015,
abstract = {We present a convolutional network capable of inferring a 3D representation of a previously unseen object given a single image of this object. Concretely, the network can predict an RGB image and a depth map of the object as seen from an arbitrary view. Several of these depth maps fused together give a full point cloud of the object. The point cloud can in turn be transformed into a surface mesh. The network is trained on renderings of synthetic 3D models of cars and chairs. It successfully deals with objects on cluttered background and generates reasonable predictions for real images of cars.},
annote = {{\_}eprint: 1511.06702},
archivePrefix = {arXiv},
arxivId = {arXiv:1511.06702v1},
author = {Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},
doi = {10.1007/978-3-319-46478-7_20},
eprint = {arXiv:1511.06702v1},
isbn = {9783319464770},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {3D from single image,Convolutional networks,Deep learning},
pages = {322--337},
title = {{Single-view to Multi-view: Reconstructing Unseen Views with a Convolutional Network}},
url = {http://arxiv.org/abs/1511.06702},
volume = {9911 LNCS},
year = {2016}
}
@inproceedings{Brock2017,
abstract = {Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks.},
archivePrefix = {arXiv},
arxivId = {1708.05344},
author = {Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
booktitle = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
eprint = {1708.05344},
title = {{SmaSH: One-shot model architecture search through hypernetworks}},
volume = {abs/1708.0},
year = {2018}
}
@article{Ning2015,
abstract = {Diffusion magnetic resonance imaging (dMRI) is the modality of choice for investigating in-vivo white matter connectivity and neural tissue architecture of the brain. The diffusion-weighted signal in dMRI reflects the diffusivity of water molecules in brain tissue and can be utilized to produce image-based biomarkers for clinical research. Due to the constraints on scanning time, a limited number of measurements can be acquired within a clinically feasible scan time. In order to reconstruct the dMRI signal from a discrete set of measurements, a large number of algorithms have been proposed in recent years in conjunction with varying sampling schemes, i.e., with varying b-values and gradient directions. Thus, it is imperative to compare the performance of these reconstruction methods on a single data set to provide appropriate guidelines to neuroscientists on making an informed decision while designing their acquisition protocols. For this purpose, the SPArse Reconstruction Challenge (SPARC) was held along with the workshop on Computational Diffusion MRI (at MICCAI 2014) to validate the performance of multiple reconstruction methods using data acquired from a physical phantom. A total of 16 reconstruction algorithms (9 teams) participated in this community challenge. The goal was to reconstruct single b-value and/or multiple b-value data from a sparse set of measurements. In particular, the aim was to determine an appropriate acquisition protocol (in terms of the number of measurements, b-values) and the analysis method to use for a neuroimaging study. The challenge did not delve on the accuracy of these methods in estimating model specific measures such as fractional anisotropy (FA) or mean diffusivity, but on the accuracy of these methods to fit the data. This paper presents several quantitative results pertaining to each reconstruction algorithm. The conclusions in this paper provide a valuable guideline for choosing a suitable algorithm and the corresponding data-sampling scheme for clinical neuroscience applications.},
author = {Ning, Lipeng and Laun, Frederik and Gur, Yaniv and DiBella, Edward V.R. and Deslauriers-Gauthier, Samuel and Megherbi, Thinhinane and Ghosh, Aurobrata and Zucchelli, Mauro and Menegaz, Gloria and Fick, Rutger and St-Jean, Samuel and Paquette, Michael and Aranda, Ramon and Descoteaux, Maxime and Deriche, Rachid and O'Donnell, Lauren and Rathi, Yogesh},
doi = {10.1016/j.media.2015.10.012},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Angular error,Diffusion MRI,Normalized mean square error,Physical phantom},
month = {dec},
number = {1},
pages = {316--331},
title = {{Sparse Reconstruction Challenge for diffusion MRI: Validation on a physical phantom to determine which acquisition scheme and analysis method to use?}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841515001541},
volume = {26},
year = {2015}
}
@article{S.Rameshkumar2016,
author = {Rameshkumar, S and Thilak, J Anish Jafrin and Suresh, P and Sathishkumar, S and Subramani, N},
doi = {10.15680/IJIRSET.2016.0512161},
issn = {2319-8753},
journal = {International Journal of Innovative Research in Science Engineering and Technology},
keywords = {filter,image denoising,mri,mse,psnr,rmse,wb},
month = {dec},
number = {12},
pages = {21079--21083},
title = {{Speckle Noise Removal in MRI Scan Image Using WB – Filter}},
volume = {5},
year = {2016}
}
@inproceedings{Rippel2015,
abstract = {Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs). We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality. This representation also enables a new form of stochastic regularization by randomized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling. Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training.},
annote = {{\_}eprint: 1506.03767},
archivePrefix = {arXiv},
arxivId = {1506.03767},
author = {Rippel, Oren and Snoek, Jasper and Adams, Ryan P},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1506.03767},
issn = {10495258},
keywords = {Computer Science - Learning,Statistics - Machine Learning},
pages = {2449--2457},
title = {{Spectral representations for convolutional neural networks}},
volume = {2015-Janua},
year = {2015}
}
@article{Jamaludin2017,
abstract = {The objective of this work is to automatically produce radiological gradings of spinal lumbar MRIs and also localize the predicted pathologies. We show that this can be achieved via a Convolutional Neural Network (CNN) framework that takes intervertebral disc volumes as inputs and is trained only on disc-specific class labels. Our contributions are: (i) a CNN architecture that predicts multiple gradings at once, and we propose variants of the architecture including using 3D convolutions; (ii) showing that this architecture can be trained using a multi-task loss function without requiring segmentation level annotation; and (iii) a localization method that clearly shows pathological regions in the disc volumes. We compare three visualization methods for the localization. The network is applied to a large corpus of MRI T2 sagittal spinal MRIs (using a standard clinical scan protocol) acquired from multiple machines, and is used to automatically compute disk and vertebra gradings for each MRI. These are: Pfirrmann grading, disc narrowing, upper/lower endplate defects, upper/lower marrow changes, spondylolisthesis, and central canal stenosis. We report near human performances across the eight gradings, and also visualize the evidence for these gradings localized on the original scans.},
author = {Jamaludin, Amir and Kadir, Timor and Zisserman, Andrew},
doi = {10.1016/j.media.2017.07.002},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {MRI analysis,Radiological classification,Spinal MRI},
pages = {63--73},
title = {{SpineNet: Automated classification and evidence visualization in spinal MRIs}},
url = {http://www.sciencedirect.com/science/article/pii/S136184151730110X},
volume = {41},
year = {2017}
}
@inproceedings{Su2018,
abstract = {We present a network architecture for processing point clouds that directly operates on a collection of points represented as a sparse set of samples in a high-dimensional lattice. Na{\~{A}}ely applying convolutions on this lattice scales poorly, both in terms of memory and computational cost, as the size of the lattice increases. Instead, our network uses sparse bilateral convolutional layers as building blocks. These layers maintain efficiency by using indexing structures to apply convolutions only on occupied parts of the lattice, and allow flexible specifications of the lattice structure enabling hierarchical and spatially-aware feature learning, as well as joint 2D-3D reasoning. Both point-based and image-based representations can be easily incorporated in a network with such layers and the resulting model can be trained in an end-to-end manner. We present results on 3D segmentation tasks where our approach outperforms existing state-of-the-art techniques.},
annote = {{\_}eprint: 1802.08275},
archivePrefix = {arXiv},
arxivId = {1802.08275},
author = {Su, Hang and Jampani, Varun and Sun, Deqing and Maji, Subhransu and Kalogerakis, Evangelos and Yang, Ming Hsuan and Kautz, Jan},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00268},
eprint = {1802.08275},
isbn = {9781538664209},
issn = {10636919},
pages = {2530--2539},
title = {{SPLATNet: Sparse Lattice Networks for Point Cloud Processing}},
url = {http://arxiv.org/abs/1802.08275},
volume = {abs/1802.0},
year = {2018}
}
@incollection{Theodoridis2015,
abstract = {http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/},
author = {Theodoridis, Sergios},
booktitle = {Machine Learning},
doi = {10.1016/b978-0-12-801522-3.00005-7},
month = {aug},
pages = {161--231},
title = {{Stochastic Gradient Descent}},
url = {https://en.wikipedia.org/wiki/Stochastic{\_}gradient{\_}descent{\#}Extensions{\_}and{\_}variants},
year = {2015}
}
@incollection{Bottou2012,
abstract = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.},
author = {Bottou, L{\'{e}}on},
booktitle = {Neural Networks: Tricks of the Trade - Second Edition},
doi = {10.1007/978-3-642-35289-8_25},
pages = {421--436},
title = {{Stochastic Gradient Descent Tricks}},
url = {https://doi.org/10.1007/978-3-642-35289-8{\_}25},
year = {2012}
}
@article{Vahadane2016a,
abstract = {Staining and scanning of tissue samples for microscopic examination is fraught with undesirable color variations arising from differences in raw materials and manufacturing techniques of stain vendors, staining protocols of labs, and color responses of digital scanners. When comparing tissue samples, color normalization and stain separation of the tissue images can be helpful for both pathologists and software. Techniques that are used for natural images fail to utilize structural properties of stained tissue samples and produce undesirable color distortions. The stain concentration cannot be negative. Tissue samples are stained with only a few stains and most tissue regions are characterized by at most one effective stain. We model these physical phenomena that define the tissue structure by first decomposing images in an unsupervised manner into stain density maps that are sparse and non-negative. For a given image, we combine its stain density maps with stain color basis of a pathologist-preferred target image, thus altering only its color while preserving its structure described by the maps. Stain density correlation with ground truth and preference by pathologists were higher for images normalized using our method when compared to other alternatives. We also propose a computationally faster extension of this technique for large whole-slide images that selects an appropriate patch sample instead of using the entire image to compute the stain color basis.},
author = {Vahadane, Abhishek and Peng, Tingying and Sethi, Amit and Albarqouni, Shadi and Wang, Lichao and Baust, Maximilian and Steiger, Katja and Schlitter, Anna Melissa and Esposito, Irene and Navab, Nassir},
doi = {10.1109/TMI.2016.2529665},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Color normalization,Unsupervised stain separation,histopathological images,non-negative matrix factorization,sparse regularization},
number = {8},
pages = {1962--1971},
title = {{Structure-Preserving Color Normalization and Sparse Stain Separation for Histological Images}},
volume = {35},
year = {2016}
}
@article{Cortes1995,
abstract = {In this paper, the optimal margin algorithm is generalized$\backslash$nto non-separable problems by the introduction of slack$\backslash$nvariables in the statement of the optimization problem.},
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1007/bf00994018},
issn = {0885-6125},
journal = {Machine Learning},
number = {3},
pages = {273--297},
title = {{Support-vector networks}},
volume = {20},
year = {1995}
}
@article{Corneanu2016,
abstract = {Facial expressions are an important way through which humans interact socially. Building a system capable of automatically recognizing facial expressions from images and video has been an intense field of study in recent years. Interpreting such expressions remains challenging and much research is needed about the way they relate to human affect. This paper presents a general overview of automatic RGB, 3D, thermal and multimodal facial expression analysis. We define a new taxonomy for the field, encompassing all steps from face detection to facial expression recognition, and describe and classify the state of the art methods accordingly. We also present the important datasets and the bench-marking of most influential methods. We conclude with a general discussion about trends, important questions and future lines of research.},
archivePrefix = {arXiv},
arxivId = {1606.03237},
author = {Corneanu, Ciprian Adrian and Sim{\'{o}}n, Marc Oliu and Cohn, Jeffrey F. and Guerrero, Sergio Escalera},
doi = {10.1109/TPAMI.2016.2515606},
eprint = {1606.03237},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {3D,Facial expression,RGB,affect,emotion recognition,multimodal,thermal},
month = {aug},
number = {8},
pages = {1548--1568},
publisher = {IEEE Computer Society},
title = {{Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-Related Applications}},
volume = {38},
year = {2016}
}
@inproceedings{Yi2016,
abstract = {In this paper, we study the problem of semantic annotation on 3D models that are represented as shape graphs. A functional view is taken to represent localized information on graphs, so that annotations such as part segment or keypoint are nothing but 0-1 indicator vertex functions. Compared with images that are 2D grids, shape graphs are irregular and non-isomorphic data structures. To enable the prediction of vertex functions on them by convolutional neural networks, we resort to spectral CNN method that enables weight sharing by parametrizing kernels in the spectral domain spanned by graph Laplacian eigenbases. Under this setting, our network, named SyncSpecCNN, strives to overcome two key challenges: how to share coefficients and conduct multi-scale analysis in different parts of the graph for a single shape, and how to share information across related but different shapes that may be represented by very different graphs. Towards these goals, we introduce a spectral parametrization of dilated convolutional kernels and a spectral transformer network. Experimentally we tested SyncSpecCNN on various tasks, including 3D shape part segmentation and keypoint prediction. State-of-the-art performance has been achieved on all benchmark datasets.},
annote = {{\_}eprint: 1612.00606},
archivePrefix = {arXiv},
arxivId = {1612.00606},
author = {Yi, Li and Su, Hao and Guo, Xingwen and Guibas, Leonidas},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.697},
eprint = {1612.00606},
isbn = {9781538604571},
pages = {6584--6592},
title = {{SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation}},
url = {http://arxiv.org/abs/1612.00606},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Abadi2016,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, generalpurpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that Tensor-Flow achieves for several real-world applications.},
annote = {{\_}eprint: 1605.08695},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek Gordon and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
booktitle = {Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016},
eprint = {1605.08695},
isbn = {9781931971331},
pages = {265--283},
title = {{TensorFlow: A system for large-scale machine learning}},
url = {http://arxiv.org/abs/1605.08695},
volume = {abs/1605.0},
year = {2016}
}
@article{Vallet2015,
abstract = {The objective of the TerraMobilita/iQmulus 3D urban analysis benchmark is to evaluate the current state of the art in urban scene analysis from mobile laser scanning (MLS) at large scale. A very detailed semantic tree for urban scenes is proposed. We call analysis the capacity of a method to separate the points of the scene into these categories (classification), and to separate the different objects of the same type for object classes (detection). A very large ground truth is produced manually in two steps using advanced editing tools developed especially for this benchmark. Based on this ground truth, the benchmark aims at evaluating the classification, detection and segmentation quality of the submitted results.},
author = {Vallet, Bruno and Br{\'{e}}dif, Mathieu and Serna, Andres and Marcotegui, Beatriz and Paparoditis, Nicolas},
doi = {10.1016/j.cag.2015.03.004},
issn = {00978493},
journal = {Computers and Graphics (Pergamon)},
keywords = {Benchmark,Classification,Laser scanning,Mobile mapping,Segmentation,Urban scene},
pages = {126--133},
title = {{TerraMobilita/iQmulus urban point cloud analysis benchmark}},
volume = {49},
year = {2015}
}
@article{Ye2015,
abstract = {This paper analyzes, compares, and contrasts technical challenges, methods, and the performance of text detection and recognition research in color imagery. It summarizes the fundamental problems and enumerates factors that should be considered when addressing these problems. Existing techniques are categorized as either stepwise or integrated and sub-problems are highlighted including text localization, verification, segmentation and recognition. Special issues associated with the enhancement of degraded text and the processing of video text, multi-oriented, perspectively distorted and multilingual text are also addressed. The categories and sub-categories of text are illustrated, benchmark datasets are enumerated, and the performance of the most representative approaches is compared. This review provides a fundamental comparison and analysis of the remaining problems in the field.},
author = {Ye, Qixiang and Doermann, David},
doi = {10.1109/TPAMI.2014.2366765},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Survey,Text Detection,Text Localization,Text Recognition},
month = {jul},
number = {7},
pages = {1480--1500},
publisher = {IEEE Computer Society},
title = {{Text Detection and Recognition in Imagery: A Survey}},
volume = {37},
year = {2015}
}
@misc{Mueller2005,
abstract = {With increasing life expectancy in developed countries, the incidence of Alzheimer's disease (AD) and its socioeconomic impact are growing. Increasing knowledge of the mechanisms of AD facilitates the development of treatment strategies aimed at slowing down or preventing neuronal death. AD treatment trials using clinical outcome measures require long observation times and large patient samples. There is increasing evidence that neuroimaging and cerebrospinal fluid and blood biomarkers may provide information that may reduce sample sizes and observation periods. The Alzheimer's Disease Neuroimaging Initiative will help identify clinical, neuroimaging, and biomarker outcome measures that provide the highest power for measurement of longitudinal changes and for prediction of transitions. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {Mueller, Susanne G and Weiner, Michael W and Thal, Leon J and Petersen, Ronald Carl and Jack, Clifford and Jagust, William and Trojanowski, John Q and Toga, Arthur W and Beckett, Laurel},
booktitle = {Neuroimaging Clinics of North America},
doi = {10.1016/j.nic.2005.09.008},
issn = {10525149},
number = {4},
pages = {869--877},
title = {{The Alzheimer's disease neuroimaging initiative}},
volume = {15},
year = {2005}
}
@article{Clark2013,
abstract = {The National Institutes of Health have placed significant emphasis on sharing of research data to support secondary research. Investigators have been encouraged to publish their clinical and imaging data as part of fulfilling their grant obligations. Realizing it was not sufficient to merely ask investigators to publish their collection of imaging and clinical data, the National Cancer Institute (NCI) created the open source National Biomedical Image Archive software package as a mechanism for centralized hosting of cancer related imaging. NCI has contracted with Washington University in Saint Louis to create The Cancer Imaging Archive (TCIA) - an open-source, open-access information resource to support research, development, and educational initiatives utilizing advanced medical imaging of cancer. In its first year of operation, TCIA accumulated 23 collections (3.3 million images). Operating and maintaining a high-availability image archive is a complex challenge involving varied archive-specific resources and driven by the needs of both image submitters and image consumers. Quality archives of any type (traditional library, PubMed, refereed journals) require management and customer service. This paper describes the management tasks and user support model for TCIA. {\textcopyright} 2013 Society for Imaging Informatics in Medicine.},
author = {Clark, Kenneth and Vendt, Bruce and Smith, Kirk and Freymann, John and Kirby, Justin and Koppel, Paul and Moore, Stephen and Phillips, Stanley and Maffitt, David and Pringle, Michael and Tarbox, Lawrence and Prior, Fred},
doi = {10.1007/s10278-013-9622-7},
issn = {08971889},
journal = {Journal of Digital Imaging},
keywords = {Biomedical image analysis,Cancer detection,Cancer imaging,Image archive,NBIA,TCIA},
number = {6},
pages = {1045--1057},
pmid = {23884657},
title = {{The cancer imaging archive (TCIA): Maintaining and operating a public information repository}},
url = {https://doi.org/10.1007/s10278-013-9622-7},
volume = {26},
year = {2013}
}
@article{Clark2013,
abstract = {The National Institutes of Health have placed significant emphasis on sharing of research data to support secondary research. Investigators have been encouraged to publish their clinical and imaging data as part of fulfilling their grant obligations. Realizing it was not sufficient to merely ask investigators to publish their collection of imaging and clinical data, the National Cancer Institute (NCI) created the open source National Biomedical Image Archive software package as a mechanism for centralized hosting of cancer related imaging. NCI has contracted with Washington University in Saint Louis to create The Cancer Imaging Archive (TCIA) - an open-source, open-access information resource to support research, development, and educational initiatives utilizing advanced medical imaging of cancer. In its first year of operation, TCIA accumulated 23 collections (3.3 million images). Operating and maintaining a high-availability image archive is a complex challenge involving varied archive-specific resources and driven by the needs of both image submitters and image consumers. Quality archives of any type (traditional library, PubMed, refereed journals) require management and customer service. This paper describes the management tasks and user support model for TCIA. {\textcopyright} 2013 Society for Imaging Informatics in Medicine.},
author = {Clark, Kenneth and Vendt, Bruce and Smith, Kirk and Freymann, John and Kirby, Justin and Koppel, Paul and Moore, Stephen and Phillips, Stanley and Maffitt, David and Pringle, Michael and Tarbox, Lawrence and Prior, Fred},
doi = {10.1007/s10278-013-9622-7},
issn = {08971889},
journal = {Journal of Digital Imaging},
keywords = {Biomedical image analysis,Cancer detection,Cancer imaging,Image archive,NBIA,TCIA},
month = {dec},
number = {6},
pages = {1045--1057},
pmid = {23884657},
title = {{The cancer imaging archive (TCIA): Maintaining and operating a public information repository}},
url = {http://link.springer.com/10.1007/s10278-013-9622-7},
volume = {26},
year = {2013}
}
@article{Lowekamp2013,
abstract = {SimpleITK is a new interface to the Insight Segmentation and Registration Toolkit (ITK) designed to facilitate rapid prototyping, education and scientific activities via high level programming languages. ITK is a templated C++ library of image processing algorithms and frameworks for biomedical and other applications, and it was designed to be generic, flexible and extensible. Initially, ITK provided a direct wrapping interface to languages such as Python and Tcl through the WrapITK system. Unlike WrapITK, which exposed ITK's complex templated interface, SimpleITK was designed to provide an easy to use and simplified interface to ITK's algorithms. It includes procedural methods, hides ITK's demand driven pipeline, and provides a template-less layer. Also SimpleITK provides practical conveniences such as binary distribution packages and overloaded operators. Our user-friendly design goals dictated a departure from the direct interface wrapping approach of WrapITK, toward a new facade class structure that only exposes the required functionality, hiding ITK's extensive template use. Internally SimpleITK utilizes a manual description of each filter with code-generation and advanced C++ meta-programming to provide the higher-level interface, bringing the capabilities of ITK to a wider audience. SimpleITK is licensed as open source software library under the Apache License Version 2.0 and more information about downloading it can be found at http://www.simpleitk.org. {\textcopyright} 2013 Lowek amp, Chen, Ib{\'{a}}{\~{n}}ez and Blezek.},
author = {Lowekamp, Bradley C. and Chen, David T. and Ib{\'{a}}{\~{n}}ez, Luis and Blezek, Daniel},
doi = {10.3389/fninf.2013.00045},
issn = {16625196},
journal = {Frontiers in Neuroinformatics},
keywords = {Image processing and analysis,Image processing software,Insight toolkit,Segmentation,Software design,Software development},
number = {DEC},
pages = {45},
title = {{The design of simpleITK}},
volume = {7},
year = {2013}
}
@misc{LeCun1998b,
address = {Cambridge, MA, USA},
annote = {Section: Convolutional Networks for Images, Speech, and Time Series},
author = {Andrew, Alex M.},
booktitle = {Kybernetes},
doi = {10.1108/k.1999.28.9.1084.1},
editor = {Arbib, Michael A},
isbn = {0-262-51102-9},
issn = {0368492X},
keywords = {Artificial intelligence,Brain,Cybernetics,Neural networks,Publication},
number = {9},
pages = {1084--1094},
publisher = {MIT Press},
title = {{The Handbook of Brain Theory and Neural Networks}},
url = {http://dl.acm.org/citation.cfm?id=303568.303704},
volume = {28},
year = {1999}
}
@misc{Ibanez2003,
abstract = {Everything you need to install, use, and extend the Insight Segmentation and Registration Toolikit ITK. Includes detailed examples, installation procedures, and system overview for ITK version 2.4. (The included examples are taken directly from the ITK source code repository and are designed to demonstrate the essential features of the software.) The book comes with a CD-ROM that contains a complete hyperlinked version of the book plus ITK source code, data, Windows binaries, and extensive class documentation. Also includes CMake binaries for managing the ITK build process on a variety of compiler and operating system configurations.},
author = {Ibanez, L and Schroeder, W and Ng, L and Cates, J},
booktitle = {The ITK Software Guide},
doi = {1�930934-15�7},
edition = {First},
isbn = {1930934157},
issn = {10445323},
number = {May},
pages = {804},
pmid = {1000070720},
publisher = {Kitware, Inc.},
title = {{The ITK Software Guide}},
url = {http://www.itk.org/ItkSoftwareGuide.pdf},
volume = {Second},
year = {2005}
}
@article{Heller2019,
abstract = {The morphometry of a kidney tumor revealed by contrast-enhanced Computed Tomography (CT) imaging is an important factor in clinical decision making surrounding the lesion's diagnosis and treatment. Quantitative study of the relationship between kidney tumor morphology and clinical outcomes is difficult due to data scarcity and the laborious nature of manually quantifying imaging predictors. Automatic semantic segmentation of kidneys and kidney tumors is a promising tool towards automatically quantifying a wide array of morphometric features, but no sizeable annotated dataset is currently available to train models for this task. We present the KiTS19 challenge dataset: A collection of multi-phase CT imaging, segmentation masks, and comprehensive clinical outcomes for 300 patients who underwent nephrectomy for kidney tumors at our center between 2010 and 2018. 210 (70{\%}) of these patients were selected at random as the training set for the 2019 MICCAI KiTS Kidney Tumor Segmentation Challenge and have been released publicly. With the presence of clinical context and surgical outcomes, this data can serve not only for benchmarking semantic segmentation models, but also for developing and studying biomarkers which make use of the imaging and semantic segmentation masks.},
annote = {{\_}eprint: 1904.00445},
archivePrefix = {arXiv},
arxivId = {1904.00445},
author = {Heller, Nicholas and Sathianathen, Niranjan and Kalapara, Arveen and Walczak, Edward and Moore, Keenan and Kaluzniak, Heather and Rosenberg, Joel and Blake, Paul and Rengel, Zachary and Oestreich, Makinna and Dean, Joshua and Tradewell, Michael and Shah, Aneri and Tejpaul, Resha and Edgerton, Zachary and Peterson, Matthew and Raza, Shaneabbas and Regmi, Subodh and Papanikolopoulos, Nikolaos and Weight, Christopher},
eprint = {1904.00445},
title = {{The KiTS19 Challenge Data: 300 Kidney Tumor Cases with Clinical Context, CT Semantic Segmentations, and Surgical Outcomes}},
url = {http://arxiv.org/abs/1904.00445},
year = {2019}
}
@article{Bilic2019,
abstract = {In this work, we report the set-up and results of the Liver Tumor Segmentation Benchmark (LITS) organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI) 2016 and International Conference On Medical Image Computing Computer Assisted Intervention (MICCAI) 2017. Twenty four valid state-of-the-art liver and liver tumor segmentation algorithms were applied to a set of 131 computed tomography (CT) volumes with different types of tumor contrast levels (hyper-/hypo-intense), abnormalities in tissues (metastasectomie) size and varying amount of lesions. The submitted algorithms have been tested on 70 undisclosed volumes. The dataset is created in collaboration with seven hospitals and research institutions and manually reviewed by independent three radiologists. We found that not a single algorithm performed best for liver and tumors. The best liver segmentation algorithm achieved a Dice score of 0.96(MICCAI) whereas for tumor segmentation the best algorithm evaluated at 0.67(ISBI) and 0.70(MICCAI). The LITS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
annote = {{\_}eprint: 1901.04056},
archivePrefix = {arXiv},
arxivId = {1901.04056},
author = {Bilic, Patrick and Christ, Patrick Ferdinand and Vorontsov, Eugene and Chlebus, Grzegorz and Chen, Hao and Dou, Qi and Fu, Chi-Wing and Han, Xiao and Heng, Pheng-Ann and Hesser, J{\"{u}}rgen and Kadoury, Samuel and Konopczynski, Tomasz and Le, Miao and Li, Chunming and Li, Xiaomeng and Lipkov{\`{a}}, Jana and Lowengrub, John and Meine, Hans and Moltz, Jan Hendrik and Pal, Chris and Piraud, Marie and Qi, Xiaojuan and Qi, Jin and Rempfler, Markus and Roth, Karsten and Schenk, Andrea and Sekuboyina, Anjany and Vorontsov, Eugene and Zhou, Ping and H{\"{u}}lsemeyer, Christian and Beetz, Marcel and Ettlinger, Florian and Gruen, Felix and Kaissis, Georgios and Loh{\"{o}}fer, Fabian and Braren, Rickmer and Holch, Julian and Hofmann, Felix and Sommer, Wieland and Heinemann, Volker and Jacobs, Colin and Mamani, Gabriel Efrain Humpire and van Ginneken, Bram and Chartrand, Gabriel and Tang, An and Drozdzal, Michal and Ben-Cohen, Avi and Klang, Eyal and Amitai, Marianne Marianne and Konen, Eli and Greenspan, Hayit and Moreau, Johan and Hostettler, Alexandre and Soler, Luc and Vivanti, Refael and Szeskin, Adi and Lev-Cohain, Naama and Sosna, Jacob and Joskowicz, Leo and Menze, Bjoern H},
eprint = {1901.04056},
journal = {CoRR},
title = {{The Liver Tumor Segmentation Benchmark (LiTS)}},
url = {http://arxiv.org/abs/1901.04056},
volume = {abs/1901.0},
year = {2019}
}
@inproceedings{Berman2017,
abstract = {The Jaccard index, also referred to as the intersection-over-union score, is commonly employed in the evaluation of image segmentation results given its perceptual qualities, scale invariance - which lends appropriate relevance to small objects, and appropriate counting of false negatives, in comparison to per-pixel losses. We present a method for direct optimization of the mean intersection-over-union loss in neural networks, in the context of semantic image segmentation, based on the convex Lov{\~{A}}¡sz extension of submodular losses. The loss is shown to perform better with respect to the Jaccard index measure than the traditionally used cross-entropy loss. We show quantitative and qualitative differences between optimizing the Jaccard index per image versus optimizing the Jaccard index taken over an entire dataset. We evaluate the impact of our method in a semantic segmentation pipeline and show substantially improved intersection-over-union segmentation scores on the Pascal VOC and Cityscapes datasets using state-of-the-art deep learning segmentation architectures.},
annote = {{\_}eprint: 1705.08790},
archivePrefix = {arXiv},
arxivId = {1705.08790},
author = {Berman, Maxim and Triki, Amal Rannen and Blaschko, Matthew B},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00464},
eprint = {1705.08790},
isbn = {9781538664209},
issn = {10636919},
pages = {4413--4421},
title = {{The Lovasz-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks}},
year = {2018}
}
@article{VanDerWalt2011,
abstract = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts. {\textcopyright} 2011 IEEE.},
archivePrefix = {arXiv},
arxivId = {1102.1523},
author = {{Van Der Walt}, St{\'{e}}fan and Colbert, S Chris and Varoquaux, Ga{\"{e}}l},
doi = {10.1109/MCSE.2011.37},
eprint = {1102.1523},
issn = {15219615},
journal = {Computing in Science and Engineering},
keywords = {NumPy,Python,numerical computations,programming libraries,scientific programming},
month = {mar},
number = {2},
pages = {22--30},
title = {{The NumPy array: A structure for efficient numerical computation}},
volume = {13},
year = {2011}
}
@article{Maloney2010,
author = {Maloney, John and Resnick, Mitchel and Rusk, Natalie and Silverman, Brian and Eastmond, Evelyn},
doi = {10.1145/1868358.1868363},
journal = {ACM Transactions on Computing Education (TOCE)},
pages = {16},
title = {{The Scratch Programming Language and Environment}},
volume = {10},
year = {2010}
}
@article{Zela2018,
abstract = {While existing work on neural architecture search (NAS) tunes hyperparameters in a separate post-processing step, we demonstrate that architectural choices and other hyperparameter settings interact in a way that can render this separation suboptimal. Likewise, we demonstrate that the common practice of using very few epochs during the main NAS and much larger numbers of epochs during a post-processing step is inefficient due to little correlation in the relative rankings for these two training regimes. To combat both of these problems, we propose to use a recent combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search.},
archivePrefix = {arXiv},
arxivId = {1807.06906},
author = {Zela, Arber and Klein, Aaron and Falkner, Stefan and Hutter, Frank},
eprint = {1807.06906},
journal = {ArXiv},
title = {{Towards Automated Deep Learning: Efficient Joint Neural Architecture and Hyperparameter Search}},
url = {http://arxiv.org/abs/1807.06906},
volume = {abs/1807.0},
year = {2018}
}
@incollection{Mendoza2016,
abstract = {Recent advances in AutoML have led to automated tools that can compete with machine learning experts on supervised learning tasks. However, current AutoML tools do not yet support modern neural networks effectively. In this work, we present a first version of Auto-Net, which provides automatically-tuned feed-forward neural networks without any human intervention. We report results on datasets from the recent AutoML challenge showing that ensembling Auto-Net with Auto-sklearn can perform better than either alone and report the first results on winning competition datasets against human experts with automatically-tuned neural networks.},
author = {Mendoza, Hector and Klein, Aaron and Feurer, Matthias and Springenberg, Jost Tobias and Urban, Matthias and Burkart, Michael and Dippel, Maximilian and Lindauer, Marius and Hutter, Frank},
booktitle = {AutoML@ICML},
doi = {10.1007/978-3-030-05318-5_7},
pages = {135--149},
title = {{Towards Automatically-Tuned Deep Neural Networks}},
year = {2019}
}
@article{Cote2013,
abstract = {We have developed the Tractometer: an online evaluation and validation system for tractography processing pipelines. One can now evaluate the results of more than 57,000 fiber tracking outputs using different acquisition settings (b-value, averaging), different local estimation techniques (tensor, q-ball, spherical deconvolution) and different tracking parameters (masking, seeding, maximum curvature, step size). At this stage, the system is solely based on a revised FiberCup analysis, but we hope that the community will get involved and provide us with new phantoms, new algorithms, third party libraries and new geometrical metrics, to name a few. We believe that the new connectivity analysis and tractography characteristics proposed can highlight limits of the algorithms and contribute in solving open questions in fiber tracking: from raw data to connectivity analysis. Overall, we show that (i) averaging improves quality of tractography, (ii) sharp angular ODF profiles helps tractography, (iii) seeding and multi-seeding has a large impact on tractography outputs and must be used with care, and (iv) deterministic tractography produces less invalid tracts which leads to better connectivity results than probabilistic tractography. {\textcopyright} 2013 Elsevier B.V.},
author = {C{\^{o}}t{\'{e}}, Marc Alexandre and Girard, Gabriel and Bor{\'{e}}, Arnaud and Garyfallidis, Eleftherios and Houde, Jean Christophe and Descoteaux, Maxime},
doi = {10.1016/j.media.2013.03.009},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Connectivity analysis,Diffusion MRI,Tractography,Validation},
month = {oct},
number = {7},
pages = {844--857},
pmid = {23706753},
title = {{Tractometer: Towards validation of tractography pipelines}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23706753},
volume = {17},
year = {2013}
}
@inproceedings{Boser1992,
abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
author = {Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
booktitle = {Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory},
doi = {10.1145/130385.130401},
isbn = {089791497X},
pages = {144--152},
publisher = {ACM Press},
title = {{Training algorithm for optimal margin classifiers}},
year = {1992}
}
@article{Mogadala2019,
abstract = {Integration of vision and language tasks has seen a significant growth in the recent times due to surge of interest from multi-disciplinary communities such as deep learning, computer vision, and natural language processing. In this survey, we focus on ten different vision and language integration tasks in terms of their problem formulation, methods, existing datasets, evaluation measures, and comparison of results achieved with the corresponding state-of-the-art methods. This goes beyond earlier surveys which are either task-specific or concentrate only on one type of visual content i.e., image or video. We then conclude the survey by discussing some possible future directions for integration of vision and language research.},
archivePrefix = {arXiv},
arxivId = {1907.09358},
author = {Mogadala, Aditya and Kalimuthu, Marimuthu and Klakow, Dietrich},
eprint = {1907.09358},
month = {jul},
title = {{Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods}},
url = {http://arxiv.org/abs/1907.09358},
year = {2019}
}
@article{Kuo2016,
abstract = {This work attempts to address two fundamental questions about the structure of the convolutional neural networks (CNN): (1) why a nonlinear activation function is essential at the filter output of all intermediate layers? (2) what is the advantage of the two-layer cascade system over the one-layer system? A mathematical model called the “REctified-COrrelations on a Sphere” (RECOS) is proposed to answer these two questions. After the CNN training process, the converged filter weights define a set of anchor vectors in the RECOS model. Anchor vectors represent the frequently occurring patterns (or the spectral components). The necessity of rectification is explained using the RECOS model. Then, the behavior of a two-layer RECOS system is analyzed and compared with its one-layer counterpart. The LeNet-5 and the MNIST dataset are used to illustrate discussion points. Finally, the RECOS model is generalized to a multilayer system with the AlexNet as an example.},
annote = {{\_}eprint: 1609.04112},
archivePrefix = {arXiv},
arxivId = {1609.04112},
author = {Kuo, C.-C. C.Jay},
doi = {10.1016/j.jvcir.2016.11.003},
eprint = {1609.04112},
issn = {10959076},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Convolutional neural network (CNN),MNIST dataset,Nonlinear activation,RECOS model,Rectified linear unit (ReLU)},
pages = {406--413},
title = {{Understanding convolutional neural networks with a mathematical model}},
url = {http://arxiv.org/abs/1609.04112},
volume = {41},
year = {2016}
}
@article{Zhanga,
author = {Zhang, Liang and Fan, Zhonghao and Li, Yuehan and Zhu, Guangming and Li, Ping and Lu, Xiaoyuan and Shen, Peiyi and Afaq, Syed},
title = {{U-net based analysis of MRI for Alzheimer ' s disease diagnosis}},
year = {2019}
}
@inproceedings{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
annote = {{\_}eprint: 1505.04597},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
isbn = {9783319245737},
issn = {16113349},
pages = {234--241},
title = {{U-net: Convolutional networks for biomedical image segmentation}},
volume = {9351},
year = {2015}
}
@unpublished{Zhang,
author = {Zhang, Liang and Kong, Xiangwen},
title = {unknow}
}
@inproceedings{Boulch2017,
abstract = {In this work, we describe a new, general, and efficient method for unstructured point cloud labeling. As the question of efficiently using deep Convolutional Neural Networks (CNNs) on 3D data is still a pending issue, we propose a framework which applies CNNs on multiple 2D image views (or snapshots) of the point cloud. The approach consists in three core ideas. (i) We pick many suitable snapshots of the point cloud. We generate two types of images: a Red-Green-Blue (RGB) view and a depth composite view containing geometric features. (ii) We then perform a pixel-wise labeling of each pair of 2D snapshots using fully convolutional networks. Different architectures are tested to achieve a profitable fusion of our heterogeneous inputs. (iii) Finally, we perform fast back-projection of the label predictions in the 3D space using efficient buffering to label every 3D point. Experiments show that our method is suitable for various types of point clouds such as Lidar or photogrammetric data.},
annote = {ISSN: 1997-0471},
author = {Boulch, Alexandre and {Le Saux}, B. and Audebert, Nicolas},
booktitle = {Eurographics Workshop on 3D Object Retrieval, EG 3DOR},
doi = {10.2312/3dor.20171047},
editor = {Pratikakis, Ioannis and Dupont, Florent and Ovsjanikov, Maks},
isbn = {9783038680307},
issn = {19970471},
pages = {17--24},
publisher = {The Eurographics Association},
title = {{Unstructured point cloud semantic labeling using deep segmentation networks}},
volume = {2017-April},
year = {2017}
}
@article{Setio2016,
abstract = {Automatic detection of pulmonary nodules in thoracic computed tomography (CT) scans has been an active area of research for the last two decades. However, there have only been few studies that provide a comparative performance evaluation of different systems on a common database. We have therefore set up the LUNA16 challenge, an objective evaluation framework for automatic nodule detection algorithms using the largest publicly available reference database of chest CT scans, the LIDC-IDRI data set. In LUNA16, participants develop their algorithm and upload their predictions on 888 CT scans in one of the two tracks: 1) the complete nodule detection track where a complete CAD system should be developed, or 2) the false positive reduction track where a provided set of nodule candidates should be classified. This paper describes the setup of LUNA16 and presents the results of the challenge so far. Moreover, the impact of combining individual systems on the detection performance was also investigated. It was observed that the leading solutions employed convolutional networks and used the provided set of nodule candidates. The combination of these solutions achieved an excellent sensitivity of over 95{\%} at fewer than 1.0 false positives per scan. This highlights the potential of combining algorithms to improve the detection performance. Our observer study with four expert readers has shown that the best system detects nodules that were missed by expert readers who originally annotated the LIDC-IDRI data. We released this set of additional nodules for further development of CAD systems.},
archivePrefix = {arXiv},
arxivId = {1612.08012},
author = {Setio, Arnaud Arindra Adiyoso and Traverso, Alberto and de Bel, Thomas and Berens, Moira S.N. and van den Bogaard, Cas and Cerello, Piergiorgio and Chen, Hao and Dou, Qi and Fantacci, Maria Evelina and Geurts, Bram and van der Gugten, Robbert and Heng, Pheng Ann and Jansen, Bart and de Kaste, Michael M.J. and Kotov, Valentin and Lin, Jack Yu Hung and Manders, Jeroen T.M.C. and S{\'{o}}{\~{n}}ora-Mengana, Alexander and Garc{\'{i}}a-Naranjo, Juan Carlos and Papavasileiou, Evgenia and Prokop, Mathias and Saletta, Marco and Schaefer-Prokop, Cornelia M. and Scholten, Ernst T. and Scholten, Luuk and Snoeren, Miranda M. and Torres, Ernesto Lopez and Vandemeulebroucke, Jef and Walasek, Nicole and Zuidhof, Guido C.A. and van Ginneken, Bram and Jacobs, Colin},
doi = {10.1016/j.media.2017.06.015},
eprint = {1612.08012},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Computed tomography,Computer-aided detection,Convolutional networks,Deep learning,Medical image challenges,Pulmonary nodules},
pages = {1--13},
pmid = {28732268},
title = {{Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: The LUNA16 challenge}},
volume = {42},
year = {2017}
}
@inproceedings{Sharma2016,
abstract = {With the advent of affordable depth sensors, 3D capture becomes more and more ubiquitous and already has made its way into commercial products. Yet, capturing the geometry or complete shapes of everyday objects using scanning devices (e.g. Kinect) still comes with several challenges that result in noise or even incomplete shapes. Recent success in deep learning has shown how to learn complex shape distributions in a data-driven way from large scale 3D CAD Model collections and to utilize them for 3D processing on volumetric representations and thereby circumventing problems of topology and tessellation. Prior work has shown encouraging results on problems ranging from shape completion to recognition.We provide an analysis of such approaches and discover that training as well as the resulting representation are strongly and unnecessarily tied to the notion of object labels. Thus, we propose a full convolutional volumetric auto encoder that learns volumetric representation from noisy data by estimating the voxel occupancy grids. The proposed method outperforms prior work on challenging tasks like denoising and shape completion. We also show that the obtained deep embedding gives competitive performance when used for classification and promising results for shape interpolation.},
annote = {{\_}eprint: 1604.03755},
archivePrefix = {arXiv},
arxivId = {1604.03755},
author = {Sharma, Abhishek and Grau, Oliver and Fritz, Mario},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-49409-8_20},
eprint = {1604.03755},
isbn = {9783319494081},
issn = {16113349},
keywords = {3D deep learning,Denoising auto-encoder,Shape blending,Shape completion},
pages = {236--250},
title = {{VConv-DAE: Deep volumetric shape learning without object labels}},
url = {http://arxiv.org/abs/1604.03755},
volume = {9915 LNCS},
year = {2016}
}
@inproceedings{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
annote = {{\_}eprint: 1409.1556},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1409.1556},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2015}
}
@article{Geiger2013,
abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide. {\textcopyright} The Author(s) 2013.},
author = {Geiger, Andreas and Lenz, Philip and Stiller, Christoph and Urtasun, Raquel},
doi = {10.1177/0278364913491297},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Dataset,GPS,KITTI,SLAM,autonomous driving,benchmarks,cameras,computer vision,field robotics,laser,mobile robotics,object detection,optical flow,stereo,tracking},
number = {11},
pages = {1231--1237},
title = {{Vision meets robotics: The KITTI dataset}},
volume = {32},
year = {2013}
}
@article{Hohman2019,
abstract = {Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W's and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.},
archivePrefix = {arXiv},
arxivId = {1801.06889},
author = {Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
doi = {10.1109/TVCG.2018.2843369},
eprint = {1801.06889},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Deep learning,information visualization,neural networks,visual analytics},
number = {8},
pages = {2674--2693},
title = {{Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841519300829},
volume = {25},
year = {2019}
}
@misc{Zhang2018,
abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
annote = {{\_}eprint: 1802.00614},
archivePrefix = {arXiv},
arxivId = {1802.00614},
author = {shi Zhang, Quan and chun Zhu, Song},
booktitle = {Frontiers of Information Technology and Electronic Engineering},
doi = {10.1631/FITEE.1700808},
eprint = {1802.00614},
issn = {20959230},
keywords = {Artificial intelligence,Deep learning,Interpretable model},
number = {1},
pages = {27--39},
title = {{Visual interpretability for deep learning: a survey}},
volume = {19},
year = {2018}
}
@article{Wu2016,
abstract = {Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. Given an image and a question in natural language, it requires reasoning over visual elements of the image and general knowledge to infer the correct answer. In the first part of this survey, we examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to connect the visual and textual modalities. In particular, we examine the common approach of combining convolutional and recurrent neural networks to map images and questions to a common feature space. We also discuss memory-augmented and modular architectures that interface with structured knowledge bases. In the second part of this survey, we review the datasets available for training and evaluating VQA systems. The various datatsets contain questions at different levels of complexity, which require different capabilities and types of reasoning. We examine in depth the question/answer pairs from the Visual Genome project, and evaluate the relevance of the structured annotations of images with scene graphs for VQA. Finally, we discuss promising future directions for the field, in particular the connection to structured knowledge bases and the use of natural language processing models.},
archivePrefix = {arXiv},
arxivId = {1607.05910},
author = {Wu, Qi and Teney, Damien and Wang, Peng and Shen, Chunhua and Dick, Anthony and van den Hengel, Anton},
doi = {10.1016/j.cviu.2017.05.001},
eprint = {1607.05910},
issn = {1090235X},
journal = {Computer Vision and Image Understanding},
keywords = {Knowledge bases,Natural language processing,Recurrent neural networks,Visual question answering},
month = {jul},
pages = {21--40},
title = {{Visual question answering: A survey of methods and datasets}},
url = {http://arxiv.org/abs/1607.05910},
volume = {163},
year = {2017}
}
@article{Smeulders2014,
abstract = {There is a large variety of trackers, which have been proposed in the literature during the last two decades with some mixed success. Object tracking in realistic scenarios is a difficult problem, therefore, it remains a most active area of research in computer vision. A good tracker should perform well in a large number of videos involving illumination changes, occlusion, clutter, camera motion, low contrast, specularities, and at least six more aspects. However, the performance of proposed trackers have been evaluated typically on less than ten videos, or on the special purpose datasets. In this paper, we aim to evaluate trackers systematically and experimentally on 315 video fragments covering above aspects. We selected a set of nineteen trackers to include a wide variety of algorithms often cited in literature, supplemented with trackers appearing in 2010 and 2011 for which the code was publicly available. We demonstrate that trackers can be evaluated objectively by survival curves, Kaplan Meier statistics, and Grubs testing. We find that in the evaluation practice the F-score is as effective as the object tracking accuracy (OTA) score. The analysis under a large variety of circumstances provides objective insight into the strengths and weaknesses of trackers. {\textcopyright} 2014 IEEE.},
author = {Smeulders, Arnold W.M. and Chu, Dung M. and Cucchiara, Rita and Calderara, Simone and Dehghan, Afshin and Shah, Mubarak},
doi = {10.1109/TPAMI.2013.230},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Camera surveillance,Computer vision,Image processing,Object tracking,Tracking dataset,Tracking evaluation,Video understanding},
number = {7},
pages = {1442--1468},
publisher = {IEEE Computer Society},
title = {{Visual tracking: An experimental survey}},
volume = {36},
year = {2014}
}
@inproceedings{Milletari2016,
abstract = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
annote = {{\_}eprint: 1606.04797},
archivePrefix = {arXiv},
arxivId = {1606.04797},
author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed Ahmad},
booktitle = {Proceedings - 2016 4th International Conference on 3D Vision, 3DV 2016},
doi = {10.1109/3DV.2016.79},
eprint = {1606.04797},
isbn = {9781509054077},
keywords = {Deep learning,convolutional neural networks,machine learning,prostate,segmentation},
pages = {565--571},
title = {{V-Net: Fully convolutional neural networks for volumetric medical image segmentation}},
year = {2016}
}
@inproceedings{Engelcke2017,
abstract = {This paper proposes a computationally efficient approach to detecting objects natively in 3D point clouds using convolutional neural networks (CNNs). In particular, this is achieved by leveraging a feature-centric voting scheme to implement novel convolutional layers which explicitly exploit the sparsity encountered in the input. To this end, we examine the trade-off between accuracy and speed for different architectures and additionally propose to use an L1 penalty on the filter activations to further encourage sparsity in the intermediate representations. To the best of our knowledge, this is the first work to propose sparse convolutional layers and L1 regularisation for efficient large-scale processing of 3D data. We demonstrate the efficacy of our approach on the KITTI object detection benchmark and show that VoteSDeep models with as few as three layers outperform the previous state of the art in both laser and laser-vision based approaches by margins of up to 40{\%} while remaining highly competitive in terms of processing time.},
archivePrefix = {arXiv},
arxivId = {1609.06666},
author = {Engelcke, Martin and Rao, Dushyant and Wang, Dominic Zeng and Tong, Chi Hay and Posner, Ingmar},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2017.7989161},
eprint = {1609.06666},
isbn = {9781509046331},
issn = {10504729},
pages = {1355--1361},
title = {{Vote3Deep: Fast object detection in 3D point clouds using efficient convolutional neural networks}},
url = {https://doi.org/10.1109/ICRA.2017.7989161},
year = {2017}
}
@inproceedings{Maturana2015,
abstract = {Robust object recognition is a crucial skill for robots operating autonomously in real world environments. Range sensors such as LiDAR and RGBD cameras are increasingly found in modern robotic systems, providing a rich source of 3D information that can aid in this task. However, many current systems do not fully utilize this information and have trouble efficiently dealing with large amounts of point cloud data. In this paper, we propose VoxNet, an architecture to tackle this problem by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN). We evaluate our approach on publicly available benchmarks using LiDAR, RGBD, and CAD data. VoxNet achieves accuracy beyond the state of the art while labeling hundreds of instances per second.},
author = {Maturana, Daniel and Scherer, Sebastian},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2015.7353481},
isbn = {9781479999941},
issn = {21530866},
pages = {922--928},
title = {{VoxNet: A 3D Convolutional Neural Network for real-time object recognition}},
url = {https://doi.org/10.1109/IROS.2015.7353481},
volume = {2015-Decem},
year = {2015}
}
@article{Yang2019,
abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
annote = {{\_}eprint: 1906.08237},
archivePrefix = {arXiv},
arxivId = {1906.08237},
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V},
eprint = {1906.08237},
title = {{XLNet: Generalized Autoregressive Pretraining for Language Understanding}},
url = {http://arxiv.org/abs/1906.08237},
year = {2019}
}
@article{Graham2018,
abstract = {Nuclear segmentation within Haematoxylin {\&} Eosin stained histology images is a fundamental prerequisite in the digital pathology work-flow, due to the ability for nuclear features to act as key diagnostic markers. The development of automated methods for nuclear segmentation enables the quantitative analysis of tens of thousands of nuclei within a whole-slide pathology image, opening up possibilities of further analysis of large-scale nuclear morphometry. However, automated nuclear segmentation is faced with a major challenge in that there are several different types of nuclei, some of them exhibiting large intra-class variability such as the tumour cells. Additionally, some of the nuclei are often clustered together. To address these challenges, we present a novel convolutional neural network for automated nuclear segmentation that leverages the instance-rich information encoded within the vertical and horizontal distances of nuclear pixels to their centres of mass. These distances are then utilised to separate clustered nuclei, resulting in an accurate segmentation, particularly in areas with overlapping instances. We demonstrate state-of-the-art performance compared to other methods on four independent multi-tissue histology image datasets. Furthermore, we propose an interpretable and reliable evaluation framework that effectively quantifies nuclear segmentation performance and overcomes the limitations of existing performance measures.},
annote = {{\_}eprint: 1812.06499},
archivePrefix = {arXiv},
arxivId = {1812.06499},
author = {Graham, Simon and Vu, Quoc Dang and Raza, Shan e Ahmed and Kwak, Jin Tae and Rajpoot, Nasir},
eprint = {1812.06499},
journal = {CoRR},
pages = {1--11},
title = {{XY Network for Nuclear Segmentation in Multi-Tissue Histology Images}},
url = {http://arxiv.org/abs/1812.06499},
volume = {abs/1812.0},
year = {2018}
}
