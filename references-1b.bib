@article{Klein2012,
abstract = {We introduce the Mindboggle-101 dataset, the largest and most complete set of free, publicly accessible, manually labeled human brain images. To manually label the macroscopic anatomy in magnetic resonance images of 101 healthy participants, we created a new cortical labeling protocol that relies on robust anatomical landmarks and minimal manual edits after initialization with automated labels. The "Desikan-Killiany-Tourville" (DKT) protocol is intended to improve the ease, consistency, and accuracy of labeling human cortical areas. Given how difficult it is to label brains, the Mindboggle-101 dataset is intended to serve as brain atlases for use in labeling other brains, as a normative dataset to establish mor-phometric variation in a healthy population for comparison against clinical populations, and contribute to the development, training, testing, and evaluation of automated registration and labeling algorithms. To this end, we also introduce benchmarks for the evaluation of such algorithms by comparing our manual labels with labels automatically generated by probabilistic and multi-atlas registration-based approaches. All data and related software and updated information are available on the http://mindboggle.info/data website. Copy; 2012 Kleinand Tourville.},
author = {Klein, Arno and Tourville, Jason},
doi = {10.3389/fnins.2012.00171},
issn = {16624548},
journal = {Frontiers in Neuroscience},
keywords = {Anatomy,Cerebral cortex,Human brain,Labeling,Mri,Parcellation,Segmentation},
number = {DEC},
title = {{101 Labeled Brain Images and a Consistent Human Cortical Labeling Protocol}},
url = {http://journal.frontiersin.org/article/10.3389/fnins.2012.00171/abstract},
volume = {6},
year = {2012}
}
@online{2018AtrialSegmentationChallenge,
title = {{2018 Atrial Segmentation Challenge – Atrial Segmentation Challenge}},
url = {http://atriaseg2018.cardiacatlas.org/}
}
@inproceedings{Seff2014,
abstract = {Enlarged lymph nodes (LNs) can provide important information for cancer diagnosis, staging, and measuring treatment reactions, making automated detection a highly sought goal. In this paper, we propose a new algorithm representation of decomposing the LN detection problem into a set of 2D object detection subtasks on sampled CT slices, largely alleviating the curse of dimensionality issue. Our 2D detection can be effectively formulated as linear classification on a single image feature type of Histogram of Oriented Gradients (HOG), covering a moderate field-of-view of 45 by 45 voxels. We exploit both max-pooling and sparse linear fusion schemes to aggregate these 2D detection scores for the final 3D LN detection. In this manner, detection is more tractable and does not need to perform perfectly at instance level (as weak hypotheses) since our aggregation process will robustly harness collective information for LN detection. Two datasets (90 patients with 389 mediastinal LNs and 86 patients with 595 abdominal LNs) are used for validation. Cross-validation demonstrates 78.0{\%} sensitivity at 6 false positives/volume (FP/vol.) (86.1{\%} at 10 FP/vol.) and 73.1{\%} sensitivity at 6 FP/vol. (87.2{\%} at 10 FP/vol.), for the mediastinal and abdominal datasets respectively. Our results compare favorably to previous state-of-the-art methods. {\textcopyright} 2014 Springer International Publishing.},
author = {Seff, Ari and Lu, Le and Cherry, Kevin M. and Roth, Holger R. and Liu, Jiamin and Wang, Shijun and Hoffman, Joanne and Turkbey, Evrim B. and Summers, Ronald M.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10404-1_68},
isbn = {9783319104034},
issn = {16113349},
title = {{2D view aggregation for lymph node detection using a shallow hierarchy of linear classifiers}},
year = {2014}
}
@inproceedings{Zhong2017b,
abstract = {Positron emission tomography – computed tomography (PET-CT) has been widely used in modern cancer imaging. Accurate tumor delineation from PET and CT plays an important role in radiation therapy. The PET-CT co-segmentation technique, which makes use of advantages of both modalities, has achieved impressive performance for tumor delineation. In this work, we propose a novel 3D image matting based semi-automated co-segmentation method for tumor delineation on dual PET-CT scans. The “matte” values generated by 3D image matting are employed to compute the region costs for the graph based co-segmentation. Compared to previous PET-CT co-segmentation methods, our method is completely data-driven in the design of cost functions, thus using much less hyper-parameters in our segmentation model. Comparative experiments on 54 PET-CT scans of lung cancer patients demonstrated the effectiveness of our method.},
author = {Zhong, Zisha and Kim, Yusung and Buatti, John and Wu, Xiaodong},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-67564-0_4},
isbn = {9783319675633},
issn = {16113349},
keywords = {Co-segmentation,Image matting,Image segmentation,Interactive segmentation,Lung tumor segmentation},
pages = {31--42},
title = {{3D alpha matting based co-segmentation of tumors on PET-CT images}},
volume = {10555 LNCS},
year = {2017}
}
@article{Khvostikov2018,
abstract = {Computer-aided early diagnosis of Alzheimers Disease (AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the subject of extensive research in recent years. Some recent studies have shown promising results in the AD and MCI determination using structural and functional Magnetic Resonance Imaging (sMRI, fMRI), Positron Emission Tomography (PET) and Diffusion Tensor Imaging (DTI) modalities. Furthermore, fusion of imaging modalities in a supervised machine learning framework has shown promising direction of research. In this paper we first review major trends in automatic classification methods such as feature extraction based methods as well as deep learning approaches in medical image analysis applied to the field of Alzheimer's Disease diagnostics. Then we propose our own algorithm for Alzheimer's Disease diagnostics based on a convolutional neural network and sMRI and DTI modalities fusion on hippocampal ROI using data from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (http://adni.loni.usc.edu). Comparison with a single modality approach shows promising results. We also propose our own method of data augmentation for balancing classes of different size and analyze the impact of the ROI size on the classification results as well.},
annote = {{\_}eprint: 1801.05968},
archivePrefix = {arXiv},
arxivId = {1801.05968},
author = {Khvostikov, Alexander and Aderghal, Karim and Benois-Pineau, Jenny and Krylov, Andrey and Catheline, Gwenaelle},
eprint = {1801.05968},
title = {{3D CNN-based classification using sMRI and MD-DTI images for Alzheimer disease studies}},
url = {http://arxiv.org/abs/1801.05968},
year = {2018}
}
@inproceedings{Dou2016,
abstract = {Automatic liver segmentation from CT volumes is a crucial prerequisite yet challenging task for computer-aided hepatic disease diagnosis and treatment. In this paper,we present a novel 3D deeply supervised network (3D DSN) to address this challenging task. The proposed 3D DSN takes advantage of a fully convolutional architecture which performs efficient end-to-end learning and inference. More importantly,we introduce a deep supervision mechanism during the learning process to combat potential optimization difficulties,and thus the model can acquire a much faster convergence rate and more powerful discrimination capability. On top of the high-quality score map produced by the 3D DSN,a conditional random field model is further employed to obtain refined segmentation results. We evaluated our framework on the public MICCAI-SLiver07 dataset. Extensive experiments demonstrated that our method achieves competitive segmentation results to state-of-the-art approaches with a much faster processing speed.},
address = {Cham},
archivePrefix = {arXiv},
arxivId = {1607.00582},
author = {Dou, Qi and Chen, Hao and Jin, Yueming and Yu, Lequan and Qin, Jing and Heng, Pheng Ann},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46723-8_18},
editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R and Unal, Gozde and Wells, William},
eprint = {1607.00582},
isbn = {9783319467221},
issn = {16113349},
pages = {149--157},
publisher = {Springer International Publishing},
title = {{3D deeply supervised network for automatic liver segmentation from CT volumes}},
volume = {9901 LNCS},
year = {2016}
}
@inproceedings{Zhong2018,
annote = {ISSN: 1945-8452},
author = {Zhong, Z and Kim, Y and Zhou, L and Plichta, K and Allen, B and Buatti, J and Wu, X},
booktitle = {2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)},
doi = {10.1109/ISBI.2018.8363561},
keywords = {3D fully convolutional networks,Biomedical imaging,Computed tomography,Image segmentation,Lung,PET-CT images,PET-CT scans,Three-dimensional displays,Tumors,automated accurate tumor delineation,biomedical MRI,cancer,cancer diagnosis,co-segmentation,co-segmentation model,computed tomography,computerised tomography,critical diagnostic information,deep learning,dual-modality imaging,final tumor segmentation results,fully convolutional networks,graph cut,image classification,image segmentation,learning (artificial intelligence),lung,lung cancer patients,lung tumor segmentation,medical image processing,positron emission tomography,probability maps,semantic segmentation framework,tumor reading,tumours},
month = {apr},
pages = {228--231},
title = {{3D fully convolutional networks for co-segmentation of tumors on PET-CT images}},
year = {2018}
}
@inproceedings{Kim2013,
abstract = {Scene understanding is an important yet very challenging problem in computer vision. In the past few years, researchers have taken advantage of the recent diffusion of depth-RGB (RGB-D) cameras to help simplify the problem of inferring scene semantics. However, while the added 3D geometry is certainly useful to segment out objects with different depth values, it also adds complications in that the 3D geometry is often incorrect because of noisy depth measurements and the actual 3D extent of the objects is usually unknown because of occlusions. In this paper we propose a new method that allows us to jointly refine the 3D reconstruction of the scene (raw depth values) while accurately segmenting out the objects or scene elements from the 3D reconstruction. This is achieved by introducing a new model which we called Voxel-CRF. The Voxel-CRF model is based on the idea of constructing a conditional random field over a 3D volume of interest which captures the semantic and 3D geometric relationships among different elements (voxels) of the scene. Such model allows to jointly estimate (1) a dense voxel-based 3D reconstruction and (2) the semantic labels associated with each voxel even in presence of partial occlusions using an approximate yet efficient inference strategy. We evaluated our method on the challenging NYU Depth dataset (Version 1 and 2). Experimental results show that our method achieves competitive accuracy in inferring scene semantics and visually appealing results in improving the quality of the 3D reconstruction. We also demonstrate an interesting application of object removal and scene completion from RGB-D images. {\textcopyright} 2013 IEEE.},
author = {Kim, Byung Soo and Kohli, Pushmeet and Savarese, Silvio},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2013.180},
isbn = {9781479928392},
keywords = {3D reconstruction,RGB-D,Scene understanding},
pages = {1425--1432},
title = {{3D scene understanding by voxel-CRF}},
url = {https://doi.org/10.1109/ICCV.2013.180},
year = {2013}
}
@article{VanGinneken2007,
abstract = {This paper describes the setup of a segmentation competition for the automatic extraction of Multiple Sclerosis (MS) lesions from brain Magnetic Resonance Imaging (MRI) data. This competition is one of three competitions that make up a comparison workshop at the 2008 Medical Image Computing and Computer Assisted Intervention (MICCAI) conference and was modeled after the successful comparison workshop on liver and caudate segmentation at the 2007 MICCAI conference. In this paper, the rationale for organizing the competition is discussed, the training and test data sets for both segmentation tasks are described and the scoring system used to evaluate the segmentation is presented.},
author = {van Ginneken, Bram and Heimann, Tobias and Styner, Martin},
journal = {International Conference on Medical Image Computing and Computer Assisted Intervention},
pages = {7--15},
title = {{3D segmentation in the clinic: A grand challeng}},
url = {http://grand-challenge2008.bigr.nl/proceedings/pdfs/msls08/Styner.pdf},
volume = {10},
year = {2007}
}
@inproceedings{Wu2014,
abstract = {3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet - a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.},
annote = {{\_}eprint: 1406.5670},
archivePrefix = {arXiv},
arxivId = {1406.5670},
author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298801},
eprint = {1406.5670},
isbn = {9781467369640},
issn = {10636919},
keywords = {Computer Science - Computer Vision and Pattern Rec},
pages = {1912--1920},
title = {{3D ShapeNets: A deep representation for volumetric shapes}},
url = {http://arxiv.org/abs/1406.5670},
volume = {07-12-June},
year = {2015}
}
@ONLINE{Landman,
author = {Landman, Bennett A and Anderson, Adam W and Schilling, Kurt and Alexander, Simon and Kerins, Fergal and Westin, C-F and Rathi, Yogesh and Dyrby, Tim B. and Descoteaux, Maxime and Houde, Jean-Christophe and Verma, Ragini and Pierpaoli, Carlo and Irfanoglu, Okan and Thomas, Cibu},
title = {{3-D Validation of Tractography with Experimental MRI (3D VoTEM)}},
url = {https://my.vanderbilt.edu/votem/}
}
@article{Arulkumaran2017a,
abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep {\$}Q{\$}-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
archivePrefix = {arXiv},
arxivId = {1708.05866},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
doi = {10.1109/MSP.2017.2743240},
eprint = {1708.05866},
month = {aug},
title = {{A Brief Survey of Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1708.05866 http://dx.doi.org/10.1109/MSP.2017.2743240},
year = {2017}
}
@inproceedings{Jurio2010,
abstract = {In this work we carry out a comparison study between different color spaces in clustering-based image segmentation. We use two similar clustering algorithms, one based on the entropy and the other on the ignorance. The study involves four color spaces and, in all cases, each pixel is represented by the values of the color channels in that space. Our purpose is to identify the best color representation, if there is any, when using this kind of clustering algorithms. {\textcopyright} Springer-Verlag Berlin Heidelberg 2010.},
address = {Berlin, Heidelberg},
author = {Jurio, Aranzazu and Pagola, Miguel and Galar, Mikel and Lopez-Molina, Carlos and Paternain, Daniel},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-642-14058-7_55},
editor = {H{\"{u}}llermeier, Eyke and Kruse, Rudolf and Hoffmann, Frank},
isbn = {9783642140570},
issn = {18650929},
keywords = {CMY,Clustering,Color space,HSV,Image segmentation,RGB,YUV},
pages = {532--541},
publisher = {Springer Berlin Heidelberg},
title = {{A comparison study of different color spaces in clustering based image segmentation}},
volume = {81 PART 2},
year = {2010}
}
@misc{Kalsotra2019,
abstract = {Background subtraction is an effective method of choice when it comes to detection of moving objects in videos and has been recognized as a breakthrough for the wide range of applications of intelligent video analytics (IVA). In recent years, a number of video datasets intended for background subtraction have been created to address the problem of large realistic datasets with accurate ground truth. The use of these datasets enables qualitative as well as quantitative comparisons and allows benchmarking of different algorithms. Finding the appropriate dataset is generally a cumbersome task for an exhaustive evaluation of algorithms. Therefore, we systematically survey standard video datasets and list their applicability for different applications. This paper presents a comprehensive account of public video datasets for background subtraction and attempts to cover the lack of a detailed description of each dataset. The video datasets are presented in chronological order of their appearance. Current trends of deep learning in background subtraction along with top-ranked background subtraction methods are also discussed in this paper. The survey introduced in this paper will assist researchers of the computer vision community in the selection of appropriate video dataset to evaluate their algorithms on the basis of challenging scenarios that exist in both indoor and outdoor environments.},
author = {Kalsotra, Rudrika and Arora, Sakshi},
booktitle = {IEEE Access},
doi = {10.1109/ACCESS.2019.2914961},
issn = {21693536},
keywords = {Background model,background subtraction,challenges,datasets,deep neural networks,foreground,intelligent video analytics (IVA),video frames},
pages = {59143--59171},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A Comprehensive Survey of Video Datasets for Background Subtraction}},
volume = {7},
year = {2019}
}
@article{Zhang2017,
abstract = {Automated cardiac image interpretation has the potential to transform clinical practice in multiple ways including enabling low-cost serial assessment of cardiac function in the primary care and rural setting. We hypothesized that advances in computer vision could enable building a fully automated, scalable analysis pipeline for echocardiogram (echo) interpretation. Our approach entailed: 1) preprocessing; 2) convolutional neural networks (CNN) for view identification, image segmentation, and phasing of the cardiac cycle; 3) quantification of chamber volumes and left ventricular mass; 4) particle tracking to compute longitudinal strain; and 5) targeted disease detection. CNNs accurately identified views (e.g. 99{\%} for apical 4-chamber) and segmented individual cardiac chambers. Cardiac structure measurements agreed with study report values (e.g. mean absolute deviations (MAD) of 7.7 mL/kg/m2 for left ventricular diastolic volume index, 2918 studies). We computed automated ejection fraction and longitudinal strain measurements (within 2 cohorts), which agreed with commercial software-derived values [for ejection fraction, MAD=5.3{\%}, N=3101 studies; for strain, MAD=1.5{\%} (n=197) and 1.6{\%} (n=110)], and demonstrated applicability to serial monitoring of breast cancer patients for trastuzumab cardiotoxicity. Overall, we found that, compared to manual measurements, automated measurements had superior performance across seven internal consistency metrics with an average increase in the Spearman correlation coefficient of 0.05 (p=0.02). Finally, we developed disease detection algorithms for hypertrophic cardiomyopathy and cardiac amyloidosis, with C-statistics of 0.93 and 0.84, respectively. Our pipeline lays the groundwork for using automated interpretation to support point-of-care handheld cardiac ultrasound and large-scale analysis of the millions of echos archived within healthcare systems.},
annote = {{\_}eprint: 1706.07342},
archivePrefix = {arXiv},
arxivId = {1706.07342},
author = {Zhang, Jeffrey and Gajjala, Sravani and Agrawal, Pulkit and Tison, Geoffrey H and Hallock, Laura A and Beussink-Nelson, Lauren and Fan, Eugene and Aras, Mandar A and Jordan, ChaRandle and Fleischmann, Kirsten E and Melisko, Michelle and Qasim, Atif and Efros, Alexei and Shah, Sanjiv J and Bajcsy, Ruzena and Deo, Rahul C},
eprint = {1706.07342},
title = {{A Computer Vision Pipeline for Automated Determination of Cardiac Structure and Function and Detection of Disease by Two-Dimensional Echocardiography}},
url = {http://arxiv.org/abs/1706.07342},
year = {2017}
}
@article{Kumar2017,
abstract = {Nuclear segmentation in digital microscopic tissue images can enable extraction of high-quality features for nuclear morphometrics and other analysis in computational pathology. Conventional image processing techniques, such as Otsu thresholding and watershed segmentation, do not work effectively on challenging cases, such as chromatin-sparse and crowded nuclei. In contrast, machine learning-based segmentation can generalize across various nuclear appearances. However, training machine learning algorithms requires data sets of images, in which a vast number of nuclei have been annotated. Publicly accessible and annotated data sets, along with widely agreed upon metrics to compare techniques, have catalyzed tremendous innovation and progress on other image classification problems, particularly in object recognition. Inspired by their success, we introduce a large publicly accessible data set of hematoxylin and eosin (HE)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries, whose quality was validated by a medical doctor. Because our data set is taken from multiple hospitals and includes a diversity of nuclear appearances from several patients, disease states, and organs, techniques trained on it are likely to generalize well and work right out-of-the-box on other HE-stained images. We also propose a new metric to evaluate nuclear segmentation results that penalizes object- and pixel-level errors in a unified manner, unlike previous metrics that penalize only one type of error. We also propose a segmentation technique based on deep learning that lays a special emphasis on identifying the nuclear boundaries, including those between the touching or overlapping nuclei, and works well on a diverse set of test images.},
author = {Kumar, Neeraj and Verma, Ruchika and Sharma, Sanuj and Bhargava, Surabhi and Vahadane, Abhishek and Sethi, Amit},
doi = {10.1109/TMI.2017.2677499},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Annotation,boundaries,dataset,deep learning,nuclear segmentation,nuclei},
month = {jul},
number = {7},
pages = {1550--1560},
title = {{A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology}},
volume = {36},
year = {2017}
}
@article{Krishnamurthy2018,
abstract = {Automatic deception detection is an important task that has gained momentum in computational linguistics due to its potential applications. In this paper, we propose a simple yet tough to beat multi-modal neural model for deception detection. By combining features from different modalities such as video, audio, and text along with Micro-Expression features, we show that detecting deception in real life videos can be more accurate. Experimental results on a dataset of real-life deception videos show that our model outperforms existing techniques for deception detection with an accuracy of 96.14{\%} and ROC-AUC of 0.9799.},
archivePrefix = {arXiv},
arxivId = {1803.00344},
author = {Krishnamurthy, Gangeshwar and Majumder, Navonil and Poria, Soujanya and Cambria, Erik},
eprint = {1803.00344},
month = {mar},
title = {{A Deep Learning Approach for Multimodal Deception Detection}},
url = {http://arxiv.org/abs/1803.00344},
year = {2018}
}
@article{Zhao2018,
abstract = {Accurate and reliable brain tumor segmentation is a critical component in cancer diagnosis, treatment planning, and treatment outcome evaluation. Build upon successful deep learning techniques, a novel brain tumor segmentation method is developed by integrating fully convolutional neural networks (FCNNs) and Conditional Random Fields (CRFs) in a unified framework to obtain segmentation results with appearance and spatial consistency. We train a deep learning based segmentation model using 2D image patches and image slices in following steps: 1) training FCNNs using image patches; 2) training CRFs as Recurrent Neural Networks (CRF-RNN) using image slices with parameters of FCNNs fixed; and 3) fine-tuning the FCNNs and the CRF-RNN using image slices. Particularly, we train 3 segmentation models using 2D image patches and slices obtained in axial, coronal and sagittal views respectively, and combine them to segment brain tumors using a voting based fusion strategy. Our method could segment brain images slice-by-slice, much faster than those based on image patches. We have evaluated our method based on imaging data provided by the Multimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015 and BRATS 2016. The experimental results have demonstrated that our method could build a segmentation model with Flair, T1c, and T2 scans and achieve competitive performance as those built with Flair, T1, T1c, and T2 scans.},
author = {Zhao, Xiaomei and Wu, Yihong and Song, Guidong and Li, Zhenye and Zhang, Yazhuo and Fan, Yong},
doi = {https://doi.org/10.1016/j.media.2017.10.002},
issn = {1361-8415},
journal = {Medical Image Analysis},
keywords = {Brain tumor segmentation,Conditional random fields,Deep learning,Fully convolutional neural networks},
pages = {98 -- 111},
title = {{A deep learning model integrating FCNNs and CRFs for brain tumor segmentation}},
url = {http://www.sciencedirect.com/science/article/pii/S136184151730141X},
volume = {43},
year = {2018}
}
@inproceedings{Zhou2017,
abstract = {Deep neural networks have been widely adopted for automatic organ segmentation from abdominal CT scans. However, the segmentation accuracy of some small organs (e.g., the pancreas) is sometimes below satisfaction, arguably because deep networks are easily disrupted by the complex and variable background regions which occupies a large fraction of the input volume. In this paper, we formulate this problem into a fixed-point model which uses a predicted segmentation mask to shrink the input region. This is motivated by the fact that a smaller input region often leads to more accurate segmentation. In the training process, we use the ground-truth annotation to generate accurate input regions and optimize network weights. On the testing stage, we fix the network parameters and update the segmentation results in an iterative manner. We evaluate our approach on the NIH pancreas segmentation dataset, and outperform the state-of-the-art by more than {\$}{\$}4$\backslash$backslash{\%}{\$}{\$}, measured by the average Dice-S{\o}rensen Coefficient (DSC). In addition, we report {\$}{\$}62.43$\backslash$backslash{\%}{\$}{\$}DSC in the worst case, which guarantees the reliability of our approach in clinical applications.},
address = {Cham},
author = {Zhou, Yuyin and Xie, Lingxi and Shen, Wei and Wang, Yan and Fishman, Elliot K and Yuille, Alan L},
booktitle = {Medical Image Computing and Computer Assisted Intervention − MICCAI 2017},
editor = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D Louis and Duchesne, Simon},
isbn = {978-3-319-66182-7},
pages = {693--701},
publisher = {Springer International Publishing},
title = {{A Fixed-Point Model for Pancreas Segmentation in Abdominal CT Scans}},
year = {2017}
}
@article{Jimenez-Carretero2019,
abstract = {Lung vessel segmentation has been widely explored by the biomedical image processing community; however, the differentiation of arterial from venous irrigation is still a challenge. Pulmonary artery–vein (AV) segmentation using computed tomography (CT) is growing in importance owing to its undeniable utility in multiple cardiopulmonary pathological states, especially those implying vascular remodelling, allowing the study of both flow systems separately. We present a new framework to approach the separation of tree-like structures using local information and a specifically designed graph-cut methodology that ensures connectivity as well as the spatial and directional consistency of the derived subtrees. This framework has been applied to the pulmonary AV classification using a random forest (RF) pre-classifier to exploit the local anatomical differences of arteries and veins. The evaluation of the system was performed using 192 bronchopulmonary segment phantoms, 48 anthropomorphic pulmonary CT phantoms, and 26 lungs from noncontrast CT images with precise voxel-based reference standards obtained by manually labelling the vessel trees. The experiments reveal a relevant improvement in the accuracy (∼ 20{\%}) of the vessel particle classification with the proposed framework with respect to using only the pre-classification based on local information applied to the whole area of the lung under study. The results demonstrated the accurate differentiation between arteries and veins in both clinical and synthetic cases, specifically when the image quality can guarantee a good airway segmentation, which opens a huge range of possibilities in the clinical study of cardiopulmonary diseases.},
author = {Jimenez-Carretero, Daniel and Bermejo-Pel{\'{a}}ez, David and Nardelli, Pietro and Fraga, Patricia and Fraile, Eduardo and {San Jos{\'{e}} Est{\'{e}}par}, Ra{\'{u}}l and Ledesma-Carbayo, Maria J},
doi = {10.1016/j.media.2018.11.011},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Arteries,Artery-vein segmentation,Graph-cuts,Lung,Noncontrast CT,Phantoms,Random forest,Veins},
pages = {144--159},
title = {{A graph-cut approach for pulmonary artery-vein segmentation in noncontrast CT images}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841518308740},
volume = {52},
year = {2019}
}
@article{Simpson2019,
abstract = {Semantic segmentation of medical images aims to associate a pixel with a label in a medical image without human initialization. The success of semantic segmentation algorithms is contingent on the availability of high-quality imaging data with corresponding labels provided by experts. We sought to create a large collection of annotated medical image datasets of various clinically relevant anatomies available under open source license to facilitate the development of semantic segmentation algorithms. Such a resource would allow: 1) objective assessment of general-purpose segmentation methods through comprehensive benchmarking and 2) open and free access to medical image data for any researcher interested in the problem domain. Through a multi-institutional effort, we generated a large, curated dataset representative of several highly variable segmentation tasks that was used in a crowd-sourced challenge - the Medical Segmentation Decathlon held during the 2018 Medical Image Computing and Computer Aided Interventions Conference in Granada, Spain. Here, we describe these ten labeled image datasets so that these data may be effectively reused by the research community.},
archivePrefix = {arXiv},
arxivId = {1902.09063},
author = {Simpson, Amber L. and Antonelli, Michela and Bakas, Spyridon and Bilello, Michel and Farahani, Keyvan and van Ginneken, Bram and Kopp-Schneider, Annette and Landman, Bennett A. and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M. and Bilic, Patrick and Christ, Patrick F. and Do, Richard K. G. and Gollub, Marc and Golia-Pernicka, Jennifer and Heckers, Stephan H. and Jarnagin, William R. and McHugo, Maureen K. and Napel, Sandy and Vorontsov, Eugene and Maier-Hein, Lena and Cardoso, M. Jorge},
eprint = {1902.09063},
month = {feb},
title = {{A large annotated medical image dataset for the development and evaluation of segmentation algorithms}},
url = {http://arxiv.org/abs/1902.09063},
year = {2019}
}
@article{Liew2018,
abstract = {Stroke is the leading cause of adult disability worldwide, with up to two-thirds of individuals experiencing long-term disabilities. Large-scale neuroimaging studies have shown promise in identifying robust biomarkers (e.g., measures of brain structure) of long-term stroke recovery following rehabilitation. However, analyzing large rehabilitation-related datasets is problematic due to barriers in accurate stroke lesion segmentation. Manually-traced lesions are currently the gold standard for lesion segmentation on T1-weighted MRIs, but are labor intensive and require anatomical expertise. While algorithms have been developed to automate this process, the results often lack accuracy. Newer algorithms that employ machine-learning techniques are promising, yet these require large training datasets to optimize performance. Here we present ATLAS (Anatomical Tracings of Lesions After Stroke), an open-source dataset of 304 T1-weighted MRIs with manually segmented lesions and metadata. This large, diverse dataset can be used to train and test lesion segmentation algorithms and provides a standardized dataset for comparing the performance of different segmentation methods. We hope ATLAS release 1.1 will be a useful resource to assess and improve the accuracy of current lesion segmentation methods.},
author = {Liew, Sook Lei and Anglin, Julia M. and Banks, Nick W. and Sondag, Matt and Ito, Kaori L. and Kim, Hosung and Chan, Jennifer and Ito, Joyce and Jung, Connie and Khoshab, Nima and Lefebvre, Stephanie and Nakamura, William and Saldana, David and Schmiesing, Allie and Tran, Cathy and Vo, Danny and Ard, Tyler and Heydari, Panthea and Kim, Bokkyu and Aziz-Zadeh, Lisa and Cramer, Steven C. and Liu, Jingchun and Soekadar, Surjo and Nordvik, Jan Egil and Westlye, Lars T. and Wang, Junping and Winstein, Carolee and Yu, Chunshui and Ai, Lei and Koo, Bonhwang and Craddock, R. Cameron and Milham, Michael and Lakich, Matthew and Pienta, Amy and Stroud, Alison},
doi = {10.1038/sdata.2018.11},
issn = {20524463},
journal = {Scientific Data},
month = {dec},
number = {1},
pages = {180011},
pmid = {29461514},
title = {{A large, open source dataset of stroke anatomical brain images and manual lesion segmentations}},
url = {http://www.nature.com/articles/sdata201811},
volume = {5},
year = {2018}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed. {\textcopyright} 1943 The University of Chicago Press.},
author = {McCulloch, Warren S and Pitts, Walter},
doi = {10.1007/BF02478259},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
number = {4},
pages = {115--133},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {https://doi.org/10.1007/BF02478259},
volume = {5},
year = {1943}
}
@book{Henderson2020,
address = {Cham},
author = {Henderson, Brad},
doi = {10.1007/978-3-030-10756-7},
isbn = {978-3-030-10754-3},
publisher = {Springer International Publishing},
title = {{A Math-Based Writing System for Engineers}},
url = {http://link.springer.com/10.1007/978-3-030-10756-7},
year = {2020}
}
@article{Kumar2019,
abstract = {Generalized nucleus segmentation techniques can contribute greatly to reducing the time to develop and validate visual biomarkers for new digital pathology datasets. We summarize the results of MoNuSeg 2018 Challenge whose objective was to develop generalizable nuclei segmentation techniques in digital pathology. The challenge was an official satellite event of the MICCAI 2018 conference in which 32 teams with more than 80 participants from geographically diverse institutes participated. Contestants were given a training set with 30 images from seven organs with annotations of 21,623 individual nuclei. A test dataset with 14 images taken from seven organs, including two organs that did not appear in the training set was released without annotations. Entries were evaluated based on average aggregated Jaccard index (AJI) on the test set to prioritize accurate instance segmentation as opposed to mere semantic segmentation. More than half the teams that completed the challenge outperformed a previous baseline. Among the trends observed that contributed to increased accuracy were the use of color normalization as well as heavy data augmentation. Additionally, fully convolutional networks inspired by variants of U-Net, FCN, and Mask-RCNN were popularly used, typically based on ResNet or VGG base architectures. Watershed segmentation on predicted semantic segmentation maps was a popular post-processing strategy. Several of the top techniques compared favorably to an individual human annotator and can be used with confidence for nuclear morphometrics.},
author = {Kumar, Neeraj and Verma, Ruchika and Anand, Deepak and Zhou, Yanning and Onder, Omer Fahri and Tsougenis, Efstratios and Chen, Hao and Heng, Pheng Ann and Li, Jiahui and Hu, Zhiqiang and Wang, Yuqin Yunzhi and Koohbanani, Navid Alemi and Jahanifar, Mostafa and Tajeddin, Neda Zamani and Gooya, Ali and Rajpoot, Nasir and Ren, Xuhua and Zhou, Sihang and Wang, Qian and Shen, Dinggang and Yang, Cheng Kun and Weng, Chi Hung and Yu, Wei Hsiang and Yeh, Chao Yuan and Yang, Shuang and Xu, Shuoyu and Yeung, Pak Hei and Sun, Peng and Mahbod, Amirreza and Schaefer, Gerald and Ellinger, Isabella and Ecker, Rupert and Smedby, Orjan and Wang, Chunliang and Chidester, Benjamin and Ton, That Vinh and Tran, Minh Triet and Ma, Jun Jian and Do, Minh N. and Graham, Simon and Vu, Quoc Dang and Kwak, Jin Tae and Gunda, Akshaykumar and Chunduri, Raviteja and Hu, Corey and Zhou, Xiaoyang and Lotfi, Dariush and Safdari, Reza and Kascenas, Antanas and O'Neil, Alison and Eschweiler, Dennis and Stegmaier, Johannes and Cui, Yanping and Yin, Baocai and Chen, Kailin and Tian, Xinmei and Gruening, Philipp and Barth, Erhardt and Arbel, Elad and Remer, Itay and Ben-Dor, Amir and Sirazitdinova, Ekaterina and Kohl, Matthias and Braunewell, Stefan and Li, Yuexiang and Xie, Xinpeng and Shen, Linlin and Ma, Jun Jian and Baksi, Krishanu Das and Khan, Mohammad Azam and Choo, Jaegul and Colomer, Adrian and Naranjo, Valery and Pei, Linmin and Iftekharuddin, Khan M. and Roy, Kaushiki and Bhattacharjee, Debotosh and Pedraza, Anibal and Bueno, Maria Gloria and Devanathan, Sabarinathan and Radhakrishnan, Saravanan and Koduganty, Praveen and Wu, Zihan and Cai, Guanyu and Liu, Xiaojie and Wang, Yuqin Yunzhi and Sethi, Amit},
doi = {10.1109/TMI.2019.2947628},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Multi-organ,aggregated Jaccard index,digital pathology,instance segmentation,nucleus segmentation},
month = {may},
number = {5},
pages = {1380--1391},
pmid = {31647422},
title = {{A Multi-Organ Nucleus Segmentation Challenge}},
url = {https://ieeexplore.ieee.org/document/8880654/},
volume = {39},
year = {2019}
}
@misc{TCIA-CT-Lymph-Nodes,
author = {Roth, H R and Lu, L and Seff, A and Cherry, K M and Hoffman, J and Wang, S and Summers, R M},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2015.AQIIDCNM},
publisher = {The Cancer Imaging Archive},
title = {{A new 2.5 d representation for lymph node detection in CT}},
url = {https://wiki.cancerimagingarchive.net/x/0gAtAQ},
year = {2015}
}
@inproceedings{Roth2014,
abstract = {Automated Lymph Node (LN) detection is an important clinical diagnostic task but very challenging due to the low contrast of surrounding structures in Computed Tomography (CT) and to their varying sizes, poses, shapes and sparsely distributed locations. State-of-the-art studies show the performance range of 52.9{\%} sensitivity at 3.1 false-positives per volume (FP/vol.), or 60.9{\%} at 6.1 FP/vol. for mediastinal LN, by one-shot boosting on 3D HAAR features. In this paper, we first operate a preliminary candidate generation stage, towards ∼100{\%} sensitivity at the cost of high FP levels (∼40 per patient), to harvest volumes of interest (VOI). Our 2.5D approach consequently decomposes any 3D VOI by resampling 2D reformatted orthogonal views N times, via scale, random translations, and rotations with respect to the VOI centroid coordinates. These random views are then used to train a deep Convolutional Neural Network (CNN) classifier. In testing, the CNN is employed to assign LN probabilities for all N random views that can be simply averaged (as a set) to compute the final classification probability per VOI. We validate the approach on two datasets: 90 CT volumes with 388 mediastinal LNs and 86 patients with 595 abdominal LNs. We achieve sensitivities of 70{\%}/83{\%} at 3 FP/vol. and 84{\%}/90{\%} at 6 FP/vol. in mediastinum and abdomen respectively, which drastically improves over the previous state-of-the-art work. {\textcopyright} 2014 Springer International Publishing.},
author = {Roth, Holger R. and Lu, Le and Seff, Ari and Cherry, Kevin M. and Hoffman, Joanne and Wang, Shijun and Liu, Jiamin and Turkbey, Evrim and Summers, Ronald M.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10404-1_65},
isbn = {9783319104034},
issn = {16113349},
pmid = {25333158},
title = {{A new 2.5D representation for lymph node detection using random sets of deep convolutional neural network observations}},
year = {2014}
}
@article{Avants2011,
abstract = {The United States National Institutes of Health (NIH) commit significant support to open-source data and software resources in order to foment reproducibility in the biomedical imaging sciences. Here, we report and evaluate a recent product of this commitment: Advanced Neuroimaging Tools (ANTs), which is approaching its 2.0 release. The ANTs open source software library consists of a suite of state-of-the-art image registration, segmentation and template building tools for quantitative morphometric analysis. In this work, we use ANTs to quantify, for the first time, the impact of similarity metrics on the affine and deformable components of a template-based normalization study. We detail the ANTs implementation of three similarity metrics: squared intensity difference, a new and faster cross-correlation, and voxel-wise mutual information. We then use two-fold cross-validation to compare their performance on openly available, manually labeled, T1-weighted MRI brain image data of 40 subjects (UCLA's LPBA40 dataset). We report evaluation results on cortical and whole brain labels for both the affine and deformable components of the registration. Results indicate that the best ANTs methods are competitive with existing brain extraction results (Jaccard = 0.958) and cortical labeling approaches. Mutual information affine mapping combined with cross-correlation diffeomorphic mapping gave the best cortical labeling results (Jaccard = 0.669. ±. 0.022). Furthermore, our two-fold cross-validation allows us to quantify the similarity of templates derived from different subgroups. Our open code, data and evaluation scripts set performance benchmark parameters for this state-of-the-art toolkit. This is the first study to use a consistent transformation framework to provide a reproducible evaluation of the isolated effect of the similarity metric on optimal template construction and brain labeling. {\textcopyright} 2010 Elsevier Inc.},
author = {Avants, Brian B and Tustison, Nicholas J and Song, Gang and Cook, Philip A and Klein, Arno and Gee, James C},
doi = {10.1016/j.neuroimage.2010.09.025},
issn = {10538119},
journal = {NeuroImage},
number = {3},
pages = {2033--2044},
pmid = {20851191},
title = {{A reproducible evaluation of ANTs similarity metric performance in brain image registration}},
url = {http://www.sciencedirect.com/science/article/pii/S1053811910012061},
volume = {54},
year = {2011}
}
@book{Blackwell2011,
address = {New York, NY},
author = {Blackwell, John and Martin, Jan},
doi = {10.1007/978-1-4419-9788-3},
isbn = {978-1-4419-9787-6},
publisher = {Springer New York},
title = {{A Scientific Approach to Scientific Writing}},
url = {http://link.springer.com/10.1007/978-1-4419-9788-3},
year = {2011}
}
@article{Hsu2016,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Fowler, Bridget},
doi = {10.1177/02632760022050997},
eprint = {0-387-31073-8},
institution = {Department of Computer Science National Taiwan University, Taipei 106, Taiwan},
isbn = {013805326X},
issn = {02632764},
journal = {Theory, Culture and Society},
month = {may},
number = {1},
pages = {39--61},
pmid = {18190633},
title = {{A sociological analysis of the satanic verses affair}},
url = {http://www.csie.ntu.edu.tw/{\%}7B{~}{\%}7Dcjlin/papers/guide/guide.pdf},
volume = {17},
year = {2000}
}
@article{Jeyavathana2016,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 305502844 A : Analysis -processing Segmentation Article CITATIONS 0 READS 171 3 , including : Balasubramanian Manonmaniam 17 SEE Anbarasa Manonmaniam 12 SEE All . The . Abstract : Pre - Processing and Segmentation Techniques are used in the application of medical images . Image segmentation is a tediousprocess due to restrictions on Image acquisitions . The most important goal of medical image segmentation is to perform operations on images to detect patterns and to retrieve information from it . In this paper , first medical image processing is discussed . Then we have been proposed approaches to segment CT and CXR images . The comparative study of various image processing techniques has been given in tabular form . This survey provides details of automated segmentation methods , specifically discussed in the context of CT images . The motive is to discuss the problems encountered in the segmentation of CT images , and the relative merits and limitations of methods currently available for segmentation of medical images .},
author = {{Beaulah Jeyavathana}, R and Balasubramanian, R and Pandian, A Anbarasa},
journal = {International Journal of Research and Scientific Innovation},
keywords = {CT,CXR,Pre - processing,Segmentation},
number = {June},
pages = {2321--2705},
title = {{A Survey : Analysis on Pre - processing and Segmentation Techniques for Medical Images}},
volume = {III},
year = {2016}
}
@inproceedings{Lee2015,
abstract = {A precise analysis of medical image is an important stage in the contouring phase throughout radiotherapy preparation. Medical images are mostly used as radiographic techniques in diagnosis, clinical studies and treatment planning Medical image processing tool are also similarly as important. With a medical image processing tool, it is possible to speed up and enhance the operation of the analysis of the medical image. This paper describes medical image processing software tool which attempts to secure the same kind of programmability advantage for exploring applications of the pipelined processors. These tools simulate complete systems consisting of several of the proposed processing components, in a configuration described by a graphical schematic diagram. In this paper, fifteen different medical image processing tools will be compared in several aspects. The main objective of the comparison is to gather and analysis on the tool in order to recommend users of different operating systems on what type of medical image tools to be used when analysing different types of imaging. A result table was attached and discussed in the paper.},
author = {Lee, Lay Khoon and Liew, Siau Chuin},
booktitle = {2015 4th International Conference on Software Engineering and Computer Systems, ICSECS 2015: Virtuous Software Solutions for Big Data},
doi = {10.1109/ICSECS.2015.7333105},
isbn = {9781467367226},
keywords = {computer vision,image processing,tools component},
pages = {171--176},
title = {{A survey of medical image processing tools}},
year = {2015}
}
@article{Yong2012,
abstract = {More than 30 students from university campus participated in the Development of Biomedical Image Processing Software Package for New Learners Survey investigating the use of software package for processing and editing image. The survey was available online for six months. Facts and opinions were sought to learn the general information, interactive image processing tool, non-interactive (automatic) tool, current status and future of image processing package tool. Composed of 19 questions, the survey built a comprehensive picture of the software package, programming language, workflow of the tool and captured the attitudes of the respondents. Result shows that MATLAB was difficult to use but it was viewed in high regard however. The result of this study is expected to be beneficial and able to assist users on effective image processing and analysis in a newly developed software package.},
author = {Yong, Ching Yee and Chew, Kim Mey and Mahmood, Nasrul Humaimi and Ariffin, Ismail},
doi = {10.1016/j.sbspro.2012.09.654},
issn = {18770428},
journal = {Procedia - Social and Behavioral Sciences},
keywords = {Image editting,Image processing,Medical imaging,Software package,Visualisation tools},
pages = {265--271},
title = {{A Survey of Visualization Tools in Medical Imaging}},
url = {http://www.sciencedirect.com/science/article/pii/S187704281204116X},
volume = {56},
year = {2012}
}
@misc{Litjens2017,
abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
archivePrefix = {arXiv},
arxivId = {1702.05747},
author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A.W.M. and van Ginneken, Bram and S{\'{a}}nchez, Clara I},
booktitle = {Medical Image Analysis},
doi = {10.1016/j.media.2017.07.005},
eprint = {1702.05747},
issn = {13618423},
keywords = {Convolutional neural networks,Deep learning,Medical imaging,Survey},
pages = {60--88},
pmid = {28778026},
title = {{A survey on deep learning in medical image analysis}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841517301135},
volume = {42},
year = {2017}
}
@article{Shorten2019,
abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
author = {Shorten, Connor and Khoshgoftaar, Taghi M},
doi = {10.1186/s40537-019-0197-0},
issn = {21961115},
journal = {Journal of Big Data},
keywords = {Big data,Data Augmentation,Deep Learning,GANs,Image data},
number = {1},
pages = {60},
title = {{A survey on Image Data Augmentation for Deep Learning}},
url = {https://doi.org/10.1186/s40537-019-0197-0},
volume = {6},
year = {2019}
}
@article{Yao2017,
abstract = {Pre-processing is an important step in digital image matting, which aims to classify more accurate foreground and background pixels from the unknown region of the input three-region mask (Trimap). This step has no relation with the well-known matting equation and only compares color differences between the current unknown pixel and those known pixels. These newly classified pure pixels are then fed to the matting process as samples to improve the quality of the final matte. However, in the research field of image matting, the importance of pre-processing step is still blurry. Moreover, there are no corresponding review articles for this step, and the quantitative comparison of Trimap and alpha mattes after this step still remains unsolved. In this paper, the necessity and the importance of pre-processing step in image matting are firstly discussed in details. Next, current pre-processing methods are introduced by using the following two categories: static thresholding methods and dynamic thresholding methods. Analyses and experimental results show that static thresholding methods, especially the most popular iterative method, can make accurate pixel classifications in those general Trimaps with relatively fewer unknown pixels. However, in a much larger Trimap, there methods are limited by the conservative color and spatial thresholds. In contrast, dynamic thresholding methods can make much aggressive classifications on much difficult cases, but still strongly suffer from noises and false classifications. In addition, the sharp boundary detector is further discussed as a prior of pure pixels. Finally, summaries and a more effective approach are presented for pre-processing compared with the existing methods.},
author = {Yao, Gui Lin},
doi = {10.1007/s11390-017-1709-z},
issn = {10009000},
journal = {Journal of Computer Science and Technology},
keywords = {Trimap expansion,image matting,pixel classification,pre-processing},
number = {1},
pages = {122--138},
title = {{A Survey on Pre-Processing in Image Matting}},
url = {https://doi.org/10.1007/s11390-017-1709-z},
volume = {32},
year = {2017}
}
@article{Hindy2018,
abstract = {With the world moving towards being increasingly dependent on computers and automation, one of the main challenges in the current decade has been to build secure applications, systems and networks. Alongside these challenges, the number of threats is rising exponentially due to the attack surface increasing through numerous interfaces offered for each service. To alleviate the impact of these threats, researchers have proposed numerous solutions; however, current tools often fail to adapt to ever-changing architectures, associated threats and 0-days. This manuscript aims to provide researchers with a taxonomy and survey of current dataset composition and current Intrusion Detection Systems (IDS) capabilities and assets. These taxonomies and surveys aim to improve both the efficiency of IDS and the creation of datasets to build the next generation IDS as well as to reflect networks threats more accurately in future datasets. To this end, this manuscript also provides a taxonomy and survey or network threats and associated tools. The manuscript highlights that current IDS only cover 25{\%} of our threat taxonomy, while current datasets demonstrate clear lack of real-network threats and attack representation, but rather include a large number of deprecated threats, hence limiting the accuracy of current machine learning IDS. Moreover, the taxonomies are open-sourced to allow public contributions through a Github repository.},
archivePrefix = {arXiv},
arxivId = {1806.03517},
author = {Hindy, Hanan and Brosset, David and Bayne, Ethan and Seeam, Amar and Tachtatzis, Christos and Atkinson, Robert and Bellekens, Xavier},
eprint = {1806.03517},
month = {jun},
title = {{A Taxonomy and Survey of Intrusion Detection System Design Techniques, Network Threats and Datasets}},
url = {http://arxiv.org/abs/1806.03517},
year = {2018}
}
@book{Staron2020,
address = {Cham},
author = {Staron, Miroslaw},
booktitle = {Action Research in Software Engineering},
doi = {10.1007/978-3-030-32610-4},
isbn = {978-3-030-32609-8},
publisher = {Springer International Publishing},
title = {{Action Research in Software Engineering}},
url = {http://link.springer.com/10.1007/978-3-030-32610-4},
year = {2020}
}
@data{GrandChallengeADAM,
author = {Fu, Huazhu and Li, Fei and Orlando, Jos{\'{e}} Ignacio and Bogunovi{\'{c}}, Hrvoje and Sun, Xu and Liao, Jingan and Xu, Yanwu and Zhang, Shaochong and Zhang, Xiulan},
doi = {10.21227/dt4f-rt59},
publisher = {IEEE Dataport},
title = {{ADAM: Automatic Detection challenge on Age-related Macular degeneration}},
url = {http://dx.doi.org/10.21227/dt4f-rt59},
year = {2020}
}
@article{Liu2016,
abstract = {In this paper, we propose an adaptive spatial pooling method for enhancing the discriminability of feature representation for image classification. The core idea is to adopt a spatial distribution matrix to define how the image patches are pooled together. By formulating the pooling distribution learning and classifier training jointly, our method can extract multiple spatial layouts of arbitrary shapes rather than regular rectangular regions. By proper mathematical transformation, the distributions can be learned via a boosting-like algorithm, which improves the efficiency of learning especially for large distribution matrices. Further, our method allows category-specific pooling operations to take advantage of the different spatial layouts of different categories. Experimental results on three benchmark datasets UIUC-Sports, 21-Land-Use and Scene 15 demonstrate the effectiveness of our method.},
author = {Liu, Yinglu and Zhang, Yan Ming and Zhang, Xu Yao and Liu, Cheng Lin},
doi = {10.1016/j.patcog.2016.01.030},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Distribution matrix,Image classification,Spatial layout,Weighted pooling},
month = {jul},
pages = {58--67},
title = {{Adaptive spatial pooling for image classification}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320316000510},
volume = {55},
year = {2016}
}
@article{Carlin2017,
abstract = {The perceptual representation of individual faces is often explained with reference to a norm-based face space. In such spaces, individuals are encoded as vectors where identity is primarily conveyed by direction and distinctiveness by eccentricity. Here we measured human fMRI responses and psychophysical similarity judgments of individual face exemplars, which were generated as realistic 3D animations using a computer-graphics model. We developed and evaluated multiple neurobiologically plausible computational models, each of which predicts a representational distance matrix and a regional-mean activation profile for 24 face stimuli. In the fusiform face area, a face-space coding model with sigmoidal ramp tuning provided a better account of the data than one based on exemplar tuning. However, an image-processing model with weighted banks of Gabor filters performed similarly. Accounting for the data required the inclusion of a measurement-level population averaging mechanism that approximates how fMRI voxels locally average distinct neuronal tunings. Our study demonstrates the importance of comparing multiple models and of modeling the measurement process in computational neuroimaging.},
author = {Carlin, Johan D. and Kriegeskorte, Nikolaus},
doi = {10.1371/journal.pcbi.1005604},
editor = {Daunizeau, Jean},
issn = {15537358},
journal = {PLoS Computational Biology},
month = {jul},
number = {7},
pages = {e1005604},
title = {{Adjudicating between face-coding models with individual-face fMRI responses}},
url = {https://dx.plos.org/10.1371/journal.pcbi.1005604},
volume = {13},
year = {2017}
}
@article{Bakas2017,
abstract = {Gliomas belong to a group of central nervous system tumors, and consist of various sub-regions. Gold standard labeling of these sub-regions in radiographic imaging is essential for both clinical and computational studies, including radiomic and radiogenomic analyses. Towards this end, we release segmentation labels and radiomic features for all pre-operative multimodal magnetic resonance imaging (MRI) (n=243) of the multi-institutional glioma collections of The Cancer Genome Atlas (TCGA), publicly available in The Cancer Imaging Archive (TCIA). Pre-operative scans were identified in both glioblastoma (TCGA-GBM, n=135) and low-grade-glioma (TCGA-LGG, n=108) collections via radiological assessment. The glioma sub-region labels were produced by an automated state-of-the-art method and manually revised by an expert board-certified neuroradiologist. An extensive panel of radiomic features was extracted based on the manually-revised labels. This set of labels and features should enable i) direct utilization of the TCGA/TCIA glioma collections towards repeatable, reproducible and comparative quantitative studies leading to new predictive, prognostic, and diagnostic assessments, as well as ii) performance evaluation of computer-aided segmentation methods, and comparison to our state-of-the-art method.},
author = {Bakas, Spyridon and Akbari, Hamed and Sotiras, Aristeidis and Bilello, Michel and Rozycki, Martin and Kirby, Justin S. and Freymann, John B. and Farahani, Keyvan and Davatzikos, Christos},
doi = {10.1038/sdata.2017.117},
issn = {20524463},
journal = {Scientific Data},
month = {dec},
number = {1},
pages = {170117},
title = {{Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features}},
url = {http://www.nature.com/articles/sdata2017117},
volume = {4},
year = {2017}
}
@article{Finlayson2018,
abstract = {The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we demonstrate that adversarial examples are capable of manipulating deep learning systems across three clinical domains. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. Our models are representative of the current state of the art in medical computer vision and, in some cases, directly reflect architectures already seeing deployment in real world clinical settings. In addition to the technical contribution of our paper, we synthesize a large body of knowledge about the healthcare system to argue that medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud and provide concrete examples of how and why such attacks could be realistically carried out. We urge practitioners to be aware of current vulnerabilities when deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.},
archivePrefix = {arXiv},
arxivId = {1804.05296},
author = {Finlayson, Samuel G. and Chung, Hyung Won and Kohane, Isaac S. and Beam, Andrew L.},
eprint = {1804.05296},
month = {apr},
title = {{Adversarial Attacks Against Medical Deep Learning Systems}},
url = {http://arxiv.org/abs/1804.05296},
year = {2018}
}
@inproceedings{Song2017,
abstract = {We propose a method to reconstruct, complete and semantically label a 3D scene from a single input depth image. We improve the accuracy of the regressed semantic 3D maps by a novel architecture based on adversarial learning. In particular, we suggest using multiple adversarial loss terms that not only enforce realistic outputs with respect to the ground truth, but also an effective embedding of the internal features. This is done by correlating the latent features of the encoder working on partial 2.5D data with the latent features extracted from a variational 3D auto-encoder trained to reconstruct the complete semantic scene. In addition, differently from other approaches that operate entirely through 3D convolutions, at test time we retain the original 2.5D structure of the input during downsampling to improve the effectiveness of the internal representation of our model. We test our approach on the main benchmark datasets for semantic scene completion to qualitatively and quantitatively assess the effectiveness of our proposal.},
archivePrefix = {arXiv},
arxivId = {1810.10901},
author = {Wang, Yida and Tan, David Joseph and Navab, Nassir and Tombari, Federico},
booktitle = {Proceedings - 2018 International Conference on 3D Vision, 3DV 2018},
doi = {10.1109/3DV.2018.00056},
eprint = {1810.10901},
isbn = {9781538684252},
keywords = {Adversarial training,Depth image,Latent space,Scene completion},
pages = {426--434},
title = {{Adversarial semantic scene completion from a single depth image}},
year = {2018}
}
@article{Fu2020,
abstract = {Angle closure glaucoma (ACG) is a more aggressive disease than open-angle glaucoma, where the abnormal anatomical structures of the anterior chamber angle (ACA) may cause an elevated intraocular pressure and gradually leads to glaucomatous optic neuropathy and eventually to visual impairment and blindness. Anterior Segment Optical Coherence Tomography (AS-OCT) imaging provides a fast and contactless way to discriminate angle closure from open angle. Although many medical image analysis algorithms have been developed for glaucoma diagnosis, only a few studies have focused on AS-OCT imaging. In particular, there is no public AS-OCT dataset available for evaluating the existing methods in a uniform way, which limits the progress in the development of automated techniques for angle closure detection and assessment. To address this, we organized the Angle closure Glaucoma Evaluation challenge (AGE), held in conjunction with MICCAI 2019. The AGE challenge consisted of two tasks: scleral spur localization and angle closure classification. For this challenge, we released a large data of 4800 annotated AS-OCT images from 199 patients, and also proposed an evaluation framework to benchmark and compare different models. During the AGE challenge, over 200 teams registered online, and more than 1100 results were submitted for online evaluation. Finally, eight teams participated in the onsite challenge. In this paper, we summarize these eight onsite challenge methods and analyze their corresponding results in the two tasks. We further discuss limitations and future directions. In the AGE challenge, the top-performing approach had an average Euclidean Distance of 10 pixel in scleral spur localization, while in the task of angle closure classification, all the algorithms achieved the satisfactory performances, especially, 100{\%} accuracy rate for top-two performances.},
archivePrefix = {arXiv},
arxivId = {2005.02258},
author = {Fu, Huazhu and Li, Fei and Sun, Xu and Cao, Xingxing and Liao, Jingan and Orlando, Jose Ignacio and Tao, Xing and Li, Yuexiang and Zhang, Shihao and Tan, Mingkui and Yuan, Chenglang and Bian, Cheng and Xie, Ruitao and Li, Jiongcheng and Li, Xiaomeng and Wang, Jing and Geng, Le and Li, Panming and Hao, Huaying and Liu, Jiang and Kong, Yan and Ren, Yongyong and Bogunovic, Hrvoje and Zhang, Xiulan and Xu, Yanwu},
eprint = {2005.02258},
title = {{AGE Challenge: Angle Closure Glaucoma Evaluation in Anterior Segment Optical Coherence Tomography}},
url = {http://arxiv.org/abs/2005.02258},
year = {2020}
}
@data{Zhang2019a,
author = {Zhang, Huazhu Fu; Fei Li; Jos{\'{e}} Ignacio Orlando; Hrvoje Bogunovi{\'{c}}; Xu Sun; Jingan Liao; Yanwu Xu; Shaochong Zhang; Xiulan},
doi = {10.21227/petb-fy10},
publisher = {IEEE Dataport},
title = {{AGE: Angle closure Glaucoma Evaluation Challenge}},
url = {http://dx.doi.org/10.21227/petb-fy10},
year = {2019}
}
@inproceedings{Khagi2019,
abstract = {Various Convolutional Neural Network (CNN) architecture has been proposed for image classification and Object recognition. For the image based classification, it is a complex task for CNN to deal with hundreds of MRI Image slices, each of almost identical nature in a single patient. So, classifying a number of patients as an AD, MCI or NC based on 3D MRI becomes vague technique using 2D CNN architecture. Hence, to address this issue, we have simplified the idea of classifying patients on basis of 3D MRI but acknowledging the 2D features generated from the CNN framework. We present our idea regarding how to obtain 2D features from MRI and transform it to be applicable to classify using machine learning algorithm. Our experiment shows the result of classifying 3 class subjects patients. We employed scratched trained CNN or pretrained Alexnet CNN as generic feature extractor of 2D image which dimensions were reduced using PCA+TSNE, and finally classifying using simple Machine learning algorithm like KNN, Navies Bayes Classifier. Although the result is not so impressive but it definitely shows that this can be better than scratch trained CNN softmax classification based on probability score. The generated feature can be well manipulated and refined for better accuracy, sensitivity, and specificity.},
author = {Khagi, Bijen and Lee, Chung Ghiu and Kwon, Goo Rak},
booktitle = {BMEiCON 2018 - 11th Biomedical Engineering International Conference},
doi = {10.1109/BMEiCON.2018.8609974},
isbn = {9781538657249},
keywords = {CNN,Classifier,Generic feature,MRI,PCA,TSNE},
title = {{Alzheimer's disease Classification from Brain MRI based on transfer learning from CNN}},
year = {2019}
}
@misc{ADNI,
abstract = {The Alzheimer's Disease Neuroimaging Initiative (ADNI) unites researchers with study data as they work to define the progression of Alzheimer's disease (AD). ADNI researchers collect, validate and utilize data, including MRI and PET images, genetics, cognitive tests, CSF and blood biomarkers as predictors of the disease. Study resources and data from the North American ADNI study are available through this website, including Alzheimer's disease patients, mild cognitive impairment subjects, and elderly controls.},
keywords = {[1] “Alzheimer's Disease Neuroimaging Initiative.”},
title = {{Alzheimer's Disease Neuroimaging Initiative}},
url = {http://adni.loni.usc.edu/},
urldate = {2020-05-25},
year = {2020}
}
@article{Aisen2015,
abstract = {Introduction This article reviews the current status of the Clinical Core of the Alzheimer's Disease Neuroimaging Initiative (ADNI), and summarizes planning for the next stage of the project. Methods Clinical Core activities and plans were synthesized based on discussions among the Core leaders and external advisors. Results The longitudinal data in ADNI-2 provide natural history data on a clinical trials population and continue to inform refinement and standardization of assessments, models of trajectories, and clinical trial methods that have been extended into sporadic preclinical Alzheimer's disease (AD). Discussion Plans for the next phase of the ADNI project include maintaining longitudinal follow-up of the normal and mild cognitive impairment cohorts, augmenting specific clinical cohorts, and incorporating novel computerized cognitive assessments and patient-reported outcomes. A major hypothesis is that AD represents a gradually progressive disease that can be identified precisely in its long presymptomatic phase, during which intervention with potentially disease-modifying agents may be most useful.},
author = {Aisen, Paul S. and Petersen, Ronald C. and Donohue, Michael and Weiner, Michael W.},
doi = {10.1016/j.jalz.2015.05.005},
issn = {15525279},
journal = {Alzheimer's and Dementia},
keywords = {Alzheimer's disease,Amyloid,Cognitive assessment},
month = {jul},
number = {7},
pages = {734--739},
title = {{Alzheimer's Disease Neuroimaging Initiative 2 Clinical Core: Progress and plans}},
url = {http://doi.wiley.com/10.1016/j.jalz.2015.05.005},
volume = {11},
year = {2015}
}
@article{Chen2018,
abstract = {Fully-connected Conditional Random Field (CRF) is often used as post-processing to refine voxel classification results by encouraging spatial coherence. In this paper, we propose a new end-to-end training method called Posterior-CRF. In contrast with previous approaches which use the original image intensity in the CRF, our approach applies 3D, fully connected CRF to the posterior probabilities from a CNN and optimizes both CNN and CRF together. The experiments on white matter hyperintensities segmentation demonstrate that our method outperforms CNN, post-processing CRF and different end-to-end training CRF approaches.},
annote = {{\_}eprint: 1811.03549},
archivePrefix = {arXiv},
arxivId = {1811.03549},
author = {Chen, Shuai and de Bruijne, Marleen},
eprint = {1811.03549},
title = {{An End-to-end Approach to Semantic Segmentation with 3D CNN and Posterior-CRF in Medical Images}},
url = {http://arxiv.org/abs/1811.03549},
year = {2018}
}
@article{Cardona2010,
abstract = {The analysis of microcircuitry (the connectivity at the level of individual neuronal processes and synapses), which is indispensable for our understanding of brain function, is based on serial transmission electron microscopy (TEM) or one of its modern variants. Due to technical limitations, most previous studies that used serial TEM recorded relatively small stacks of individual neurons. As a result, our knowledge of microcircuitry in any nervous system is very limited. We applied the software package TrakEM2 to reconstruct neuronal microcircuitry from TEM sections of a small brain, the early larval brain of Drosophila melanogaster. TrakEM2 enables us to embed the analysis of the TEM image volumes at the microcircuit level into a light microscopically derived neuro-anatomical framework, by registering confocal stacks containing sparsely labeled neural structures with the TEM image volume. We imaged two sets of serial TEM sections of the Drosophila first instar larval brain neuropile and one ventral nerve cord segment, and here report our first results pertaining to Drosophila brain microcircuitry. Terminal neurites fall into a small number of generic classes termed globular, varicose, axiform, and dendritiform. Globular and varicose neurites have large diameter segments that carry almost exclusively presynaptic sites. Dendritiform neurites are thin, highly branched processes that are almost exclusively postsynaptic. Due to the high branching density of dendritiform fibers and the fact that synapses are polyadic, neurites are highly interconnected even within small neuropile volumes. We describe the network motifs most frequently encountered in the Drosophila neuropile. Our study introduces an approach towards a comprehensive anatomical reconstruction of neuronal microcircuitry and delivers microcircuitry comparisons between vertebrate and insect neuropile. {\textcopyright} 2010 Cardona et al.},
author = {Cardona, Albert and Hartenstein, Volker and Saalfeld, Stephan and Preibisch, Stephan and Schmid, Benjamin and Cheng, Anchi and Pulokas, Jim and Tomancak, Pavel},
doi = {10.1371/journal.pbio.1000502},
editor = {Harris, Kristen M.},
issn = {15457885},
journal = {PLoS Biology},
month = {oct},
number = {10},
pages = {e1000502},
title = {{An integrated micro- and macroarchitectural analysis of the Drosophila brain by computer-assisted serial section electron microscopy}},
url = {https://dx.plos.org/10.1371/journal.pbio.1000502},
volume = {8},
year = {2010}
}
@article{Ruder2016,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
annote = {{\_}eprint: 1609.04747},
archivePrefix = {arXiv},
arxivId = {1609.04747},
author = {Ruder, Sebastian},
eprint = {1609.04747},
journal = {CoRR},
title = {{An overview of gradient descent optimization algorithms}},
url = {http://arxiv.org/abs/1609.04747},
volume = {abs/1609.0},
year = {2016}
}
@book{Langer2020,
address = {Cham},
author = {Langer, Arthur M.},
booktitle = {Analysis and Design of Next-Generation Software Architectures},
doi = {10.1007/978-3-030-36899-9},
isbn = {978-3-030-36898-2},
publisher = {Springer International Publishing},
title = {{Analysis and Design of Next-Generation Software Architectures}},
url = {http://link.springer.com/10.1007/978-3-030-36899-9},
year = {2020}
}
@article{Thomas2014,
abstract = {Tractography based on diffusion-weighted MRI (DWI) is widely used for mapping the structural connections of the human brain. Its accuracy is known to be limited by technical factors affecting in vivo data acquisition, such as noise, artifacts, and data undersampling resulting from scan time constraints. It generally is assumed that improvements in data quality and implementation of sophisticated tractography methods will lead to increasingly accurate maps of human anatomical connections. However, assessing the anatomical accuracy of DWI tractography is difficult because of the lack of independent knowledge of the true anatomical connections in humans. Here we investigate the future prospects of DWI-based connectional imaging by applying advanced tractography methods to an ex vivo DWI dataset of the macaque brain. The results of different tractography methods were compared with maps of known axonal projections from previous tracer studies in the macaque. Despite the exceptional quality of the DWI data, none of the methods demonstrated high anatomical accuracy. The methods that showed the highest sensitivity showed the lowest specificity, and vice versa. Additionally, anatomical accuracy was highly dependent upon parameters of the tractography algorithm, with different optimal values for mapping different pathways. These results suggest that there is an inherent limitation in determining long-range anatomical projections based on voxelaveraged estimates of local fiber orientation obtained from DWI data that is unlikely to be overcome by improvements in data acquisition and analysis alone.},
author = {Thomas, Cibu and Ye, Frank Q. and Irfanoglu, M. Okan and Modi, Pooja and Saleem, Kadharbatcha S. and Leopold, David A. and Pierpaoli, Carlo},
doi = {10.1073/pnas.1405672111},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = {nov},
number = {46},
pages = {16574--16579},
title = {{Anatomical accuracy of brain connections derived from diffusion MRI tractography is inherently limited}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1405672111},
volume = {111},
year = {2014}
}
@online{ANT-Day2019,
author = {Day, Trevor K. M. and Madyastha, Tara M. and Boord, Peter and Askren, Mary K. and Montine, Thomas J. and Grabowski, Thomas J.},
title = {{ANT: Healthy aging and Parkinson's disease}},
url = {https://openneuro.org/datasets/ds001907/versions/2.0.3},
urldate = {2020-05-27},
year = {2019}
}
@book{Khalaf2019,
abstract = {In the recent years, the number of web logs, and the amount of opinionated data on the World Wide Web, have been grown substantially. The ability to determine the political orientation of an article automatically can be beneficial in many areas from academia to security. However, the sentiment classification of web log posts (political web log posts in particular), is apparently more complex than the sentiment classification of conventional text. In this paper, a supervised machine learning with two feature extraction techniques Term Frequency (TF) and Term Frequency-Inverse Document Frequency (TF-IDF) are used for the classification process. For investigation, SVM with four kernels for supervised machine learning have been employed. Subsequent to testing, the results reveal that the linear with TF achieved the results in accuracy of 91.935{\%} also with TF-IDF achieved the 95.161{\%}. The linear kernel was deemed the most suitable for our model.},
address = {Cham},
author = {Khalaf, Mohammed I and Al-jumeily, Dhiya and Eds, Alexei Lisitsa},
doi = {10.1007/978-3-030-38752-5},
editor = {Khalaf, Mohammed I. and Al-Jumeily, Dhiya and Lisitsa, Alexei},
isbn = {9783030387518},
keywords = {Term Frequency-Inverse Document Fre,Term frequency,frequency-inverse document frequency {\'{a}},political and machine,support vector machine,svm,term frequency {\'{a}} term,{\'{a}} arabic article {\'{a}}},
pages = {79--94},
publisher = {Springer International Publishing},
series = {Communications in Computer and Information Science},
title = {{Applied Computing to Support Industry}},
url = {http://dx.doi.org/10.1007/978-3-030-38752-5{\_}7},
volume = {2},
year = {2019}
}
@online{KaggleAPTOS2019,
title = {{APTOS 2019 Blindness Detection | Kaggle}},
url = {https://www.kaggle.com/c/aptos2019-blindness-detection}
}
@incollection{Lempitsky2020,
address = {Cham},
author = {Lempitsky, Victor},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_862-1},
pages = {1--6},
publisher = {Springer International Publishing},
title = {{Autoencoder}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}862-1},
year = {2020}
}
@inproceedings{Jin2018,
abstract = {Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet [51], PNAS [29], usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Extensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The code and documentation are available at https://autokeras.com. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.},
archivePrefix = {arXiv},
arxivId = {1806.10282},
author = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/3292500.3330648},
eprint = {1806.10282},
isbn = {9781450362016},
keywords = {AutoML,Automated Machine Learning,Bayesian Optimization,Network Morphism,Neural Architecture Search},
pages = {1946--1956},
title = {{Auto-keras: An efficient neural architecture search system}},
year = {2019}
}
@article{Yue2006,
abstract = {Craniofacial landmark localization and anatomical structure tracing on cephalograms are two important ways to obtain the cephalometric analysis. In order to computerize them in parallel, a model-based approach is proposed to locate 262 craniofacial feature points, including 90 landmarks and 172 auxiliary points. In model training, 12 landmarks are selected as reference points and used to divide every training shape to 10 regions according to the anatomical knowledge; principle components analysis is employed to characterize the region shape variations and the statistical grey profile of every feature point. Locating feature points on an input image is a two-stage procedure. First, we identify the reference landmarks by image processing and pattern matching techniques, so that the shape partition is performed on the input image. Then, for each region, its feature points are located by a modified active shape model. All craniofacial anatomical structures can be traced out by connecting the located points with subdivision curves according to the prior knowledge. Users are permitted to modify the results interactively in many different ways. Experimental results show the advantage and reliability of the proposed method. {\textcopyright} 2006 IEEE.},
author = {Yue, Weining and Yin, Dali and Li, Chengjun and Wang, Guoping and Xu, Tianmin},
doi = {10.1109/TBME.2006.876638},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {ASM,Cephalometry,Edge detection,Landmarks,PCA,Pattern matching,Structure tracing,Superimposition},
month = {aug},
number = {8},
pages = {1615--1623},
title = {{Automated 2-D cephalometric analysis on X-ray images by a model-based approach}},
url = {http://ieeexplore.ieee.org/document/1658156/},
volume = {53},
year = {2006}
}
@article{Liu2019,
abstract = {Accurate diagnosis of thyroid nodules using ultrasonography is a valuable but tough task even for experienced radiologists, considering both benign and malignant nodules have heterogeneous appearances. Computer-aided diagnosis (CAD) methods could potentially provide objective suggestions to assist radiologists. However, the performance of existing learning-based approaches is still limited, for direct application of general learning models often ignores critical domain knowledge related to the specific nodule diagnosis. In this study, we propose a novel deep-learning-based CAD system, guided by task-specific prior knowledge, for automated nodule detection and classification in ultrasound images. Our proposed CAD system consists of two stages. First, a multi-scale region-based detection network is designed to learn pyramidal features for detecting nodules at different feature scales. The region proposals are constrained by the prior knowledge about size and shape distributions of real nodules. Then, a multi-branch classification network is proposed to integrate multi-view diagnosis-oriented features, in which each network branch captures and enhances one specific group of characteristics that were generally used by radiologists. We evaluated and compared our method with the state-of-the-art CAD methods and experienced radiologists on two datasets, i.e. Dataset I and Dataset II. The detection and diagnostic accuracy on Dataset I were 97.5{\%} and 97.1{\%}, respectively. Besides, our CAD system also achieved better performance than experienced radiologists on Dataset II, with improvements of accuracy for 8{\%}. The experimental results demonstrate that our proposed method is effective in the discrimination of thyroid nodules.},
author = {Liu, Tianjiao and Guo, Qianqian and Lian, Chunfeng and Ren, Xuhua and Liang, Shujun and Yu, Jing and Niu, Lijuan and Sun, Weidong and Shen, Dinggang},
doi = {10.1016/j.media.2019.101555},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Clinical knowledge,Convolutional neural networks,Thyroid nodule,Ultrasound image},
pages = {101555},
pmid = {31520984},
title = {{Automated detection and classification of thyroid nodules in ultrasound images using clinical-knowledge-guided convolutional neural networks}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841519300970},
volume = {58},
year = {2019}
}
@article{Yang2014,
abstract = {Purpose To develop an automated magnetic resonance imaging (MRI) parotid segmentation method to monitor radiation-induced parotid gland changes in patients after head and neck radiation therapy (RT).
Methods and Materials The proposed method combines the atlas registration method, which captures the global variation of anatomy, with a machine learning technology, which captures the local statistical features, to automatically segment the parotid glands from the MRIs. The segmentation method consists of 3 major steps. First, an atlas (pre-RT MRI and manually contoured parotid gland mask) is built for each patient. A hybrid deformable image registration is used to map the pre-RT MRI to the post-RT MRI, and the transformation is applied to the pre-RT parotid volume. Second, the kernel support vector machine (SVM) is trained with the subject-specific atlas pair consisting of multiple features (intensity, gradient, and others) from the aligned pre-RT MRI and the transformed parotid volume. Third, the well-trained kernel SVM is used to differentiate the parotid from surrounding tissues in the post-RT MRIs by statistically matching multiple texture features. A longitudinal study of 15 patients undergoing head and neck RT was conducted: baseline MRI was acquired prior to RT, and the post-RT MRIs were acquired at 3-, 6-, and 12-month follow-up examinations. The resulting segmentations were compared with the physicians' manual contours.
Results Successful parotid segmentation was achieved for all 15 patients (42 post-RT MRIs). The average percentage of volume differences between the automated segmentations and those of the physicians' manual contours were 7.98{\%} for the left parotid and 8.12{\%} for the right parotid. The average volume overlap was 91.1{\%} ± 1.6{\%} for the left parotid and 90.5{\%} ± 2.4{\%} for the right parotid. The parotid gland volume reduction at follow-up was 25{\%} at 3 months, 27{\%} at 6 months, and 16{\%} at 12 months.
Conclusions We have validated our automated parotid segmentation algorithm in a longitudinal study. This segmentation method may be useful in future studies to address radiation-induced xerostomia in head and neck radiation therapy.},
author = {Yang, Xiaofeng and Wu, Ning and Cheng, Guanghui and Zhou, Zhengyang and Yu, David S. and Beitler, Jonathan J. and Curran, Walter J. and Liu, Tian},
doi = {10.1016/j.ijrobp.2014.08.350},
issn = {1879355X},
journal = {International Journal of Radiation Oncology Biology Physics},
month = {dec},
number = {5},
pages = {1225--1233},
title = {{Automated segmentation of the parotid gland based on atlas registration and machine learning: A longitudinal mri study in head-and-neck radiation therapy}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0360301614040577},
volume = {90},
year = {2014}
}
@article{Lu2017,
abstract = {Purpose: Segmentation of the liver from abdominal computed tomography (CT) images is an essential step in some computer-assisted clinical interventions, such as surgery planning for living donor liver transplant, radiotherapy and volume measurement. In this work, we develop a deep learning algorithm with graph cut refinement to automatically segment the liver in CT scans. Methods: The proposed method consists of two main steps: (i) simultaneously liver detection and probabilistic segmentation using 3D convolutional neural network; (ii) accuracy refinement of the initial segmentation with graph cut and the previously learned probability map. Results: The proposed approach was validated on forty CT volumes taken from two public databases MICCAI-Sliver07 and 3Dircadb1. For the MICCAI-Sliver07 test dataset, the calculated mean ratios of volumetric overlap error (VOE), relative volume difference (RVD), average symmetric surface distance (ASD), root-mean-square symmetric surface distance (RMSD) and maximum symmetric surface distance (MSD) are 5.9, 2.7 {\%}, 0.91, 1.88 and 18.94 mm, respectively. For the 3Dircadb1 dataset, the calculated mean ratios of VOE, RVD, ASD, RMSD and MSD are 9.36, 0.97 {\%}, 1.89, 4.15 and 33.14 mm, respectively. Conclusions: The proposed method is fully automatic without any user interaction. Quantitative results reveal that the proposed approach is efficient and accurate for hepatic volume estimation in a clinical setup. The high correlation between the automatic and manual references shows that the proposed method can be good enough to replace the time-consuming and nonreproducible manual segmentation method.},
archivePrefix = {arXiv},
arxivId = {1605.03012},
author = {Lu, Fang and Wu, Fa and Hu, Peijun and Peng, Zhiyi and Kong, Dexing},
doi = {10.1007/s11548-016-1467-3},
eprint = {1605.03012},
issn = {18616429},
journal = {International Journal of Computer Assisted Radiology and Surgery},
keywords = {3D convolution neural network,CT images,Graph cut,Liver segmentation},
number = {2},
pages = {171--182},
title = {{Automatic 3D liver location and segmentation via convolutional neural network and graph cut}},
url = {https://doi.org/10.1007/s11548-016-1467-3},
volume = {12},
year = {2017}
}
@article{Kaur2015,
abstract = {Cephalometry is an essential clinical and research tool in orthodontics. It has been used for decades to obtain absolute and relative measures of the craniofacial skeleton. Since manual identification of predefined anatomical landmarks is a very tedious approach, there is a strong need for automated methods. This paper explores the use of Zernike moment-based global features for initial landmark estimation and computing small expectation window for each landmark. Using this expectation window and local template matching based on ring and central projection method, a closer approximation of landmark position is obtained. A smaller search window based on this approximation is used to find the exact location of landmark positions based on template matching using a combination of sum of squared distance and normalized cross-correlation. The system was tested on 18 commonly used landmarks using a dataset of 85 randomly selected cephalograms. A total of 89.5 {\%} of the localization of 18 selected landmarks are within a window of {\$}{\$}$\backslash$le $\backslash$!$\backslash$!$\backslash$pm 2$\backslash$text{\{} mm{\}}{\$}{\$}. The average mean error for the 18 landmarks is 1.84 mm and average SD of mean error is 1.24.},
author = {Kaur, Amandeep and Singh, Chandan},
doi = {10.1007/s11760-013-0432-7},
issn = {18631711},
journal = {Signal, Image and Video Processing},
keywords = {Central projections,Cephalometry,Landmarks,Template matching,Zernike moments},
month = {jan},
number = {1},
pages = {117--132},
title = {{Automatic cephalometric landmark detection using Zernike moments and template matching}},
url = {http://link.springer.com/10.1007/s11760-013-0432-7},
volume = {9},
year = {2015}
}
@inproceedings{Paszke2017,
abstract = {In this article, we describe an automatic differentiation module of PyTorch-a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and Facebook, Zachary Devito and Research, A I and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Srl, Orobix and Lerer, Adam},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
title = {{Automatic differentiation in PyTorch}},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
year = {2019}
}
@article{Deselaers2008,
abstract = {In this paper, the automatic medical annotation task of the 2007 CLEF cross language image retrieval campaign (ImageCLEF) is described. The paper focusses on the images used, the task setup, and the results obtained in the evaluation campaign. Since 2005, the medical automatic image annotation task exists in ImageCLEF with increasing complexity to evaluate the performance of state-of-the-art methods for completely automatic annotation of medical images based on visual properties. The paper also describes the evolution of the task from its origin in 2005-2007. The 2007 task, comprising 11,000 fully annotated training images and 1000 test images to be annotated, is a realistic task with a large number of possible classes at different levels of detail. Detailed analysis of the methods across participating groups is presented with respect to the (i) image representation, (ii) classification method, and (iii) use of the class hierarchy. The results show that methods which build on local image descriptors and discriminative models are able to provide good predictions of the image classes, mostly by using techniques that were originally developed in the machine learning and computer vision domain for object recognition in non-medical images. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Deselaers, Thomas and Deserno, Thomas M. and M{\"{u}}ller, Henning},
doi = {10.1016/j.patrec.2008.03.001},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Automatic image annotation,Benchmark,Evaluation,Medical images},
title = {{Automatic medical image annotation in ImageCLEF 2007: Overview, results, and discussion}},
year = {2008}
}
@article{Peng2017,
author = {Peng, Hanchuan and Zhou, Zhi and Meijering, Erik and Zhao, Ting and Ascoli, Giorgio A. and Hawrylycz, Michael},
doi = {10.1038/nmeth.4233},
issn = {15487105},
journal = {Nature Methods},
month = {apr},
number = {4},
pages = {332--333},
pmid = {28362437},
title = {{Automatic tracing of ultra-volumes of neuronal images}},
url = {http://www.nature.com/articles/nmeth.4233},
volume = {14},
year = {2017}
}
@article{Jaeger2014,
abstract = {Tuberculosis is a major health threat in many regions of the world. Opportunistic infections in immunocompromised HIV/AIDS patients and multi-drug-resistant bacterial strains have exacerbated the problem, while diagnosing tuberculosis still remains a challenge. When left undiagnosed and thus untreated, mortality rates of patients with tuberculosis are high. Standard diagnostics still rely on methods developed in the last century. They are slow and often unreliable. In an effort to reduce the burden of the disease, this paper presents our automated approach for detecting tuberculosis in conventional posteroanterior chest radiographs. We first extract the lung region using a graph cut segmentation method. For this lung region, we compute a set of texture and shape features, which enable the X-rays to be classified as normal or abnormal using a binary classifier. We measure the performance of our system on two datasets: a set collected by the tuberculosis control program of our local county's health department in the United States, and a set collected by Shenzhen Hospital, China. The proposed computer-aided diagnostic system for TB screening, which is ready for field deployment, achieves a performance that approaches the performance of human experts. We achieve an area under the ROC curve (AUC) of 87{\%} (78.3{\%} accuracy) for the first set, and an AUC of 90{\%} (84{\%} accuracy) for the second set. For the first set, we compare our system performance with the performance of radiologists. When trying not to miss any positive cases, radiologists achieve an accuracy of about 82{\%} on this set, and their false positive rate is about half of our system's rate. {\textcopyright} 1982-2012 IEEE.},
author = {Jaeger, Stefan and Karargyris, Alexandros and Candemir, Sema and Folio, Les and Siegelman, Jenifer and Callaghan, Fiona and Xue, Zhiyun and Palaniappan, Kannappan and Singh, Rahul K. and Antani, Sameer and Thoma, George and Wang, Yi Xiang and Lu, Pu Xuan and McDonald, Clement J.},
doi = {10.1109/TMI.2013.2284099},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Computer-aided detection and diagnosis,X-ray imaging,lung,pattern recognition and classification,segmentation,tuberculosis (TB)},
title = {{Automatic tuberculosis screening using chest radiographs}},
year = {2014}
}
@article{He2019,
abstract = {Deep-learning techniques have penetrated all aspects of our lives and brought us great convenience. However, the process of building a high-quality deep-learning system for a specific task is time-consuming, requires extensive resources and relies on human expertise, hindering the further development of deep learning applications in both industry and academia. To alleviate this problem, a growing number of research projects focus on automated machine learning (AutoML). In this paper, we provide a comprehensive and up-to-date study on the state-of-the-art (SOTA) in AutoML. First, we introduce the AutoML techniques in detail, in relation to the machine-learning pipeline. We then summarize existing research on neural architecture search (NAS), as this is one of the most popular topics in the field of AutoML. We also compare the performance of models generated by NAS algorithms with that of human-designed models. Finally, we present several open problems for future research.},
annote = {{\_}eprint: 1908.00709},
archivePrefix = {arXiv},
arxivId = {1908.00709},
author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
eprint = {1908.00709},
title = {{AutoML: A Survey of the State-of-the-Art}},
url = {http://arxiv.org/abs/1908.00709},
year = {2019}
}
@article{Yang2018,
abstract = {Purpose: This report presents the methods and results of the Thoracic Auto-Segmentation Challenge organized at the 2017 Annual Meeting of American Association of Physicists in Medicine. The purpose of the challenge was to provide a benchmark dataset and platform for evaluating performance of autosegmentation methods of organs at risk (OARs) in thoracic CT images. Methods : Sixty thoracic CT scans provided by three different institutions were separated into 36 training, 12 offline testing, and 12 online testing scans. Eleven participants completed the offline challenge, and seven completed the online challenge. The OARs were left and right lungs, heart, esophagus, and spinal cord. Clinical contours used for treatment planning were quality checked and edited to adhere to the RTOG 1106 contouring guidelines. Algorithms were evaluated using the Dice coefficient, Hausdorff distance, and mean surface distance. A consolidated score was computed by normalizing the metrics against interrater variability and averaging over all patients and structures. Results : The interrater study revealed highest variability in Dice for the esophagus and spinal cord, and in surface distances for lungs and heart. Five out of seven algorithms that participated in the online challenge employed deep-learning methods. Although the top three participants using deep learning produced the best segmentation for all structures, there was no significant difference in the performance among them. The fourth place participant used a multi-atlas-based approach. The highest Dice scores were produced for lungs, with averages ranging from 0.95 to 0.98, while the lowest Dice scores were produced for esophagus, with a range of 0.55–0.72. Conclusion : The results of the challenge showed that the lungs and heart can be segmented fairly accurately by various algorithms, while deep-learning methods performed better on the esophagus. Our dataset together with the manual contours for all training cases continues to be available publicly as an ongoing benchmarking resource.},
author = {Yang, Jinzhong and Veeraraghavan, Harini and Armato, Samuel G. and Farahani, Keyvan and Kirby, Justin S. and Kalpathy-Kramer, Jayashree and van Elmpt, Wouter and Dekker, Andre and Han, Xiao and Feng, Xue and Aljabar, Paul and Oliveira, Bruno and van der Heyden, Brent and Zamdborg, Leonid and Lam, Dao and Gooding, Mark and Sharp, Gregory C.},
doi = {10.1002/mp.13141},
issn = {00942405},
journal = {Medical Physics},
keywords = {automatic segmentation,grand challenge,lung cancer,radiation therapy},
number = {10},
pages = {4568--4581},
pmid = {30144101},
title = {{Autosegmentation for thoracic radiation treatment planning: A grand challenge at AAPM 2017}},
volume = {45},
year = {2018}
}
@inproceedings{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82{\%} top-5 test error, exceeding the accuracy of human raters.},
annote = {{\_}eprint: 1502.03167},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1502.03167},
isbn = {9781510810587},
pages = {448--456},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
url = {http://arxiv.org/abs/1502.03167},
volume = {1},
year = {2015}
}
@book{Tsitoara2020,
address = {Berkeley, CA},
author = {Tsitoara, Mariot},
booktitle = {Beginning Git and GitHub},
doi = {10.1007/978-1-4842-5313-7},
isbn = {9781484253120},
publisher = {Apress},
title = {{Beginning Git and GitHub}},
url = {http://link.springer.com/10.1007/978-1-4842-5313-7},
year = {2020}
}
@book{Clark2020,
address = {Berkeley, CA},
author = {Clark, Dan},
doi = {10.1007/978-1-4842-5620-6},
isbn = {978-1-4842-5619-0},
publisher = {Apress},
title = {{Beginning Microsoft Power BI}},
url = {http://link.springer.com/10.1007/978-1-4842-5620-6},
year = {2020}
}
@article{Tobon-Gomez2015,
abstract = {Knowledge of left atrial (LA) anatomy is important for atrial fibrillation ablation guidance, fibrosis quantification and biophysical modelling. Segmentation of the LA from Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) images is a complex problem. This manuscript presents a benchmark to evaluate algorithms that address LA segmentation. The datasets, ground truth and evaluation code have been made publicly available through the http://www.cardiacatlas.org website. This manuscript also reports the results of the Left Atrial Segmentation Challenge (LASC) carried out at the STACOM'13 workshop, in conjunction with MICCAI'13. Thirty CT and 30 MRI datasets were provided to participants for segmentation. Each participant segmented the LA including a short part of the LA appendage trunk and proximal sections of the pulmonary veins (PVs). We present results for nine algorithms for CT and eight algorithms for MRI. Results showed that methodologies combining statistical models with region growing approaches were the most appropriate to handle the proposed task. The ground truth and automatic segmentations were standardised to reduce the influence of inconsistently defined regions (e.g., mitral plane, PVs end points, LA appendage). This standardisation framework, which is a contribution of this work, can be used to label and further analyse anatomical regions of the LA. By performing the standardisation directly on the left atrial surface, we can process multiple input data, including meshes exported from different electroanatomical mapping systems.},
author = {Tobon-Gomez, Catalina and Geers, Arjan J. and Peters, Jochen and Weese, J{\"{u}}rgen and Pinto, Karen and Karim, Rashed and Ammar, Mohammed and Daoudi, Abdelaziz and Margeta, Jan and Sandoval, Zulma and Stender, Birgit and Zheng, Yefeng and Zuluaga, Maria A. and Betancur, Julian and Ayache, Nicholas and Chikh, Mohammed Amine and Dillenseger, Jean Louis and Kelm, B. Michael and Mahmoudi, Sa{\"{i}}d and Ourselin, S{\'{e}}bastien and Schlaefer, Alexander and Schaeffter, Tobias and Razavi, Reza and Rhode, Kawal S.},
doi = {10.1109/TMI.2015.2398818},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Image segmentation,benchmark testing,cardiovascular disease,computed tomography,left atrium,magnetic resonance imaging},
title = {{Benchmark for Algorithms Segmenting the Left Atrium From 3D CT and MRI Datasets}},
year = {2015}
}
@article{Wang2019,
abstract = {Accurate segmentation of infant brain magnetic resonance (MR) images into white matter (WM), gray matter (GM), and cerebrospinal fluid (CSF) is an indispensable foundation for early studying of brain growth patterns and morphological changes in neurodevelopmental disorders. Nevertheless, in the isointense phase (approximately 6-9 months of age), due to inherent myelination and maturation process, WM and GM exhibit similar levels of intensity in both T1-weighted (T1w) and T2-weighted (T2w) MR images, making tissue segmentation very challenging. Despite many efforts were devoted to brain segmentation, only few studies have focused on the segmentation of 6-month infant brain images. With the idea of boosting methodological development in the community, iSeg-2017 challenge (http://iseg2017.web.unc.edu) provides a set of 6-month infant subjects with manual labels for training and testing the participating methods. Among the 21 automatic segmentation methods participating in iSeg-2017, we review the 8 top-ranked teams, in terms of Dice ratio, modified Hausdorff distance and average surface distance, and introduce their pipelines, implementations, as well as source codes. We further discuss limitations and possible future directions. We hope the dataset in iSeg-2017 and this review article could provide insights into methodological development for the community.},
author = {Wang, Li and Nie, Dong and Li, Guannan and Puybareau, Elodie and Dolz, Jose and Zhang, Qian and Wang, Fan and Xia, Jing and Wu, Zhengwang and Chen, Jia-Wei and Thung, Kim-Han and Bui, Toan Duc and Shin, Jitae and Zeng, Guodong and Zheng, Guoyan and Fonov, Vladimir S. and Doyle, Andrew and Xu, Yongchao and Moeskops, Pim and Pluim, Josien P. W. and Desrosiers, Christian and Ayed, Ismail Ben and Sanroma, Gerard and Benkarim, Oualid M. and Casamitjana, Adria and Vilaplana, Veronica and Lin, Weili and Li, Gang and Shen, Dinggang},
doi = {10.1109/tmi.2019.2901712},
issn = {0278-0062},
journal = {IEEE Transactions on Medical Imaging},
month = {sep},
number = {9},
pages = {2219--2230},
title = {{Benchmark on Automatic Six-Month-Old Infant Brain Segmentation Algorithms: The iSeg-2017 Challenge}},
url = {https://ieeexplore.ieee.org/document/8654000/},
volume = {38},
year = {2019}
}
@article{Devlin2018,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5{\%} (7.7{\%} point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
annote = {{\_}eprint: 1810.04805},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
eprint = {1810.04805},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805},
year = {2018}
}
@article{Werbos1974,
author = {Thesis, Science and Appl, Ph D and Harvard, Math},
number = {January 1974},
title = {{Beyond Regression : New Tools for Prediction and Analysis in the Behavioral}},
year = {2018}
}
@inproceedings{Wei2016,
abstract = {It is obvious that big data can bring us new opportunities to discover valuable information. Apparently, corresponding big datasets are powerful tools for scholars, which connect theoretical studies to reality. They can help scholars to evaluate their achievements and find new problems. In recent years, there has been a significant growth in research data repositories and registries. However, these infrastructures are fragmented across institutions, countries and research domains. As such, finding research datasets is not a trivial task for many researchers. Thus we investigated 195 papers regarding big data on some notable international conferences in recent 3 years, and also gathered 285 datasets mentioned in them. In this paper, we present and analyze our survey results in terms of the status quo of big data research and datasets from different aspects. In particular, we propose two different taxonomies of big datasets and classify our surveyed datasets into them. In addition, we also give a brief introduction about 7 widely accepted data collections online. Finally, some basic principles for scholars in choosing and using big datasets are given.},
author = {Wei, Yi and Liu, Shijun and Sun, Jiao and Cui, Lizhen and Pan, Li and Wu, Lei},
booktitle = {Proceedings - 2016 IEEE International Congress on Big Data, BigData Congress 2016},
doi = {10.1109/BigDataCongress.2016.62},
isbn = {9781509026227},
keywords = {Big data,Datasets,Survey},
month = {oct},
pages = {394--401},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Big datasets for research: A survey on flagship conferences}},
year = {2016}
}
@book{Pan2020,
address = {Singapore},
doi = {10.1007/978-981-15-3425-6},
editor = {Pan, Linqiang and Liang, Jing and Qu, Boyang},
isbn = {978-981-15-3424-9},
publisher = {Springer Singapore},
series = {Communications in Computer and Information Science},
title = {{Bio-inspired Computing: Theories and Applications}},
url = {http://link.springer.com/10.1007/978-981-15-3425-6},
volume = {1159},
year = {2020}
}
@article{Zhangb,
abstract = {Multi-organ segmentation is a challenging task due to the label imbalance and  structural differences between different organs. In this work, we propose an efficient cascaded V-Net model to improve the performance of multi-organ segmentation by establishing dense Block Level Skip Connections (BLSC) across cascaded V-Net. Our model can take full advantage of features from the first stage network and make the cascaded structure more efficient. We also combine stacked small and large kernels with an inception-like structure to help our model to learn more patterns, which produces superior results for multi-organ segmentation. In addition, some small organs are commonly occluded by large organs and have unclear boundaries with other surrounding tissues, which makes them hard to be segmented. We therefore first locate the small organs through a multi-class network and crop them randomly with the surrounding region, then segment them with a single-class network. We evaluated our model on SegTHOR 2019 challenge unseen testing set and Multi-Atlas Labeling Beyond the Cranial Vault challenge validation set. Our model has achieved an average dice score gain of 1.62 percents and 3.90 percents compared to traditional cascaded networks on these two datasets, respectively. For hard-to-segment small organs, such as the esophagus in SegTHOR 2019 challenge, our technique has achieved a gain of 5.63 percents on dice score, and four organs in Multi-Atlas Labeling Beyond the Cranial Vault challenge have achieved a gain of 5.27 percents on average dice score.},
author = {Zhang, Liang and Zhang, Jiaming and Shen, Peiyi and Zhu, Guangming and Li, Ping and Lu, Xiaoyuan and Zhang, Huan and Shah, Syed Afaq and Bennamoun, Mohammed},
doi = {10.1109/tmi.2020.2975347},
issn = {0278-0062},
journal = {IEEE Transactions on Medical Imaging},
pages = {1--1},
title = {{Block Level Skip Connections across Cascaded V-Net for Multi-organ Segmentation}},
url = {https://ieeexplore.ieee.org/document/9006924},
year = {2020}
}
@misc{ue-bvs,
abstract = {Brenden Sewell is a lead game designer at E-Line Media, and has spent the last 5 years designing and creating games that are both fun to play and have educational or social impact. He has been building games since 2002, when Neverwinter Nights taught him an invaluable lesson about the expressive power of game design. In 2010, he graduated with a degree in cognitive science from Indiana University. Since then, he has focused on enhancing his own craft of game design while harnessing its power to do good in the world, and exposing more people to the joy the profession holds.},
author = {Sewell, Brenden},
isbn = {9781785286018},
pages = {273},
title = {{Blueprints Visual Scripting for Unreal Engine}},
year = {2015}
}
@article{Chang2019,
abstract = {Vision science, particularly machine vision, has been revolutionized by introducing large-scale image datasets and statistical learning approaches. Yet, human neuroimaging studies of visual perception still rely on small numbers of images (around 100) due to time-constrained experimental procedures. To apply statistical learning approaches that include neuroscience, the number of images used in neuroimaging must be significantly increased. We present BOLD5000, a human functional MRI (fMRI) study that includes almost 5,000 distinct images depicting real-world scenes. Beyond dramatically increasing image dataset size relative to prior fMRI studies, BOLD5000 also accounts for image diversity, overlapping with standard computer vision datasets by incorporating images from the Scene UNderstanding (SUN), Common Objects in Context (COCO), and ImageNet datasets. The scale and diversity of these image datasets, combined with a slow event-related fMRI design, enables fine-grained exploration into the neural representation of a wide range of visual features, categories, and semantics. Concurrently, BOLD5000 brings us closer to realizing Marr's dream of a singular vision science-the intertwined study of biological and computer vision.},
archivePrefix = {arXiv},
arxivId = {1809.01281},
author = {Chang, Nadine and Pyles, John A. and Marcus, Austin and Gupta, Abhinav and Tarr, Michael J. and Aminoff, Elissa M.},
doi = {10.1038/s41597-019-0052-3},
eprint = {1809.01281},
issn = {20524463},
journal = {Scientific data},
month = {sep},
number = {1},
pages = {49},
title = {{BOLD5000, a public fMRI dataset while viewing 5000 visual images}},
url = {http://arxiv.org/abs/1809.01281 http://dx.doi.org/10.1038/s41597-019-0052-3},
volume = {6},
year = {2019}
}
@article{Pereira2016,
abstract = {Among brain tumors, gliomas are the most common and aggressive, leading to a very short life expectancy in their highest grade. Thus, treatment planning is a key stage to improve the quality of life of oncological patients. Magnetic resonance imaging (MRI) is a widely used imaging technique to assess these tumors, but the large amount of data produced by MRI prevents manual segmentation in a reasonable time, limiting the use of precise quantitative measurements in the clinical practice. So, automatic and reliable segmentation methods are required; however, the large spatial and structural variability among brain tumors make automatic segmentation a challenging problem. In this paper, we propose an automatic segmentation method based on Convolutional Neural Networks (CNN), exploring small 3 x 3 kernels. The use of small kernels allows designing a deeper architecture, besides having a positive effect against overfitting, given the fewer number of weights in the network. We also investigated the use of intensity normalization as a pre-processing step, which though not common in CNN-based segmentation methods, proved together with data augmentation to be very effective for brain tumor segmentation in MRI images. Our proposal was validated in the Brain Tumor Segmentation Challenge 2013 database (BRATS 2013), obtaining simultaneously the first position for the complete, core, and enhancing regions in Dice Similarity Coefficient metric (0.88, 0.83, 0.77) for the Challenge data set. Also, it obtained the overall first position by the online evaluation platform. We also participated in the on-site BRATS 2015 Challenge using the same model, obtaining the second place, with Dice Similarity Coefficient metric of 0.78, 0.65, and 0.75 for the complete, core, and enhancing regions, respectively.},
author = {Pereira, Sergio and Pinto, Adriano and Alves, Victor and Silva, Carlos A},
doi = {10.1109/TMI.2016.2538465},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Brain tumor,brain tumor segmentation,convolutional neural networks,deep learning,glioma,magnetic resonance imaging},
number = {5},
pages = {1240--1251},
title = {{Brain Tumor Segmentation Using Convolutional Neural Networks in MRI Images}},
volume = {35},
year = {2016}
}
@misc{Schmainda2018,
abstract = {This collection includes datasets from 20 subjects with primary newly diagnosed glioblastoma who were treated with surgery and standard concomitant chemo-radiation therapy (CRT) followed by adjuvant chemotherapy. Two MRI exams are included for each patient: within 90 days following CRT completion and at progression (determined clinically, and based on a combination of clinical performance and/or imaging findings, and punctuated by a change in treatment or intervention). All image sets are in DICOM format and contain T1w (pre and post-contrast agent), FLAIR, T2w, ADC, normalized cerebral blood flow, normalized relative cerebral blood volume, standardized relative cerebral blood volume, and binary tumor masks (generated using T1w images). The perfusion images were generated from dynamic susceptibility contrast (GRE-EPI DSC) imaging following a preload of contrast agent. All of the series are co-registered with the T1+C images. The intent of this dataset is for assessing deep learning algorithm performance to predict tumor progression.},
author = {Schmainda, K.M. and Prah, M.},
booktitle = {Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2018.15quzvnb},
title = {{Brain-Tumor-Progression}},
url = {http://doi.org/10.7937/K9/TCIA.2018.15quzvnb},
year = {2018}
}
@online{brats13,
title = {{BRATS13 - SICAS Medical Image Repository}},
url = {https://www.virtualskeleton.ch/BRATS/Start2013}
}
@book{Rojas2020,
address = {Berkeley, CA},
author = {Rojas, Carlos},
booktitle = {Building Progressive Web Applications with Vue.js},
doi = {10.1007/978-1-4842-5334-2},
isbn = {978-1-4842-5333-5},
publisher = {Apress},
title = {{Building Progressive Web Applications with Vue.js}},
url = {http://link.springer.com/10.1007/978-1-4842-5334-2},
year = {2020}
}
@book{Olsson2020,
address = {Berkeley, CA},
author = {Olsson, Mikael},
booktitle = {C{\#} 8 Quick Syntax Reference},
doi = {10.1007/978-1-4842-5577-3},
isbn = {978-1-4842-5576-6},
publisher = {Apress},
title = {{C{\#} 8 Quick Syntax Reference}},
url = {http://link.springer.com/10.1007/978-1-4842-5577-3},
year = {2020}
}
@online{CADDementia,
author = {Bron, Esther E. and Klein, Stefan and Smits, Marion and van Swieten, John C. and Niessen, Wiro J.},
title = {{CADDementia - Home}},
url = {https://caddementia.grand-challenge.org/},
year = {2014}
}
@article{Grammatikopoulou2019,
abstract = {Video feedback provides a wealth of information about surgical procedures and is the main sensory cue for surgeons. Scene understanding is crucial to computer assisted interventions (CAI) and to post-operative analysis of the surgical procedure. A fundamental building block of such capabilities is the identification and localization of surgical instruments and anatomical structures through semantic segmentation. Deep learning has advanced semantic segmentation techniques in the recent years but is inherently reliant on the availability of labeled datasets for model training. This paper introduces a dataset for semantic segmentation of cataract surgery videos. The annotated images are part of the publicly available CATARACTS challenge dataset. In addition, we benchmark the performance of several state-of-the-art deep learning models for semantic segmentation on the presented dataset. The dataset is publicly available at https://cataracts.grand-challenge.org/CaDIS/ .},
archivePrefix = {arXiv},
arxivId = {1906.11586},
author = {Grammatikopoulou, Maria and Flouty, Evangello and Kadkhodamohammadi, Abdolrahim and Quellec, Gwenol'e and Chow, Andre and Nehme, Jean and Luengo, Imanol and Stoyanov, Danail},
eprint = {1906.11586},
month = {jun},
title = {{CaDIS: Cataract Dataset for Image Segmentation}},
url = {http://arxiv.org/abs/1906.11586},
year = {2019}
}
@inproceedings{Jia2014,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying generalpurpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (≈ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
booktitle = {MM 2014 - Proceedings of the 2014 ACM Conference on Multimedia},
doi = {10.1145/2647868.2654889},
eprint = {1408.5093},
isbn = {9781450330633},
keywords = {Computer vision,Machine learning,Neural networks,Open source,Parallel computation},
pages = {675--678},
title = {{Caffe: Convolutional architecture for fast feature embedding}},
year = {2014}
}
@article{AlHajj2019,
author = {{Al Hajj}, Hassan and Lamard, Mathieu and Conze, Pierre-Henri and Roychowdhury, Soumali and Hu, Xiaowei and Mar{\v{s}}alkaitė, Gabija and Zisimopoulos, Odysseas and Dedmari, Muneer Ahmad and Zhao, Fenqiang and Prellberg, Jonas and Sahu, Manish and Galdran, Adrian and Ara{\'{u}}jo, Teresa and Vo, Duc My and Panda, Chandan and Dahiya, Navdeep and Kondo, Satoshi and Bian, Zhengbing and Vahdat, Arash and Bialopetravi{\v{c}}ius, Jonas and Flouty, Evangello and Qiu, Chenhui and Dill, Sabrina and Mukhopadhyay, Anirban and Costa, Pedro and Aresta, Guilherme and Ramamurthy, Senthil and Lee, Sang-Woong and Campilho, Aur{\'{e}}lio and Zachow, Stefan and Xia, Shunren and Conjeti, Sailesh and Stoyanov, Danail and Armaitis, Jogundas and Heng, Pheng-Ann and Macready, William G. and Cochener, B{\'{e}}atrice and Quellec, Gwenol{\'{e}}},
doi = {10.1016/j.media.2018.11.008},
issn = {13618415},
journal = {Medical Image Analysis},
month = {feb},
pages = {24--41},
title = {{CATARACTS: Challenge on automatic tool annotation for cataRACT surgery}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S136184151830865X},
volume = {52},
year = {2019}
}
@article{Tessa2019,
author = {Tessa, Carlo and Toschi, Nicola and Orsolini, Stefano and Valenza, Gaetano and Lucetti, Claudio and Barbieri, Riccardo and Diciotti, Stefano},
doi = {10.1371/journal.pone.0210324},
editor = {Koenig, Julian},
issn = {1932-6203},
journal = {PLOS ONE},
month = {jan},
number = {1},
pages = {e0210324},
title = {{Central modulation of parasympathetic outflow is impaired in de novo Parkinson's disease patients}},
url = {https://dx.plos.org/10.1371/journal.pone.0210324},
volume = {14},
year = {2019}
}
@article{Pantoni2010,
abstract = {The term cerebral small vessel disease refers to a group of pathological processes with various aetiologies that affect the small arteries, arterioles, venules, and capillaries of the brain. Age-related and hypertension-related small vessel diseases and cerebral amyloid angiopathy are the most common forms. The consequences of small vessel disease on the brain parenchyma are mainly lesions located in the subcortical structures such as lacunar infarcts, white matter lesions, large haemorrhages, and microbleeds. Because lacunar infarcts and white matter lesions are easily detected by neuroimaging, whereas small vessels are not, the term small vessel disease is frequently used to describe the parenchyma lesions rather than the underlying small vessel alterations. This classification, however, restricts the definition of small vessel disease to ischaemic lesions and might be misleading. Small vessel disease has an important role in cerebrovascular disease and is a leading cause of cognitive decline and functional loss in the elderly. Small vessel disease should be a main target for preventive and treatment strategies, but all types of presentation and complications should be taken into account. {\textcopyright} 2010 Elsevier Ltd. All rights reserved.},
author = {Pantoni, Leonardo},
doi = {10.1016/S1474-4422(10)70104-6},
issn = {14744422},
journal = {The Lancet Neurology},
month = {jul},
number = {7},
pages = {689--701},
pmid = {20610345},
title = {{Cerebral small vessel disease: from pathogenesis and clinical characteristics to therapeutic challenges}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1474442210701046},
volume = {9},
year = {2010}
}
@article{Bontempi2019,
abstract = {Many functional and structural neuroimaging studies call for accurate morphometric segmentation of different brain structures starting from image intensity values of MRI scans. Current automatic (multi-) atlas-based segmentation strategies often lack accuracy on difficult-to-segment brain structures and, since these methods rely on atlas-to-scan alignment, they may take long processing times. Alternatively, recent methods deploying solutions based on Convolutional Neural Networks (CNNs) are enabling the direct analysis of out-of-the-scanner data. However, current CNN-based solutions partition the test volume into 2D or 3D patches, which are processed independently. This process entails a loss of global contextual information, thereby negatively impacting the segmentation accuracy. In this work, we design and test an optimised end-to-end CNN architecture that makes the exploitation of global spatial information computationally tractable, allowing to process a whole MRI volume at once. We adopt a weakly supervised learning strategy by exploiting a large dataset composed of 947 out-of-the-scanner (3 Tesla T1-weighted 1mm isotropic MP-RAGE 3D sequences) MR Images. The resulting model is able to produce accurate multi-structure segmentation results in only a few seconds. Different quantitative measures demonstrate an improved accuracy of our solution when compared to state-of-the-art techniques. Moreover, through a randomised survey involving expert neuroscientists, we show that subjective judgements favour our solution with respect to widely adopted atlas-based software.},
archivePrefix = {arXiv},
arxivId = {1909.05085},
author = {Bontempi, Dennis and Benini, Sergio and Signoroni, Alberto and Svanera, Michele and Muckli, Lars},
doi = {10.1016/j.media.2020.101688},
eprint = {1909.05085},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {3D Image analysis,Brain MRI segmentation,Convolutional neural networks,Weakly supervised learning},
month = {sep},
title = {{CEREBRUM: a fast and fully-volumetric Convolutional Encoder-decodeR for weakly-supervised sEgmentation of BRain strUctures from out-of-the-scanner MRI}},
url = {http://arxiv.org/abs/1909.05085},
volume = {62},
year = {2020}
}
@article{Goodfellow2015,
abstract = {The ICML 2013 Workshop on Challenges in Representation Learning. 11http://deeplearning.net/icml2013-workshop-competition. focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions.},
archivePrefix = {arXiv},
arxivId = {1307.0414},
author = {Goodfellow, Ian J and Erhan, Dumitru and {Luc Carrier}, Pierre and Courville, Aaron and Mirza, Mehdi and Hamner, Ben and Cukierski, Will and Tang, Yichuan and Thaler, David and Lee, Dong Hyun and Zhou, Yingbo and Ramaiah, Chetan and Feng, Fangxiang and Li, Ruifan and Wang, Xiaojie and Athanasakis, Dimitris and Shawe-Taylor, John and Milakov, Maxim and Park, John and Ionescu, Radu and Popescu, Marius and Grozea, Cristian and Bergstra, James and Xie, Jingjing and Romaszko, Lukasz and Xu, Bing and Chuang, Zhang and Bengio, Yoshua},
doi = {10.1016/j.neunet.2014.09.005},
eprint = {1307.0414},
issn = {18792782},
journal = {Neural Networks},
keywords = {Competition,Dataset,Representation learning},
pages = {59--63},
title = {{Challenges in representation learning: A report on three machine learning contests}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608014002159},
volume = {64},
year = {2015}
}
@dataset{CHAOSdata2019,
archivePrefix = {arXiv},
arxivId = {2001.06535v1},
author = {Kavur, Ali Emre and Selver, M. Alper and Dicle, Oğuz and Barış, Mustafa and Gezer, N. Sinem},
booktitle = {arXive:2001.06535v1},
doi = {10.5281/zenodo.3362844},
eprint = {2001.06535v1},
pages = {1--10},
publisher = {Zenodo},
title = {{CHAOS - Combined (CT-MR) Healthy Abdominal Organ Segmentation}},
url = {https://chaos.grand-challenge.org/Combined{\_}Healthy{\_}Abdominal{\_}Organ{\_}Segmentation/},
year = {2019}
}
@article{Kavur2020,
abstract = {Segmentation of abdominal organs has been a comprehensive, yet unresolved, research field for many years. In the last decade, intensive developments in deep learning (DL) have introduced new state-of-the-art segmentation systems. In order to expand the knowledge on these topics, the CHAOS - Combined (CT-MR) Healthy Abdominal Organ Segmentation challenge has been organized in conjunction with IEEE International Symposium on Biomedical Imaging (ISBI), 2019, in Venice, Italy. CHAOS provides both abdominal CT and MR data from healthy subjects for single and multiple abdominal organ segmentation. Five different but complementary tasks have been designed to analyze the capabilities of current approaches from multiple perspectives. The results are investigated thoroughly, compared with manual annotations and interactive methods. The analysis shows that the performance of DL models for single modality (CT / MR) can show reliable volumetric analysis performance (DICE: 0.98 {\$}\backslashpm{\$} 0.00 / 0.95 {\$}\backslashpm{\$} 0.01) but the best MSSD performance remain limited (21.89 {\$}\backslashpm{\$} 13.94 / 20.85 {\$}\backslashpm{\$} 10.63 mm). The performances of participating models decrease significantly for cross-modality tasks for the liver (DICE: 0.88 {\$}\backslashpm{\$} 0.15 MSSD: 36.33 {\$}\backslashpm{\$} 21.97 mm) and all organs (DICE: 0.85 {\$}\backslashpm{\$} 0.21 MSSD: 33.17 {\$}\backslashpm{\$} 38.93 mm). Despite contrary examples on different applications, multi-tasking DL models designed to segment all organs seem to perform worse compared to organ-specific ones (performance drop around 5$\backslash${\%}). Besides, such directions of further research for cross-modality segmentation would significantly support real-world clinical applications. Moreover, having more than 1500 participants, another important contribution of the paper is the analysis on shortcomings of challenge organizations such as the effects of multiple submissions and peeking phenomena.},
archivePrefix = {arXiv},
arxivId = {2001.06535},
author = {Kavur, A. Emre and Gezer, N. Sinem and Barış, Mustafa and Aslan, Sinem and Conze, Pierre-Henri and Groza, Vladimir and Pham, Duc Duy and Chatterjee, Soumick and Ernst, Philipp and {\"{O}}zkan, Savaş and Baydar, Bora and Lachinov, Dmitry and Han, Shuo and Pauli, Josef and Isensee, Fabian and Perkonigg, Matthias and Sathish, Rachana and Rajan, Ronnie and Sheet, Debdoot and Dovletov, Gurbandurdy and Speck, Oliver and N{\"{u}}rnberger, Andreas and Maier-Hein, Klaus H. and Akar, G{\"{o}}zde Bozdağı and {\"{U}}nal, G{\"{o}}zde and Dicle, Oğuz and Selver, M. Alper},
eprint = {2001.06535},
month = {jan},
title = {{CHAOS Challenge -- Combined (CT-MR) Healthy Abdominal Organ Segmentation}},
url = {http://arxiv.org/abs/2001.06535},
year = {2020}
}
@inproceedings{Stirenko2018,
abstract = {The results of chest X-ray (CXR) analysis of 2D images to get the statistically reliable predictions (availability of tuberculosis) by computer-aided diagnosis (CADx) on the basis of deep learning are presented. They demonstrate the efficiency of lung segmentation, lossless and lossy data augmentation for CADx of tuberculosis by deep convolutional neural network (CNN) applied to the small and not well-balanced dataset even. CNN demonstrates ability to train (despite overfitting) on the pre-processed dataset obtained after lung segmentation in contrast to the original not-segmented dataset. Lossless data augmentation of the segmented dataset leads to the lowest validation loss (without overfitting) and nearly the same accuracy (within the limits of standard deviation) in comparison to the original and other pre-processed datasets after lossy data augmentation. The additional limited lossy data augmentation results in the lower validation loss, but with a decrease of the validation accuracy. In conclusion, besides the more complex deep CNNs and bigger datasets, the better progress of CADx for the small and not well-balanced datasets even could be obtained by better segmentation, data augmentation, dataset stratification, and exclusion of non-evident outliers.},
author = {Stirenko, Sergii and Kochura, Yuriy and Alienin, Oleg and Rokovyi, Oleksandr and Gordienko, Yuri and Gang, Peng and Zeng, Wei},
booktitle = {2018 IEEE 38th International Conference on Electronics and Nanotechnology, ELNANO 2018 - Proceedings},
doi = {10.1109/ELNANO.2018.8477564},
isbn = {9781538663837},
keywords = {TensorFlow,chest X-ray,computer-aided diagnosis,convolutional neural network,data augmentation,deep learning,lung,mask,open dataset,segmentation,tuberculosis},
title = {{Chest X-Ray Analysis of Tuberculosis by Deep Learning with Segmentation and Augmentation}},
year = {2018}
}
@inproceedings{Madani2018,
abstract = {{\textcopyright} COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only. Medical imaging datasets are limited in size due to privacy issues and the high cost of obtaining annotations. Augmentation is a widely used practice in deep learning to enrich the data in data-limited scenarios and to avoid overfitting. However, standard augmentation methods that produce new examples of data by varying lighting, field of view, and spatial rigid transformations do not capture the biological variance of medical imaging data and could result in unrealistic images. Generative adversarial networks (GANs) provide an avenue to understand the underlying structure of image data which can then be utilized to generate new realistic samples. In this work, we investigate the use of GANs for producing chest X-ray images to augment a dataset. This dataset is then used to train a convolutional neural network to classify images for cardiovascular abnormalities. We compare our augmentation strategy with traditional data augmentation and show higher accuracy for normal vs abnormal classification in chest X-rays.},
annote = {Backup Publisher: International Society for Optics and Photonics},
author = {Moradi, Mehdi and Madani, Ali and Karargyris, Alexandros and Syeda-Mahmood, Tanveer F.},
booktitle = {Medical Imaging 2018: Image Processing},
doi = {10.1117/12.2293971},
editor = {Angelini, Elsa D and Landman, Bennett A},
isbn = {9781510616370},
issn = {16057422},
keywords = {Convolutional networks,Generative adversarial networks,data augmentation},
pages = {57},
publisher = {SPIE},
title = {{Chest x-ray generation and data augmentation for cardiovascular abnormality classification}},
url = {https://doi.org/10.1117/12.2293971},
volume = {10574},
year = {2018}
}
@inproceedings{Khvostikov2017,
abstract = {Computer-aided early diagnosis of Alzheimer's Disease (AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the subject of extensive research in recent years. Some recent studies have shown promising results in the AD and MCI determination using structural and functional Magnetic Resonance Imaging (sMRI, fMRI), Positron Emission Tomography (PET) and Diffusion Tensor Imaging (DTI) modalities. This paper reviews the major trends in automatic classification methods such as feature extraction based methods as well as deep learning approaches in medical image analysis applied to the field of Alzheimer's Disease diagnostics. Different fusion methodologies to combine heterogeneous image modalities to improve classification scores are also considered.},
author = {Khvostikov, Alexander and Benois-Pineau, Jenny and Krylov, Andrey and Catheline, Gwenaelle},
booktitle = {GraphiCon 2017 - 27th International Conference on Computer Graphics and Vision},
isbn = {9785794429633},
keywords = {Alzheimer's Disease,Convolutional neural networks,Deep learning,Fusion,Machine Learning,Medical imaging,Mild cognitive impairment,Review},
pages = {237--242},
title = {{Classification methods on different brain imaging modalities for Alzheimer disease studies}},
year = {2017}
}
@inproceedings{Wegmayr2018,
abstract = {{\textcopyright} 2018 SPIE. Our ever-aging society faces the growing problem of neurodegenerative diseases, in particular dementia. Magnetic Resonance Imaging provides a unique tool for non-invasive investigation of these brain diseases. However, it is extremely difficult for neurologists to identify complex disease patterns from large amounts of three-dimensional images. In contrast, machine learning excels at automatic pattern recognition from large amounts of data. In particular, deep learning has achieved impressive results in image classification. Unfortunately, its application to medical image classification remains difficult. We consider two reasons for this difficulty: First, volumetric medical image data is considerably scarcer than natural images. Second, the complexity of 3D medical images is much higher compared to common 2D images. To address the problem of small data set size, we assemble the largest dataset ever used for training a deep 3D convolutional neural network to classify brain images as healthy (HC), mild cognitive impairment (MCI) or Alzheimers disease (AD). We use more than 20.000 images from subjects of these three classes, which is almost 9x the size of the previously largest data set. The problem of high dimensionality is addressed by using a deep 3D convolutional neural network, which is state-of-the-art in large-scale image classification. We exploit its ability to process the images directly, only with standard preprocessing, but without the need for elaborate feature engineering. Compared to other work, our workflow is considerably simpler, which increases clinical applicability. Accuracy is measured on the ADNI+AIBL data sets, and the independent CADDementia benchmark.},
author = {Wegmayr, Viktor and Aitharaju, Sai and Buhmann, Joachim},
booktitle = {Medical Imaging 2018: Computer-Aided Diagnosis},
doi = {10.1117/12.2293719},
editor = {Mori, Kensaku and Petrick, Nicholas},
isbn = {9781510616394},
issn = {16057422},
month = {feb},
pages = {63},
publisher = {SPIE},
title = {{Classification of brain MRI with big data and deep 3D convolutional neural networks}},
url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10575/2293719/Classification-of-brain-MRI-with-big-data-and-deep-3D/10.1117/12.2293719.full},
year = {2018}
}
@article{Jimenez-Del-Toro2016,
abstract = {Variations in the shape and appearance of anatomical structures in medical images are often relevant radiological signs of disease. Automatic tools can help automate parts of this manual process. A cloud-based evaluation framework is presented in this paper including results of benchmarking current state-of-the-art medical imaging algorithms for anatomical structure segmentation and landmark detection: the VISCERAL Anatomy benchmarks. The algorithms are implemented in virtual machines in the cloud where participants can only access the training data and can be run privately by the benchmark administrators to objectively compare their performance in an unseen common test set. Overall, 120 computed tomography and magnetic resonance patient volumes were manually annotated to create a standard Gold Corpus containing a total of 1295 structures and 1760 landmarks. Ten participants contributed with automatic algorithms for the organ segmentation task, and three for the landmark localization task. Different algorithms obtained the best scores in the four available imaging modalities and for subsets of anatomical structures. The annotation framework, resulting data set, evaluation setup, results and performance analysis from the three VISCERAL Anatomy benchmarks are presented in this article. Both the VISCERAL data set and Silver Corpus generated with the fusion of the participant algorithms on a larger set of non-manually-annotated medical images are available to the research community.},
author = {Jimenez-Del-Toro, Oscar and Muller, Henning and Krenn, Markus and Gruenberg, Katharina and Taha, Abdel Aziz and Winterstein, Marianne and Eggel, Ivan and Foncubierta-Rodriguez, Antonio and Goksel, Orcun and Jakab, Andras and Kontokotsios, Georgios and Langs, Georg and Menze, Bjoern H. and {Salas Fernandez}, Tomas and Schaer, Roger and Walleyo, Anna and Weber, Marc Andre and {Dicente Cid}, Yashin and Gass, Tobias and Heinrich, Mattias and Jia, Fucang and Kahl, Fredrik and Kechichian, Razmig and Mai, Dominic and Spanier, Assaf B. and Vincent, Graham and Wang, Chunliang and Wyeth, Daniel and Hanbury, Allan},
doi = {10.1109/TMI.2016.2578680},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Evaluation framework,landmark detection,organ segmentation},
month = {nov},
number = {11},
pages = {2459--2475},
title = {{Cloud-Based Evaluation of Anatomical Structure Segmentation and Landmark Detection Algorithms: VISCERAL Anatomy Benchmarks}},
url = {http://ieeexplore.ieee.org/document/7488206/},
volume = {35},
year = {2016}
}
@misc{Zhang2020,
author = {Zhang, S and Yoshida, W and Mano, H and Yanagisawa, T and Shibata, K and Kawato, M and Seymour, B},
doi = {10.18112/OPENNEURO.DS002596.V1.0.0},
publisher = {Openneuro},
title = {{Cognitive control of sensory pain encoding in the pregenual anterior cingulate cortex. d1 - decoder construction in day 1, d2 - adaptive control in day 2.}},
url = {https://openneuro.org/datasets/ds002596/versions/1.0.0},
year = {2020}
}
@article{Kumar2018,
abstract = {The analysis of multi-modality positron emission tomography and computed tomography (PET-CT) images for computer-aided diagnosis applications (e.g., detection and segmentation) requires combining the sensitivity of PET to detect abnormal regions with anatomical localization from CT. Current methods for PET-CT image analysis either process the modalities separately or fuse information from each modality based on knowledge about the image analysis task. These methods generally do not consider the spatially varying visual characteristics that encode different information across different modalities, which have different priorities at different locations. For example, a high abnormal PET uptake in the lungs is more meaningful for tumor detection than physiological PET uptake in the heart. Our aim is to improve the fusion of the complementary information in multi-modality PET-CT with a new supervised convolutional neural network (CNN) that learns to fuse complementary information for multi-modality medical image analysis. Our CNN first encodes modality-specific features and then uses them to derive a spatially varying fusion map that quantifies the relative importance of each modality's feature across different spatial locations. These fusion maps are then multiplied with the modality-specific feature maps to obtain a representation of the complementary multi-modality information at different locations, which can then be used for image analysis. We evaluated the ability of our CNN to detect and segment multiple regions (lungs, mediastinum, and tumors) with different fusion requirements using a dataset of PET-CT images of lung cancer. We compared our method to baseline techniques for multi-modality image fusion (fused inputs (FSs), multi-branch (MB) techniques, and multi-channel (MC) techniques) and segmentation. Our findings show that our CNN had a significantly higher foreground detection accuracy (99.29{\%}, p {\textless} {\{}0.05) than the fusion baselines (FS: 99.00{\%}, MB: 99.08{\%}, and TC: 98.92{\%}) and a significantly higher Dice score (63.85{\%}) than the recent PET-CT tumor segmentation methods.}},
annote = {{\_}eprint: 1810.02492},
archivePrefix = {arXiv},
arxivId = {1810.02492},
author = {Kumar, Ashnil and Fulham, Michael and Feng, Dagan and Kim, Jinman},
doi = {10.1109/TMI.2019.2923601},
eprint = {1810.02492},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Multi-modality imaging,PET-CT,deep learning,fusion learning},
number = {1},
pages = {204--217},
title = {{Co-Learning Feature Fusion Maps from PET-CT Images of Lung Cancer}},
volume = {39},
year = {2020}
}
@incollection{Ebner2020,
address = {Cham},
author = {Ebner, Marc},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_454-1},
pages = {1--9},
publisher = {Springer International Publishing},
title = {{Color Constancy}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}454-1},
year = {2020}
}
@article{Reinhard2001,
author = {Reinhard, Erik and Ashikhmin, Michael and Gooch, Bruce and Shirley, Peter},
doi = {10.1109/38.946629},
issn = {02721716},
journal = {IEEE Computer Graphics and Applications},
number = {5},
pages = {34--41},
title = {{Color transfer between images}},
volume = {21},
year = {2001}
}
@article{Magee2009,
abstract = {Abstract. Colour consistency in light microscopy based histology is an increasingly important problem with the advent of Gigapixel digital slide scanners and automatic image analysis. This paper presents an evaluation of two novel colour normalisation approaches against the previously utilised method of linear normalisation in l$\alpha$$\beta$ colourspace. These approaches map the colour distribution of an over/under stained image to that of a well stained target image. The first novel approach presented is a multi-modal extension to linear normalisation in l$\alpha$$\beta$ colourspace using an automatic image segmentation method and defining separate transforms for each class. The second approach normalises in a representation space obtained using stain specific colour deconvolution. Additionally, we present a method for estimation of the required colour deconvolution vectors directly from the image data. Our evaluation demonstrates the inherent variability in the original data, the known theoretical problems with linear normalisation in l$\alpha$$\beta$ colourspace, and that a multi-modal colour deconvolution based approach overcomes these problems. The segmentation based approach, while producing good results on the majority of images, is less successful than the colour deconvolution method for a significant minority of images as robust segmentation is required to avoid introducing artifacts.},
author = {{Magee D., Treanor D., Crellin D., Shires M., Smith K., Mohee K.}, Quirke P and {Magee D., Treanor D., Crellin D., Shires M., Smith K., Mohee K.}, and Quirke P},
journal = {Optical Tissue Image analysis in Microscopy, Histopathology and Endoscopy (MICCAI Workshop)},
pages = {100--111},
title = {{Colour Normalisation in Digital Histopathology Images}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.157.5405},
year = {2009}
}
@article{BrendanMcMahan2017,
abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10–100x as compared to synchronized stochastic gradient descent.},
archivePrefix = {arXiv},
arxivId = {1602.05629},
author = {{Brendan McMahan}, H. and Moore, Eider and Ramage, Daniel and Hampson, Seth and {Ag{\"{u}}era y Arcas}, Blaise},
eprint = {1602.05629},
journal = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017},
month = {feb},
title = {{Communication-efficient learning of deep networks from decentralized data}},
url = {http://arxiv.org/abs/1602.05629},
year = {2017}
}
@article{Heimann2009,
abstract = {This paper presents a comparison study between 10 automatic and six interactive methods for liver segmentation from contrast-enhanced CT images. It is based on results from the "MICCAI 2007 Grand Challenge" workshop, where 16 teams evaluated their algorithms on a common database. A collection of 20 clinical images with reference segmentations was provided to train and tune algorithms in advance. Participants were also allowed to use additional proprietary training data for that purpose. All teams then had to apply their methods to 10 test datasets and submit the obtained results. Employed algorithms include statistical shape models, atlas registration, level-sets, graph-cuts and rule-based systems. All results were compared to reference segmentations five error measures that highlight different aspects of segmentation accuracy. All measures were combined according to a specific scoring system relating the obtained values to human expert variability. In general, interactive methods reached higher average scores than automatic approaches and featured a better consistency of segmentation quality. However, the best automatic methods (mainly based on statistical shape models with some additional free deformation) could compete well on the majority of test images. The study provides an insight in performance of different segmentation approaches under real-world conditions and highlights achievements and limitations of current image analysis techniques. {\textcopyright} 2009 IEEE.},
author = {Heimann, Tobias and {Van Ginneken}, Brain and Styner, Martin A. and Arzhaeva, Yulia and Aurich, Volker and Bauer, Christian and Beck, Andreas and Becker, Christoph and Beichel, Reinhard and Bekes, Gy{\"{o}}rgy and Bello, Fernando and Binnig, Gerd and Bischof, Horst and Bornik, Alexander and Cashman, Peter M.M. and Chi, Ying and C{\'{o}}rdova, Andr{\'{e}}s and Dawant, Benoit M. and Fidrich, M{\'{a}}rta and Furst, Jacob D. and Furukawa, Daisuke and Grenacher, Lars and Hornegger, Joachim and Kainm{\"{u}}ller, Dagmar and Kitney, Richard I. and Kobatake, Hidefumi and Lamecker, Hans and Lange, Thomas and Lee, Jeongjin and Lennon, Brian and Li, Rui and Li, Senhu and Meinzer, Hans Peter and N{\'{e}}meth, G{\'{a}}bor and Raicu, Daniela S. and Rau, Anne Mareike and {Van Rikxoort}, Eva M. and Rousson, Mika{\"{e}}l and Rusk{\'{o}}, L{\'{a}}szlo and Saddi, Kinda A. and Schmidt, G{\"{u}}nter and Seghers, Dieter and Shimizu, Akinobu and Slagmolen, Pieter and Sorantin, Erich and Soza, Grzegorz and Susomboon, Ruchaneewan and Waite, Jonathan M. and Wimmer, Andreas and Wolf, Ivo},
doi = {10.1109/TMI.2009.2013851},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Evaluation,Liver,Segmentation},
pmid = {19211338},
title = {{Comparison and evaluation of methods for liver segmentation from CT datasets}},
year = {2009}
}
@article{Kavur2020a,
abstract = {PURPOSE We aimed to compare the accuracy and repeatability of emerging machine learning-based (i.e., deep learning) automatic segmentation algorithms with those of well-established interactive semi-automatic methods for determining liver volume in living liver transplant donors at computed tomography (CT) imaging. METHODS A total of 12 methods (6 semi-automatic, 6 full-automatic) were evaluated. The semi-automatic segmentation algorithms were based on both traditional iterative models including watershed, fast marching, region growing, active contours and modern techniques including robust statistics segmenter and super-pixels. These methods entailed some sort of interaction mechanism such as placing initialization seeds on images or determining a parameter range. The automatic methods were based on deep learning and included three framework templates (DeepMedic, NiftyNet and U-Net), the first two of which were applied with default parameter sets and the last two involved adapted novel model designs. For 20 living donors (8 training and 12 test data-sets), a group of imaging scientists and radiologists created ground truths by performing manual segmentations on contrast-enhanced CT images. Each segmentation was evaluated using five metrics (i.e., volume overlap and relative volume errors, average/root-mean-square/maximum symmetrical surface distances). The results were mapped to a scoring system and a final grade was calculated by taking their average. Accuracy and repeatability were evaluated using slice-by-slice comparisons and volumetric analysis. Diversity and complementarity were observed through heatmaps. Majority voting (MV) and simultaneous truth and performance level estimation (STAPLE) algorithms were utilized to obtain the fusion of the individual results. RESULTS The top four methods were automatic deep learning models, with scores of 79.63, 79.46, 77.15, and 74.50. Intra-user score was determined as 95.14. Overall, automatic deep learning segmentation outperformed interactive techniques on all metrics. The mean volume of liver of ground truth was 1409.93±271.28 mL, while it was calculated as 1342.21±231.24 mL using automatic and 1201.26±258.13 mL using interactive methods, showing higher accuracy and less variation with automatic methods. The qualitative analysis of segmentation results showed significant diversity and complementarity, enabling the idea of using ensembles to obtain superior results. The fusion score of automatic methods reached 83.87 with MV and 86.20 with STAPLE, which were only slightly less than fusion of all methods (MV, 86.70) and (STAPLE, 88.74). CONCLUSION Use of the new deep learning-based automatic segmentation algorithms substantially increases the accuracy and repeatability for segmentation and volumetric measurements of liver. Fusion of automatic methods based on ensemble approaches exhibits best results with almost no additional time cost due to potential parallel execution of multiple models.},
author = {Kavur, A. Emre and Gezer, Naciye Sinem and Barış, Mustafa and Şahin, Yusuf and {\"{O}}zkan, Savaş and Baydar, Bora and Y{\"{u}}ksel, Ulaş and Kılık{\c{c}}ıer, {\c{C}}ağlar and Olut, Şahin and Akar, G{\"{o}}zde Bozdağı and {\"{U}}nal, G{\"{o}}zde and Dicle, Oğuz and Selver, M. Alper},
doi = {10.5152/dir.2019.19025},
issn = {13053612},
journal = {Diagnostic and Interventional Radiology},
month = {jan},
number = {1},
pages = {11--21},
pmid = {31904568},
title = {{Comparison of semi-automatic and deep learning-based automatic methods for liver segmentation in living liver transplant donors}},
url = {https://www.dirjournal.org/en/comparison-of-semi-automatic-and-deep-learning-based-automatic-methods-for-liver-segmentation-in-living-liver-transplant-donors-132076},
volume = {26},
year = {2020}
}
@incollection{Sankaranarayanan2020,
address = {Cham},
author = {Sankaranarayanan, Aswin C. and Baraniuk, Richard G.},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_647-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Compressive Sensing}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}647-1},
year = {2020}
}
@article{Oh2016,
abstract = {In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.},
archivePrefix = {arXiv},
arxivId = {1605.09128},
author = {Oh, Junhyuk and Chockalingam, Valliappa and Singh, Satinder and Lee, Honglak},
eprint = {1605.09128},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
month = {may},
pages = {4067--4089},
title = {{Control of memory, active perception, and action in minecraft}},
url = {http://arxiv.org/abs/1605.09128},
volume = {6},
year = {2016}
}
@inproceedings{Mou2014,
abstract = {Programming language processing (similar to natural language processing) is a hot research topic in the field of software engineering; it has also aroused growing interest in the artificial intelligence community. However, different from a natural language sentence, a program contains rich, explicit, and complicated structural information. Hence, traditional NLP models may be inappropriate for programs. In this paper, we propose a novel tree-based convolutional neural network (TBCNN) for programming language processing, in which a convolution kernel is designed over programs' abstract syntax trees to capture structural information. TBCNN is a generic architecture for programming language processing; our experiments show its effectiveness in two different program analysis tasks: classifying programs according to functionality, and detecting code snippets of certain patterns. TBCNN outperforms baseline methods, including several neural models for NLP.},
annote = {{\_}eprint: 1409.5718},
archivePrefix = {arXiv},
arxivId = {1409.5718},
author = {Mou, Lili and Li, Ge and Zhang, Lu and Wang, Tao and Jin, Zhi},
booktitle = {30th AAAI Conference on Artificial Intelligence, AAAI 2016},
eprint = {1409.5718},
isbn = {9781577357605},
pages = {1287--1293},
title = {{Convolutional neural networks over tree structures for programming language processing}},
url = {http://arxiv.org/abs/1409.5718},
volume = {abs/1409.5},
year = {2016}
}
@article{Rusu2017,
abstract = {Objective: To develop an approach for radiology-pathology fusion of ex vivo histology of surgically excised pulmonary nodules with pre-operative CT, to radiologically map spatial extent of the invasive adenocarcinomatous component of the nodule. Methods: Six subjects (age: 75 ± 11 years) with pre-operative CT and surgically excised ground-glass nodules (size: 22.5 ± 5.1 mm) with a significant invasive adenocarcinomatous component ({\textgreater}5 mm) were included. The pathologist outlined disease extent on digitized histology specimens; two radiologists and a pulmonary critical care physician delineated the entire nodule on CT (in-plane resolution: {\textless}0.8 mm, inter-slice distance: 1–5 mm). We introduced a novel reconstruction approach to localize histology slices in 3D relative to each other while using CT scan as spatial constraint. This enabled the spatial mapping of the extent of tumour invasion from histology onto CT. Results: Good overlap of the 3D reconstructed histology and the nodule outlined on CT was observed (65.9 ± 5.2{\%}). Reduction in 3D misalignment of corresponding anatomical landmarks on histology and CT was observed (1.97 ± 0.42 mm). Moreover, the CT attenuation (HU) distributions were different when comparing invasive and in situ regions. Conclusion: This proof-of-concept study suggests that our fusion method can enable the spatial mapping of the invasive adenocarcinomatous component from 2D histology slices onto in vivo CT. Key Points: • 3D reconstructions are generated from 2D histology specimens of ground glass nodules. • The reconstruction methodology used pre-operative in vivo CT as 3D spatial constraint. • The methodology maps adenocarcinoma extent from digitized histology onto in vivo CT. • The methodology potentially facilitates the discovery of CT signature of invasive adenocarcinoma.},
author = {Rusu, Mirabela and Rajiah, Prabhakar and Gilkeson, Robert and Yang, Michael and Donatelli, Christopher and Thawani, Rajat and Jacono, Frank J and Linden, Philip and Madabhushi, Anant},
doi = {10.1007/s00330-017-4813-0},
issn = {14321084},
journal = {European Radiology},
keywords = {Computed tomography,Computer-assisted image processing,Lung adenocarcinoma,Multimodal imaging,Pathology},
number = {10},
pages = {4209--4217},
title = {{Co-registration of pre-operative CT with ex vivo surgically excised ground glass nodules to define spatial extent of invasive adenocarcinoma on in vivo imaging: a proof-of-concept study}},
url = {https://doi.org/10.1007/s00330-017-4813-0},
volume = {27},
year = {2017}
}
@book{Raposo2020,
address = {Cham},
author = {Raposo, Maria and Ribeiro, Paulo and S{\'{e}}rio, Susana and Staiano, Antonino and Ciaramella, Angelo},
doi = {10.1007/978-3-030-34585-3_31},
editor = {Raposo, Maria and Ribeiro, Paulo and S{\'{e}}rio, Susana and Staiano, Antonino and Ciaramella, Angelo},
isbn = {978-3-030-34584-6},
pages = {C1--C1},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Correction to: Computational Intelligence Methods for Bioinformatics and Biostatistics}},
url = {http://link.springer.com/10.1007/978-3-030-34585-3},
volume = {11925},
year = {2020}
}
@article{Carreras2015,
abstract = {To stimulate progress in automating the reconstruction of neural circuits, we organized the first international challenge on 2D segmentation of electron microscopic(EM) images of the brain. Participants submitted boundary map spredicted for a test set of images, and were scored based on their agreement with a on sensus of human expert annotations. The winning team had no prior experience with EM images, and employed a convolutional network. This “deeplearning” approach has since become accepted as a standard for segmentation of EM images. The challenge has continued to accept submissions, and the best so far has resulted from co-operation between two teams. The challenge has probably saturated, as algorithms cannot progress beyond limits set by ambiguities inherent in 2D scoring and the size of the test data set. Retrospective evaluation of the challenges coring system reveals that it was not sufficiently robust to variations in the widths of neurite borders. We propose a solution to this problem, which should be useful for a future 3D segmentation challenge.},
author = {Carreras, Ignacio Arganda and Turaga, Srinivas C. and Berger, Daniel R. and San, Dan Cire and Giusti, Alessandro and Gambardella, Luca M. and Schmidhuber, J{\"{u}}rgen and Laptev, Dmitry and Dwivedi, Sarvesh and Buhmann, Joachim M. and Liu, Ting and Seyedhosseini, Mojtaba and Tasdizen, Tolga and Kamentsky, Lee and Burget, Radim and Uher, Vaclav and Tan, Xiao and Sun, Changming and Pham, Tuan D. and Bas, Erhan and Uzunbas, Mustafa G. and Cardona, Albert and Schindelin, Johannes and Seung, H. Sebastian},
doi = {10.3389/fnana.2015.00142},
issn = {16625129},
journal = {Frontiers in Neuroanatomy},
keywords = {Connectomics,Electron microscopy,Image segmentation,Machine learning,Reconstruction},
month = {nov},
number = {November},
pages = {1--13},
title = {{Crowdsourcing the creation of image segmentation algorithms for connectomics}},
url = {http://journal.frontiersin.org/Article/10.3389/fnana.2015.00142/abstract},
volume = {9},
year = {2015}
}
@article{Rister2018a,
abstract = {Fully-convolutional neural networks have achieved superior performance in a variety of image segmentation tasks. However, their training requires laborious manual annotation of large datasets, as well as acceleration by parallel processors with high-bandwidth memory, such as GPUs. We show that simple models can achieve competitive accuracy for organ segmentation on CT images when trained with extensive data augmentation, which leverages existing graphics hardware to quickly apply geometric and photometric transformations to 3D image data. On 3 mm{\^{}}3 CT volumes, our GPU implementation is 2.6-8X faster than a widely-used CPU version, including communication overhead. We also show how to automatically generate training labels using rudimentary morphological operations, which are efficiently computed by 3D Fourier transforms. We combined fully-automatic labels for the lungs and bone with semi-automatic ones for the liver, kidneys and bladder, to create a dataset of 130 labeled CT scans. To achieve the best results from data augmentation, our model uses the intersection-over-union (IOU) loss function, a close relative of the Dice loss. We discuss its mathematical properties and explain why it outperforms the usual weighted cross-entropy loss for unbalanced segmentation tasks. We conclude that there is no unique IOU loss function, as the naive one belongs to a broad family of functions with the same essential properties. When combining data augmentation with the IOU loss, our model achieves a Dice score of 78-92{\%} for each organ. The trained model, code and dataset will be made publicly available, to further medical imaging research.},
archivePrefix = {arXiv},
arxivId = {1811.11226},
author = {Rister, Blaine and Yi, Darvin and Shivakumar, Kaushik and Nobashi, Tomomi and Rubin, Daniel L.},
eprint = {1811.11226},
month = {nov},
title = {{CT organ segmentation using GPU data augmentation, unsupervised labels and IOU loss}},
url = {http://arxiv.org/abs/1811.11226},
year = {2018}
}
@article{Rister2018,
abstract = {Fully-convolutional neural networks have achieved superior performance in a variety of image segmentation tasks. However, their training requires laborious manual annotation of large datasets, as well as acceleration by parallel processors with high-bandwidth memory, such as GPUs. We show that simple models can achieve competitive accuracy for organ segmentation on CT images when trained with extensive data augmentation, which leverages existing graphics hardware to quickly apply geometric and photometric transformations to 3D image data. On 3 mm{\^{}}3 CT volumes, our GPU implementation is 2.6-8X faster than a widely-used CPU version, including communication overhead. We also show how to automatically generate training labels using rudimentary morphological operations, which are efficiently computed by 3D Fourier transforms. We combined fully-automatic labels for the lungs and bone with semi-automatic ones for the liver, kidneys and bladder, to create a dataset of 130 labeled CT scans. To achieve the best results from data augmentation, our model uses the intersection-over-union (IOU) loss function, a close relative of the Dice loss. We discuss its mathematical properties and explain why it outperforms the usual weighted cross-entropy loss for unbalanced segmentation tasks. We conclude that there is no unique IOU loss function, as the naive one belongs to a broad family of functions with the same essential properties. When combining data augmentation with the IOU loss, our model achieves a Dice score of 78-92{\%} for each organ. The trained model, code and dataset will be made publicly available, to further medical imaging research.},
annote = {{\_}eprint: 1811.11226},
archivePrefix = {arXiv},
arxivId = {1811.11226},
author = {Rister, Blaine and Yi, Darvin and Shivakumar, Kaushik and Nobashi, Tomomi and Rubin, Daniel L},
eprint = {1811.11226},
journal = {CoRR},
title = {{CT organ segmentation using GPU data augmentation, unsupervised labels and IOU loss}},
url = {http://arxiv.org/abs/1811.11226},
volume = {abs/1811.1},
year = {2018}
}
@article{Mirsky2019,
abstract = {In 2018, clinics and hospitals were hit with numerous attacks leading to significant data breaches and interruptions in medical services. An attacker with access to medical records can do much more than hold the data for ransom or sell it on the black market. In this paper, we show how an attacker can use deep-learning to add or remove evidence of medical conditions from volumetric (3D) medical scans. An attacker may perform this act in order to stop a political candidate, sabotage research, commit insurance fraud, perform an act of terrorism, or even commit murder. We implement the attack using a 3D conditional GAN and show how the framework (CT-GAN) can be automated. Although the body is complex and 3D medical scans are very large, CT-GAN achieves realistic results which can be executed in milliseconds. To evaluate the attack, we focused on injecting and removing lung cancer from CT scans. We show how three expert radiologists and a state-of-the-art deep learning AI are highly susceptible to the attack. We also explore the attack surface of a modern radiology network and demonstrate one attack vector: we intercepted and manipulated CT scans in an active hospital network with a covert penetration test.},
archivePrefix = {arXiv},
arxivId = {1901.03597},
author = {Mirsky, Yisroel and Mahler, Tom and Shelef, Ilan and Elovici, Yuval},
eprint = {1901.03597},
isbn = {9781939133069},
journal = {Proceedings of the 28th USENIX Security Symposium},
month = {jan},
pages = {461--478},
title = {{CT-GAN: Malicious tampering of 3D medical imagery using deep learning}},
url = {http://arxiv.org/abs/1901.03597},
year = {2019}
}
@misc{TCIA-CT-ORG,
author = {Rister, Blaine and Shivakumar, Kaushik and Nobashi, Tomomi and Rubin, Daniel L},
doi = {10.7937/TCIA.2019.TT7F4V7O},
publisher = {The Cancer Imaging Archive},
title = {{CT-ORG: A Dataset of CT Volumes With Multiple Organ Segmentations}},
url = {https://wiki.cancerimagingarchive.net/x/OgWkAw},
year = {2019}
}
@article{Chetlur2014,
abstract = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36{\%} on a standard model while also reducing memory consumption.},
archivePrefix = {arXiv},
arxivId = {1410.0759},
author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
eprint = {1410.0759},
journal = {arXiv: Neural and Evolutionary Computing},
title = {{cuDNN: Efficient Primitives for Deep Learning}},
url = {http://arxiv.org/abs/1410.0759},
year = {2014}
}
@book{Florentina2019,
address = {Cham},
author = {Florentina, Laura and Eds, Stoica},
doi = {10.1007/978-3-030-39237-6},
editor = {Simian, Dana and Stoica, Laura Florentina},
isbn = {9783030392369},
publisher = {Springer International Publishing},
series = {Communications in Computer and Information Science},
title = {{Dana Simian Modelling and Development of Intelligent Systems}},
url = {http://link.springer.com/10.1007/978-3-030-39237-6},
volume = {1126},
year = {2019}
}
@inproceedings{Liu2018,
abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
archivePrefix = {arXiv},
arxivId = {1806.09055},
author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
booktitle = {7th International Conference on Learning Representations, ICLR 2019},
eprint = {1806.09055},
title = {{DARTS: Differentiable architecture search}},
volume = {abs/1806.0},
year = {2019}
}
@article{Takahashi2019,
abstract = {Deep convolutional neural networks (CNNs) have achieved remarkable results in image processing tasks. However, their high expression ability risks overfitting. Consequently, data augmentation techniques have been proposed to prevent overfitting while enriching datasets. Recent CNN architectures with more parameters are rendering traditional data augmentation techniques insufficient. In this study, we propose a new data augmentation technique called random image cropping and patching (RICAP) which randomly crops four images and patches them to create a new training image. Moreover, RICAP mixes the class labels of the four images, resulting in an advantage similar to label smoothing. We evaluated RICAP with current state-of-the-art CNNs (e.g., the shake-shake regularization model) by comparison with competitive data augmentation techniques such as cutout and mixup. RICAP achieves a new state-of-the-art test error of {\$}2.19\backslash{\%}{\$} on CIFAR-10. We also confirmed that deep CNNs with RICAP achieve better results on classification tasks using CIFAR-100 and ImageNet and an image-caption retrieval task using Microsoft COCO.},
annote = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
archivePrefix = {arXiv},
arxivId = {1811.09030},
author = {Takahashi, Ryo and Matsubara, Takashi and Uehara, Kuniaki},
doi = {10.1109/tcsvt.2019.2935128},
eprint = {1811.09030},
issn = {1051-8215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
pages = {1--1},
title = {{Data Augmentation using Random Image Cropping and Patching for Deep CNNs}},
url = {http://dx.doi.org/10.1109/TCSVT.2019.2935128},
year = {2019}
}
@article{Lu2020,
abstract = {The rapidly emerging field of computational pathology has the potential to enable objective diagnosis, therapeutic response prediction and identification of new morphological features of clinical relevance. However, deep learning-based computational pathology approaches either require manual annotation of gigapixel whole slide images (WSIs) in fully-supervised settings or thousands of WSIs with slide-level labels in a weakly-supervised setting. Moreover, whole slide level computational pathology methods also suffer from domain adaptation and interpretability issues. These challenges have prevented the broad adaptation of computational pathology for clinical and research purposes. Here we present CLAM - Clustering-constrained attention multiple instance learning, an easy-to-use, high-throughput, and interpretable WSI-level processing and learning method that only requires slide-level labels while being data efficient, adaptable and capable of handling multi-class subtyping problems. CLAM is a deep-learning-based weakly-supervised method that uses attention-based learning to automatically identify sub-regions of high diagnostic value in order to accurately classify the whole slide, while also utilizing instance-level clustering over the representative regions identified to constrain and refine the feature space. In three separate analyses, we demonstrate the data efficiency and adaptability of CLAM and its superior performance over standard weakly-supervised classification. We demonstrate that CLAM models are interpretable and can be used to identify well-known and new morphological features. We further show that models trained using CLAM are adaptable to independent test cohorts, cell phone microscopy images, and biopsies. CLAM is a general-purpose and adaptable method that can be used for a variety of different computational pathology tasks in both clinical and research settings.},
archivePrefix = {arXiv},
arxivId = {2004.09666},
author = {Lu, Ming Y. and Williamson, Drew F. K. and Chen, Tiffany Y. and Chen, Richard J. and Barbieri, Matteo and Mahmood, Faisal},
eprint = {2004.09666},
month = {apr},
title = {{Data Efficient and Weakly Supervised Computational Pathology on Whole Slide Images}},
url = {http://arxiv.org/abs/2004.09666},
year = {2020}
}
@misc{Cardenas2019,
author = {Cardenas, Carlos E and Mohamed, A and Sharp, G. and Gooding, M. and Veeraraghavan, H. and Yang, J.},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/tcia.2019.bcfjqfqb},
publisher = {The Cancer Imaging Archive},
title = {{Data from AAPM RT-MAC Grand Challenge 2019}},
url = {https://wiki.cancerimagingarchive.net/x/bAP9Ag},
year = {2019}
}
@misc{TCIA-C4KC-KiTS,
author = {Heller, N. and Sathianathen, N. and {Kalapara, A., Walczak}, E. and Moore, K. and Kaluzniak, H. and Rosenberg, J. and Blake, P. and Rengel, Z. and Oestreich, M. and Dean, J. and Tradewell, M. and Shah, A. and Tejpaul, R. and Edgerton, Z. and Peterson, M. and Raza, S. and Regmi, S. and Papanikolopoulos, N. and Weight, C.},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/TCIA.2019.IX49E8NX},
publisher = {The Cancer Imaging Archive},
title = {{Data from C4KC-KiTS [Data set]}},
url = {https://wiki.cancerimagingarchive.net/x/UwakAw},
year = {2019}
}
@misc{Vallieres2017a,
author = {Valli{\`{e}}res, Martin and Kay-Rivest, Emily and Perrin, L{\'{e}}o Jean and Liem, Xavier and Furstoss, Christophe and Khaouam, Nader and Nguyen-Tan, Phuc Felix and Wang, Chang-Shu and Sultanem, Khalil},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2017.8oje5q00},
publisher = {The Cancer Imaging Archive},
title = {{Data from Head-Neck-PET-CT}},
url = {https://wiki.cancerimagingarchive.net/x/24pyAQ},
year = {2017}
}
@misc{TCIAHead-Neck-Radiomics-HN1,
author = {Wee, Leonard and Dekker, Andre},
doi = {10.7937/TCIA.2019.8KAP372N},
publisher = {The Cancer Imaging Archive},
title = {{Data from Head-Neck-Radiomics-HN1}},
url = {https://wiki.cancerimagingarchive.net/x/iBglAw},
year = {2019}
}
@misc{Erickson2017,
author = {Erickson, Bradley and Akkus, Zeynettin and Sedlar, Jiri and Korfiatis, Panagiotis},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2017.dwehtz9v},
publisher = {The Cancer Imaging Archive},
title = {{Data From LGG-1p19qDeletion}},
url = {https://wiki.cancerimagingarchive.net/x/coKJAQ},
year = {2017}
}
@misc{ArmatoIIISamuelG.2015,
abstract = {The Lung Image Database Consortium image collection (LIDC-IDRI) consists of diagnostic and lung cancer screening thoracic computed tomography (CT) scans with marked-up annotated lesions. It is a web-accessible international resource for development, training, and evaluation of computer-assisted diagnostic (CAD) methods for lung cancer detection and diagnosis. Initiated by the National Cancer Institute (NCI), further advanced by the Foundation for the National Institutes of Health (FNIH), and accompanied by the Food and Drug Administration (FDA) through active participation, this public-private partnership demonstrates the success of a consortium founded on a consensus-based process. Seven academic centers and eight medical imaging companies collaborated to create this data set which contains 1018 cases. Each subject includes images from a clinical thoracic CT scan and an associated XML file that records the results of a two-phase image annotation process performed by four experienced thoracic radiologists. In the initial blinded-read phase, each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories ("nodule {\textgreater} or =3 mm," "nodule {\textless}3 mm," and "non-nodule {\textgreater} or =3 mm"). In the subsequent unblinded-read phase, each radiologist independently reviewed their own marks along with the anonymized marks of the three other radiologists to render a final opinion. The goal of this process was to identify as completely as possible all lung nodules in each CT scan without requiring forced consensus.},
author = {III, Armato and G., Samuel and McLennan and Geoffrey and Bidaut and Luc and McNitt-Gray and Michael, F. and R., Meyer and Charles and Reeves},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2015.LO9QL9SX},
publisher = {The Cancer Imaging Archive},
title = {{Data From LIDC-IDRI}},
url = {https://wiki.cancerimagingarchive.net/x/rgAe},
year = {2015}
}
@misc{TCIA-Lung-CT-Segmentation-Challenge,
author = {Yang, Jinzhong and Sharp, Greg and Veeraraghavan, Harini and {Van Elmpt}, Wouter and Dekker, Andre and Lustberg, Tim and Gooding, Mark},
doi = {10.7937/K9/TCIA.2017.3R3FVZ08},
publisher = {The Cancer Imaging Archive},
title = {{Data from Lung CT Segmentation Challenge}},
url = {https://wiki.cancerimagingarchive.net/x/e41yAQ},
year = {2017}
}
@misc{TCIA-Pancreas-CT,
author = {Roth, Holger and Farag, Amal and Turkbey, Evrim B and Lu, Le and Liu, Jiamin and Summers, Ronald M},
doi = {10.7937/K9/TCIA.2016.TNB1KQBU},
publisher = {The Cancer Imaging Archive},
title = {{Data From Pancreas-CT}},
url = {https://wiki.cancerimagingarchive.net/x/eIlXAQ},
year = {2016}
}
@misc{TCIA-Prostate-3T,
author = {{Litjens Geert; Futterer}, Jurgen; Huisman Henkjan;},
doi = {10.7937/K9/TCIA.2015.QJTV5IL5},
publisher = {The Cancer Imaging Archive},
title = {{Data From Prostate-3T}},
url = {http://dx.doi.org/10.7937/K9/TCIA.2015.QJTV5IL5},
year = {2015}
}
@misc{TCIA-PROSTATE-DIAGNOSIS,
author = {Bloch, B. Nicolas and Jain, Ashali and Jaffe, C. Carl},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2015.FOQEUJVT},
publisher = {The Cancer Imaging Archive},
title = {{Data From PROSTATE-DIAGNOSIS}},
url = {https://wiki.cancerimagingarchive.net/x/xgEy},
year = {2015}
}
@incollection{Viswanathan2020,
address = {Cham},
author = {Viswanathan, Ramanarayanan},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_298-1},
pages = {1--3},
publisher = {Springer International Publishing},
title = {{Data Fusion}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}298-1},
year = {2020}
}
@book{AnounciaS.M.GohelH.A.&Vairamuthu2020,
address = {Singapore},
author = {{Anouncia, S.M., Gohel, H.A., {\&} Vairamuthu}, S.},
doi = {10.1007/978-981-15-2282-6},
editor = {Anouncia, S. Margret and Gohel, Hardik A. and Vairamuthu, Subbiah},
isbn = {978-981-15-2281-9},
publisher = {Springer Singapore},
title = {{Data Visualization: Trends and Challenges Toward Multidisciplinary Perception. Data Visualization.}},
url = {https://link.springer.com/book/10.1007{\%}2F978-981-15-2282-6},
year = {2020}
}
@article{Chapman2019,
abstract = {Generating value from data requires the ability to find, access and make sense of datasets. There are many efforts underway to encourage data sharing and reuse, from scientific publishers asking authors to submit data alongside manuscripts to data marketplaces, open data portals and data communities. Google recently beta released a search service for datasets, which allows users to discover data stored in various online repositories via keyword queries. These developments foreshadow an emerging research field around dataset search or retrieval that broadly encompasses frameworks, methods and tools that help match a user data need against a collection of datasets. Here, we survey the state of the art of research and commercial systems in dataset retrieval. We identify what makes dataset search a research field in its own right, with unique challenges and methods and highlight open problems. We look at approaches and implementations from related areas dataset search is drawing upon, including information retrieval, databases, entity-centric and tabular search in order to identify possible paths to resolve these open problems as well as immediate next steps that will take the field forward.},
archivePrefix = {arXiv},
arxivId = {1901.00735},
author = {Chapman, Adriane and Simperl, Elena and Koesten, Laura and Konstantinidis, George and Ib{\'{a}}{\~{n}}ez-Gonzalez, Luis-Daniel and Kacprzak, Emilia and Groth, Paul},
eprint = {1901.00735},
month = {jan},
title = {{Dataset search: a survey}},
url = {http://arxiv.org/abs/1901.00735},
year = {2019}
}
@article{Wu2018,
abstract = {We present a system for covert automated deception detection using information available in a video. We study the importance of different modalities like vision, audio and text for this task. On the vision side, our system uses classifiers trained on low level video features which predict human micro-expressions. We show that predictions of high-level micro-expressions can be used as features for deception prediction. Surprisingly, IDT (Improved Dense Trajectory) features which have been widely used for action recognition, are also very good at predicting deception in videos. We fuse the score of classifiers trained on IDT features and high-level micro-expressions to improve performance. MFCC (Mel-frequency Cepstral Coefficients) features from the audio domain also provide a significant boost in performance, while information from transcripts is not very beneficial for our system. Using various classifiers, our automated system obtains an AUC of 0.877 (10-fold cross-validation) when evaluated on subjects which were not part of the training set. Even though state-of-the-art methods use human annotations of micro-expressions for deception detection, our fully automated approach outperforms them by 5{\%}. When combined with human annotations of micro-expressions, our AUC improves to 0.922. We also present results of a user-study to analyze how well do average humans perform on this task, what modalities they use for deception detection and how they perform if only one modality is accessible.},
archivePrefix = {arXiv},
arxivId = {1712.04415},
author = {Wu, Zhe and Singh, Bharat and Davis, Larry S. and Subrahmanian, V. S.},
eprint = {1712.04415},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
month = {dec},
pages = {1695--1702},
title = {{Deception detection in videos}},
url = {http://arxiv.org/abs/1712.04415},
year = {2018}
}
@misc{Ikutani2020,
author = {Ikutani, Yoshiharu and {Takatomi Kubo} and Nishida, Satoshi and Hata, Hideaki and Matsumoto, Kenichi and Ikeda, Kazushi and Nishimoto, Shinji},
doi = {10.18112/OPENNEURO.DS002411.V1.1.0},
publisher = {Openneuro},
title = {{Decoding functional category of source code from the brain (fMRI on Java program comprehension)}},
url = {https://openneuro.org/datasets/ds002411/versions/1.1.0},
year = {2020}
}
@article{Aerts2014b,
abstract = {Human cancers exhibit strong phenotypic differences that can be visualized noninvasively by medical imaging. Radiomics refers to the comprehensive quantification of tumour phenotypes by applying a large number of quantitative image features. Here we present a radiomic analysis of 440 features quantifying tumour image intensity, shape and texture, which are extracted from computed tomography data of 1,019 patients with lung or head-and-neck cancer. We find that a large number of radiomic features have prognostic power in independent data sets of lung and head-and-neck cancer patients, many of which were not identified as significant before. Radiogenomics analysis reveals that a prognostic radiomic signature, capturing intratumour heterogeneity, is associated with underlying gene-expression patterns. These data suggest that radiomics identifies a general prognostic phenotype existing in both lung and head-and-neck cancer. This may have a clinical impact as imaging is routinely used in clinical practice, providing an unprecedented opportunity to improve decision-support in cancer treatment at low cost. {\textcopyright} 2014 Macmillan Publishers Limited. All rights reserved.},
author = {Aerts, Hugo J.W.L. and Velazquez, Emmanuel Rios and Leijenaar, Ralph T.H. and Parmar, Chintan and Grossmann, Patrick and Cavalho, Sara and Bussink, Johan and Monshouwer, Ren{\'{e}} and Haibe-Kains, Benjamin and Rietveld, Derek and Hoebers, Frank and Rietbergen, Michelle M. and Leemans, C. Ren{\'{e}} and Dekker, Andre and Quackenbush, John and Gillies, Robert J. and Lambin, Philippe},
doi = {10.1038/ncomms5006},
issn = {20411723},
journal = {Nature Communications},
pmid = {24892406},
title = {{Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach}},
volume = {5},
year = {2014}
}
@article{Aerts2014,
abstract = {Human cancers exhibit strong phenotypic differences that can be visualized noninvasively by medical imaging. Radiomics refers to the comprehensive quantification of tumour phenotypes by applying a large number of quantitative image features. Here we present a radiomic analysis of 440 features quantifying tumour image intensity, shape and texture, which are extracted from computed tomography data of 1,019 patients with lung or head-and-neck cancer. We find that a large number of radiomic features have prognostic power in independent data sets of lung and head-and-neck cancer patients, many of which were not identified as significant before. Radiogenomics analysis reveals that a prognostic radiomic signature, capturing intratumour heterogeneity, is associated with underlying gene-expression patterns. These data suggest that radiomics identifies a general prognostic phenotype existing in both lung and head-and-neck cancer. This may have a clinical impact as imaging is routinely used in clinical practice, providing an unprecedented opportunity to improve decision-support in cancer treatment at low cost. {\textcopyright} 2014 Macmillan Publishers Limited. All rights reserved.},
author = {Aerts, Hugo J.W.L. and Velazquez, Emmanuel Rios and Leijenaar, Ralph T.H. and Parmar, Chintan and Grossmann, Patrick and Cavalho, Sara and Bussink, Johan and Monshouwer, Ren{\'{e}} and Haibe-Kains, Benjamin and Rietveld, Derek and Hoebers, Frank and Rietbergen, Michelle M. and Leemans, C. Ren{\'{e}} and Dekker, Andre and Quackenbush, John and Gillies, Robert J. and Lambin, Philippe},
doi = {10.1038/ncomms5006},
issn = {20411723},
journal = {Nature Communications},
number = {1},
pages = {4006},
pmid = {24892406},
title = {{Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach}},
url = {https://doi.org/10.1038/ncomms5006},
volume = {5},
year = {2014}
}
@incollection{Bansal2020,
address = {Cham},
author = {Bansal, Ankan and Ranjan, Rajeev and Castillo, Carlos D. and Chellappa, Rama},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_880-1},
pages = {1--9},
publisher = {Springer International Publishing},
title = {{Deep CNN-Based Face Recognition}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}880-1},
year = {2020}
}
@inproceedings{Tran2016,
abstract = {Over the last few years deep learning methods have emerged as one of the most prominent approaches for video analysis. However, so far their most successful applications have been in the area of video classification and detection, i.e., problems involving the prediction of a single class label or a handful of output variables per video. Furthermore, while deep networks are commonly recognized as the best models to use in these domains, there is a widespread perception that in order to yield successful results they often require time-consuming architecture search, manual tweaking of parameters and computationally intensive preprocessing or post-processing methods. In this paper we challenge these views by presenting a deep 3D convolutional architecture trained end to end to perform voxel-level prediction, i.e., to output a variable at every voxel of the video. Most importantly, we show that the same exact architecture can be used to achieve competitive results on three widely different voxel-prediction tasks: video semantic segmentation, optical flow estimation, and video coloring. The three networks learned on these problems are trained from raw video without any form of preprocessing and their outputs do not require post-processing to achieve outstanding performance. Thus, they offer an efficient alternative to traditional and much more computationally expensive methods in these video domains.},
address = {United States},
archivePrefix = {arXiv},
arxivId = {1511.06681},
author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2016.57},
eprint = {1511.06681},
isbn = {9781467388504},
issn = {21607516},
pages = {402--409},
publisher = {IEEE Computer Society},
title = {{Deep End2End Voxel2Voxel Prediction}},
year = {2016}
}
@misc{GuohuaShen2020,
author = {{Guohua Shen} and {Tomoyasu Horikawa} and Majima, Kei and {Yukiyasu Kamitani}},
doi = {10.18112/OPENNEURO.DS001506.V1.3.1},
publisher = {Openneuro},
title = {{Deep Image Reconstruction}},
url = {https://openneuro.org/datasets/ds001506/versions/1.3.1},
year = {2020}
}
@article{Shen2019,
abstract = {The mental contents of perception and imagery are thought to be encoded in hierarchical representations in the brain, but previous attempts to visualize perceptual contents have failed to capitalize on multiple levels of the hierarchy, leaving it challenging to reconstruct internal imagery. Recent work showed that visual cortical activity measured by functional magnetic resonance imaging (fMRI) can be decoded (translated) into the hierarchical features of a pre-trained deep neural network (DNN) for the same input image, providing a way to make use of the information from hierarchical visual features. Here, we present a novel image reconstruction method, in which the pixel values of an image are optimized to make its DNN features similar to those decoded from human brain activity at multiple layers. We found that our method was able to reliably produce reconstructions that resembled the viewed natural images. A natural image prior introduced by a deep generator neural network effectively rendered semantically meaningful details to the reconstructions. Human judgment of the reconstructions supported the effectiveness of combining multiple DNN layers to enhance the visual quality of generated images. While our model was solely trained with natural images, it successfully generalized to artificial shapes, indicating that our model was not simply matching to exemplars. The same analysis applied to mental imagery demonstrated rudimentary reconstructions of the subjective content. Our results suggest that our method can effectively combine hierarchical neural representations to reconstruct perceptual and subjective images, providing a new window into the internal contents of the brain.},
author = {Shen, Guohua and Horikawa, Tomoyasu and Majima, Kei and Kamitani, Yukiyasu},
doi = {10.1371/journal.pcbi.1006633},
editor = {O'Reilly, Jill},
issn = {15537358},
journal = {PLoS Computational Biology},
month = {jan},
number = {1},
pages = {e1006633},
title = {{Deep image reconstruction from human brain activity}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1006633},
volume = {15},
year = {2019}
}
@book{Goodfellow2016DeepLearning,
author = {Goodfellow, Ian},
isbn = {0262035618},
publisher = {The MIT Press},
title = {{Deep Learning (Adaptive Computation and Machine Learning series)}},
url = {https://www.xarg.org/ref/a/0262035618/},
year = {2016}
}
@article{Litjens2016,
author = {Litjens, Geert and S{\'{a}}nchez, Clara I. and Timofeeva, Nadya and Hermsen, Meyke and Nagtegaal, Iris and Kovacs, Iringo and {Hulsbergen - van de Kaa}, Christina and Bult, Peter and van Ginneken, Bram and van der Laak, Jeroen},
doi = {10.1038/srep26286},
issn = {2045-2322},
journal = {Scientific Reports},
month = {sep},
number = {1},
pages = {26286},
title = {{Deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis}},
url = {http://www.nature.com/articles/srep26286},
volume = {6},
year = {2016}
}
@article{Hosny2018,
abstract = {Background: Non-small-cell lung cancer (NSCLC) patients often demonstrate varying clinical courses and outcomes, even within the same tumor stage. This study explores deep learning applications in medical imaging allowing for the automated quantification of radiographic characteristics and potentially improving patient stratification. Methods and findings: We performed an integrative analysis on 7 independent datasets across 5 institutions totaling 1,194 NSCLC patients (age median = 68.3 years [range 32.5–93.3], survival median = 1.7 years [range 0.0–11.7]). Using external validation in computed tomography (CT) data, we identified prognostic signatures using a 3D convolutional neural network (CNN) for patients treated with radiotherapy (n = 771, age median = 68.0 years [range 32.5–93.3], survival median = 1.3 years [range 0.0–11.7]). We then employed a transfer learning approach to achieve the same for surgery patients (n = 391, age median = 69.1 years [range 37.2–88.0], survival median = 3.1 years [range 0.0–8.8]). We found that the CNN predictions were significantly associated with 2-year overall survival from the start of respective treatment for radiotherapy (area under the receiver operating characteristic curve [AUC] = 0.70 [95{\%} CI 0.63–0.78], p {\textless} 0.001) and surgery (AUC = 0.71 [95{\%} CI 0.60–0.82], p {\textless} 0.001) patients. The CNN was also able to significantly stratify patients into low and high mortality risk groups in both the radiotherapy (p {\textless} 0.001) and surgery (p = 0.03) datasets. Additionally, the CNN was found to significantly outperform random forest models built on clinical parameters—including age, sex, and tumor node metastasis stage—as well as demonstrate high robustness against test–retest (intraclass correlation coefficient = 0.91) and inter-reader (Spearman's rank-order correlation = 0.88) variations. To gain a better understanding of the characteristics captured by the CNN, we identified regions with the most contribution towards predictions and highlighted the importance of tumor-surrounding tissue in patient stratification. We also present preliminary findings on the biological basis of the captured phenotypes as being linked to cell cycle and transcriptional processes. Limitations include the retrospective nature of this study as well as the opaque black box nature of deep learning networks. Conclusions: Our results provide evidence that deep learning networks may be used for mortality risk stratification based on standard-of-care CT images from NSCLC patients. This evidence motivates future research into better deciphering the clinical and biological basis of deep learning networks as well as validation in prospective data.},
annote = {From Duplicate 2 (Deep learning for lung cancer prognostication: A retrospective multi-cohort radiomics study - Hosny, Ahmed; Parmar, Chintan; Coroller, Thibaud P; Grossmann, Patrick; Zeleznik, Roman; Kumar, Avnish; Bussink, Johan; Gillies, Robert J; Mak, Raymond H; Aerts, Hugo J.W.L.)

Publisher: Public Library of Science},
author = {Hosny, Ahmed and Parmar, Chintan and Coroller, Thibaud P. and Grossmann, Patrick and Zeleznik, Roman and Kumar, Avnish and Bussink, Johan and Gillies, Robert J. and Mak, Raymond H. and Aerts, Hugo J.W.L.},
doi = {10.1371/journal.pmed.1002711},
issn = {15491676},
journal = {PLoS Medicine},
number = {11},
pmid = {27212078},
title = {{Deep learning for lung cancer prognostication: A retrospective multi-cohort radiomics study}},
url = {https://doi.org/10.1371/journal.pmed.1002711},
volume = {15},
year = {2018}
}
@article{Leclerc2019,
abstract = {Delineation of the cardiac structures from 2D echocardiographic images is a common clinical task to establish a diagnosis. Over the past decades, the automation of this task has been the subject of intense research. In this paper, we evaluate how far the state-of-the-art encoder-decoder deep convolutional neural network methods can go at assessing 2D echocardiographic images, i.e., segmenting cardiac structures and estimating clinical indices, on a dataset, especially, designed to answer this objective. We, therefore, introduce the cardiac acquisitions for multi-structure ultrasound segmentation dataset, the largest publicly-available and fully-annotated dataset for the purpose of echocardiographic assessment. The dataset contains two and four-chamber acquisitions from 500 patients with reference measurements from one cardiologist on the full dataset and from three cardiologists on a fold of 50 patients. Results show that encoder-decoder-based architectures outperform state-of-the-art non-deep learning methods and faithfully reproduce the expert analysis for the end-diastolic and end-systolic left ventricular volumes, with a mean correlation of 0.95 and an absolute mean error of 9.5 ml. Concerning the ejection fraction of the left ventricle, results are more contrasted with a mean correlation coefficient of 0.80 and an absolute mean error of 5.6{\%}. Although these results are below the inter-observer scores, they remain slightly worse than the intra-observer's ones. Based on this observation, areas for improvement are defined, which open the door for accurate and fully-automatic analysis of 2D echocardiographic images.},
archivePrefix = {arXiv},
arxivId = {1908.06948},
author = {Leclerc, Sarah and Smistad, Erik and Pedrosa, Joao and Ostvik, Andreas and Cervenansky, Frederic and Espinosa, Florian and Espeland, Torvald and Berg, Erik Andreas Rye and Jodoin, Pierre Marc and Grenier, Thomas and Lartizien, Carole and Dhooge, Jan and Lovstakken, Lasse and Bernard, Olivier},
doi = {10.1109/TMI.2019.2900516},
eprint = {1908.06948},
issn = {1558254X},
journal = {IEEE transactions on medical imaging},
month = {sep},
number = {9},
pages = {2198--2210},
pmid = {30802851},
title = {{Deep Learning for Segmentation Using an Open Large-Scale Dataset in 2D Echocardiography}},
url = {https://ieeexplore.ieee.org/document/8649738/},
volume = {38},
year = {2019}
}
@inproceedings{Dolph2017,
abstract = {This work proposes multiclass deep learning classification of Alzheimer's disease (AD) using novel texture and other associated features extracted from structural MRI. Two distinct learning models (Model 1 and 2) are presented where both include subcortical area specific feature extraction, feature selection and stacked auto-encoder (SAE) deep neural network (DNN). The models learn highly complex and subtle differences in spatial atrophy patterns using white matter volumes, gray matter volumes, cortical surface area, cortical thickness, and different types of Fractal Brownian Motion co-occurrence matrices for texture as features to classify AD from cognitive normal (CN) and mild cognitive impairment (MCI) in dementia patients. A five layer SAE with state-of-the-art dropout learning is trained on a publicly available ADNI dataset and the model performances are evaluated at two levels: one using in-house tenfold cross validation and another using the publicly available CADDementia competition. The in-house evaluations of our two models achieve 56.6{\%} and 58.0{\%} tenfold cross validation accuracies using 504 ADNI subjects. For the public domain evaluation, we are the first to report DNN to CADDementia and our methods yield competitive classification accuracies of 51.4{\%} and 56.8{\%}. Further, both of our proposed models offer higher True Positive Fraction (TPF) for AD class when compared to the top-overall ranked algorithm while Model 1 also ties for top diseased class sensitivity at 58.2{\%} in the CADDementia challenge. Finally, Model 2 achieves strong disease class sensitivity with improvement in specificity and overall accuracy. Our algorithms have the potential to provide a rapid, objective, and non-invasive assessment of AD.},
author = {Dolph, C. V. and Alam, M. and Shboul, Z. and Samad, M. D. and Iftekharuddin, K. M.},
booktitle = {Proceedings of the International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2017.7966129},
isbn = {9781509061815},
keywords = {ADNI,Alzheimer's disease,Biomarkers,Deep learning,Dropout learning,Hippocampus,Neuroimaging classification},
month = {may},
pages = {2259--2266},
publisher = {IEEE},
title = {{Deep learning of texture and structural features for multiclass Alzheimer's disease classification}},
url = {http://ieeexplore.ieee.org/document/7966129/},
volume = {2017-May},
year = {2017}
}
@book{Singh2020,
address = {Berkeley, CA},
author = {Singh, Himanshu and Lone, Yunis Ahmad},
booktitle = {Deep Neuro-Fuzzy Systems with Python},
doi = {10.1007/978-1-4842-5361-8},
isbn = {978-1-4842-5360-1},
publisher = {Apress},
title = {{Deep Neuro-Fuzzy Systems with Python}},
url = {http://link.springer.com/10.1007/978-1-4842-5361-8},
year = {2020}
}
@article{Hausknecht2015,
abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting Deep Recurrent Q-Network (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
archivePrefix = {arXiv},
arxivId = {1507.06527},
author = {Hausknecht, Matthew and Stone, Peter},
eprint = {1507.06527},
isbn = {9781577357520},
journal = {AAAI Fall Symposium - Technical Report},
month = {jul},
pages = {29--37},
title = {{Deep recurrent q-learning for partially observable MDPs}},
url = {http://arxiv.org/abs/1507.06527},
volume = {FS-15-06},
year = {2015}
}
@article{Arulkumaran2017,
abstract = {Deep reinforcement learning (DRL) is poised to revolutionize the field of artificial intelligence (AI) and represents a step toward building autonomous systems with a higherlevel understanding of the visual world. Currently, deep learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to play video games directly from pixels. DRL algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of RL, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep RL, including the deep Q-network (DQN), trust region policy optimization (TRPO), and asynchronous advantage actor critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via RL. To conclude, we describe several current areas of research within the field.},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
doi = {10.1109/MSP.2017.2743240},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
month = {nov},
number = {6},
pages = {26--38},
title = {{Deep reinforcement learning: A brief survey}},
url = {http://ieeexplore.ieee.org/document/8103164/},
volume = {34},
year = {2017}
}
@article{Li2017,
abstract = {We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions. Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.},
archivePrefix = {arXiv},
arxivId = {1701.07274},
author = {Li, Yuxi},
eprint = {1701.07274},
month = {jan},
title = {{Deep Reinforcement Learning: An Overview}},
url = {http://arxiv.org/abs/1701.07274},
year = {2017}
}
@inproceedings{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
isbn = {9781467388504},
issn = {10636919},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
url = {http://arxiv.org/abs/1512.03385},
volume = {2016-Decem},
year = {2016}
}
@article{Kulkarni2016,
abstract = {Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms. There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map. The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. The value function of a state can be computed as the inner product between the successor map and the reward weights. In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework. DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy. We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine.},
archivePrefix = {arXiv},
arxivId = {1606.02396},
author = {Kulkarni, Tejas D. and Saeedi, Ardavan and Gautam, Simanta and Gershman, Samuel J.},
eprint = {1606.02396},
month = {jun},
title = {{Deep Successor Reinforcement Learning}},
url = {http://arxiv.org/abs/1606.02396},
year = {2016}
}
@inproceedings{Xie2016,
abstract = {As 3D movie viewing becomes mainstream and the Virtual Reality (VR) market emerges, the demand for 3D contents is growing rapidly. Producing 3D videos, however, remains challenging. In this paper we propose to use deep neural networks to automatically convert 2Dvideos and images to a stereoscopic 3D format. In contrast to previous automatic 2D-to-3D conversion algorithms, which have separate stages and need ground truth depth map as supervision, our approach is trained endto-end directly on stereo pairs extracted from existing 3D movies. This novel training scheme makes it possible to exploit orders of magnitude more data and significantly increases performance. Indeed, Deep3D outperforms baselines in both quantitative and human subject evaluations.},
annote = {From Duplicate 2 (Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks - Xie, Junyuan; Girshick, Ross; Farhadi, Ali)

{\_}eprint: 1604.03650},
archivePrefix = {arXiv},
arxivId = {1604.03650},
author = {Xie, Junyuan and Girshick, Ross and Farhadi, Ali},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46493-0_51},
eprint = {1604.03650},
isbn = {9783319464923},
issn = {16113349},
keywords = {Deep convolutional neural networks,Monocular stereo reconstruction},
pages = {842--857},
title = {{Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks}},
url = {http://arxiv.org/abs/1604.03650},
volume = {9908 LNCS},
year = {2016}
}
@inproceedings{Zhu2018,
abstract = {In this work, we present a fully automated lung computed tomography (CT) cancer diagnosis system, DeepLung. DeepLung consists of two components, nodule detection (identifying the locations of candidate nodules) and classification (classifying candidate nodules into benign or malignant). Considering the 3D nature of lung CT data and the compactness of dual path networks (DPN), two deep 3D DPN are designed for nodule detection and classification respectively. Specifically, a 3D Faster Regions with Convolutional Neural Net (R-CNN) is designed for nodule detection with 3D dual path blocks and a U-net-like encoder-decoder structure to effectively learn nodule features. For nodule classification, gradient boosting machine (GBM) with 3D dual path network features is proposed. The nodule classification subnetwork was validated on a public dataset from LIDC-IDRI, on which it achieved better performance than state-of-the-art approaches and surpassed the performance of experienced doctors based on image modality. Within the DeepLung system, candidate nodules are detected first by the nodule detection subnetwork, and nodule diagnosis is conducted by the classification subnetwork. Extensive experimental results demonstrate that DeepLung has performance comparable to experienced doctors both for the nodule-level and patient-level diagnosis on the LIDC-IDRI dataset.},
annote = {{\_}eprint: 1801.09555},
archivePrefix = {arXiv},
arxivId = {1801.09555},
author = {Zhu, Wentao and Liu, Chaochun and Fan, Wei and Xie, Xiaohui},
booktitle = {Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018},
doi = {10.1109/WACV.2018.00079},
eprint = {1801.09555},
isbn = {9781538648865},
pages = {673--681},
title = {{DeepLung: Deep 3D dual path nets for automated pulmonary nodule detection and classification}},
volume = {2018-Janua},
year = {2018}
}
@article{Beers2018,
abstract = {Translating neural networks from theory to clinical practice has unique challenges, specifically in the field of neuroimaging. In this paper, we present DeepNeuro, a deep learning framework that is best-suited to putting deep learning algorithms for neuroimaging in practical usage with a minimum of friction. We show how this framework can be used to both design and train neural network architectures, as well as modify state-of-the-art architectures in a flexible and intuitive way. We display the pre- and postprocessing functions common in the medical imaging community that DeepNeuro offers to ensure consistent performance of networks across variable users, institutions, and scanners. And we show how pipelines created in DeepNeuro can be concisely packaged into shareable Docker containers and command-line interfaces using DeepNeuro's pipeline resources.},
annote = {{\_}eprint: 1808.04589},
archivePrefix = {arXiv},
arxivId = {1808.04589},
author = {Beers, Andrew and Brown, James and Chang, Ken and Hoebel, Katharina and Gerstner, Elizabeth and Rosen, Bruce and Kalpathy-Cramer, Jayashree},
eprint = {1808.04589},
title = {{DeepNeuro: an open-source deep learning toolbox for neuroimaging}},
url = {http://arxiv.org/abs/1808.04589},
year = {2018}
}
@incollection{Roth2015,
abstract = {Automatic organ segmentation is an important yet challenging problem for medical image analysis. The pancreas is an abdominal organ with very high anatomical variability. This inhibits previous segmentation methods from achieving high accuracies, especially compared to other organs such as the liver, heart or kidneys. In this paper, we present a probabilistic bottom-up approach for pancreas segmentation in abdominal computed tomography (CT) scans, using multi-level deep convolutional networks (ConvNets). We propose and evaluate several variations of deep ConvNets in the context of hierarchical, coarse-tofine classification on image patches and regions, i.e. superpixels. We first present a dense labeling of local image patches via P-ConvNet and nearest neighbor fusion. Then we describe a regional ConvNet (R1−ConvNet) that samples a set of bounding boxes around each image superpixel at different scales of contexts in a “zoom-out” fashion. Our ConvNets learn to assign class probabilities for each superpixel region of being pancreas. Last, we study a stacked R2−ConvNet leveraging the joint space of CT intensities and the P−ConvNet dense probability maps. Both 3D Gaussian smoothing and 2D conditional random fields are exploited as structured predictions for post-processing. We evaluate on CT images of 82 patients in 4-fold cross-validation. We achieve a Dice Similarity Coefficient of 83.6±6.3{\%} in training and 71.8±10.7{\%} in testing.},
archivePrefix = {arXiv},
arxivId = {1506.06448},
author = {Roth, Holger R. and Lu, Le and Farag, Amal and Shin, Hoo Chang and Liu, Jiamin and Turkbey, Evrim B. and Summers, Ronald M.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24553-9_68},
eprint = {1506.06448},
issn = {16113349},
title = {{Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation}},
year = {2015}
}
@article{Flynn2015,
annote = {From Duplicate 2 (DeepStereo: Learning to Predict New Views from the World's Imagery - Flynn, John; Neulander, Ivan; Philbin, James; Snavely, Noah)

{\_}eprint: 1506.06825},
archivePrefix = {arXiv},
arxivId = {1506.06825},
author = {Flynn, John and Neulander, Ivan and Philbin, James and Snavely, Noah},
eprint = {1506.06825},
journal = {CoRR},
title = {{DeepStereo: Learning to Predict New Views from the World's Imagery}},
url = {http://arxiv.org/abs/1506.06825},
volume = {abs/1506.0},
year = {2015}
}
@inproceedings{He2015a,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94{\%} top-5 test error on the ImageNet 2012 classification dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%} [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1{\%}, [26]) on this dataset.},
annote = {{\_}eprint: 1502.01852},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.123},
eprint = {1502.01852},
isbn = {9781467383912},
issn = {15505499},
pages = {1026--1034},
title = {{Delving deep into rectifiers: Surpassing human-level performance on imagenet classification}},
volume = {2015 Inter},
year = {2015}
}
@inproceedings{Huang2016,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L2+1) direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
annote = {{\_}eprint: 1608.06993},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.243},
eprint = {1608.06993},
isbn = {9781538604571},
pages = {2261--2269},
title = {{Densely connected convolutional networks}},
url = {http://arxiv.org/abs/1608.06993},
volume = {2017-Janua},
year = {2017}
}
@article{Lafferty2001,
author = {Lafferty, John and Mccallum, Andrew and Pereira, Fernando C N},
journal = {Machine Learning},
number = {Icml},
pages = {282--289},
title = {{Departmental Papers ( CIS ) Conditional Random Fields : Probabilistic Models for Segmenting and Labeling Sequence Data Conditional Random Fields : Probabilistic Models}},
volume = {2001},
year = {2001}
}
@inproceedings{Baker2016,
abstract = {At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Qlearning with an -greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.},
archivePrefix = {arXiv},
arxivId = {1611.02167},
author = {Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1611.02167},
title = {{Designing neural network architectures using reinforcement learning}},
volume = {abs/1611.0},
year = {2019}
}
@article{Lepping2016a,
abstract = {Music is a strong emotional stimulus; however, it is difficult to differentiate the effects of arousal and valence. While emotional stimuli sets have been created from words and pictures, a normed set of musical stimuli is unavailable. The goal of this project was to identify a set of ecologically valid musical stimuli for use in research studies of emotion and mood that differ on valence but are matched for arousal, and are also matched with emotionally evocative non-musical stimuli. Three rating experiments were conducted. In the first Stimulus Selection experiment, participants rated short music clips for valence and arousal, and the most evocative clips were selected. In two follow-up Stimulus Validation experiments, two additional groups of participants rated the selected musical and non-musical stimuli, with Experiment 3 confirming that these stimuli work effectively in a noisy environment (within an MRI scanner). The result of these three studies is a set of emotionally evocative, positive and negative musical stimuli, matched for valence and arousal with a subset of previously validated non-musical stimuli.},
author = {Lepping, Rebecca J. and Atchley, Ruth Ann and Savage, Cary R.},
doi = {10.1177/0305735615604509},
issn = {17413087},
journal = {Psychology of Music},
keywords = {arousal,auditory perception/cognition,emotion,experimental aesthetics,negative emotions,valence},
month = {sep},
number = {5},
pages = {1012--1028},
title = {{Development of a validated emotionally provocative musical stimulus set for research}},
url = {http://journals.sagepub.com/doi/10.1177/0305735615604509},
volume = {44},
year = {2016}
}
@online{KaggleDiabeticRetinopathyDetection,
abstract = {Diabetic retinopathy is becoming a more prevalent disease in diabetic patients nowadays. The surprising fact about the disease is it leaves no symptoms at the beginning stage and the patient can realize the disease only when his vision starts to fall. If the disease is not found at the earliest it leads to a stage where the probability of curing the disease is less. But if we find the disease at that stage, the patient might be in a situation of losing the vision completely. Hence, this paper aims at finding the disease at the earliest possible stage by extracting two features from the retinal image namely Microaneurysms which is found to be the starting symptom showing feature and Hemorrhage which shows symptoms of the other stages. Based on these two features we classify the stage of the disease as normal, beginning, mild and severe using convolutional neural network, a deep learning technique which reduces the burden of manual feature extraction and gives higher accuracy. We also locate the position of these features in the disease affected retinal images to help the doctors offer better medical treatment.},
booktitle = {International Journal of Engineering and Advanced Technology},
doi = {10.35940/ijeat.d7786.049420},
number = {4},
pages = {1022--1026},
title = {{Diabetic Retinopathy Detection}},
url = {https://www.kaggle.com/c/diabetic-retinopathy-detection},
volume = {9},
year = {2020}
}
@article{Sorensen2017,
abstract = {This paper presents a brain T1-weighted structural magnetic resonance imaging (MRI) biomarker that combines several individual MRI biomarkers (cortical thickness measurements, volumetric measurements, hippocampal shape, and hippocampal texture). The method was developed, trained, and evaluated using two publicly available reference datasets: a standardized dataset from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and the imaging arm of the Australian Imaging Biomarkers and Lifestyle flagship study of ageing (AIBL). In addition, the method was evaluated by participation in the Computer-Aided Diagnosis of Dementia (CADDementia) challenge. Cross-validation using ADNI and AIBL data resulted in a multi-class classification accuracy of 62.7{\%} for the discrimination of healthy normal controls (NC), subjects with mild cognitive impairment (MCI), and patients with Alzheimer's disease (AD). This performance generalized to the CADDementia challenge where the method, trained using the ADNI and AIBL data, achieved a classification accuracy 63.0{\%}. The obtained classification accuracy resulted in a first place in the challenge, and the method was significantly better (McNemar's test) than the bottom 24 methods out of the total of 29 methods contributed by 15 different teams in the challenge. The method was further investigated with learning curve and feature selection experiments using ADNI and AIBL data. The learning curve experiments suggested that neither more training data nor a more complex classifier would have improved the obtained results. The feature selection experiment showed that both common and uncommon individual MRI biomarkers contributed to the performance; hippocampal volume, ventricular volume, hippocampal texture, and parietal lobe thickness were the most important. This study highlights the need for both subtle, localized measurements and global measurements in order to discriminate NC, MCI, and AD simultaneously based on a single structural MRI scan. It is likely that additional non-structural MRI features are needed to further improve the obtained performance, especially to improve the discrimination between NC and MCI.},
author = {S{\o}rensen, Lauge and Igel, Christian and Pai, Akshay and Balas, Ioana and Anker, Cecilie and Lillholm, Martin and Nielsen, Mads},
doi = {10.1016/j.nicl.2016.11.025},
issn = {22131582},
journal = {NeuroImage: Clinical},
keywords = {Alzheimer's disease,Biomarker,Classification,Machine learning,Mild cognitive impairment,Structural MRI},
pages = {470--482},
title = {{Differential diagnosis of mild cognitive impairment and Alzheimer's disease using structural MRI cortical thickness, hippocampal shape, hippocampal texture, and volumetry}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2213158216302340},
volume = {13},
year = {2017}
}
@incollection{Gonzalez-Diaz2020,
address = {Cham},
author = {Gonzalez-Diaz, Rocio and Stelldinger, Peer and Latecki, Longin Jan},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_645-1},
pages = {1--8},
publisher = {Springer International Publishing},
title = {{Digitization}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}645-1},
year = {2020}
}
@incollection{Zaki2018,
address = {Cham},
author = {Zaki, Mohammed J. and {Meira, Jr}, Wagner},
booktitle = {Data Mining and Analysis},
doi = {10.1017/cbo9780511810114.008},
pages = {183--214},
publisher = {Springer International Publishing},
title = {{Dimensionality Reduction}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}652-1},
year = {2018}
}
@incollection{Kafieh2008,
abstract = {This paper introduces a new method for automatic landmark detection in cephalometry. In first step, some feature points of bony structures are extracted to model the size, rotation, and translation of skull, we propose two different methods for bony structure discrimination in cephalograms. The first method is using bit slices of a gray level image to create a layered version of the same image and the second method is to make use of a SUSAN edge detector and discriminate the pixels with enough thickness as bony structures. Then a neural network is used to classify images according to their geometrical specifications. Using NN for every new image, the possible coordinates of landmarks are estimated. Then a modified ASM is applied to locate the exact location of landmarks.On average the first method can discriminate feature points of bony structures in 78{\%} of cephalograms and the second method can do it in 94{\%} of them. {\textcopyright} 2008 Springer-Verlag.},
author = {Kafieh, Rahele and Sadri, Saeed and Mehri, Alireza and Raji, Hamid},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-540-89985-3_75},
isbn = {3540899847},
issn = {18650929},
keywords = {Active Shape Model,Bit slicing,Learning Vector Quantization,Susan edge detector,cephalometry},
pages = {609--620},
title = {{Discrimination of bony structures in cephalograms for automatic landmark detection}},
url = {http://link.springer.com/10.1007/978-3-540-89985-3{\_}75},
volume = {6 CCIS},
year = {2008}
}
@book{Srikant2020,
abstract = {Graph Analytics is important in different domains: social networks, computer networks, and computational biology to name a few. This paper describes the challenges involved in programming the underlying graph algorithms for graph analytics for distributed systems with CPU, GPU, and multi-GPU machines and how to deal with them. It emphasizes how language abstractions and good compilation can ease programming graph analytics on such platforms without sacrificing implementation efficiency.},
address = {Cham},
author = {Srikant, Y. N.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-36987-3_1},
isbn = {9783030369866},
issn = {16113349},
keywords = {GPU computation,Graph analytics,Graph frameworks,Graph processing languages,Multi-core processors,Parallel algorithms,Social networks},
pages = {3--20},
publisher = {Springer International Publishing},
title = {{Distributed Graph Analytics}},
url = {http://link.springer.com/10.1007/978-3-030-41886-1},
volume = {11969 LNCS},
year = {2020}
}
@book{Carter2020,
address = {Singapore},
author = {Carter, Susan and Guerin, Cally and Aitchison, Claire},
doi = {10.1007/978-981-15-1808-9},
isbn = {978-981-15-1807-2},
publisher = {Springer Singapore},
title = {{Doctoral Writing}},
url = {http://link.springer.com/10.1007/978-981-15-1808-9},
year = {2020}
}
@article{Wachinger2016,
abstract = {With the increasing prevalence of Alzheimer's disease, research focuses on the early computer-aided diagnosis of dementia with the goal to understand the disease process, determine risk and preserving factors, and explore preventive therapies. By now, large amounts of data from multi-site studies have been made available for developing, training, and evaluating automated classifiers. Yet, their translation to the clinic remains challenging, in part due to their limited generalizability across different datasets. In this work, we describe a compact classification approach that mitigates overfitting by regularizing the multinomial regression with the mixed ℓ1/ℓ2 norm. We combine volume, thickness, and anatomical shape features from MRI scans to characterize neuroanatomy for the three-class classification of Alzheimer's disease, mild cognitive impairment and healthy controls. We demonstrate high classification accuracy via independent evaluation within the scope of the CADDementia challenge. We, furthermore, demonstrate that variations between source and target datasets can substantially influence classification accuracy. The main contribution of this work addresses this problem by proposing an approach for supervised domain adaptation based on instance weighting. Integration of this method into our classifier allows us to assess different strategies for domain adaptation. Our results demonstrate (i) that training on only the target training set yields better results than the na{\"{i}}ve combination (union) of source and target training sets, and (ii) that domain adaptation with instance weighting yields the best classification results, especially if only a small training component of the target dataset is available. These insights imply that successful deployment of systems for computer-aided diagnostics to the clinic depends not only on accurate classifiers that avoid overfitting, but also on a dedicated domain adaptation strategy.},
author = {Wachinger, Christian and Reuter, Martin},
doi = {10.1016/j.neuroimage.2016.05.053},
issn = {10959572},
journal = {NeuroImage},
keywords = {Alzheimer's disease,Classification,Computer-aided diagnosis,Domain adaptation},
month = {oct},
pages = {470--479},
title = {{Domain adaptation for Alzheimer's disease diagnostics}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811916301732},
volume = {139},
year = {2016}
}
@incollection{Patel2020,
address = {Cham},
author = {Patel, Vishal M. and Nguyen, Hien Van},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_819-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Domain Adaptation Using Dictionaries}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}819-1},
year = {2020}
}
@misc{GrandChallengeDRIVE,
title = {{DRIVE: Digital Retinal Images for Vessel Extraction}}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets. {\textcopyright} 2014 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Deep learning,Model combination,Neural networks,Regularization},
pages = {1929--1958},
title = {{Dropout: A simple way to prevent neural networks from overfitting}},
url = {http://jmlr.org/papers/v15/srivastava14a.html},
volume = {15},
year = {2014}
}
@inproceedings{Chen2017,
abstract = {In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64 × 4d) with 26{\%} smaller model size, 25{\%} less computational cost and 8{\%} lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.},
archivePrefix = {arXiv},
arxivId = {1707.01629},
author = {Chen, Yunpeng and Li, Jianan and Xiao, Huaxin and Jin, Xiaojie and Yan, Shuicheng and Feng, Jiashi},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1707.01629},
issn = {10495258},
pages = {4468--4476},
title = {{Dual path networks}},
volume = {2017-Decem},
year = {2017}
}
@article{Madhyastha2015,
abstract = {Consistent spatial patterns of coherent activity, representing large-scale networks, have been reliably identified in multiple populations. Most often, these studies have examined "stationary" connectivity. However, there is a growing recognition that there is a wealth of information in the time-varying dynamics of networks which has neural underpinnings, which changes with age and disease and that supports behavior. Using factor analysis of overlapping sliding windows across 25 participants with Parkinson disease (PD) and 21 controls (ages 41-86), we identify factors describing the covarying correlations of regions (dynamic connectivity) within attention networks and the default mode network, during two baseline resting-state and task runs. Cortical regions that support attention networks are affected early in PD, motivating the potential utility of dynamic connectivity as a sensitive way to characterize physiological disruption to these networks. We show that measures of dynamic connectivity are more reliable than comparable measures of stationary connectivity. Factors in the dorsal attention network (DAN) and fronto-parietal task control network, obtained at rest, are consistently related to the alerting and orienting reaction time effects in the subsequent Attention Network Task. In addition, the same relationship between the same DAN factor and the alerting effect was present during tasks. Although reliable, dynamic connectivity was not invariant, and changes between factor scores across sessions were related to changes in accuracy. In summary, patterns of time-varying correlations among nodes in an intrinsic network have a stability that has functional relevance.},
author = {Madhyastha, Tara M. and Askren, Mary K. and Boord, Peter and Grabowski, Thomas J.},
doi = {10.1089/brain.2014.0248},
issn = {21580022},
journal = {Brain Connectivity},
keywords = {Parkinson disease,attention network task,dynamic functional connectivity,resting-state connectivity,task connectivity},
month = {feb},
number = {1},
pages = {45--59},
title = {{Dynamic connectivity at rest predicts attention task performance}},
url = {http://www.liebertpub.com/doi/10.1089/brain.2014.0248},
volume = {5},
year = {2015}
}
@inproceedings{Simonovsky2017,
abstract = {A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches.},
annote = {{\_}eprint: 1704.02901},
archivePrefix = {arXiv},
arxivId = {1704.02901},
author = {Simonovsky, Martin and Komodakis, Nikos},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.11},
eprint = {1704.02901},
isbn = {9781538604571},
pages = {29--38},
title = {{Dynamic edge-conditioned filters in convolutional neural networks on graphs}},
url = {http://arxiv.org/abs/1704.02901},
volume = {2017-Janua},
year = {2017}
}
@article{Wang2018,
abstract = {Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information, so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds, including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks, including ModelNet40, ShapeNetPart, and S3DIS.},
annote = {{\_}eprint: 1801.07829},
archivePrefix = {arXiv},
arxivId = {1801.07829},
author = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E and Bronstein, Michael M and Solomon, Justin M},
doi = {10.1145/3326362},
eprint = {1801.07829},
issn = {15577368},
journal = {ACM Transactions on Graphics},
keywords = {Classification,Point cloud,Segmentation},
number = {5},
title = {{Dynamic graph Cnn for learning on point clouds}},
url = {http://arxiv.org/abs/1801.07829},
volume = {38},
year = {2019}
}
@incollection{Blanchini2015,
abstract = {In this section a jump back in the history of control is made and we consider problems which had been theoretically faced in the early 70s, and thereafter almost abandoned. The main reason is that the computational effort necessary to practically implement these techniques was not suitable for the computer technology of the time. Today, the situation is different, and many authors are reconsidering the approach. In this section, the main focus will be put on discrete-time systems, although it will also be shown, given the existing relation between continuous- and discrete-time invariant sets presented in Lemma 4.26, how the proposed algorithms can be used to deal with continuous-time systems as well.},
address = {Cham},
author = {Blanchini, Franco and Miani, Stefano},
booktitle = {Systems and Control: Foundations and Applications},
doi = {10.1007/978-3-319-17933-9_5},
issn = {23249757},
number = {9783319179322},
pages = {193--234},
publisher = {Springer International Publishing},
title = {{Dynamic programming}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}690-1},
year = {2015}
}
@inproceedings{Cai2017,
abstract = {Techniques for automatically designing deep neural network architectures such as reinforcement learning based approaches have recently shown promising results. However, their success is based on vast computational resources (e.g. hundreds of GPUs), making them difficult to be widely used. A noticeable limitation is that they still design and train each network from scratch during the exploration of the architecture space, which is highly inefficient. In this paper, we propose a new framework toward efficient architecture search by exploring the architecture space based on the current network and reusing its weights. We employ a reinforcement learning agent as the meta-controller, whose action is to grow the network depth or layer width with function-preserving transformations. As such, the previously validated networks can be reused for further exploration, thus saves a large amount of computational cost. We apply our method to explore the architecture space of the plain convolutional neural networks (no skip-connections, branching etc.) on image benchmark datasets (CIFAR-10, SVHN) with restricted computational resources (5 GPUs). Our method can design highly competitive networks that outperform existing networks using the same design scheme. On CIFAR-10, our model without skip-connections achieves 4.23{\%} test error rate, exceeding a vast majority of modern architectures and approaching DenseNet. Furthermore, by applying our method to explore the DenseNet architecture space, we are able to achieve more accurate networks with fewer parameters.},
archivePrefix = {arXiv},
arxivId = {1707.04873},
author = {Cai, Han and Chen, Tianyao and Zhang, Weinan and Yu, Yong and Wang, Jun},
booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
eprint = {1707.04873},
isbn = {9781577358008},
pages = {2787--2794},
title = {{Efficient architecture search by network transformation}},
year = {2018}
}
@article{Kamnitsas2016,
abstract = {We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumours, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.},
annote = {{\_}eprint: 1603.05959},
archivePrefix = {arXiv},
arxivId = {1603.05959},
author = {Kamnitsas, Konstantinos and Ledig, Christian and Newcombe, Virginia F.J. and Simpson, Joanna P and Kane, Andrew D and Menon, David K and Rueckert, Daniel and Glocker, Ben},
doi = {10.1016/j.media.2016.10.004},
eprint = {1603.05959},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {3D convolutional neural network,Brain lesions,Deep learning,Fully connected CRF,Segmentation},
pages = {61--78},
title = {{Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation}},
url = {http://arxiv.org/abs/1603.05959},
volume = {36},
year = {2017}
}
@inproceedings{Pham2018,
abstract = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. ENAS constructs a large computational graph, where each subgraph represents a neural network architecture, hence forcing all architectures to share their parameters. A controller is trained with policy gradient to search for a subgraph that maximizes the expected reward on a validation set. Meanwhile a model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Sharing parameters among child models allows ENAS to deliver strong empirical performances, whilst using much fewer GPU-hours than existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On Penn Treebank, ENAS discovers a novel architecture that achieves a test perplexity of 56.3, on par with the existing state-of-the-art among all methods without post-training processing. On CIFAR-10, ENAS finds a novel architecture that achieves 2.89{\%} test error, which is on par with the 2.65{\%} test error of NASNet (Zoph et al., 2018).},
archivePrefix = {arXiv},
arxivId = {1802.03268},
author = {Pham, Hieu and Guan, Melody Y and Zoph, Barret and Le, Quoc V and Dean, Jeff},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1802.03268},
isbn = {9781510867963},
pages = {6522--6531},
title = {{Efficient Neural Architecture Search via parameter Sharing}},
volume = {9},
year = {2018}
}
@incollection{Yamamoto2020,
address = {Cham},
author = {Yamamoto, Masanobu},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_873-1},
pages = {1--7},
publisher = {Springer International Publishing},
title = {{Ego-Motion and EPI Analysis}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}873-1},
year = {2020}
}
@incollection{Takahashi2020,
address = {Cham},
author = {Takahashi, Tomokazu and Murase, Hiroshi},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_711-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Eigenspace Methods}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}711-1},
year = {2020}
}
@misc{docker,
author = {Johnston, Scott},
keywords = {docker},
title = {{Empowering App Development for Developers | Docker}},
url = {https://www.docker.com/}
}
@book{Wallwork2016c,
address = {Cham},
author = {Wallwork, Adrian},
doi = {10.1007/978-3-319-26435-6},
isbn = {978-3-319-26433-2},
publisher = {Springer International Publishing},
title = {{English for Academic Correspondence}},
url = {http://link.springer.com/10.1007/978-3-319-26435-6},
year = {2016}
}
@book{Wallwork2019,
address = {Cham},
author = {Wallwork, Adrian},
doi = {10.1007/978-3-030-11090-1},
isbn = {978-3-030-11089-5},
publisher = {Springer International Publishing},
series = {English for Academic Research},
title = {{English for Academic CVs, Resumes, and Online Profiles}},
url = {http://link.springer.com/10.1007/978-3-030-11090-1},
year = {2019}
}
@book{Wallwork2016a,
address = {Cham},
author = {Wallwork, Adrian},
doi = {10.1007/978-3-319-32687-0},
isbn = {978-3-319-32685-6},
publisher = {Springer International Publishing},
title = {{English for Academic Research: A Guide for Teachers}},
url = {http://link.springer.com/10.1007/978-3-319-32687-0},
year = {2016}
}
@book{Wallwork2013a,
address = {Boston, MA},
author = {Wallwork, Adrian},
doi = {10.1007/978-1-4614-4289-9},
isbn = {978-1-4614-4288-2},
publisher = {Springer US},
title = {{English for Academic Research: Grammar Exercises}},
url = {http://link.springer.com/10.1007/978-1-4614-4289-9},
year = {2013}
}
@book{Wallwork2013b,
address = {Boston, MA},
author = {Wallwork, Adrian},
doi = {10.1007/978-1-4614-4268-4},
isbn = {978-1-4614-4267-7},
publisher = {Springer US},
title = {{English for Academic Research: Vocabulary Exercises}},
url = {http://link.springer.com/10.1007/978-1-4614-4268-4},
year = {2013}
}
@book{Wallwork2016d,
address = {Cham},
author = {Wallwork, Adrian},
doi = {10.1007/978-3-319-28734-8},
isbn = {978-3-319-28732-4},
publisher = {Springer International Publishing},
title = {{English for Interacting on Campus}},
url = {http://link.springer.com/10.1007/978-3-319-28734-8},
year = {2016}
}
@book{Wallwork2016,
address = {Cham},
author = {Wallwork, Adrian},
doi = {10.1007/978-3-319-26330-4},
isbn = {978-3-319-26328-1},
publisher = {Springer International Publishing},
title = {{English for Presentations at International Conferences}},
url = {http://link.springer.com/10.1007/978-3-319-26330-4},
year = {2016}
}
@book{Wallwork2013,
address = {Boston, MA},
author = {Wallwork, Adrian},
doi = {10.1007/978-1-4614-1593-0},
isbn = {978-1-4614-1592-3},
publisher = {Springer US},
title = {{English for Research: Grammar, Usage and Style}},
url = {http://link.springer.com/10.1007/978-1-4614-1593-0},
year = {2013}
}
@book{Wallwork2016b,
address = {Cham},
author = {Wallwork, Adrian},
doi = {10.1007/978-3-319-26094-5},
isbn = {978-3-319-26092-1},
publisher = {Springer International Publishing},
title = {{English for Writing Research Papers}},
url = {http://link.springer.com/10.1007/978-3-319-26094-5},
year = {2016}
}
@online{ENIGMACerebellum2017,
title = {{ENIGMA Cerebellum | MICCAI 2017 Workshop {\&} Challenge}},
url = {https://my.vanderbilt.edu/enigmacerebellum/},
urldate = {2020-05-12}
}
@inproceedings{Klokov2017,
abstract = {We present a new deep learning architecture (called Kdnetwork) that is designed for 3D model recognition tasks and works with unstructured point clouds. The new architecture performs multiplicative transformations and shares parameters of these transformations according to the subdivisions of the point clouds imposed onto them by kdtrees. Unlike the currently dominant convolutional architectures that usually require rasterization on uniform twodimensional or three-dimensional grids, Kd-networks do not rely on such grids in any way and therefore avoid poor scaling behavior. In a series of experiments with popular shape recognition benchmarks, Kd-networks demonstrate competitive performance in a number of shape recognition tasks such as shape classification, shape retrieval and shape part segmentation.},
annote = {{\_}eprint: 1704.01222},
archivePrefix = {arXiv},
arxivId = {1704.01222},
author = {Klokov, Roman and Lempitsky, Victor},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.99},
eprint = {1704.01222},
isbn = {9781538610329},
issn = {15505499},
pages = {863--872},
title = {{Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models}},
url = {http://arxiv.org/abs/1704.01222},
volume = {2017-Octob},
year = {2017}
}
@article{Hameeteman2011,
abstract = {This paper describes an evaluation framework that allows a standardized and objective quantitative comparison of carotid artery lumen segmentation and stenosis grading algorithms. We describe the data repository comprising 56 multi-center, multi-vendor CTA datasets, their acquisition, the creation of the reference standard and the evaluation measures. This framework has been introduced at the MICCAI 2009 workshop 3D Segmentation in the Clinic: A Grand Challenge III, and we compare the results of eight teams that participated. These results show that automated segmentation of the vessel lumen is possible with a precision that is comparable to manual annotation. The framework is open for new submissions through the website http://cls2009.bigr.nl. {\textcopyright} 2011 Elsevier B.V.},
author = {Hameeteman, K. and Zuluaga, M. A. and Freiman, M. and Joskowicz, L. and Cuisenaire, O. and Valencia, L. Fl{\'{o}}rez and G{\"{u}}ls{\"{u}}n, M. A. and Krissian, K. and Mille, J. and Wong, W. C.K. and Orkisz, M. and Tek, H. and Hoyos, M. Hern{\'{a}}ndez and Benmansour, F. and Chung, A. C.S. and Rozie, S. and van Gils, M. and van den Borne, L. and Sosna, J. and Berman, P. and Cohen, N. and Douek, P. C. and S{\'{a}}nchez, I. and Aissat, M. and Schaap, M. and Metz, C. T. and Krestin, G. P. and van der Lugt, A. and Niessen, W. J. and {Van Walsum}, T.},
doi = {10.1016/j.media.2011.02.004},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {CTA,Carotid,Evaluation framework,Lumen segmentation,Stenosis grading},
month = {aug},
number = {4},
pages = {477--488},
title = {{Evaluation framework for carotid bifurcation lumen segmentation and stenosis grading}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841511000260},
volume = {15},
year = {2011}
}
@article{Isgum2015,
abstract = {A number of algorithms for brain segmentation in preterm born infants have been published, but a reliable comparison of their performance is lacking. The NeoBrainS12 study (http://neobrains12.isi.uu.nl), providing three different image sets of preterm born infants, was set up to provide such a comparison. These sets are (i) axial scans acquired at 40. weeks corrected age, (ii) coronal scans acquired at 30. weeks corrected age and (iii) coronal scans acquired at 40. weeks corrected age. Each of these three sets consists of three T1- and T2-weighted MR images of the brain acquired with a 3T MRI scanner. The task was to segment cortical grey matter, non-myelinated and myelinated white matter, brainstem, basal ganglia and thalami, cerebellum, and cerebrospinal fluid in the ventricles and in the extracerebral space separately. Any team could upload the results and all segmentations were evaluated in the same way. This paper presents the results of eight participating teams. The results demonstrate that the participating methods were able to segment all tissue classes well, except myelinated white matter.},
author = {I{\v{s}}gum, Ivana and Benders, Manon J.N.L. and Avants, Brian and Cardoso, M. Jorge and Counsell, Serena J. and Gomez, Elda Fischi and Gui, Laura and Huppi, Petra S. and Kersbergen, Karina J. and Makropoulos, Antonios and Melbourne, Andrew and Moeskops, Pim and Mol, Christian P. and Kuklisova-Murgasova, Maria and Rueckert, Daniel and Schnabel, Julia A. and Srhoj-Egekher, Vedran and Wu, Jue and Wang, Siying and de Vries, Linda S. and Viergever, Max A.},
doi = {10.1016/j.media.2014.11.001},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Brain segmentation,MRI,Neonatal brain,Segmentation comparison,Segmentation evaluation},
month = {feb},
number = {1},
pages = {135--151},
title = {{Evaluation of automatic neonatal brain segmentation algorithms: The NeoBrainS12 challenge}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841514001583},
volume = {20},
year = {2015}
}
@article{Litjens2014,
abstract = {Prostate MRI image segmentation has been an area of intense research due to the increased use of MRI as a modality for the clinical workup of prostate cancer. Segmentation is useful for various tasks, e.g. to accurately localize prostate boundaries for radiotherapy or to initialize multi-modal registration algorithms. In the past, it has been difficult for research groups to evaluate prostate segmentation algorithms on multi-center, multi-vendor and multi-protocol data. Especially because we are dealing with MR images, image appearance, resolution and the presence of artifacts are affected by differences in scanners and/or protocols, which in turn can have a large influence on algorithm accuracy. The Prostate MR Image Segmentation (PROMISE12) challenge was setup to allow a fair and meaningful comparison of segmentation methods on the basis of performance and robustness. In this work we will discuss the initial results of the online PROMISE12 challenge, and the results obtained in the live challenge workshop hosted by the MICCAI2012 conference. In the challenge, 100 prostate MR cases from 4 different centers were included, with differences in scanner manufacturer, field strength and protocol. A total of 11 teams from academic research groups and industry participated. Algorithms showed a wide variety in methods and implementation, including active appearance models, atlas registration and level sets. Evaluation was performed using boundary and volume based metrics which were combined into a single score relating the metrics to human expert performance. The winners of the challenge where the algorithms by teams Imorphics and ScrAutoProstate, with scores of 85.72 and 84.29 overall. Both algorithms where significantly better than all other algorithms in the challenge (p {\textless} 0.05) and had an efficient implementation with a run time of 8. min and 3. s per case respectively. Overall, active appearance model based approaches seemed to outperform other approaches like multi-atlas registration, both on accuracy and computation time. Although average algorithm performance was good to excellent and the Imorphics algorithm outperformed the second observer on average, we showed that algorithm combination might lead to further improvement, indicating that optimal performance for prostate segmentation is not yet obtained. All results are available online at http://promise12.grand-challenge.org/. {\textcopyright} 2013 Elsevier B.V.},
author = {Litjens, Geert and Toth, Robert and van de Ven, Wendy and Hoeks, Caroline and Kerkstra, Sjoerd and van Ginneken, Bram and Vincent, Graham and Guillard, Gwenael and Birbeck, Neil and Zhang, Jindang and Strand, Robin and Malmberg, Filip and Ou, Yangming and Davatzikos, Christos and Kirschner, Matthias and Jung, Florian and Yuan, Jing and Qiu, Wu and Gao, Qinquan and Edwards, Philip Eddie and Maan, Bianca and van der Heijden, Ferdinand and Ghose, Soumya and Mitra, Jhimli and Dowling, Jason and Barratt, Dean and Huisman, Henkjan and Madabhushi, Anant},
doi = {10.1016/j.media.2013.12.002},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Challenge,MRI,Prostate,Segmentation},
title = {{Evaluation of prostate segmentation algorithms for MRI: The PROMISE12 challenge}},
year = {2014}
}
@article{Litjens2014a,
abstract = {Prostate MRI image segmentation has been an area of intense research due to the increased use of MRI as a modality for the clinical workup of prostate cancer. Segmentation is useful for various tasks, e.g. to accurately localize prostate boundaries for radiotherapy or to initialize multi-modal registration algorithms. In the past, it has been difficult for research groups to evaluate prostate segmentation algorithms on multi-center, multi-vendor and multi-protocol data. Especially because we are dealing with MR images, image appearance, resolution and the presence of artifacts are affected by differences in scanners and/or protocols, which in turn can have a large influence on algorithm accuracy. The Prostate MR Image Segmentation (PROMISE12) challenge was setup to allow a fair and meaningful comparison of segmentation methods on the basis of performance and robustness. In this work we will discuss the initial results of the online PROMISE12 challenge, and the results obtained in the live challenge workshop hosted by the MICCAI2012 conference. In the challenge, 100 prostate MR cases from 4 different centers were included, with differences in scanner manufacturer, field strength and protocol. A total of 11 teams from academic research groups and industry participated. Algorithms showed a wide variety in methods and implementation, including active appearance models, atlas registration and level sets. Evaluation was performed using boundary and volume based metrics which were combined into a single score relating the metrics to human expert performance. The winners of the challenge where the algorithms by teams Imorphics and ScrAutoProstate, with scores of 85.72 and 84.29 overall. Both algorithms where significantly better than all other algorithms in the challenge (p {\textless} 0.05) and had an efficient implementation with a run time of 8. min and 3. s per case respectively. Overall, active appearance model based approaches seemed to outperform other approaches like multi-atlas registration, both on accuracy and computation time. Although average algorithm performance was good to excellent and the Imorphics algorithm outperformed the second observer on average, we showed that algorithm combination might lead to further improvement, indicating that optimal performance for prostate segmentation is not yet obtained. All results are available online at http://promise12.grand-challenge.org/. {\textcopyright} 2013 Elsevier B.V.},
author = {Litjens, Geert and Toth, Robert and van de Ven, Wendy and Hoeks, Caroline and Kerkstra, Sjoerd and van Ginneken, Bram and Vincent, Graham and Guillard, Gwenael and Birbeck, Neil and Zhang, Jindang and Strand, Robin and Malmberg, Filip and Ou, Yangming and Davatzikos, Christos and Kirschner, Matthias and Jung, Florian and Yuan, Jing and Qiu, Wu and Gao, Qinquan and Edwards, Philip Eddie and Maan, Bianca and van der Heijden, Ferdinand and Ghose, Soumya and Mitra, Jhimli and Dowling, Jason and Barratt, Dean and Huisman, Henkjan and Madabhushi, Anant},
doi = {10.1016/j.media.2013.12.002},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Challenge,MRI,Prostate,Segmentation},
title = {{Evaluation of prostate segmentation algorithms for MRI: The PROMISE12 challenge}},
year = {2014}
}
@article{Raudaschl2017,
abstract = {Purpose: Automated delineation of structures and organs is a key step in medical imaging. However, due to the large number and diversity of structures and the large variety of segmentation algorithms, a consensus is lacking as to which automated segmentation method works best for certain applications. Segmentation challenges are a good approach for unbiased evaluation and comparison of segmentation algorithms. Methods: In this work, we describe and present the results of the Head and Neck Auto-Segmentation Challenge 2015, a satellite event at the Medical Image Computing and Computer Assisted Interventions (MICCAI) 2015 conference. Six teams participated in a challenge to segment nine structures in the head and neck region of CT images: brainstem, mandible, chiasm, bilateral optic nerves, bilateral parotid glands, and bilateral submandibular glands. Results: This paper presents the quantitative results of this challenge using multiple established error metrics and a well-defined ranking system. The strengths and weaknesses of the different auto-segmentation approaches are analyzed and discussed. Conclusions: The Head and Neck Auto-Segmentation Challenge 2015 was a good opportunity to assess the current state-of-the-art in segmentation of organs at risk for radiotherapy treatment. Participating teams had the possibility to compare their approaches to other methods under unbiased and standardized circumstances. The results demonstrate a clear tendency toward more general purpose and fewer structure-specific segmentation algorithms.},
author = {Raudaschl, Patrik F. and Zaffino, Paolo and Sharp, Gregory C. and Spadea, Maria Francesca and Chen, Antong and Dawant, Benoit M. and Albrecht, Thomas and Gass, Tobias and Langguth, Christoph and Luthi, Marcel and Jung, Florian and Knapp, Oliver and Wesarg, Stefan and Mannion-Haworth, Richard and Bowes, Mike and Ashman, Annaliese and Guillard, Gwenael and Brett, Alan and Vincent, Graham and Orbes-Arteaga, Mauricio and Cardenas-Pena, David and Castellanos-Dominguez, German and Aghdasi, Nava and Li, Yangming and Berens, Angelique and Moe, Kris and Hannaford, Blake and Schubert, Rainer and Fritscher, Karl D.},
doi = {10.1002/mp.12197},
issn = {00942405},
journal = {Medical Physics},
keywords = {Atlas-based segmentation,Automated segmentation,Model-based segmentation,Segmentation challenge},
month = {may},
number = {5},
pages = {2020--2036},
title = {{Evaluation of segmentation methods on head and neck CT: Auto-segmentation challenge 2015}},
url = {http://doi.wiley.com/10.1002/mp.12197},
volume = {44},
year = {2017}
}
@book{Johannsen2011,
abstract = {This paper presents a novel hybrid approach for solving the Container Loading (CL) problem based on the combination of Integer$\backslash$n  Linear Programming (ILP) and Genetic Algorithms (GAs). More precisely, a GA engine works as a generator of reduced instances$\backslash$n  for the original CL problem, which are formulated as ILP models. These instances, in turn, are solved by an exact optimization$\backslash$n  technique (solver), and the performance measures accomplished by the respective models are interpreted as fitness values by$\backslash$n  the genetic algorithm, thus guiding its evolutionary process. Computational experiments performed on standard benchmark problems,$\backslash$n  as well as a practical case study developed in a metallurgic factory, are also reported and discussed here in a manner as$\backslash$n  to testify the potentialities behind the novel approach.},
address = {Cham},
author = {Johannsen, Daniel},
doi = {10.1142/9789814282673_0003},
editor = {Paquete, Lu{\'{i}}s and Zarges, Christine},
isbn = {978-3-030-43679-7},
pages = {53--99},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Evolutionary Computation in Combinatorial Optimization}},
url = {http://link.springer.com/10.1007/978-3-030-43680-3},
volume = {12102},
year = {2011}
}
@article{Stanley2001,
abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
author = {Stanley, Kenneth O and Miikkulainen, Risto},
doi = {10.1162/106365602320169811},
issn = {10636560},
journal = {Evolutionary Computation},
keywords = {Competing conventions,Genetic algorithms,Network topologies,Neural networks,Neuroevolution,Speciation},
number = {2},
pages = {99--127},
pmid = {12180173},
title = {{Evolving neural networks through augmenting topologies}},
volume = {10},
year = {2002}
}
@article{Boord2017,
abstract = {Attention dysfunction is a common but often undiagnosed cognitive impairment in Parkinson's disease that significantly reduces quality of life. We sought to increase understanding of the mechanisms underlying attention dysfunction using functional neuroimaging. Functional MRI was acquired at two repeated sessions in the resting state and during the Attention Network Test, for 25 non-demented subjects with Parkinson's disease and 21 healthy controls. Behavioral and MRI contrasts were calculated for alerting, orienting, and executive control components of attention. Brain regions showing group differences in attention processing were used as seeds in a functional connectivity analysis of a separate resting state run. Parkinson's disease subjects showed more activation during increased executive challenge in four regions of the dorsal attention and frontoparietal networks, namely right frontal eye field, left and right intraparietal sulcus, and precuneus. In three regions we saw reduced resting state connectivity to the default mode network. Further, whereas higher task activation in the right intraparietal sulcus correlated with reduced resting state connectivity between right intraparietal sulcus and the precuneus in healthy controls, this relationship was absent in Parkinson's disease subjects. Our results suggest that a weakened interaction between the default mode and task positive networks might alter the way in which the executive response is processed in PD.},
author = {Boord, Peter and Madhyastha, Tara M. and Askren, Mary K. and Grabowski, Thomas J.},
doi = {10.1016/j.nicl.2016.11.004},
issn = {22131582},
journal = {NeuroImage: Clinical},
keywords = {Attention network test,Default mode network,Executive attention,Functional connectivity,Parkinson's disease},
pages = {1--8},
title = {{Executive attention networks show altered relationship with default mode network in PD}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2213158216302108},
volume = {13},
year = {2017}
}
@incollection{Flach2020,
abstract = {tep involves the computation of the expectation of the likelihood of allmodel parameters by including the hid- den variables as if they were observed. Eachmaximiza- tion step involves the computation of the maximum likelihood estimates of the parameters by maximizing the expected likelihood found during the expectation step. The parameters produced by the maximization step are then used to begin another expectation step, and the process is repeated. It can be shown that an EM iteration will not decrease the observed data likelihood function. How- ever, there is no guarantee that the iteration converges to amaximum likelihood estimator. “Expectation-maximization” has developed to be a general recipe and umbrella term for a class of algo- rithms that iterates between a type of expectation and maximization step. The Baum–Welch algorithm is an example of an EM algorithm specifically suited to HMMs.},
address = {Cham},
author = {Flach, Boris and Hlavac, Vaclav},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_692-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Expectation-Maximization Algorithm}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}692-1},
year = {2020}
}
@article{Ikutani2020a,
abstract = {Expertise enables humans to achieve outstanding performance on domain-specific tasks, and programming is no exception. Many have shown that expert programmers exhibit remarkable differences from novices in behavioral performance, knowledge structure, and selective attention. However, the underlying differences in the brain are still unclear. We here address this issue by associating the cortical representation of source code with individual programming expertise using a data-driven decoding approach. This approach enabled us to identify seven brain regions, widely distributed in the frontal, parietal, and temporal cortices, that have a tight relationship with programming expertise. In these brain regions, functional categories of source code could be decoded from brain activity and the decoding accuracies were significantly correlated with individual behavioral performances on source-code categorization. Our results suggest that programming expertise is built up on fine-tuned cortical representations specialized for the domain of programming.},
author = {Ikutani, Yoshiharu and Kubo, Takatomi and Nishida, Satoshi and Hata, Hideaki and Matsumoto, Kenichi and Ikeda, Kazushi and Nishimoto, Shinji},
doi = {10.1101/2020.01.28.923953},
journal = {bioRxiv},
pages = {2020.01.28.923953},
publisher = {Cold Spring Harbor Laboratory},
title = {{Expert programmers have fine-tuned cortical representations of source code}},
url = {https://www.biorxiv.org/content/10.1101/2020.01.28.923953v1},
year = {2020}
}
@article{Lo2012,
abstract = {This paper describes a framework for establishing a reference airway tree segmentation, which was used to quantitatively evaluate 15 different airway tree extraction algorithms in a standardized manner. Because of the sheer difficulty involved in manually constructing a complete reference standard from scratch, we propose to construct the reference using results from all algorithms that are to be evaluated. We start by subdividing each segmented airway tree into its individual branch segments. Each branch segment is then visually scored by trained observers to determine whether or not it is a correctly segmented part of the airway tree. Finally, the reference airway trees are constructed by taking the union of all correctly extracted branch segments. Fifteen airway tree extraction algorithms from different research groups are evaluated on a diverse set of 20 chest computed tomography (CT) scans of subjects ranging from healthy volunteers to patients with severe pathologies, scanned at different sites, with different CT scanner brands, models, and scanning protocols. Three performance measures covering different aspects of segmentation quality were computed for all participating algorithms. Results from the evaluation showed that no single algorithm could extract more than an average of 74{\%} of the total length of all branches in the reference standard, indicating substantial differences between the algorithms. A fusion scheme that obtained superior results is presented, demonstrating that there is complementary information provided by the different algorithms and there is still room for further improvements in airway segmentation algorithms. {\textcopyright} 1982-2012 IEEE.},
author = {Lo, Pechin and {Van Ginneken}, Bram and Reinhardt, Joseph M. and Yavarna, Tarunashree and {De Jong}, Pim A. and Irving, Benjamin and Fetita, Catalin and Ortner, Margarete and Pinho, R{\^{o}}mulo and Sijbers, Jan and Feuerstein, Marco and Fabijanska, Anna and Bauer, Christian and Beichel, Reinhard and Mendoza, Carlos S. and Wiemker, Rafael and Lee, Jaesung and Reeves, Anthony P. and Born, Silvia and Weinheimer, Oliver and {Van Rikxoort}, Eva M. and Tschirren, Juerg and Mori, Ken and Odry, Benjamin and Naidich, David P. and Hartmann, Ieneke and Hoffman, Eric A. and Prokop, Mathias and Pedersen, Jesper H. and {De Bruijne}, Marleen},
doi = {10.1109/TMI.2012.2209674},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Computed tomography,evaluation,pulmonary airways,segmentation},
pmid = {22855226},
title = {{Extraction of airways from CT (EXACT'09)}},
year = {2012}
}
@incollection{Kumar2020,
abstract = {https://www.pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/},
address = {Cham},
author = {Kumar, Amit and Chellappa, Rama},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_879-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Face Alignment}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}879-1},
year = {2020}
}
@incollection{Loy2020,
abstract = {In recent years, face recognition has attracted much attention and its research has rapidly expanded by not only engineers but also neuroscientists, since it has many potential applications in computer vision},
address = {Cham},
author = {Loy, Chen Change},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_798-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Face Detection}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}798-1},
year = {2020}
}
@incollection{Thom2020,
address = {Cham},
author = {Thom, Nathan and Hand, Emily M.},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_815-1},
pages = {1--13},
publisher = {Springer International Publishing},
title = {{Facial Attribute Recognition: A Survey}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}815-1},
year = {2020}
}
@article{Qaiser2019,
abstract = {Tumor segmentation in whole-slide images of histology slides is an important step towards computer-assisted diagnosis. In this work, we propose a tumor segmentation framework based on the novel concept of persistent homology profiles (PHPs). For a given image patch, the homology profiles are derived by efficient computation of persistent homology, which is an algebraic tool from homology theory. We propose an efficient way of computing topological persistence of an image, alternative to simplicial homology. The PHPs are devised to distinguish tumor regions from their normal counterparts by modeling the atypical characteristics of tumor nuclei. We propose two variants of our method for tumor segmentation: one that targets speed without compromising accuracy and the other that targets higher accuracy. The fast version is based on a selection of exemplar image patches from a convolution neural network (CNN) and patch classification by quantifying the divergence between the PHPs of exemplars and the input image patch. Detailed comparative evaluation shows that the proposed algorithm is significantly faster than competing algorithms while achieving comparable results. The accurate version combines the PHPs and high-level CNN features and employs a multi-stage ensemble strategy for image patch labeling. Experimental results demonstrate that the combination of PHPs and CNN features outperform competing algorithms. This study is performed on two independently collected colorectal datasets containing adenoma, adenocarcinoma, signet, and healthy cases. Collectively, the accurate tumor segmentation produces the highest average patch-level F1-score, as compared with competing algorithms, on malignant and healthy cases from both the datasets. Overall the proposed framework highlights the utility of persistent homology for histopathology image analysis.},
archivePrefix = {arXiv},
arxivId = {1805.03699},
author = {Qaiser, Talha and Tsang, Yee Wah and Taniyama, Daiki and Sakamoto, Naoya and Nakane, Kazuaki and Epstein, David and Rajpoot, Nasir},
doi = {10.1016/j.media.2019.03.014},
eprint = {1805.03699},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Colorectal (colon) cancer,Computational pathology,Deep learning,Histology image analysis,Persistent homology,Tumor segmentation},
pages = {1--14},
pmid = {30991188},
title = {{Fast and accurate tumor segmentation of histology images using persistent homology and deep convolutional features}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841518302688},
volume = {55},
year = {2019}
}
@inproceedings{Klein2016,
abstract = {Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for large datasets, training and validating a single configuration often takes hours, days, or even weeks, which limits the achievable performance. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed FABOLAS, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that FABOLAS often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods or the recently proposed bandit strategy Hyperband.},
archivePrefix = {arXiv},
arxivId = {1605.07079},
author = {Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017},
eprint = {1605.07079},
title = {{Fast Bayesian optimization of machine learning hyperparameters on large datasets}},
volume = {abs/1605.0},
year = {2017}
}
@article{Ren2015,
annote = {From Duplicate 1 (Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks - Ren, Shaoqing; He, Kaiming; Girshick, Ross B; Sun, Jian)

{\_}eprint: 1506.01497},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross B and Sun, Jian},
eprint = {1506.01497},
journal = {CoRR},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
url = {http://arxiv.org/abs/1506.01497},
volume = {abs/1506.0},
year = {2015}
}
@article{Zbontar2018,
abstract = {Accelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements has the potential to reduce medical costs, minimize stress to patients and make MRI possible in applications where it is currently prohibitively slow or expensive. We introduce the fastMRI dataset, a large-scale collection of both raw MR measurements and clinical MR images, that can be used for training and evaluation of machine-learning approaches to MR image reconstruction. By introducing standardized evaluation criteria and a freely-accessible dataset, our goal is to help the community make rapid advances in the state of the art for MR image reconstruction. We also provide a self-contained introduction to MRI for machine learning researchers with no medical imaging background.},
archivePrefix = {arXiv},
arxivId = {1811.08839},
author = {Zbontar, Jure and Knoll, Florian and Sriram, Anuroop and Murrell, Tullie and Huang, Zhengnan and Muckley, Matthew J. and Defazio, Aaron and Stern, Ruben and Johnson, Patricia and Bruno, Mary and Parente, Marc and Geras, Krzysztof J. and Katsnelson, Joe and Chandarana, Hersh and Zhang, Zizhao and Drozdzal, Michal and Romero, Adriana and Rabbat, Michael and Vincent, Pascal and Yakubova, Nafissa and Pinkerton, James and Wang, Duo and Owens, Erich and Zitnick, C. Lawrence and Recht, Michael P. and Sodickson, Daniel K. and Lui, Yvonne W.},
eprint = {1811.08839},
month = {nov},
title = {{fastMRI: An Open Dataset and Benchmarks for Accelerated MRI}},
url = {http://arxiv.org/abs/1811.08839},
year = {2018}
}
@article{Henschel2019,
abstract = {Traditional neuroimage analysis pipelines involve computationally intensive, time-consuming optimization steps, and thus, do not scale well to large cohort studies with thousands or tens of thousands of individuals. In this work we propose a fast and accurate deep learning based neuroimaging pipeline for the automated processing of structural human brain MRI scans, including surface reconstruction and cortical parcellation. To this end, we introduce an advanced deep learning architecture capable of whole brain segmentation into 95 classes in under 1 minute, mimicking FreeSurfer's anatomical segmentation and cortical parcellation. The network architecture incorporates local and global competition via competitive dense blocks and competitive skip pathways, as well as multi-slice information aggregation that specifically tailor network performance towards accurate segmentation of both cortical and sub-cortical structures. Further, we perform fast cortical surface reconstruction and thickness analysis by introducing a spectral spherical embedding and by directly mapping the cortical labels from the image to the surface. This approach provides a full FreeSurfer alternative for volumetric analysis (within 1 minute) and surface-based thickness analysis (within only around 1h run time). For sustainability of this approach we perform extensive validation: we assert high segmentation accuracy on several unseen datasets, measure generalizability and demonstrate increased test-retest reliability, and increased sensitivity to disease effects relative to traditional FreeSurfer.},
annote = {{\_}eprint: 1910.03866},
archivePrefix = {arXiv},
arxivId = {1910.03866},
author = {Henschel, Leonie and Conjeti, Sailesh and Estrada, Santiago and Diers, Kersten and Fischl, Bruce and Reuter, Martin},
eprint = {1910.03866},
title = {{FastSurfer -- A fast and accurate deep learning based neuroimaging pipeline}},
url = {http://arxiv.org/abs/1910.03866},
year = {2019}
}
@article{Ramaswamy2019,
abstract = {We show that a word-level recurrent neural network can predict emoji from text typed on a mobile keyboard. We demonstrate the usefulness of transfer learning for predicting emoji by pretraining the model using a language modeling task. We also propose mechanisms to trigger emoji and tune the diversity of candidates. The model is trained using a distributed on-device learning framework called federated learning. The federated model is shown to achieve better performance than a server-trained model. This work demonstrates the feasibility of using federated learning to train production-quality models for natural language understanding tasks while keeping users' data on their devices.},
archivePrefix = {arXiv},
arxivId = {1906.04329},
author = {Ramaswamy, Swaroop and Mathews, Rajiv and Rao, Kanishka and Beaufays, Fran{\c{c}}oise},
eprint = {1906.04329},
month = {jun},
title = {{Federated Learning for Emoji Prediction in a Mobile Keyboard}},
url = {http://arxiv.org/abs/1906.04329},
year = {2019}
}
@article{Hard2018,
abstract = {We train a recurrent neural network language model using a distributed, on-device learning framework called federated learning for the purpose of next-word prediction in a virtual keyboard for smartphones. Server-based training using stochastic gradient descent is compared with training on client devices using the Federated Averaging algorithm. The federated algorithm, which enables training on a higher-quality dataset for this use case, is shown to achieve better prediction recall. This work demonstrates the feasibility and benefit of training language models on client devices without exporting sensitive user data to servers. The federated learning environment gives users greater control over the use of their data and simplifies the task of incorporating privacy by default with distributed training and aggregation across a population of client devices.},
archivePrefix = {arXiv},
arxivId = {1811.03604},
author = {Hard, Andrew and Rao, Kanishka and Mathews, Rajiv and Ramaswamy, Swaroop and Beaufays, Fran{\c{c}}oise and Augenstein, Sean and Eichner, Hubert and Kiddon, Chlo{\'{e}} and Ramage, Daniel},
eprint = {1811.03604},
month = {nov},
title = {{Federated Learning for Mobile Keyboard Prediction}},
url = {http://arxiv.org/abs/1811.03604},
year = {2018}
}
@article{Konecny2016,
abstract = {Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.},
archivePrefix = {arXiv},
arxivId = {1610.05492},
author = {Kone{\v{c}}n{\'{y}}, Jakub and McMahan, H. Brendan and Yu, Felix X. and Richt{\'{a}}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
eprint = {1610.05492},
month = {oct},
title = {{Federated Learning: Strategies for Improving Communication Efficiency}},
url = {http://arxiv.org/abs/1610.05492},
year = {2016}
}
@article{Yang2019a,
abstract = {Today's artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security.We propose a possible solution to these challenges: Secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning.We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.},
archivePrefix = {arXiv},
arxivId = {1902.04885},
author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
doi = {10.1145/3298981},
eprint = {1902.04885},
issn = {21576912},
journal = {ACM Transactions on Intelligent Systems and Technology},
keywords = {Federated learning,GDPR,transfer learning},
month = {feb},
number = {2},
title = {{Federated machine learning: Concept and applications}},
url = {http://arxiv.org/abs/1902.04885},
volume = {10},
year = {2019}
}
@incollection{Larochelle2020,
address = {Cham},
author = {Larochelle, Hugo},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_861-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Few-Shot Learning}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}861-1},
year = {2020}
}
@article{Neher2014,
abstract = {Purpose: Phantom-based validation of diffusion-weighted image processing techniques is an important key to innovation in the field and is widely used. Openly available and user friendly tools for the flexible generation of tailor-made datasets for the specific tasks at hand can greatly facilitate the work of researchers around the world.
Methods: We present an open-source framework, Fiberfox, that enables (1) the intuitive definition of arbitrary artificial white matter fiber tracts, (2) signal generation from those fibers by means of the most recent multi-compartment modeling techniques, and (3) simulation of the actual MR acquisition that allows for the introduction of realistic MRI-related effects into the final image.
Results: We show that real acquisitions can be closely approximated by simulating the acquisition of the well-known FiberCup phantom. We further demonstrate the advantages of our framework by evaluating the effects of imaging artifacts and acquisition settings on the outcome of 12 tractography algorithms.
Conclusion: Our findings suggest that experiments on a realistic software phantom might change the conclusions drawn from earlier hardware phantom experiments. Fiberfox may find application in validating and further developing methods such as tractography, super-resolution, diffusion modeling or artifact correction.},
author = {Neher, Peter F. and Laun, Frederik B. and Stieltjes, Bram and Maier-Hein, Klaus H.},
doi = {10.1002/mrm.25045},
issn = {15222594},
journal = {Magnetic Resonance in Medicine},
keywords = {Artifact simulation,Diffusion-weighted imaging,Open-source software,S synthetic white matter fibers,Software phantoms},
month = {nov},
number = {5},
pages = {1460--1470},
pmid = {24323973},
title = {{Fiberfox: Facilitating the creation of realistic white matter software phantoms}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24323973},
volume = {72},
year = {2014}
}
@incollection{Maybank2020,
address = {Cham},
author = {Maybank, Stephen J.},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_657-1},
pages = {1--3},
publisher = {Springer International Publishing},
title = {{Fisher-Rao Metric}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}657-1},
year = {2020}
}
@book{Fiadeiro2015,
address = {Cham},
author = {Fiadeiro, Jos{\'{e}} Luiz and Liu, Zhiming},
booktitle = {Science of Computer Programming},
doi = {10.1016/j.scico.2015.11.001},
editor = {Arbab, Farhad and Jongmans, Sung-Shik},
isbn = {978-3-030-40913-5},
issn = {01676423},
pages = {221--222},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Formal Aspects of Component Software (FACS 2013)}},
url = {http://link.springer.com/10.1007/978-3-030-40914-2},
volume = {113},
year = {2015}
}
@misc{Fischl2012,
abstract = {FreeSurfer is a suite of tools for the analysis of neuroimaging data that provides an array of algorithms to quantify the functional, connectional and structural properties of the human brain. It has evolved from a package primarily aimed at generating surface representations of the cerebral cortex into one that automatically creates models of most macroscopically visible structures in the human brain given any reasonable T1-weighted input image. It is freely available, runs on a wide variety of hardware and software platforms, and is open source. {\textcopyright} 2012 Elsevier Inc.},
author = {Fischl, Bruce},
booktitle = {NeuroImage},
doi = {10.1016/j.neuroimage.2012.01.021},
issn = {10538119},
keywords = {MRI,Morphometry,Registration,Segmentation},
number = {2},
pages = {774--781},
pmid = {22248573},
title = {{FreeSurfer}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3685476/},
volume = {62},
year = {2012}
}
@article{Peng2015,
author = {Peng, Hanchuan and Meijering, Erik and Ascoli, Giorgio A.},
doi = {10.1007/s12021-015-9270-9},
issn = {15392791},
journal = {Neuroinformatics},
month = {jul},
number = {3},
pages = {259--260},
title = {{From DIADEM to BigNeuron}},
url = {http://link.springer.com/10.1007/s12021-015-9270-9},
volume = {13},
year = {2015}
}
@article{Radul2001,
abstract = {We introduce a class of Lawson monads and show that these monads have a functional representation. A characterisation of such a representation for the inclusion hyperspace monad is given.},
author = {Radul, Taras},
doi = {10.1023/A:1012052928198},
issn = {09272852},
journal = {Applied Categorical Structures},
keywords = {Functional representations,Lawson monad},
number = {5},
pages = {457--463},
title = {{Functional representations of Lawson monads}},
volume = {9},
year = {2001}
}
@book{Chowdhary2020,
address = {New Delhi},
author = {Chowdhary, K.R.},
booktitle = {Fundamentals of Artificial Intelligence},
doi = {10.1007/978-81-322-3972-7},
isbn = {978-81-322-3970-3},
publisher = {Springer India},
title = {{Fundamentals of Artificial Intelligence}},
url = {http://link.springer.com/10.1007/978-81-322-3972-7},
year = {2020}
}
@incollection{Lecun1989,
author = {Lecun, Yann},
booktitle = {Connectionism in perspective},
editor = {Pfeifer, R and Schreter, Z and Fogelman, F and Steels, L},
publisher = {Elsevier},
title = {{Generalization and network design strategies}},
year = {1989}
}
@article{Yi2019,
abstract = {Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.},
author = {Yi, Xin and Walia, Ekta and Babyn, Paul},
doi = {https://doi.org/10.1016/j.media.2019.101552},
issn = {1361-8415},
journal = {Medical Image Analysis},
keywords = {Deep learning,Generative adversarial network,Generative model,Medical imaging,Review},
pages = {101552},
title = {{Generative adversarial network in medical imaging: A review}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841518308430},
volume = {58},
year = {2019}
}
@article{Brock2016,
abstract = {When working with three-dimensional data, choice of representation is key. We explore voxel-based models, and present evidence for the viability of voxellated representations in applications including shape modeling and object classification. Our key contributions are methods for training voxel-based variational autoencoders, a user interface for exploring the latent space learned by the autoencoder, and a deep convolutional neural network architecture for object classification. We address challenges unique to voxel-based representations, and empirically evaluate our models on the ModelNet benchmark, where we demonstrate a 51.5{\%} relative improvement in the state of the art for object classification.},
annote = {{\_}eprint: 1608.04236},
archivePrefix = {arXiv},
arxivId = {1608.04236},
author = {Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
eprint = {1608.04236},
journal = {CoRR},
title = {{Generative and Discriminative Voxel Modeling with Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1608.04236},
volume = {abs/1608.0},
year = {2016}
}
@article{Zamir2017,
annote = {From Duplicate 1 (Generic 3D Representation via Pose Estimation and Matching - Zamir, Amir Roshan; Wekel, Tilman; Agrawal, Pulkit; Wei, Colin; Malik, Jitendra; Savarese, Silvio)

{\_}eprint: 1710.08247},
archivePrefix = {arXiv},
arxivId = {1710.08247},
author = {Zamir, Amir Roshan and Wekel, Tilman and Agrawal, Pulkit and Wei, Colin and Malik, Jitendra and Savarese, Silvio},
eprint = {1710.08247},
journal = {CoRR},
title = {{Generic 3D Representation via Pose Estimation and Matching}},
url = {http://arxiv.org/abs/1710.08247},
volume = {abs/1710.0},
year = {2017}
}
@article{Horikawa2017,
abstract = {Object recognition is a key function in both human and machine vision. While brain decoding of seen and imagined objects has been achieved, the prediction is limited to training examples. We present a decoding approach for arbitrary objects using the machine vision principle that an object category is represented by a set of features rendered invariant through hierarchical processing. We show that visual features, including those derived from a deep convolutional neural network, can be predicted from fMRI patterns, and that greater accuracy is achieved for low-/high-level features with lower-/higher-level visual areas, respectively. Predicted features are used to identify seen/imagined object categories (extending beyond decoder training) from a set of computed features for numerous object images. Furthermore, decoding of imagined objects reveals progressive recruitment of higher-to-lower visual representations. Our results demonstrate a homology between human and machine vision and its utility for brain-based information retrieval.},
archivePrefix = {arXiv},
arxivId = {1510.06479},
author = {Horikawa, Tomoyasu and Kamitani, Yukiyasu},
doi = {10.1038/ncomms15037},
eprint = {1510.06479},
issn = {20411723},
journal = {Nature Communications},
month = {aug},
number = {1},
pages = {15037},
pmid = {28530228},
title = {{Generic decoding of seen and imagined objects using hierarchical visual features}},
url = {http://www.nature.com/articles/ncomms15037},
volume = {8},
year = {2017}
}
@misc{Bronstein2016,
abstract = {Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.},
annote = {{\_}eprint: 1611.08097},
archivePrefix = {arXiv},
arxivId = {1611.08097},
author = {Bronstein, Michael M and Bruna, Joan and Lecun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
booktitle = {IEEE Signal Processing Magazine},
doi = {10.1109/MSP.2017.2693418},
eprint = {1611.08097},
issn = {10535888},
number = {4},
pages = {18--42},
title = {{Geometric Deep Learning: Going beyond Euclidean data}},
url = {http://arxiv.org/abs/1611.08097},
volume = {34},
year = {2017}
}
@article{Meagher1982,
abstract = {A geometric modeling technique called Octree Encoding is presented. Arbitrary 3-D objects can be represented to any specified resolution in a hierarchical 8-ary tree structure or "octree" Objects may be concave or convex, have holes (including interior holes), consist of disjoint parts, and possess sculptured (i.e., "free-form") surfaces. The memory required for representation and manipulation is on the order of the surface area of the object. A complexity metric is proposed based on the number of nodes in an object's tree representation. Efficient (linear time) algorithms have been developed for the Boolean operations (union, intersection and difference), geometric operations (translation, scaling and rotation), N-dimensional interference detection, and display from any point in space with hidden surfaces removed. The algorithms require neither floating-point operations, integer multiplications, nor integer divisions. In addition, many independent sets of very simple calculations are typically generated, allowing implementation over many inexpensive high-bandwidth processors operating in parallel. Real time analysis and manipulation of highly complex situations thus becomes possible. {\textcopyright} 1982.},
author = {Meagher, Donald},
doi = {10.1016/0146-664X(82)90104-6},
issn = {0146664X},
journal = {Computer Graphics and Image Processing},
number = {2},
pages = {129--147},
title = {{Geometric modeling using octree encoding}},
url = {https://www.sciencedirect.com/science/article/pii/0146664X82901046{\%}0Ahttp://fab.cba.mit.edu/classes/S62.12/docs/Meagher{\_}octree.pdf},
volume = {19},
year = {1982}
}
@book{Strauss2020,
address = {Berkeley, CA},
author = {Strauss, Dirk},
booktitle = {Getting Started with Visual Studio 2019},
doi = {10.1007/978-1-4842-5449-3},
isbn = {978-1-4842-5448-6},
publisher = {Apress},
title = {{Getting Started with Visual Studio 2019}},
url = {http://link.springer.com/10.1007/978-1-4842-5449-3},
year = {2020}
}
@inproceedings{Han2011,
abstract = {Tumor segmentation in PET and CT images is notoriously challenging due to the low spatial resolution in PET and low contrast in CT images. In this paper, we have proposed a general framework to use both PET and CT images simultaneously for tumor segmentation. Our method utilizes the strength of each imaging modality: the superior contrast of PET and the superior spatial resolution of CT. We formulate this problem as a Markov Random Field (MRF) based segmentation of the image pair with a regularized term that penalizes the segmentation difference between PET and CT. Our method simulates the clinical practice of delineating tumor simultaneously using both PET and CT, and is able to concurrently segment tumor from both modalities, achieving globally optimal solutions in low-order polynomial time by a single maximum flow computation. The method was evaluated on clinically relevant tumor segmentation problems. The results showed that our method can effectively make use of both PET and CT image information, yielding segmentation accuracy of 0.85 in Dice similarity coefficient and the average median hausdorff distance (HD) of 6.4 mm, which is 10 {\%} (resp., 16 {\%}) improvement compared to the graph cuts method solely using the PET (resp., CT) images. {\textcopyright} 2011 Springer-Verlag.},
author = {Han, Dongfeng and Bayouth, John and Song, Qi and Taurani, Aakant and Sonka, Milan and Buatti, John and Wu, Xiaodong},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-22092-0_21},
isbn = {9783642220913},
issn = {03029743},
pages = {245--256},
title = {{Globally optimal tumor segmentation in PET-CT images: A graph-based co-segmentation method}},
volume = {6801 LNCS},
year = {2011}
}
@incollection{Xu2020,
address = {Cham},
author = {Xu, Chenyang and Prince, Jerry L.},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_712-1},
pages = {1--8},
publisher = {Springer International Publishing},
title = {{Gradient Vector Flow}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}712-1},
year = {2020}
}
@article{LeCun1998a,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Vinyals2019,
abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8{\%} of officially ranked human players.},
author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"{e}}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'{e}}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"{u}}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
doi = {10.1038/s41586-019-1724-z},
issn = {14764687},
journal = {Nature},
month = {nov},
number = {7782},
pages = {350--354},
pmid = {31666705},
title = {{Grandmaster level in StarCraft II using multi-agent reinforcement learning}},
url = {http://www.nature.com/articles/s41586-019-1724-z},
volume = {575},
year = {2019}
}
@article{Koenders2016,
abstract = {Cannabis is the most frequently used illicit drug worldwide. Cross-sectional neuroimaging studies suggest that chronic cannabis exposure and the development of cannabis use disorders may affect brain morphology. However, cross-sectional studies cannot make a conclusive distinction between cause and consequence and longitudinal neuroimaging studies are lacking. In this prospective study we investigate whether continued cannabis use and higher levels of cannabis exposure in young adults are associated with grey matter reductions. Heavy cannabis users (N = 20, age baseline M = 20.5, SD = 2.1) and non-cannabis using healthy controls (N = 22, age baseline M = 21.6, SD = 2.45) underwent a comprehensive psychological assessment and a T1- structural MRI scan at baseline and 3 years follow-up. Grey matter volumes (orbitofrontal cortex, anterior cingulate cortex, insula, striatum, thalamus, amygdala, hippocampus and cerebellum) were estimated using the software package SPM (VBM-8 module). Continued cannabis use did not have an effect on GM volume change at follow-up. Cross-sectional analyses at baseline and follow-up revealed consistent negative correlations between cannabis related problems and cannabis use (in grams) and regional GM volume of the left hippocampus, amygdala and superior temporal gyrus. These results suggests that small GM volumes in the medial temporal lobe are a risk factor for heavy cannabis use or that the effect of cannabis on GM reductions is limited to adolescence with no further damage of continued use after early adulthood. Long-term prospective studies starting in early adolescence are needed to reach final conclusions.},
author = {Koenders, Laura and Cousijn, Janna and Vingerhoets, Wilhelmina A.M. and {Van Den Brink}, Wim and Wiers, Reinout W. and Meijer, Carin J. and Machielsen, Marise W.J. and Veltman, Dick J. and Goudriaan, Anneke E. and {De Haan}, Lieuwe},
doi = {10.1371/journal.pone.0152482},
editor = {Chen, Kewei},
issn = {19326203},
journal = {PLoS ONE},
month = {may},
number = {5},
pages = {e0152482},
pmid = {27224247},
title = {{Grey matter changes associated with heavy cannabis use: A longitudinal sMRI study}},
url = {https://dx.plos.org/10.1371/journal.pone.0152482},
volume = {11},
year = {2016}
}
@book{Streib2011,
address = {Cham},
author = {Streib, James T.},
booktitle = {Guide to Assembly Language},
doi = {10.1007/978-0-85729-271-1},
isbn = {978-3-030-35638-5},
publisher = {Springer International Publishing},
series = {Undergraduate Topics in Computer Science},
title = {{Guide to Assembly Language}},
url = {http://link.springer.com/10.1007/978-3-030-35639-2},
year = {2011}
}
@book{Skansi2020,
address = {Cham},
booktitle = {Guide to Deep Learning Basics},
doi = {10.1007/978-3-030-37591-1},
editor = {Skansi, Sandro},
isbn = {978-3-030-37590-4},
publisher = {Springer International Publishing},
title = {{Guide to Deep Learning Basics}},
url = {http://link.springer.com/10.1007/978-3-030-37591-1},
year = {2020}
}
@incollection{Ge2020,
abstract = {Real-time and accurate hand pose estimation can open new doors for making the entire world more interactive. The existing systems for hand pose estimation fail to produce an accurate and a physically valid pose in real-time. The approach in this project aims to tackle these problems by applying a discriminative model for real-time prediction of joint locations and, incorporates kinematic constraints for producing a geometrically valid pose, thus leading to accurate pose estimation. The discriminative model is a deep network consisting of convolutional, fully connected and dropout layers. The kinematic constraints are incorporated as a kinematic layer towards the end of the network, which acts as a prior for the hand pose. The results are evaluated on the NYU hand pose data-set and compared with state-of-the-art methods.},
address = {Cham},
author = {Ge, Liuhao and Yuan, Junsong},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_875-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Hand Pose Estimation}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}875-1},
year = {2020}
}
@book{Sabharwal2020,
address = {Berkeley, CA},
author = {Sabharwal, Navin and Edward, Shakuntala Gupta},
booktitle = {Hands On Google Cloud SQL and Cloud Spanner},
doi = {10.1007/978-1-4842-5537-7},
isbn = {978-1-4842-5536-0},
publisher = {Apress},
title = {{Hands On Google Cloud SQL and Cloud Spanner}},
url = {http://link.springer.com/10.1007/978-1-4842-5537-7},
year = {2020}
}
@incollection{Karaman2020,
address = {Cham},
author = {Karaman, Svebor and Chang, Shih-Fu},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_817-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Hashing for Face Search}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}817-1},
year = {2020}
}
@article{Marlow2010,
abstract = {Haskell is a general purpose, purely functional programming language incorporating many recent innovations in programming language design. Haskell provides higher-order functions, non-strict semantics, static poly- morphic typing, user-defined algebraic datatypes, pattern-matching, list comprehensions, a module system, a monadic I/O system, and a rich set of primitive datatypes, including lists, arrays, arbitrary and fixed precision integers, and floating-point numbers. Haskell is both the culmination and solidification of many years of research on non-strict functional languages.},
author = {Marlow, Simon},
journal = {Language},
pages = {329},
title = {{Haskell 2010 Language Report}},
url = {http://haskell.org/definition/haskell2010.pdf},
year = {2010}
}
@article{Shao2018,
abstract = {We present a novel spatial hashing based data structure to facilitate 3D shape analysis using convolutional neural networks (CNNs). Our method builds hierarchical hash tables for an input model under different resolutions that leverage the sparse occupancy of 3D shape boundary. Based on this data structure, we design two efficient GPU algorithms namely hash2col and col2hash so that the CNN operations like convolution and pooling can be efficiently parallelized. The perfect spatial hashing is employed as our spatial hashing scheme, which is not only free of hash collision but also nearly minimal so that our data structure is almost of the same size as the raw input. Compared with existing 3D CNN methods, our data structure significantly reduces the memory footprint during the CNN training. As the input geometry features are more compactly packed, CNN operations also run faster with our data structure. The experiment shows that, under the same network structure, our method yields comparable or better benchmark results compared with the state-of-the-art while it has only one-third memory consumption when under high resolutions (i.e. 256 3).},
annote = {{\_}eprint: 1803.11385},
archivePrefix = {arXiv},
arxivId = {1803.11385},
author = {Shao, Tianjia and Yang, Yin and Weng, Yanlin and Hou, Qiming and Zhou, Kun},
doi = {10.1109/TVCG.2018.2887262},
eprint = {1803.11385},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Computational modeling,Convolution,Data structures,Shape,Solid modeling,Three-dimensional displays,Two dimensional displays,convolutional neural network,perfect hashing,shape classification,shape retrieval,shape segmentation},
title = {{H-CNN: Spatial Hashing Based CNN for 3D Shape Analysis}},
year = {2018}
}
@inproceedings{Liu2017,
abstract = {We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6{\%} on CIFAR-10 and 20.3{\%} when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3{\%} less top-1 accuracy on CIFAR-10 and 0.1{\%} less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.},
archivePrefix = {arXiv},
arxivId = {1711.00436},
author = {Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
booktitle = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
eprint = {1711.00436},
title = {{Hierarchical representations for efficient architecture search}},
volume = {abs/1711.0},
year = {2018}
}
@book{Performance2013,
address = {Cham},
author = {Performance, High},
doi = {10.1007/978-3-030-41050-6},
editor = {Bianchini, Calebe and Osthoff, Carla and Souza, Paulo and Ferreira, Renato},
isbn = {9783319102139},
publisher = {Springer International Publishing},
series = {Communications in Computer and Information Science},
title = {{High Performance Computing Systems}},
url = {http://link.springer.com/10.1007/978-3-030-41050-6},
volume = {1171},
year = {2013}
}
@article{Mascalchi2018,
abstract = {Purpose To assess the potential of histogram metrics of diffusion-tensor imaging (DTI)-derived indices in revealing neurodegeneration and its progression in spinocerebellar ataxia type 2 (SCA2). Materials and methods Nine SCA2 patients and 16 age-matched healthy controls, were examined twice (SCA2 patients 3.6±0.7 years and controls 3.3±1.0 years apart) on the same 1.5T scanner by acquiring T1-weighted and diffusion-weighted (b-value = 1000 s/mm2) images. Cerebrum and brainstem-cerebellum regions were segmented using FreeSurfer suite. Histogram analysis of DTI-derived indices, including mean diffusivity (MD), fractional anisotropy (FA), axial (AD) / radial (RD) diffusivity and mode of anisotropy (MO), was performed. Results At baseline, significant differences between SCA2 patients and controls were confined to brainstem-cerebellum. Median values of MD/AD/RD and FA/MO were significantly (p{\textless}0.001) higher and lower, respectively, in SCA2 patients (1.11/1.30/1.03×10−3 mm2/s and 0.14/0.19) than in controls (0.80/1.00/0.70×10−3 mm2/s and 0.20/0.41). Also, peak location values of MD/AD/RD and FA were significantly (p{\textless}0.001) higher and lower, respectively, in SCA2 patients (0.91/1.11/0.81×10−3 mm2/s and 0.12) than in controls (0.71/0.91/0.63×10−3 mm2/s and 0.18). Peak height values of FA and MD/AD/RD/MO were significantly (p{\textless}0.001) higher and lower, respectively, in SCA2 patients (0.20 and 0.07/0.06/0.07×10−3 mm2/s/year /0.07) than in controls (0.15 and 0.14/0.11/0.12/×10−3 mm2/s/year /0.09). The rate of change of MD median values was significantly (p{\textless}0.001) higher (i.e., increased) in SCA2 patients (0.010×10−3 mm2/s/year) than in controls (-0.003×10−3 mm2/s/year) in the brainstem-cerebellum, whereas no significant difference was found for other indices and in the cerebrum. Conclusion Histogram analysis of DTI-derived indices is a relatively straightforward approach which reveals microstructural changes associated with pontocerebellar degeneration in SCA2 and the median value of MD is capable to track its progression.},
author = {Mascalchi, Mario and Marzi, Chiara and Giannelli, Marco and Ciulli, Stefano and Bianchi, Andrea and Ginestroni, Andrea and Tessa, Carlo and Nicolai, Emanuele and Aiello, Marco and Salvatore, Elena and Soricelli, Andrea and Diciotti, Stefano},
doi = {10.1371/journal.pone.0200258},
editor = {Lenglet, Christophe},
issn = {19326203},
journal = {PLoS ONE},
month = {jul},
number = {7},
pages = {e0200258},
title = {{Histogram analysis of dti-derived indices reveals pontocerebellar degeneration and its progression in SCA2}},
url = {https://dx.plos.org/10.1371/journal.pone.0200258},
volume = {13},
year = {2018}
}
@online{GrandChallengLOLA11,
title = {{Home - LOLA11 - Grand Challenge}},
url = {https://lola11.grand-challenge.org/Home/}
}
@article{Graham2019,
abstract = {Nuclear segmentation and classification within Haematoxylin {\&} Eosin stained histology images is a fundamental prerequisite in the digital pathology work-flow. The development of automated methods for nuclear segmentation and classification enables the quantitative analysis of tens of thousands of nuclei within a whole-slide pathology image, opening up possibilities of further analysis of large-scale nuclear morphometry. However, automated nuclear segmentation and classification is faced with a major challenge in that there are several different types of nuclei, some of them exhibiting large intra-class variability such as the nuclei of tumour cells. Additionally, some of the nuclei are often clustered together. To address these challenges, we present a novel convolutional neural network for simultaneous nuclear segmentation and classification that leverages the instance-rich information encoded within the vertical and horizontal distances of nuclear pixels to their centres of mass. These distances are then utilised to separate clustered nuclei, resulting in an accurate segmentation, particularly in areas with overlapping instances. Then, for each segmented instance the network predicts the type of nucleus via a devoted up-sampling branch. We demonstrate state-of-the-art performance compared to other methods on multiple independent multi-tissue histology image datasets. As part of this work, we introduce a new dataset of Haematoxylin {\&} Eosin stained colorectal adenocarcinoma image tiles, containing 24,319 exhaustively annotated nuclei with associated class labels.},
archivePrefix = {arXiv},
arxivId = {1812.06499},
author = {Graham, Simon and Vu, Quoc Dang and Raza, Shan E.Ahmed and Azam, Ayesha and Tsang, Yee Wah and Kwak, Jin Tae and Rajpoot, Nasir},
doi = {10.1016/j.media.2019.101563},
eprint = {1812.06499},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Computational pathology,Deep learning,Nuclear classification,Nuclear segmentation},
title = {{Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images}},
volume = {58},
year = {2019}
}
@book{Pequegnat2011,
address = {Boston, MA},
doi = {10.1007/978-1-4419-1454-5},
editor = {Pequegnat, Willo and Stover, Ellen and Boyce, Cheryl Anne},
isbn = {978-1-4419-1453-8},
publisher = {Springer US},
title = {{How to Write a Successful Research Grant Application}},
url = {http://link.springer.com/10.1007/978-1-4419-1454-5},
year = {2011}
}
@book{Hering2010,
address = {Berlin, Heidelberg},
author = {Hering, Lutz and Hering, Heike},
doi = {10.1007/978-3-540-69929-3},
isbn = {978-3-540-69928-6},
publisher = {Springer Berlin Heidelberg},
title = {{How to Write Technical Reports}},
url = {http://link.springer.com/10.1007/978-3-540-69929-3},
year = {2010}
}
@book{Hering2019,
address = {Berlin, Heidelberg},
author = {Hering, Heike},
doi = {10.1007/978-3-662-58107-0},
isbn = {978-3-662-58105-6},
publisher = {Springer Berlin Heidelberg},
title = {{How to Write Technical Reports}},
url = {http://link.springer.com/10.1007/978-3-662-58107-0},
year = {2019}
}
@article{Dolz2018,
abstract = {Recently, dense connections have attracted substantial attention in computer vision because they facilitate gradient flow and implicit deep supervision during training. Particularly, DenseNet that connects each layer to every other layer in a feed-forward fashion and has shown impressive performances in natural image classification tasks. We propose HyperDenseNet, a 3-D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems. Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path but also between those across different paths. This contrasts with the existing multi-modal CNN approaches, in which modeling several modalities relies entirely on a single joint layer (or level of abstraction) for fusion, typically either at the input or at the output of the network. Therefore, the proposed network has total freedom to learn more complex combinations between the modalities, within and in-between all the levels of abstraction, which increases significantly the learning representation. We report extensive evaluations over two different and highly competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing on six month infant data and the latter on adult images. HyperDenseNet yielded significant improvements over many state-of-the-art segmentation networks, ranking at the top on both benchmarks. We further provide a comprehensive experimental analysis of features re-use, which confirms the importance of hyper-dense connections in multi-modal representation learning. Our code is publicly available.},
annote = {{\_}eprint: 1804.02967},
archivePrefix = {arXiv},
arxivId = {1804.02967},
author = {Dolz, Jose and Gopinath, Karthik and Yuan, Jing and Lombaert, Herve and Desrosiers, Christian and {Ben Ayed}, Ismail},
doi = {10.1109/TMI.2018.2878669},
eprint = {1804.02967},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {3-D CNN,Deep learning,brain MRI,multi-modal imaging,segmentation},
number = {5},
pages = {1116--1126},
title = {{HyperDense-Net: A Hyper-Densely Connected CNN for Multi-Modal Image Segmentation}},
url = {http://arxiv.org/abs/1804.02967},
volume = {38},
year = {2019}
}
@article{Singanamalli2016,
abstract = {Background To identify computer extracted in vivo dynamic contrast enhanced (DCE) MRI markers associated with quantitative histomorphometric (QH) characteristics of microvessels and Gleason scores (GS) in prostate cancer. Methods This study considered retrospective data from 23 biopsy confirmed prostate cancer patients who underwent 3 Tesla multiparametric MRI before radical prostatectomy (RP). Representative slices from RP specimens were stained with vascular marker CD31. Tumor extent was mapped from RP sections onto DCE MRI using nonlinear registration methods. Seventy-seven microvessel QH features and 18 DCE MRI kinetic features were extracted and evaluated for their ability to distinguish low from intermediate and high GS. The effect of temporal sampling on kinetic features was assessed and correlations between those robust to temporal resolution and microvessel features discriminative of GS were examined. Results A total of 12 microvessel architectural features were discriminative of low and intermediate/high grade tumors with area under the receiver operating characteristic curve (AUC)  0.7. These features were most highly correlated with mean washout gradient (WG) (max rho = −0.62). Independent analysis revealed WG to be moderately robust to temporal resolution (intraclass correlation coefficient [ICC] = 0.63) and WG variance, which was poorly correlated with microvessel features, to be predictive of low grade tumors (AUC = 0.77). Enhancement ratio was the most robust (ICC = 0.96) and discriminative (AUC = 0.78) kinetic feature but was moderately correlated with microvessel features (max rho = −0.52). Conclusion Computer extracted features of prostate DCE MRI appear to be correlated with microvessel architecture and may be discriminative of low versus intermediate and high GS. J. MAGN. RESON. IMAGING 2016;43:149–158.},
annote = {{\_}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jmri.24975},
author = {Singanamalli, Asha and Rusu, Mirabela and Sparks, Rachel E and Shih, Natalie N C and Ziober, Amy and Wang, Li-Ping and Tomaszewski, John and Rosen, Mark and Feldman, Michael and Madabhushi, Anant},
doi = {10.1002/jmri.24975},
journal = {Journal of Magnetic Resonance Imaging},
keywords = {DCE MRI,Gleason grades,imaging biomarkers,microvessel architecture,prostate cancer,quantitative histomorphometry},
number = {1},
pages = {149--158},
title = {{Identifying in vivo DCE MRI markers associated with microvessel architecture and gleason grades of prostate cancer}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jmri.24975},
volume = {43},
year = {2016}
}
@article{Kermany2018,
abstract = {The implementation of clinical-decision support algorithms for medical imaging faces challenges with reliability and interpretability. Here, we establish a diagnostic tool based on a deep-learning framework for the screening of patients with common treatable blinding retinal diseases. Our framework utilizes transfer learning, which trains a neural network with a fraction of the data of conventional approaches. Applying this approach to a dataset of optical coherence tomography images, we demonstrate performance comparable to that of human experts in classifying age-related macular degeneration and diabetic macular edema. We also provide a more transparent and interpretable diagnosis by highlighting the regions recognized by the neural network. We further demonstrate the general applicability of our AI system for diagnosis of pediatric pneumonia using chest X-ray images. This tool may ultimately aid in expediting the diagnosis and referral of these treatable conditions, thereby facilitating earlier treatment, resulting in improved clinical outcomes. Video Abstract: [Figure presented] Image-based deep learning classifies macular degeneration and diabetic retinopathy using retinal optical coherence tomography images and has potential for generalized applications in biomedical image interpretation and medical decision making.},
author = {Kermany, Daniel S. and Goldbaum, Michael and Cai, Wenjia and Valentim, Carolina C.S. and Liang, Huiying and Baxter, Sally L. and McKeown, Alex and Yang, Ge and Wu, Xiaokang and Yan, Fangbing and Dong, Justin and Prasadha, Made K. and Pei, Jacqueline and Ting, Magdalena and Zhu, Jie and Li, Christina and Hewett, Sierra and Dong, Jason and Ziyar, Ian and Shi, Alexander and Zhang, Runze and Zheng, Lianghong and Hou, Rui and Shi, William and Fu, Xin and Duan, Yaou and Huu, Viet A.N. and Wen, Cindy and Zhang, Edward D. and Zhang, Charlotte L. and Li, Oulan and Wang, Xiaobo and Singer, Michael A. and Sun, Xiaodong and Xu, Jie and Tafreshi, Ali and Lewis, M. Anthony and Xia, Huimin and Zhang, Kang},
doi = {10.1016/j.cell.2018.02.010},
issn = {10974172},
journal = {Cell},
keywords = {age-related macular degeneration,artificial intelligence,choroidal neovascularization,deep learning,diabetic macular edema,diabetic retinopathy,optical coherence tomography,pneumonia,screening,transfer learning},
month = {feb},
number = {5},
pages = {1122--1131.e9},
pmid = {29474911},
title = {{Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0092867418301545},
volume = {172},
year = {2018}
}
@article{Bakas2018,
abstract = {Gliomas are the most common primary brain malignancies, with different degrees of aggressiveness, variable prognosis and various heterogeneous histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic core, active and non-enhancing core. This intrinsic heterogeneity is also portrayed in their radio-phenotype, as their sub-regions are depicted by varying intensity profiles disseminated across multi-parametric magnetic resonance imaging (mpMRI) scans, reflecting varying biological properties. Their heterogeneous shape, extent, and location are some of the factors that make these tumors difficult to resect, and in some cases inoperable. The amount of resected tumor is a factor also considered in longitudinal scans, when evaluating the apparent tumor for potential diagnosis of progression. Furthermore, there is mounting evidence that accurate segmentation of the various tumor sub-regions can offer the basis for quantitative image analysis towards prediction of patient overall survival. This study assesses the state-of-the-art machine learning (ML) methods used for brain tumor image analysis in mpMRI scans, during the last seven instances of the International Brain Tumor Segmentation (BraTS) challenge, i.e., 2012-2018. Specifically, we focus on i) evaluating segmentations of the various glioma sub-regions in pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue of longitudinal growth of tumor sub-regions, beyond use of the RECIST/RANO criteria, and iii) predicting the overall survival from pre-operative mpMRI scans of patients that underwent gross total resection. Finally, we investigate the challenge of identifying the best ML algorithms for each of these tasks, considering that apart from being diverse on each instance of the challenge, the multi-institutional mpMRI BraTS dataset has also been a continuously evolving/growing dataset.},
archivePrefix = {arXiv},
arxivId = {1811.02629},
author = {Bakas, Spyridon and Reyes, Mauricio and Jakab, Andras and Bauer, Stefan and Rempfler, Markus and Crimi, Alessandro and Shinohara, Russell Takeshi and Berger, Christoph and Ha, Sung Min and Rozycki, Martin and Prastawa, Marcel and Alberts, Esther and Lipkova, Jana and Freymann, John and Kirby, Justin and Bilello, Michel and Fathallah-Shaykh, Hassan and Wiest, Roland and Kirschke, Jan and Wiestler, Benedikt and Colen, Rivka and Kotrotsou, Aikaterini and Lamontagne, Pamela and Marcus, Daniel and Milchenko, Mikhail and Nazeri, Arash and Weber, Marc-Andre and Mahajan, Abhishek and Baid, Ujjwal and Gerstner, Elizabeth and Kwon, Dongjin and Acharya, Gagan and Agarwal, Manu and Alam, Mahbubul and Albiol, Alberto and Albiol, Antonio and Albiol, Francisco J. and Alex, Varghese and Allinson, Nigel and Amorim, Pedro H. A. and Amrutkar, Abhijit and Anand, Ganesh and Andermatt, Simon and Arbel, Tal and Arbelaez, Pablo and Avery, Aaron and Azmat, Muneeza and B., Pranjal and Bai, W and Banerjee, Subhashis and Barth, Bill and Batchelder, Thomas and Batmanghelich, Kayhan and Battistella, Enzo and Beers, Andrew and Belyaev, Mikhail and Bendszus, Martin and Benson, Eze and Bernal, Jose and Bharath, Halandur Nagaraja and Biros, George and Bisdas, Sotirios and Brown, James and Cabezas, Mariano and Cao, Shilei and Cardoso, Jorge M. and Carver, Eric N and Casamitjana, Adri{\`{a}} and Castillo, Laura Silvana and Cat{\`{a}}, Marcel and Cattin, Philippe and Cerigues, Albert and Chagas, Vinicius S. and Chandra, Siddhartha and Chang, Yi-Ju and Chang, Shiyu and Chang, Ken and Chazalon, Joseph and Chen, Shengcong and Chen, Wei and Chen, Jefferson W and Chen, Zhaolin and Cheng, Kun and Choudhury, Ahana Roy and Chylla, Roger and Cl{\'{e}}rigues, Albert and Colleman, Steven and Colmeiro, Ramiro German Rodriguez and Combalia, Marc and Costa, Anthony and Cui, Xiaomeng and Dai, Zhenzhen and Dai, Lutao and Daza, Laura Alexandra and Deutsch, Eric and Ding, Changxing and Dong, Chao and Dong, Shidu and Dudzik, Wojciech and Eaton-Rosen, Zach and Egan, Gary and Escudero, Guilherme and Estienne, Th{\'{e}}o and Everson, Richard and Fabrizio, Jonathan and Fan, Yong and Fang, Longwei and Feng, Xue and Ferrante, Enzo and Fidon, Lucas and Fischer, Martin and French, Andrew P. and Fridman, Naomi and Fu, Huan and Fuentes, David and Gao, Yaozong and Gates, Evan and Gering, David and Gholami, Amir and Gierke, Willi and Glocker, Ben and Gong, Mingming and Gonz{\'{a}}lez-Vill{\'{a}}, Sandra and Grosges, T. and Guan, Yuanfang and Guo, Sheng and Gupta, Sudeep and Han, Woo-Sup and Han, Il Song and Harmuth, Konstantin and He, Huiguang and Hern{\'{a}}ndez-Sabat{\'{e}}, Aura and Herrmann, Evelyn and Himthani, Naveen and Hsu, Winston and Hsu, Cheyu and Hu, Xiaojun and Hu, Xiaobin and Hu, Yan and Hu, Yifan and Hua, Rui and Huang, Teng-Yi and Huang, Weilin and {Van Huffel}, Sabine and Huo, Quan and HV, Vivek and Iftekharuddin, Khan M. and Isensee, Fabian and Islam, Mobarakol and Jackson, Aaron S. and Jambawalikar, Sachin R. and Jesson, Andrew and Jian, Weijian and Jin, Peter and Jose, V Jeya Maria and Jungo, Alain and Kainz, B and Kamnitsas, Konstantinos and Kao, Po-Yu and Karnawat, Ayush and Kellermeier, Thomas and Kermi, Adel and Keutzer, Kurt and Khadir, Mohamed Tarek and Khened, Mahendra and Kickingereder, Philipp and Kim, Geena and King, Nik and Knapp, Haley and Knecht, Urspeter and Kohli, Lisa and Kong, Deren and Kong, Xiangmao and Koppers, Simon and Kori, Avinash and Krishnamurthi, Ganapathy and Krivov, Egor and Kumar, Piyush and Kushibar, Kaisar and Lachinov, Dmitrii and Lambrou, Tryphon and Lee, Joon and Lee, Chengen and Lee, Yuehchou and Lee, M and Lefkovits, Szidonia and Lefkovits, Laszlo and Levitt, James and Li, Tengfei and Li, Hongwei and Li, Wenqi and Li, Hongyang and Li, Xiaochuan and Li, Yuexiang and Li, Heng and Li, Zhenye and Li, Xiaoyu and Li, Zeju and Li, XiaoGang and Li, Wenqi and Lin, Zheng-Shen and Lin, Fengming and Lio, Pietro and Liu, Chang and Liu, Boqiang and Liu, Xiang and Liu, Mingyuan and Liu, Ju and Liu, Luyan and Llado, Xavier and Lopez, Marc Moreno and Lorenzo, Pablo Ribalta and Lu, Zhentai and Luo, Lin and Luo, Zhigang and Ma, Jun and Ma, Kai and Mackie, Thomas and Madabushi, Anant and Mahmoudi, Issam and Maier-Hein, Klaus H. and Maji, Pradipta and Mammen, CP and Mang, Andreas and Manjunath, B. S. and Marcinkiewicz, Michal and McDonagh, S and McKenna, Stephen and McKinley, Richard and Mehl, Miriam and Mehta, Sachin and Mehta, Raghav and Meier, Raphael and Meinel, Christoph and Merhof, Dorit and Meyer, Craig and Miller, Robert and Mitra, Sushmita and Moiyadi, Aliasgar and Molina-Garcia, David and Monteiro, Miguel A. B. and Mrukwa, Grzegorz and Myronenko, Andriy and Nalepa, Jakub and Ngo, Thuyen and Nie, Dong and Ning, Holly and Niu, Chen and Nuechterlein, Nicholas K and Oermann, Eric and Oliveira, Arlindo and Oliveira, Diego D. C. and Oliver, Arnau and Osman, Alexander F. I. and Ou, Yu-Nian and Ourselin, Sebastien and Paragios, Nikos and Park, Moo Sung and Paschke, Brad and Pauloski, J. Gregory and Pawar, Kamlesh and Pawlowski, Nick and Pei, Linmin and Peng, Suting and Pereira, Silvio M. and Perez-Beteta, Julian and Perez-Garcia, Victor M. and Pezold, Simon and Pham, Bao and Phophalia, Ashish and Piella, Gemma and Pillai, G. N. and Piraud, Marie and Pisov, Maxim and Popli, Anmol and Pound, Michael P. and Pourreza, Reza and Prasanna, Prateek and Prkovska, Vesna and Pridmore, Tony P. and Puch, Santi and Puybareau, {\'{E}}lodie and Qian, Buyue and Qiao, Xu and Rajchl, Martin and Rane, Swapnil and Rebsamen, Michael and Ren, Hongliang and Ren, Xuhua and Revanuru, Karthik and Rezaei, Mina and Rippel, Oliver and Rivera, Luis Carlos and Robert, Charlotte and Rosen, Bruce and Rueckert, Daniel and Safwan, Mohammed and Salem, Mostafa and Salvi, Joaquim and Sanchez, Irina and S{\'{a}}nchez, Irina and Santos, Heitor M. and Sartor, Emmett and Schellingerhout, Dawid and Scheufele, Klaudius and Scott, Matthew R. and Scussel, Artur A. and Sedlar, Sara and Serrano-Rubio, Juan Pablo and Shah, N. Jon and Shah, Nameetha and Shaikh, Mazhar and Shankar, B. Uma and Shboul, Zeina and Shen, Haipeng and Shen, Dinggang and Shen, Linlin and Shen, Haocheng and Shenoy, Varun and Shi, Feng and Shin, Hyung Eun and Shu, Hai and Sima, Diana and Sinclair, M and Smedby, Orjan and Snyder, James M. and Soltaninejad, Mohammadreza and Song, Guidong and Soni, Mehul and Stawiaski, Jean and Subramanian, Shashank and Sun, Li and Sun, Roger and Sun, Jiawei and Sun, Kay and Sun, Yu and Sun, Guoxia and Sun, Shuang and Suter, Yannick R and Szilagyi, Laszlo and Talbar, Sanjay and Tao, Dacheng and Tao, Dacheng and Teng, Zhongzhao and Thakur, Siddhesh and Thakur, Meenakshi H and Tharakan, Sameer and Tiwari, Pallavi and Tochon, Guillaume and Tran, Tuan and Tsai, Yuhsiang M. and Tseng, Kuan-Lun and Tuan, Tran Anh and Turlapov, Vadim and Tustison, Nicholas and Vakalopoulou, Maria and Valverde, Sergi and Vanguri, Rami and Vasiliev, Evgeny and Ventura, Jonathan and Vera, Luis and Vercauteren, Tom and Verrastro, C. A. and Vidyaratne, Lasitha and Vilaplana, Veronica and Vivekanandan, Ajeet and Wang, Guotai and Wang, Qian and Wang, Chiatse J. and Wang, Weichung and Wang, Duo and Wang, Ruixuan and Wang, Yuanyuan and Wang, Chunliang and Wang, Guotai and Wen, Ning and Wen, Xin and Weninger, Leon and Wick, Wolfgang and Wu, Shaocheng and Wu, Qiang and Wu, Yihong and Xia, Yong and Xu, Yanwu and Xu, Xiaowen and Xu, Peiyuan and Yang, Tsai-Ling and Yang, Xiaoping and Yang, Hao-Yu and Yang, Junlin and Yang, Haojin and Yang, Guang and Yao, Hongdou and Ye, Xujiong and Yin, Changchang and Young-Moxon, Brett and Yu, Jinhua and Yue, Xiangyu and Zhang, Songtao and Zhang, Angela and Zhang, Kun and Zhang, Xuejie and Zhang, Lichi and Zhang, Xiaoyue and Zhang, Yazhuo and Zhang, Lei and Zhang, Jianguo and Zhang, Xiang and Zhang, Tianhao and Zhao, Sicheng and Zhao, Yu and Zhao, Xiaomei and Zhao, Liang and Zheng, Yefeng and Zhong, Liming and Zhou, Chenhong and Zhou, Xiaobing and Zhou, Fan and Zhu, Hongtu and Zhu, Jin and Zhuge, Ying and Zong, Weiwei and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Davatzikos, Christos and van Leemput, Koen and Menze, Bjoern},
eprint = {1811.02629},
month = {nov},
title = {{Identifying the Best Machine Learning Algorithms for Brain Tumor Segmentation, Progression Assessment, and Overall Survival Prediction in the BRATS Challenge}},
url = {http://arxiv.org/abs/1811.02629},
year = {2018}
}
@article{Porwal2020,
abstract = {Diabetic Retinopathy (DR) is the most common cause of avoidable vision loss, predominantly affecting the working-age population across the globe. Screening for DR, coupled with timely consultation and treatment, is a globally trusted policy to avoid vision loss. However, implementation of DR screening programs is challenging due to the scarcity of medical professionals able to screen a growing global diabetic population at risk for DR. Computer-aided disease diagnosis in retinal image analysis could provide a sustainable approach for such large-scale screening effort. The recent scientific advances in computing capacity and machine learning approaches provide an avenue for biomedical scientists to reach this goal. Aiming to advance the state-of-the-art in automatic DR diagnosis, a grand challenge on “Diabetic Retinopathy – Segmentation and Grading” was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI - 2018). In this paper, we report the set-up and results of this challenge that is primarily based on Indian Diabetic Retinopathy Image Dataset (IDRiD). There were three principal sub-challenges: lesion segmentation, disease severity grading, and localization of retinal landmarks and segmentation. These multiple tasks in this challenge allow to test the generalizability of algorithms, and this is what makes it different from existing ones. It received a positive response from the scientific community with 148 submissions from 495 registrations effectively entered in this challenge. This paper outlines the challenge, its organization, the dataset used, evaluation methods and results of top-performing participating solutions. The top-performing approaches utilized a blend of clinical information, data augmentation, and an ensemble of models. These findings have the potential to enable new developments in retinal image analysis and image-based DR screening in particular.},
author = {Porwal, Prasanna and Pachade, Samiksha and Kokare, Manesh and Deshmukh, Girish and Son, Jaemin and Bae, Woong and Liu, Lihong and Wang, Jianzong and Liu, Xinhui and Gao, Liangxin and Wu, Tian Bo and Xiao, Jing and Wang, Fengyan and Yin, Baocai and Wang, Yunzhi and Danala, Gopichandh and He, Linsheng and Choi, Yoon Ho and Lee, Yeong Chan and Jung, Sang Hyuk and Li, Zhongyu and Sui, Xiaodan and Wu, Junyan and Li, Xiaolong and Zhou, Ting and Toth, Janos and Baran, Agnes and Kori, Avinash and Chennamsetty, Sai Saketh and Safwan, Mohammed and Alex, Varghese and Lyu, Xingzheng and Cheng, Li and Chu, Qinhao and Li, Pengcheng and Ji, Xin and Zhang, Sanyuan and Shen, Yaxin and Dai, Ling and Saha, Oindrila and Sathish, Rachana and Melo, T{\^{a}}nia and Ara{\'{u}}jo, Teresa and Harangi, Balazs and Sheng, Bin and Fang, Ruogu and Sheet, Debdoot and Hajdu, Andras and Zheng, Yuanjie and Mendon{\c{c}}a, Ana Maria and Zhang, Shaoting and Campilho, Aur{\'{e}}lio and Zheng, Bin and Shen, Dinggang and Giancardo, Luca and Quellec, Gwenol{\'{e}} and M{\'{e}}riaudeau, Fabrice},
doi = {10.1016/j.media.2019.101561},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Challenge,Deep learning,Diabetic Retinopathy,Retinal image analysis},
month = {jan},
pages = {101561},
pmid = {31671320},
title = {{IDRiD: Diabetic Retinopathy – Segmentation and Grading Challenge}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841519301033},
volume = {59},
year = {2020}
}
@incollection{Lepetit2020,
address = {Cham},
author = {Lepetit, Vincent},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_797-1},
pages = {1--8},
publisher = {Springer International Publishing},
title = {{Image Descriptors and Similarity Measures}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}797-1},
year = {2020}
}
@incollection{Yu2020,
address = {Cham},
author = {Yu, Guoshen and Sapiro, Guillermo},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_233-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Image Enhancement and Restoration: Traditional Approaches}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}233-1},
year = {2020}
}
@incollection{Farid2020,
address = {Cham},
author = {Farid, Hany},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_877-1},
pages = {1--10},
publisher = {Springer International Publishing},
title = {{Image Forensics}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}877-1},
year = {2020}
}
@article{Wang2004,
abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.},
author = {Wang, Zhou and Bovik, Alan Conrad and Sheikh, Hamid Rahim and Simoncelli, Eero P.},
doi = {10.1109/TIP.2003.819861},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Error sensitivity,Human visual system (HVS),Image coding,Image quality assessment,JPEG,JPEG2000,Perceptual quality,Structural information,Structural similarity (SSIM)},
month = {apr},
number = {4},
pages = {600--612},
pmid = {15376593},
title = {{Image quality assessment: From error visibility to structural similarity}},
url = {http://ieeexplore.ieee.org/document/1284395/},
volume = {13},
year = {2004}
}
@incollection{Brown2020,
abstract = {Image stitching or photo stitching is the process of combining multiple photographic images with overlapping fields of view to produce a segmented panorama or high-resolution image. Commonly performed through the use of computer software, most approaches to image stitching require nearly exact overlaps between images and identical exposures to produce seamless results, although some stitching algorithms actually benefit from differently exposed images by doing HDR (High Dynamic Range) imaging in regions of overlap. Some digital cameras can stitch their photos internally. Image stitching is widely used in today's world in applications such as$\backslash$n“Image Stabilization” feature in camcorders which use frame-rate image alignment.$\backslash$nHigh resolution photo mosaics in digital maps and satellite photos.$\backslash$nMedical Imaging.$\backslash$nMultiple image super-resolution.$\backslash$nVideo Stitching.$\backslash$nObject Insertion.},
address = {Cham},
author = {Brown, Matthew},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_13-1},
pages = {1--3},
publisher = {Springer International Publishing},
title = {{Image Stitching}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}13-1},
year = {2020}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {10.1145/3065386},
editor = {Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q},
issn = {15577317},
journal = {Communications of the ACM},
number = {6},
pages = {84--90},
publisher = {Curran Associates, Inc.},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
volume = {60},
year = {2017}
}
@book{Turner2020,
address = {Cham},
author = {Turner, Phil},
doi = {10.1007/978-3-030-37348-1},
isbn = {978-3-030-37347-4},
publisher = {Springer International Publishing},
series = {Human–Computer Interaction Series},
title = {{Imagination + Technology}},
url = {http://link.springer.com/10.1007/978-3-030-37348-1},
year = {2020}
}
@article{Paper2015,
author = {Paper, Scientific and Lillholm, M and Pai, A and Balas, I and Anker, C and Igel, C and Nielsen, M},
title = {{Improved Alzheimer ' s disease diagnostic performance using structural MRI : validation of the MRI combination biomarker that won the CADDementia challenge}},
year = {2015}
}
@book{Summers2018,
annote = {{\_}eprint: 1805.11272},
author = {Summers, Cecilia and Dinneen, Michael J},
title = {{Improved Mixed-Example Data Augmentation}},
year = {2018}
}
@inproceedings{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and nonresidual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08{\%} top-5 error on the test set of the ImageNet classification (CLS) challenge.},
annote = {{\_}eprint: 1602.07261},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
booktitle = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
eprint = {1602.07261},
pages = {4278--4284},
title = {{Inception-v4, inception-ResNet and the impact of residual connections on learning}},
year = {2017}
}
@data{GrandChallengeIDRID,
abstract = {Diabetic Retinopathy is the most prevalent cause of avoidable vision impairment, mainly affecting working age population in the world. Recent research has given a better understanding of requirement in clinical eye care practice to identify better and cheaper ways of identification, management, diagnosis and treatment of retinal disease. The importance of diabetic retinopathy screening programs and difficulty in achieving reliable early diagnosis of diabetic retinopathy at a reasonable cost needs attention to develop computer-aided diagnosis tool. Computer-aided disease diagnosis in retinal image analysis could ease mass screening of population with diabetes mellitus and help clinicians in utilizing their time more efficiently. The recent technological advances in computing power, communication systems, and machine learning techniques provide opportunities to the biomedical engineers and computer scientists to meet the requirements of clinical practice. Diverse and representative retinal image sets are essential for developing and testing digital screening programs and the automated algorithms at their core. To the best of our knowledge, the database for this challenge, IDRiD (Indian Diabetic Retinopathy Image Dataset), is the first database representative of an Indian population. Moreover, it is the only dataset constituting typical diabetic retinopathy lesions and also normal retinal structures annotated at a pixel level. This dataset provides information on the disease severity of diabetic retinopathy, and diabetic macular edema for each image. This makes it perfect for development and evaluation of image analysis algorithms for early detection of diabetic retinopathy.},
author = {Porwal, Prasanna and Pachade, Samiksha and Kamble, Ravi and Kokare, Manesh and Deshmukh, Girish and Sahasrabuddhe, Vivek and Meriaudeau, Fabrice},
booktitle = {IEEE Dataport},
doi = {10.21227/H25W98},
publisher = {IEEE Dataport},
title = {{Indian Diabetic Retinopathy Image Dataset (IDRiD)}},
url = {http://dx.doi.org/10.21227/H25W98},
year = {2018}
}
@article{Porwal2018,
abstract = {Diabetic Retinopathy is the most prevalent cause of avoidable vision impairment, mainly affecting the working-age population in the world. Recent research has given a better understanding of the requirement in clinical eye care practice to identify better and cheaper ways of identification, management, diagnosis and treatment of retinal disease. The importance of diabetic retinopathy screening programs and difficulty in achieving reliable early diagnosis of diabetic retinopathy at a reasonable cost needs attention to develop computer-aided diagnosis tool. Computer-aided disease diagnosis in retinal image analysis could ease mass screening of populations with diabetes mellitus and help clinicians in utilizing their time more efficiently. The recent technological advances in computing power, communication systems, and machine learning techniques provide opportunities to the biomedical engineers and computer scientists to meet the requirements of clinical practice. Diverse and representative retinal image sets are essential for developing and testing digital screening programs and the automated algorithms at their core. To the best of our knowledge, IDRiD (Indian Diabetic Retinopathy Image Dataset), is the first database representative of an Indian population. It constitutes typical diabetic retinopathy lesions and normal retinal structures annotated at a pixel level. The dataset provides information on the disease severity of diabetic retinopathy, and diabetic macular edema for each image. This makes it perfect for development and evaluation of image analysis algorithms for early detection of diabetic retinopathy.},
author = {Porwal, Prasanna and Pachade, Samiksha and Kamble, Ravi and Kokare, Manesh and Deshmukh, Girish and Sahasrabuddhe, Vivek and Meriaudeau, Fabrice},
doi = {10.3390/data3030025},
issn = {23065729},
journal = {Data},
keywords = {Diabetic macular edema,Diabetic retinopathy,Retinal fundus images},
month = {jul},
number = {3},
pages = {25},
title = {{Indian diabetic retinopathy image dataset (IDRiD): A database for diabetic retinopathy screening research}},
url = {http://www.mdpi.com/2306-5729/3/3/25},
volume = {3},
year = {2018}
}
@inproceedings{Silberman2012,
abstract = {We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation. {\textcopyright} 2012 Springer-Verlag.},
address = {Berlin, Heidelberg},
annote = {event-place: Florence, Italy},
author = {Silberman, Nathan and Hoiem, Derek and Kohli, Pushmeet and Fergus, Rob},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33715-4_54},
isbn = {9783642337147},
issn = {03029743},
number = {PART 5},
pages = {746--760},
publisher = {Springer-Verlag},
series = {ECCV'12},
title = {{Indoor segmentation and support inference from RGBD images}},
url = {http://dx.doi.org/10.1007/978-3-642-33715-4{\_}54},
volume = {7576 LNCS},
year = {2012}
}
@article{Crankshaw2018,
abstract = {The dominant cost in production machine learning workloads is not training individual models but serving predictions from increasingly complex prediction pipelines spanning multiple models, machine learning frameworks, and parallel hardware accelerators. Due to the complex interaction between model configurations and parallel hardware, prediction pipelines are challenging to provision and costly to execute when serving interactive latency-sensitive applications. This challenge is exacerbated by the unpredictable dynamics of bursty workloads. In this paper we introduce InferLine, a system which efficiently provisions and executes ML inference pipelines subject to end-to-end latency constraints by proactively optimizing and reactively controlling per-model configuration in a fine-grained fashion. Unpredictable changes in the serving workload are dynamically and cost-optimally accommodated with minimal service level degradation. InferLine introduces (1) automated model profiling and pipeline lineage extraction, (2) a fine-grain, cost-minimizing pipeline configuration planner, and (3) a fine-grain reactive controller. InferLine is able to configure and deploy prediction pipelines across a wide range of workload patterns and latency goals. It outperforms coarse-grained configuration alternatives by up 7.6x in cost while achieving up to 32x lower SLO miss rate on real workloads and generalizes across state-of-the-art model serving frameworks.},
annote = {{\_}eprint: 1812.01776},
archivePrefix = {arXiv},
arxivId = {1812.01776},
author = {Crankshaw, Daniel and Sela, Gur-Eyal and Zumar, Corey and Mo, Xiangxi and Gonzalez, Joseph E and Stoica, Ion and Tumanov, Alexey},
eprint = {1812.01776},
title = {{InferLine: ML Inference Pipeline Composition Framework}},
url = {http://arxiv.org/abs/1812.01776},
year = {2018}
}
@article{Faust2019,
abstract = {Deep learning is an emerging transformative tool in diagnostic medicine, yet limited access and the interpretability of learned parameters hinders widespread adoption. Here we have generated a diverse repository of 838,644 histopathologic images and used them to optimize and discretize learned representations into 512-dimensional feature vectors. Importantly, we show that individual machine-engineered features correlate with salient human-derived morphologic constructs and ontological relationships. Deciphering the overlap between human and machine reasoning may aid in eliminating biases and improving automation and accountability for artificial intelligence-assisted medicine.},
author = {Faust, Kevin and Bala, Sudarshan and van Ommeren, Randy and Portante, Alessia and {Al Qawahmed}, Raniah and Djuric, Ugljesa and Diamandis, Phedias},
doi = {10.1038/s42256-019-0068-6},
journal = {Nature Machine Intelligence},
number = {7},
pages = {316--321},
title = {{Intelligent feature engineering and ontological mapping of brain tumour histomorphologies by deep learning}},
volume = {1},
year = {2019}
}
@inproceedings{Pace2015,
abstract = {We present an interactive algorithm to segment the heart chambers and epicardial surfaces, including the great vessel walls, in pediatric cardiac MRI of congenital heart disease. Accurate whole-heart segmentation is necessary to create patient-specific 3D heart models for surgical planning in the presence of complex heart defects. Anatomical variability due to congenital defects precludes fully automatic atlas-based segmentation. Our interactive segmentation method exploits expert segmentations of a small set of short-axis slice regions to automatically delineate the remaining volume using patch-based segmentation. We also investigate the potential of active learning to automatically solicit user input in areas where segmentation error is likely to be high. Validation is performed on four subjects with double outlet right ventricle, a severe congenital heart defect. We show that strategies asking the user to manually segment regions of interest within short-axis slices yield higher accuracy with less user input than those querying entire short-axis slices.},
author = {Pace, Danielle F. and Dalca, Adrian V. and Geva, Tal and Powell, Andrew J. and Moghari, Mehdi H. and Golland, Polina},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24574-4_10},
isbn = {9783319245737},
issn = {16113349},
title = {{Interactive whole-heart segmentation in congenital heart disease}},
year = {2015}
}
@article{Hssayeni2020,
abstract = {Traumatic brain injuries may cause intracranial hemorrhages (ICH). ICH could lead to disability or death if it is not accurately diagnosed and treated in a time-sensitive procedure. The current clinical protocol to diagnose ICH is examining Computerized Tomography (CT) scans by radiologists to detect ICH and localize its regions. However, this process relies heavily on the availability of an experienced radiologist. In this paper, we designed a study protocol to collect a dataset of 82 CT scans of subjects with a traumatic brain injury. Next, the ICH regions were manually delineated in each slice by a consensus decision of two radiologists. The dataset is publicly available online at the PhysioNet repository for future analysis and comparisons. In addition to publishing the dataset, which is the main purpose of this manuscript, we implemented a deep Fully Convolutional Networks (FCNs), known as U-Net, to segment the ICH regions from the CT scans in a fully-automated manner. The method as a proof of concept achieved a Dice coefficient of 0.31 for the ICH segmentation based on 5-fold cross-validation.},
archivePrefix = {arXiv},
arxivId = {1910.08643},
author = {Hssayeni, Murtadha D. and Croock, Muayad S. and Salman, Aymen D. and Al-Khafaji, Hassan Falah and Yahya, Zakaria A. and Ghoraani, Behnaz},
doi = {10.3390/data5010014},
eprint = {1910.08643},
issn = {23065729},
journal = {Data},
keywords = {CT scans dataset,Fully convolutional network,ICH detection,Intracranial hemorrhage segmentation,U-Net},
month = {feb},
number = {1},
pages = {14},
title = {{Intracranial hemorrhage segmentation using a deep convolutional model}},
url = {https://www.mdpi.com/2306-5729/5/1/14},
volume = {5},
year = {2020}
}
@book{Manelli2020,
address = {Berkeley, CA},
author = {Manelli, Luciano},
booktitle = {Introducing Algorithms in C},
doi = {10.1007/978-1-4842-5623-7},
isbn = {978-1-4842-5622-0},
publisher = {Apress},
title = {{Introducing Algorithms in C}},
url = {http://link.springer.com/10.1007/978-1-4842-5623-7},
year = {2020}
}
@book{DeRosa2014,
address = {Berkeley, CA},
author = {{De Rosa}, Aurelio},
booktitle = {SidePoint},
doi = {10.1007/978-1-4842-5735-7},
isbn = {978-1-4842-5734-0},
keywords = {Speech Recognition,Web Speech API},
publisher = {Apress},
title = {{Introducing the Web Speech API}},
url = {http://www.sitepoint.com/introducing-web-speech-api/},
year = {2014}
}
@article{Maier2017,
abstract = {Ischemic stroke is the most common cerebrovascular disease, and its diagnosis, treatment, and study relies on non-invasive imaging. Algorithms for stroke lesion segmentation from magnetic resonance imaging (MRI) volumes are intensely researched, but the reported results are largely incomparable due to different datasets and evaluation schemes. We approached this urgent problem of comparability with the Ischemic Stroke Lesion Segmentation (ISLES) challenge organized in conjunction with the MICCAI 2015 conference. In this paper we propose a common evaluation framework, describe the publicly available datasets, and present the results of the two sub-challenges: Sub-Acute Stroke Lesion Segmentation (SISS) and Stroke Perfusion Estimation (SPES). A total of 16 research groups participated with a wide range of state-of-the-art automatic segmentation algorithms. A thorough analysis of the obtained data enables a critical evaluation of the current state-of-the-art, recommendations for further developments, and the identification of remaining challenges. The segmentation of acute perfusion lesions addressed in SPES was found to be feasible. However, algorithms applied to sub-acute lesion segmentation in SISS still lack accuracy. Overall, no algorithmic characteristic of any method was found to perform superior to the others. Instead, the characteristics of stroke lesion appearances, their evolution, and the observed challenges should be studied in detail. The annotated ISLES image datasets continue to be publicly available through an online evaluation system to serve as an ongoing benchmarking resource (www.isles-challenge.org).},
author = {Maier, Oskar and Menze, Bjoern H. and von der Gablentz, Janina and H{\"{a}}ni, Levin and Heinrich, Mattias P. and Liebrand, Matthias and Winzeck, Stefan and Basit, Abdul and Bentley, Paul and Chen, Liang and Christiaens, Daan and Dutil, Francis and Egger, Karl and Feng, Chaolu and Glocker, Ben and G{\"{o}}tz, Michael and Haeck, Tom and Halme, Hanna Leena and Havaei, Mohammad and Iftekharuddin, Khan M. and Jodoin, Pierre Marc and Kamnitsas, Konstantinos and Kellner, Elias and Korvenoja, Antti and Larochelle, Hugo and Ledig, Christian and Lee, Jia Hong and Maes, Frederik and Mahmood, Qaiser and Maier-Hein, Klaus H. and McKinley, Richard and Muschelli, John and Pal, Chris and Pei, Linmin and Rangarajan, Janaki Raman and Reza, Syed M.S. and Robben, David and Rueckert, Daniel and Salli, Eero and Suetens, Paul and Wang, Ching Wei and Wilms, Matthias and Kirschke, Jan S. and Kr{\"{a}}mer, Ulrike M. and M{\"{u}}nte, Thomas F. and Schramm, Peter and Wiest, Roland and Handels, Heinz and Reyes, Mauricio},
doi = {10.1016/j.media.2016.07.009},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Benchmark,Challenge,Comparison,Ischemic stroke,MRI,Segmentation},
month = {jan},
pages = {250--269},
pmid = {27475911},
title = {{ISLES 2015 - A public evaluation benchmark for ischemic stroke lesion segmentation from multispectral MRI}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841516301268},
volume = {35},
year = {2017}
}
@online{ISLES2015,
author = {Oskar, Maier and Bj{\"{o}}rn, Menze and Mauricio, Reyes},
title = {{ISLES: Ischemic Stroke Lesion Segmentation Challenge 2015}},
url = {http://www.isles-challenge.org/ISLES2015/},
urldate = {2020-05-20},
year = {2015}
}
@online{ISLES2016,
author = {Egger, Karl and Maier, Oskar and Reyes, Mauricio and Wiest, Roland},
title = {{ISLES: Ischemic Stroke Lesion Segmentation Challenge 2016}},
url = {http://www.isles-challenge.org/ISLES2017/{\%}0Ahttp://www.isles-challenge.org/ISLES2016/},
year = {2016}
}
@online{ISLES2017,
author = {Hakim, Arsany and Reyes, Mauricio and Wiest, Roland and Winzeck, Stefan},
title = {{ISLES: Ischemic Stroke Lesion Segmentation Challenge 2017}},
url = {http://www.isles-challenge.org/ISLES2017/},
urldate = {2020-05-20},
year = {2017}
}
@misc{ISLES2018,
author = {Hakim, Arsany and Reyes, Mauricio and Wiest, Roland and Lansberg, Maarten G and Christensen, S{\o}ren and Zaharchuk, Greg},
title = {{ISLES: Ischemic Stroke Lesion Segmentation Challenge 2018}},
year = {2018}
}
@article{Armeni2017,
abstract = {We present a dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. The dataset covers over 6,000m2 and contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360{\{}$\backslash$deg{\}} equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces. The dataset is available here: http://3Dsemantics.stanford.edu/},
annote = {{\_}eprint: 1702.01105},
archivePrefix = {arXiv},
arxivId = {1702.01105},
author = {Armeni, Iro and Sax, Sasha and Zamir, Amir Roshan and Savarese, Silvio},
eprint = {1702.01105},
journal = {CoRR},
keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Robotics},
title = {{Joint 2D-3D-Semantic Data for Indoor Scene Understanding}},
url = {http://arxiv.org/abs/1702.01105},
volume = {abs/1702.0},
year = {2017}
}
@article{Chiu2015,
abstract = {{\textcopyright} 2015 Optical Society of America. We present a fully automatic algorithm to identify fluid-filled regions and seven retinal layers on spectral domain optical coherence tomography images of eyes with diabetic macular edema (DME). To achieve this, we developed a kernel regression (KR)-based classification method to estimate fluid and retinal layer positions. We then used these classification estimates as a guide to more accurately segment the retinal layer boundaries using our previously described graph theory and dynamic programming (GTDP) framework. We validated our algorithm on 110 Bscans from ten patients with severe DME pathology, showing an overall mean Dice coefficient of 0.78 when comparing our KR + GTDP algorithm to an expert grader. This is comparable to the inter-observer Dice coefficient of 0.79. The entire data set is available online, including our automatic and manual segmentation results. To the best of our knowledge, this is the first validated, fully-automated, seven-layer and fluid segmentation method which has been applied to real-world images containing severe DME.},
author = {Chiu, Stephanie J. and Allingham, Michael J. and Mettu, Priyatham S. and Cousins, Scott W. and Izatt, Joseph A. and Farsiu, Sina},
doi = {10.1364/boe.6.001172},
issn = {2156-7085},
journal = {Biomedical Optics Express},
month = {apr},
number = {4},
pages = {1172},
title = {{Kernel regression based segmentation of optical coherence tomography images with diabetic macular edema}},
url = {https://www.osapublishing.org/abstract.cfm?URI=boe-6-4-1172},
volume = {6},
year = {2015}
}
@article{Ghiasi2016,
annote = {From Duplicate 1 (Laplacian Reconstruction and Refinement for Semantic Segmentation - Ghiasi, Golnaz; Fowlkes, Charless C)

{\_}eprint: 1605.02264},
archivePrefix = {arXiv},
arxivId = {1605.02264},
author = {Ghiasi, Golnaz and Fowlkes, Charless C},
eprint = {1605.02264},
journal = {CoRR},
title = {{Laplacian Reconstruction and Refinement for Semantic Segmentation}},
url = {http://arxiv.org/abs/1605.02264},
volume = {abs/1605.0},
year = {2016}
}
@article{DanielKermanyKangZhang2018,
abstract = {Be sure to download the most recent version of this dataset to maintain accuracy. This dataset contains thousands of validated OCT and Chest X-Ray images described and analyzed in "Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning". The images are split into a training set and a testing set of independent patients. Images are labeled as (disease)-(randomized patient ID)-(image number by this patient) and split into 4 directories: CNV, DME, DRUSEN, and NORMAL. This repository of images is made available for use in research only. How to cite this data:},
author = {{Daniel Kermany, Kang Zhang}, Michael Goldbaum},
doi = {10.17632/RSCBJBR9SJ.3},
title = {{Large Dataset of Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images}},
url = {https://data.mendeley.com/datasets/rscbjbr9sj/3},
volume = {3},
year = {2018}
}
@inproceedings{Real2017,
abstract = {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6{\%} (95.6{\%} for ensemble) and 77.0{\%}, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.},
archivePrefix = {arXiv},
arxivId = {1703.01041},
author = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V and Kurakin, Alexey},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
eprint = {1703.01041},
isbn = {9781510855144},
pages = {4429--4446},
title = {{Large-scale evolution of image classifiers}},
volume = {6},
year = {2017}
}
@article{Paper2015a,
author = {Paper, Scientific and Bron, E and Smits, M and Barkhof, F and Niessen, W and Klein, S},
keywords = {applications-detection,cad,comparative studies,computer,computer applications,diagnosis,mr,neuroradiology brain},
pages = {1--18},
title = {{Large-scale objective comparison of 29 novel algorithms for computer-aided diagnosis of dementia based on structural}},
year = {2015}
}
@book{Datta2017,
address = {Cham},
author = {Datta, Dilip},
doi = {10.1007/978-3-319-47831-9},
isbn = {978-3-319-47830-2},
publisher = {Springer International Publishing},
title = {{LaTeX in 24 Hours}},
url = {http://link.springer.com/10.1007/978-3-319-47831-9},
year = {2017}
}
@inproceedings{Vahadane2016,
abstract = {For better perception and analysis of images, good quality and high resolution (HR) are always preferred over degraded and low resolution (LR) images. Getting HR images can be cost and time prohibitive. Super resolution (SR) techniques can be an affordable alternative for small zoom factors. In medical imaging, specifically in the case of histological images, estimating an HR image from an LR one requires preservation of complex textures and edges defining various biological features (nuclei, cytoplasm etc.). This challenge is further aggravated by the scale variance of histological images that are taken of a flat biopsy slide instead of a 3D world. We propose an algorithm for SR of histological images that learns a mapping from zero-phase component analysis (ZCA)-whitened LR patches to ZCA-whitened HR patches at the desired scale. ZCA-whitening exploits the redundancy in data and enhances the texture and edges energies to better learn the desired LR to HR mapping, which we learn using a neural network. The qualitative and quantitative validation shows that improvements in HR estimation by proposed algorithm are statistically significant over benchmark learning-based SR algorithms.},
author = {Vahadane, Abhishek and Kumar, Neeraj and Sethi, Amit},
booktitle = {Proceedings - International Symposium on Biomedical Imaging},
doi = {10.1109/ISBI.2016.7493391},
isbn = {9781479923502},
issn = {19458452},
keywords = {Image super-resolution,histological image,neural network},
month = {apr},
pages = {816--819},
title = {{Learning based super-resolution of histological images}},
volume = {2016-June},
year = {2016}
}
@incollection{Concepts2020,
address = {Cham},
author = {Concepts, Related},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_823-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Learning from a Neuroscience Perspective}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}823-1},
year = {2020}
}
@inproceedings{Esteves2017,
abstract = {We address the problem of 3D rotation equivariance in convolutional neural networks. 3D rotations have been a challenging nuisance in 3D classification tasks requiring higher capacity and extended data augmentation in order to tackle it. We model 3D data with multi-valued spherical functions and we propose a novel spherical convolutional network that implements exact convolutions on the sphere by realizing them in the spherical harmonic domain. Resulting filters have local symmetry and are localized by enforcing smooth spectra. We apply a novel pooling on the spectral domain and our operations are independent of the underlying spherical resolution throughout the network. We show that networks with much lower capacity and without requiring data augmentation can exhibit performance comparable to the state of the art in standard retrieval and classification benchmarks.},
annote = {{\_}eprint: 1711.06721},
archivePrefix = {arXiv},
arxivId = {1711.06721},
author = {Esteves, Carlos and Allen-Blanchette, Christine and Makadia, Ameesh and Daniilidis, Kostas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01261-8_4},
eprint = {1711.06721},
isbn = {9783030012601},
issn = {16113349},
number = {November},
pages = {54--70},
title = {{Learning SO(3) Equivariant Representations with Spherical CNNs}},
url = {http://arxiv.org/abs/1711.06721},
volume = {11217 LNCS},
year = {2018}
}
@inproceedings{Tran2014,
abstract = {We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets, 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets, and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8{\%} accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.},
annote = {{\_}eprint: 1412.0767},
archivePrefix = {arXiv},
arxivId = {1412.0767},
author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.510},
eprint = {1412.0767},
isbn = {9781467383912},
issn = {15505499},
keywords = {Computer Science - Computer Vision and Pattern Rec},
pages = {4489--4497},
title = {{Learning spatiotemporal features with 3D convolutional networks}},
volume = {2015 Inter},
year = {2015}
}
@article{Swiderska-Chadaj2019,
abstract = {The immune system is of critical importance in the development of cancer. The evasion of destruction by the immune system is one of the emerging hallmarks of cancer. We have built a dataset of 171,166 manually annotated CD3+ and CD8+ cells, which we used to train deep learning algorithms for automatic detection of lymphocytes in histopathology images to better quantify immune response. Moreover, we investigate the effectiveness of four deep learning based methods when different subcompartments of the whole-slide image are considered: normal tissue areas, areas with immune cell clusters, and areas containing artifacts. We have compared the proposed methods in breast, colon and prostate cancer tissue slides collected from nine different medical centers. Finally, we report the results of an observer study on lymphocyte quantification, which involved four pathologists from different medical centers, and compare their performance with the automatic detection. The results give insights on the applicability of the proposed methods for clinical use. U-Net obtained the highest performance with an F1-score of 0.78 and the highest agreement with manual evaluation ($\kappa$=0.72), whereas the average pathologists agreement with reference standard was $\kappa$=0.64. The test set and the automatic evaluation procedure are publicly available at lyon19.grand-challenge.org.},
author = {Swiderska-Chadaj, Zaneta and Pinckaers, Hans and van Rijthoven, Mart and Balkenhol, Maschenka and Melnikova, Margarita and Geessink, Oscar and Manson, Quirine and Sherman, Mark and Polonia, Antonio and Parry, Jeremy and Abubakar, Mustapha and Litjens, Geert and van der Laak, Jeroen and Ciompi, Francesco},
doi = {10.1016/j.media.2019.101547},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Computational pathology,Deep learning,Immune cell detection,Immunohistochemistry},
pages = {101547},
title = {{Learning to detect lymphocytes in immunohistochemistry with deep learning}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841519300829},
volume = {58},
year = {2019}
}
@inproceedings{Zoph2017,
abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the 'NASNet search space') which enables transferability. In our experiments, we search for the best convolutional layer (or 'cell') on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a 'NASNet architecture'. We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4{\%} error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7{\%} top-1 and 96.2{\%} top-5 on ImageNet. Our model is 1.2{\%} better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28{\%} in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74{\%} top-1 accuracy, which is 3.1{\%} better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0{\%} achieving 43.1{\%} mAP on the COCO dataset.},
archivePrefix = {arXiv},
arxivId = {1707.07012},
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00907},
eprint = {1707.07012},
isbn = {9781538664209},
issn = {10636919},
pages = {8697--8710},
title = {{Learning Transferable Architectures for Scalable Image Recognition}},
year = {2018}
}
@inproceedings{Seff2015,
abstract = {Histograms of oriented gradients (HOG) are widely employed image descriptors in modern computer-aided diagnosis systems. Built upon a set of local, robust statistics of low-level image gradients, HOG features are usually computed on raw intensity images. In this paper, we explore a learned image transformation scheme for producing higher-level inputs to HOG. Leveraging semantic object boundary cues, our methods compute data-driven image feature maps via a supervised boundary detector. Compared with the raw image map, boundary cues offer mid-level, more object-specific visual responses that can be suited for subsequent HOG encoding. We validate integrations of several image transformation maps with an application of computer-aided detection of lymph nodes on thoracoabdominal CT images. Our experiments demonstrate that semantic boundary cues based HOG descriptors complement and enrich the raw intensity alone. We observe an overall system with substantially improved results (∼ 78{\%} versus 60{\%} recall at 3 FP/volume for two target regions). The proposed system also moderately outperforms the state-of-the-art deep convolutional neural network (CNN) system in the mediastinum region, without relying on data augmentation and requiring significantly fewer training samples.},
author = {Seff, Ari and Lu, Le and Barbu, Adrian and Roth, Holger and Shin, Hoo Chang and Summers, Ronald M.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24571-3_7},
isbn = {9783319245706},
issn = {16113349},
title = {{Leveraging mid-level semantic boundary cues for automated lymph node detection}},
year = {2015}
}
@article{Zhang2019,
abstract = {Reconstructing 3D geometry from satellite imagery is an important topic of research. However, disparities exist between how this 3D reconstruction problem is handled in the remote sensing context and how multi-view reconstruction pipelines have been developed in the computer vision community. In this paper, we explore whether state-of-the-art reconstruction pipelines from the vision community can be applied to the satellite imagery. Along the way, we address several challenges adapting vision-based structure from motion and multi-view stereo methods. We show that vision pipelines can offer competitive speed and accuracy in the satellite context.},
annote = {{\_}eprint: 1910.02989},
archivePrefix = {arXiv},
arxivId = {1910.02989},
author = {Zhang, Kai and Snavely, Noah and Sun, Jin},
eprint = {1910.02989},
title = {{Leveraging Vision Reconstruction Pipelines for Satellite Imagery}},
url = {http://arxiv.org/abs/1910.02989},
year = {2019}
}
@incollection{Govindu2020,
address = {Cham},
author = {Govindu, Venu Madhav},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_871-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Lie Algebra}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}871-1},
year = {2020}
}
@article{Dugre2019,
abstract = {Objective: It has long been assumed that paranoid ideation may stem from an aberrant limbic response to threatening stimuli. However, results from functional neuroimaging studies using negative emotional stimuli have failed to confirm this assumption. One of the potential reasons for the lack of effect is that study participants with psychosis may display aberrant brain responses to neutral material rather than to threatening stimuli. The authors conducted a functional neuroimaging meta-analysis to test this hypothesis. Methods: A literature search was performed with PubMed, Google Scholar, and Embase to identify functional neuroimaging studies examining brain responses to neutral material in patients with psychosis. A total of 23 studies involving schizophrenia patients were retrieved. Using t-maps of peak coordinates to calculate effect sizes, a random-effects model meta-analysis was performed with the anisotropic effect-size version of Seed-based d Mapping software. Results: In schizophrenia patients relative to healthy control subjects, increased activations were observed in the left and right amygdala and parahippocampus and the left putamen, hippocampus, and insula in response to neutral stimuli. Conclusions: Given that several limbic regions were found to be more activated in schizophrenia patients than in control subjects, the results of this meta-analysis strongly suggest that these patients confer aberrant emotional significance to nonthreatening stimuli. In theory, this abnormal brain reactivity may fuel delusional thoughts. Studies are needed in individuals at risk of psychosis to determine whether aberrant limbic reactivity to neutral stimuli is an early neurofunctional marker of psychosis vulnerability.},
author = {Dugr{\'{e}}, Jules R. and Bitar, Nathalie and Dumais, Alexandre and Potvin, St{\'{e}}phane},
doi = {10.1176/appi.ajp.2019.19030247},
issn = {15357228},
journal = {American Journal of Psychiatry},
month = {dec},
number = {12},
pages = {1021--1029},
pmid = {31509006},
title = {{Limbic hyperactivity in response to emotionally neutral stimuli in schizophrenia: A neuroimaging meta-analysis of the hypervigilant mind}},
url = {http://ajp.psychiatryonline.org/doi/10.1176/appi.ajp.2019.19030247},
volume = {176},
year = {2019}
}
@incollection{Sugihara2020,
address = {Cham},
author = {Sugihara, Kokichi},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_390-1},
pages = {1--3},
publisher = {Springer International Publishing},
title = {{Line Drawing Labeling}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}390-1},
year = {2020}
}
@incollection{Murota1955,
address = {Cham},
author = {Murota, Kazuo},
booktitle = {The Economic Studies Quarterly (Tokyo. 1950)},
doi = {10.11398/economics1950.5.3-4_156},
issn = {2185-4408},
number = {3-4},
pages = {156--163},
publisher = {Springer International Publishing},
title = {{Linear Programmingモデルの簡単な応用について}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}648-1},
volume = {5},
year = {1955}
}
@article{Carass2017a,
abstract = {The data presented in this article is related to the research article entitled “Longitudinal multiple sclerosis lesion segmentation: Resource and challenge” (Carass et al., 2017) [1]. In conjunction with the 2015 International Symposium on Biomedical Imaging, we organized a longitudinal multiple sclerosis (MS) lesion segmentation challenge providing training and test data to registered participants. The training data consists of five subjects with a mean of 4.4 (±0.55) time-points, and test data of fourteen subjects with a mean of 4.4 (±0.67) time-points. All 82 data sets had the white matter lesions associated with multiple sclerosis delineated by two human expert raters. The training data including multi-modal scans and manually delineated lesion masks is available for download.1 In addition, the testing data is also being made available in conjunction with a website for evaluating the automated analysis of the testing data.},
author = {Carass, Aaron and Roy, Snehashis and Jog, Amod and Cuzzocreo, Jennifer L. and Magrath, Elizabeth and Gherman, Adrian and Button, Julia and Nguyen, James and Bazin, Pierre Louis and Calabresi, Peter A. and Crainiceanu, Ciprian M. and Ellingsen, Lotta M. and Reich, Daniel S. and Prince, Jerry L. and Pham, Dzung L.},
doi = {10.1016/j.dib.2017.04.004},
issn = {23523409},
journal = {Data in Brief},
month = {jun},
pages = {346--350},
title = {{Longitudinal multiple sclerosis lesion segmentation data resource}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2352340917301361},
volume = {12},
year = {2017}
}
@article{Carass2017,
abstract = {In conjunction with the ISBI 2015 conference, we organized a longitudinal lesion segmentation challenge providing training and test data to registered participants. The training data consisted of five subjects with a mean of 4.4 time-points, and test data of fourteen subjects with a mean of 4.4 time-points. All 82 data sets had the white matter lesions associated with multiple sclerosis delineated by two human expert raters. Eleven teams submitted results using state-of-the-art lesion segmentation algorithms to the challenge, with ten teams presenting their results at the conference. We present a quantitative evaluation comparing the consistency of the two raters as well as exploring the performance of the eleven submitted results in addition to three other lesion segmentation algorithms. The challenge presented three unique opportunities: (1) the sharing of a rich data set; (2) collaboration and comparison of the various avenues of research being pursued in the community; and (3) a review and refinement of the evaluation metrics currently in use. We report on the performance of the challenge participants, as well as the construction and evaluation of a consensus delineation. The image data and manual delineations will continue to be available for download, through an evaluation website2 The Challenge Evaluation Website is: http://smart-stats-tools.org/lesion-challenge-2015 as a resource for future researchers in the area. This data resource provides a platform to compare existing methods in a fair and consistent manner to each other and multiple manual raters.},
author = {Carass, Aaron and Roy, Snehashis and Jog, Amod and Cuzzocreo, Jennifer L. and Magrath, Elizabeth and Gherman, Adrian and Button, Julia and Nguyen, James and Prados, Ferran and Sudre, Carole H. and {Jorge Cardoso}, Manuel and Cawley, Niamh and Ciccarelli, Olga and Wheeler-Kingshott, Claudia A.M. and Ourselin, S{\'{e}}bastien and Catanese, Laurence and Deshpande, Hrishikesh and Maurel, Pierre and Commowick, Olivier and Barillot, Christian and Tomas-Fernandez, Xavier and Warfield, Simon K. and Vaidya, Suthirth and Chunduru, Abhijith and Muthuganapathy, Ramanathan and Krishnamurthi, Ganapathy and Jesson, Andrew and Arbel, Tal and Maier, Oskar and Handels, Heinz and Iheme, Leonardo O. and Unay, Devrim and Jain, Saurabh and Sima, Diana M. and Smeets, Dirk and Ghafoorian, Mohsen and Platel, Bram and Birenbaum, Ariel and Greenspan, Hayit and Bazin, Pierre Louis and Calabresi, Peter A. and Crainiceanu, Ciprian M. and Ellingsen, Lotta M. and Reich, Daniel S. and Prince, Jerry L. and Pham, Dzung L.},
doi = {10.1016/j.neuroimage.2016.12.064},
issn = {10959572},
journal = {NeuroImage},
month = {mar},
pages = {77--102},
title = {{Longitudinal multiple sclerosis lesion segmentation: Resource and challenge}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811916307819},
volume = {148},
year = {2017}
}
@incollection{Gortler1996,
abstract = {This paper discusses a new method for capturing the complete appearance of both synthetic and real world objects and scenes, representing this information, and then using this representation to render images of the object from new camera positions. Unlike the shape capture process traditionally used in computer vision and the rendering process traditionally used in computer graphics, our approach does not rely on geometric representations. Instead we sample and reconstruct a 4D function, which we call a Lumigraph. The Lumigraph is a subset of the complete plenoptic function that describes the flow of light at all positions in all directions. With the Lumigraph, new images of the object can be generated very quickly, independent of the geometric or illumination complexity of the scene or object. The paper discusses a complete working system including the capture of samples, the construction of the Lumigraph, and the subsequent rendering of images from this new representation.},
address = {Cham},
author = {Gortler, Steven J. and Grzeszczuk, Radek and Szeliski, Richard and Cohen, Michael F.},
booktitle = {Proceedings of the ACM SIGGRAPH Conference on Computer Graphics},
doi = {10.1007/978-3-030-03243-2_8-1},
pages = {43--54},
publisher = {Springer International Publishing},
title = {{Lumigraph}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}8-1},
year = {1996}
}
@article{Candemir2014,
abstract = {The National Library of Medicine (NLM) is developing a digital chest X-ray (CXR) screening system for deployment in resource constrained communities and developing countries worldwide with a focus on early detection of tuberculosis. A critical component in the computer-aided diagnosis of digital CXRs is the automatic detection of the lung regions. In this paper, we present a nonrigid registration-driven robust lung segmentation method using image retrieval-based patient specific adaptive lung models that detects lung boundaries, surpassing state-of-the-art performance. The method consists of three main stages: 1) a content-based image retrieval approach for identifying training images (with masks) most similar to the patient CXR using a partial Radon transform and Bhattacharyya shape similarity measure, 2) creating the initial patient-specific anatomical model of lung shape using SIFT-flow for deformable registration of training masks to the patient CXR, and 3) extracting refined lung boundaries using a graph cuts optimization approach with a customized energy function. Our average accuracy of 95.4{\%} on the public JSRT database is the highest among published results. A similar degree of accuracy of 94.1{\%} and 91.7{\%} on two new CXR datasets from Montgomery County, MD, USA, and India, respectively, demonstrates the robustness of our lung segmentation approach. {\textcopyright} 2013 IEEE.},
author = {Candemir, Sema and Jaeger, Stefan and Palaniappan, Kannappan and Musco, Jonathan P. and Singh, Rahul K. and Xue, Zhiyun and Karargyris, Alexandros and Antani, Sameer and Thoma, George and McDonald, Clement J.},
doi = {10.1109/TMI.2013.2290491},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Chest X-ray imaging,Computer-aided detection,Image registration,Image segmentation,Tuberculosis (TB)},
title = {{Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration}},
year = {2014}
}
@book{Mitchell1997MachineLearning,
author = {Mitchell, Tom M},
isbn = {0070428077},
publisher = {McGraw-Hill Education},
title = {{Machine Learning}},
url = {https://www.xarg.org/ref/a/0070428077/},
year = {1997}
}
@article{Komura2018,
abstract = {Abundant accumulation of digital histopathological images has led to the increased demand for their analysis, such as computer-aided diagnosis using machine learning techniques. However, digital pathological images and related tasks have some issues to be considered. In this mini-review, we introduce the application of digital pathological image analysis using machine learning algorithms, address some problems specific to such analysis, and propose possible solutions.},
author = {Komura, Daisuke and Ishikawa, Shumpei},
doi = {10.1016/j.csbj.2018.01.001},
issn = {20010370},
journal = {Computational and Structural Biotechnology Journal},
keywords = {Computer assisted diagnosis,Deep learning,Digital image analysis,Histopathology,Machine learning,Whole slide images},
pages = {34--42},
title = {{Machine Learning Methods for Histopathological Image Analysis}},
volume = {16},
year = {2018}
}
@book{Norris2020,
address = {Berkeley, CA},
author = {Norris, Donald J.},
booktitle = {Machine Learning with the Raspberry Pi},
doi = {10.1007/978-1-4842-5174-4},
isbn = {978-1-4842-5173-7},
publisher = {Apress},
title = {{Machine Learning with the Raspberry Pi}},
url = {http://link.springer.com/10.1007/978-1-4842-5174-4},
year = {2020}
}
@book{Flach2012,
abstract = {The emerging field of Ecosystem Informatics applies methods from computer science and mathematics to address fundamental and applied problems in the ecosystem sciences. The ecosystem sciences are in the midst of a revolution driven by a combination of emerging technologies for improved sensing and the critical need for better science to help manage global climate change. This paper describes several initiatives at Oregon State University in ecosystem informatics. At the level of sensor technologies, this paper describes two projects: (a) wireless, battery-free sensor networks for forests and (b) rapid throughput automated arthropod population counting. At the level of data preparation and data cleaning, this paper describes the application of linear gaussian dynamic Bayesian networks to automated anomaly detection in temperature data streams. Finally, the paper describes two educational activities: (a) a summer institute in ecosystem informatics and (b) an interdisciplinary Ph.D. program in Ecosystem Informatics for mathematics, computer science, and the ecosystem sciences.},
author = {Flach, Peter},
isbn = {1107422221, 9781107422223},
publisher = {Posts Telecom Press and Cambridge University Press},
title = {{Machine Learning: The Art and Science of Algorithms That Make Sense of Data}},
translator = {Duan, Fei},
year = {2012}
}
@incollection{Zheng2009,
address = {Cham},
author = {Zheng, Nanning and Xue, Jianru},
booktitle = {Computer Vision},
doi = {10.1007/978-1-84882-312-9_4},
pages = {87--119},
publisher = {Springer International Publishing},
title = {{Manifold Learning}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}824-1},
year = {2009}
}
@article{Skibbe2019,
abstract = {Understanding the connectivity in the brain is an important prerequisite for understanding how the brain processes information. In the Brain/MINDS project, a connectivity study on marmoset brains uses two-photon microscopy fluorescence images of axonal projections to collect the neuron connectivity from defined brain regions at the mesoscopic scale. The processing of the images requires the detection and segmentation of the axonal tracer signal. The objective is to detect as much tracer signal as possible while not misclassifying other background structures as the signal. This can be challenging because of imaging noise, a cluttered image background, distortions or varying image contrast cause problems. We are developing MarmoNet, a pipeline that processes and analyzes tracer image data of the common marmoset brain. The pipeline incorporates state-of-the-art machine learning techniques based on artificial convolutional neural networks (CNN) and image registration techniques to extract and map all relevant information in a robust manner. The pipeline processes new images in a fully automated way. This report introduces the current state of the tracer signal analysis part of the pipeline.},
annote = {{\_}eprint: 1908.00876},
archivePrefix = {arXiv},
arxivId = {1908.00876},
author = {Skibbe, Henrik and Watakabe, Akiya and Nakae, Ken and Gutierrez, Carlos Enrique and Tsukada, Hiromichi and Hata, Junichi and Kawase, Takashi and Gong, Rui and Woodward, Alexander and Doya, Kenji and Okano, Hideyuki and Yamamori, Tetsuo and Ishii, Shin},
eprint = {1908.00876},
title = {{MarmoNet: a pipeline for automated projection mapping of the common marmoset brain from whole-brain serial two-photon tomography}},
url = {http://arxiv.org/abs/1908.00876},
year = {2019}
}
@article{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron.},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
doi = {10.1109/TPAMI.2018.2844175},
eprint = {1703.06870},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Instance segmentation,convolutional neural network,object detection,pose estimation},
number = {2},
pages = {386--397},
pmid = {29994331},
title = {{Mask R-CNN}},
volume = {42},
year = {2020}
}
@book{Rogers2014,
address = {Berlin, Heidelberg},
author = {Rogers, Silvia M.},
doi = {10.1007/978-3-642-39446-1},
isbn = {978-3-642-39445-4},
publisher = {Springer Berlin Heidelberg},
title = {{Mastering Scientific and Medical Writing}},
url = {http://link.springer.com/10.1007/978-3-642-39446-1},
year = {2014}
}
@book{Osterhage2020,
address = {Berlin, Heidelberg},
author = {Osterhage, Wolfgang W.},
booktitle = {Mathematical Theory of Advanced Computing},
doi = {10.1007/978-3-662-60359-8},
isbn = {978-3-662-60358-1},
publisher = {Springer Berlin Heidelberg},
title = {{Mathematical Theory of Advanced Computing}},
url = {http://link.springer.com/10.1007/978-3-662-60359-8},
year = {2020}
}
@article{Dice1945,
abstract = {The coefficient of association of Forbes indicates the amount of association be- tween two given species compared to the amount of association between them expected by chance. In order to provide a simple direct measure of the amount of association of one species with another the association index is proposed. If a is the number of random samples of a given series in which species A occurs and h is the number of samples in which another species B occurs together with A, then the association index B/A = h/a. Similarly, if b is the number of samples in which species B occurs, then the associa- tion index A/B = h/b. There is also proposed a coincidence index, 2h/(a + b), whose value is intermediate between the two reciprocal association indices. As a measure of the statistical reliability of the deviation shown by the samples of a given series from the amount of associa- tion expected by chance, the chi-square test may be used.},
author = {Dice, Lee R.},
doi = {10.2307/1932409},
issn = {0012-9658},
journal = {Ecology},
number = {3},
pages = {297--302},
title = {{Measures of the Amount of Ecologic Association Between Species}},
volume = {26},
year = {1945}
}
@article{Zhang2020a,
abstract = {In the past decade, deep learning (DL) has achieved unprecedented success in numerous fields including computer vision, natural language processing, and healthcare. In particular, DL is experiencing an increasing development in applications for advanced medical image analysis in terms of analysis, segmentation, classification, and furthermore. On the one hand, tremendous needs that leverage the power of DL for medical image analysis are arising from the research community of a medical, clinical, and informatics background to jointly share their expertise, knowledge, skills, and experience. On the other hand, barriers between disciplines are on the road for them often hampering a full and efficient collaboration. To this end, we propose our novel open-source platform, i. e., MeDaS–the MeDical open-source platform as Service. To the best of our knowledge, MeDaS is the first open-source platform proving a collaborative and interactive service for researchers from a medical background easily using DL related toolkits, and at the same time for scientists or engineers from information sciences to understand the medical knowledge side. Based on a series of toolkits and utilities from the idea of RINV (Rapid Implementation aNd Verification), our proposed MeDaS platform can implement pre-processing, postprocessing, augmentation, visualization, and other phases needed in medical image analysis. Five tasks including the subjects of lung, liver, brain, chest, and pathology, are validated and demonstrated to be efficiently realisable by using MeDaS.},
author = {Zhang, Liang and Li, Johann and Li, Ping and Lu, Xiaoyuan and Shen, Peiyi and Zhu, Guangming and Shah, Syed Afaq and Bennarmoun, Mohammed and Qian, Kun and Schuller, Bj{\"{o}}rn W.},
journal = {IEEE Computational Intelligence Magazine},
title = {{MeDaS: An open-source platform as service to help break the walls between medicine and informatics}},
year = {2020}
}
@dataset{Commowick2018a,
author = {Commowick, Olivier and Istace, Audrey and Kain, Michael and Laurent, Baptiste and Leray, Florent and Simon, Mathieu and Camarasu-Pop, Sorina and Girard, Pascal and Ameli, Roxana and Ferr{\'{e}}, Jean-Christophe and Kerbrat, Anne and Tourdias, Thomas and Cervenansky, Fr{\'{e}}d{\'{e}}ric and Glatard, Tristan and Beaumont, J{\'{e}}r{\'{e}}my and Doyle, Senan and Forbes, Florence and Knight, Jesse and Khademi, April and Mahbod, Amirreza and Wang, Chunliang and {Mc Kinley}, Richard and Wagner, Franca and Muschelli, John and Sweeney, Elizabeth and Roura, Eloy and Llad{\`{o}}, Xavier and Santos, Michel M and Santos, Wellington P and Silva-Filho, Abel G and Tomas-Fernandez, Xavier and Urien, H{\'{e}}l{\`{e}}ne and Bloch, Isabelle and Valverde, Sergi and Cabezas, Mariano and Vera-Olmos, Francisco Javier and Malpica, Norberto and Guttmann, Charles and Vukusic, Sandra and Edan, Gilles and Dojat, Michel and Styner, Martin and Warfield, Simon K and Cotton, Fran{\c{c}}ois and Barillot, Christian},
doi = {10.5281/zenodo.1307653},
publisher = {Zenodo},
title = {{MICCAI 2016 MS lesion segmentation challenge: supplementary results}},
url = {https://doi.org/10.5281/zenodo.1307653},
year = {2018}
}
@online{Bakas2020,
author = {Bakas, Spyridon and Menze, Bjoern and Davatzikos, Christos and Tiwari, Pallavi and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Freymann, John B. and Kirby, Justin S. and Davatzikos, Christos and Huang, Raymond and Colen, Rivka R. and Marcus, Daniel and Fathallah-Shaykh, Hassan and Wiest, Roland and Jakab, Andras and Weber, Marc-Andre and Mahajan, Abhishek},
title = {{MICCAI BRATS - The Multimodal Brain Tumor Segmentation Challenge 2020}},
url = {http://braintumorsegmentation.org/},
year = {2020}
}
@online{brats12,
title = {{MICCAI BRATS 2012}},
url = {http://www2.imm.dtu.dk/projects/BRATS2012/}
}
@online{Bakas2017a,
author = {Bakas, Spyridon and Menze, Bjoern and Davatzikos, Christos and Reyes, Mauricio and Farahani, Keyvan and Freymann, John B. and Kirby, Justin S. and Davatzikos, Christos and Shaykh, Hassan Fathallah and Wiest, Roland and Jakab, Andras and Weber, Marc-Andre},
keywords = {analysis,biomedical,brain,computational,image,imaging,medical,research},
title = {{MICCAI BraTS 2017: Scope | Section for Biomedical Image Analysis (SBIA) | Perelman School of Medicine at the University of Pennsylvania}},
url = {https://www.med.upenn.edu/sbia/brats2017.html},
year = {2017}
}
@online{brats18,
keywords = {analysis,biomedical,brain,computational,image,imaging,medical,research},
title = {{MICCAI BraTS 2018: Scope | Section for Biomedical Image Analysis (SBIA) | Perelman School of Medicine at the University of Pennsylvania}},
url = {https://www.med.upenn.edu/sbia/brats2018.html}
}
@misc{Wang2019a,
author = {Wang, Li and Bui, Toan Duc and Li, Gang and Lin, Weili and Shen, Dinggang},
title = {{MICCAI Grand Challenge on 6-month Infant Brain MRI Segmentation}},
url = {http://iseg2019.web.unc.edu/},
year = {2019}
}
@misc{,
title = {{MICCAI Grand Challenge on 6-month Infant Brain MRI Segmentation from Multiple Sites}}
}
@book{Bucchiarone2020,
address = {Cham},
booktitle = {Microservices},
doi = {10.1007/978-3-030-31646-4},
editor = {Bucchiarone, Antonio and Dragoni, Nicola and Dustdar, Schahram and Lago, Patricia and Mazzara, Manuel and Rivera, Victor and Sadovykh, Andrey},
isbn = {978-3-030-31645-7},
publisher = {Springer International Publishing},
title = {{Microservices}},
url = {http://link.springer.com/10.1007/978-3-030-31646-4},
year = {2020}
}
@article{Muller2019,
annote = {{\_}eprint: 1910.09308},
archivePrefix = {arXiv},
arxivId = {1910.09308},
author = {M{\"{u}}ller, Dominik and Kramer, Frank},
eprint = {1910.09308},
title = {{MIScnn: A Framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning}},
year = {2019}
}
@article{Chen2016,
abstract = {The number of mitoses per tissue area gives an important aggressiveness indication of the invasive breast carcinoma. However, automatic mitosis detection in histology images remains a challenging problem. Traditional methods either employ hand-crafted features to discriminate mitoses from other cells or construct a pixel-wise classifier to label every pixel in a sliding window way. While the former suffers from the large shape variation of mitoses and the existence of many mimics with similar appearance, the slow speed of the later prohibits its use in clinical practice. In order to overcome these shortcomings, we propose a fast and accurate method to detect mitosis by designing a novel deep cascaded convolutional neural network, which is composed of two components. First, by leveraging the fully convolutional neural network, we propose a coarse retrieval model to identify and locate the candidates of mitosis while preserving a high sensitivity. Based on these candidates, a fine discrimination model utilizing knowledge transferred from cross-domain is developed to further single out mitoses from hard mimics. Our approach outperformed other methods by a large margin in 2014 ICPR MITOS-ATYPIA challenge in terms of detection accuracy. When compared with the state-of-the-art methods on the 2012 ICPR MITOSIS data (a smaller and less challenging dataset), our method achieved comparable or better results with a roughly 60 times faster speed.},
author = {Chen, Hao and Dou, Qi and Wang, Xi and Qin, Jing and Heng, Pheng Ann},
isbn = {9781577357605},
journal = {30th AAAI Conference on Artificial Intelligence, AAAI 2016},
pages = {1160--1166},
title = {{Mitosis detection in breast cancer histology images via deep cascaded networks}},
year = {2016}
}
@article{Blundell2016,
abstract = {State of the art deep reinforcement learning algorithms take many millions of interactions to attain human-level performance. Humans, on the other hand, can very quickly exploit highly rewarding nuances of an environment upon first discovery. In the brain, such rapid learning is thought to depend on the hippocampus and its capacity for episodic memory. Here we investigate whether a simple model of hippocampal episodic control can learn to solve difficult sequential decision-making tasks. We demonstrate that it not only attains a highly rewarding strategy significantly faster than state-of-the-art deep reinforcement learning algorithms, but also achieves a higher overall reward on some of the more challenging domains.},
archivePrefix = {arXiv},
arxivId = {1606.04460},
author = {Blundell, Charles and Uria, Benigno and Pritzel, Alexander and Li, Yazhe and Ruderman, Avraham and Leibo, Joel Z and Rae, Jack and Wierstra, Daan and Hassabis, Demis},
eprint = {1606.04460},
month = {jun},
title = {{Model-Free Episodic Control}},
url = {http://arxiv.org/abs/1606.04460},
year = {2016}
}
@book{Guevarra2020,
address = {Berkeley, CA},
author = {Guevarra, Ezra Thess Mendoza},
booktitle = {Modeling and Animation Using Blender},
doi = {10.1007/978-1-4842-5340-3},
isbn = {978-1-4842-5339-7},
publisher = {Apress},
title = {{Modeling and Animation Using Blender}},
url = {http://link.springer.com/10.1007/978-1-4842-5340-3},
year = {2020}
}
@book{Davis2020,
address = {Berkeley, CA},
author = {Davis, Adam L.},
booktitle = {Modern Programming Made Easy},
doi = {10.1007/978-1-4842-5569-8},
isbn = {978-1-4842-5568-1},
publisher = {Apress},
title = {{Modern Programming Made Easy}},
url = {http://link.springer.com/10.1007/978-1-4842-5569-8},
year = {2020}
}
@article{Alkhasli2019,
abstract = {Background: The fronto-striatal network is involved in various motor, cognitive, and emotional processes, such as spatial attention, working memory, decision-making, and emotion regulation. Intermittent theta burst transcranial magnetic stimulation (iTBS) has been shown to modulate functional connectivity of brain networks. Long stimulation intervals, as well as high stimulation intensities are typically applied in transcranial magnetic stimulation (TMS) therapy for mood disorders. The role of stimulation intensity on network function and homeostasis has not been explored systematically yet. Objective: In this pilot study, we aimed to modulate fronto-striatal connectivity by applying iTBS at different intensities to the left dorso-lateral prefrontal cortex (DLPFC). We measured individual and group changes by comparing resting state functional magnetic resonance imaging (rsfMRI) both pre-iTBS and post-iTBS. Differential effects of individual sub- vs. supra-resting motor-threshold stimulation intensities were assessed. Methods: Sixteen healthy subjects underwent excitatory iTBS at two intensities [90{\%} and 120{\%} of individual resting motor threshold (rMT)] on separate days. Six-hundred pulses (2 s trains, 8 s pauses, duration of 3 min, 20 s) were applied over the left DLPFC. Directly before and 7 min after stimulation, task-free rsfMRI sessions, lasting 10 min each, were conducted. Individual seed-to-seed functional connectivity changes were calculated for 10 fronto-striatal and amygdala regions of interest with the SPM toolbox DPABI. Results: Sub-threshold-iTBS increased functional connectivity directly between the left DLPFC and the left and right caudate, respectively. Supra-threshold stimulation did not change fronto-striatal functional connectivity but increased functional connectivity between the right amygdala and the right caudate. Conclusion: A short iTBS protocol applied at sub-threshold intensities was not only sufficient, but favorable, in order to increase bilateral fronto-striatal functional connectivity, while minimizing side effects. The absence of an increase in functional connectivity after supra-threshold stimulation was possibly caused by network homeostatic effects.},
author = {Alkhasli, Isabel and Sakreida, Katrin and Mottaghy, Felix M. and Binkofski, Ferdinand},
doi = {10.3389/fnhum.2019.00190},
issn = {16625161},
journal = {Frontiers in Human Neuroscience},
keywords = {DLPFC,Fronto-striatal network,Functional connectivity,Intermittent theta burst stimulation (iTBS),Prefrontal cortex,Resting state,Striatum},
month = {jun},
title = {{Modulation of fronto-striatal functional connectivity using transcranial magnetic stimulation}},
url = {https://www.frontiersin.org/article/10.3389/fnhum.2019.00190/full},
volume = {13},
year = {2019}
}
@incollection{Luo2020,
address = {Cham},
author = {Luo, Chong and Zeng, Wenjun},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_872-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Monocular and Binocular People Tracking}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}872-1},
year = {2020}
}
@incollection{Kemp2019,
address = {Cham},
author = {Kemp, Jonathan and Kemp, Jonathan},
booktitle = {Film on Video},
doi = {10.4324/9780429468872-5},
pages = {45--54},
publisher = {Springer International Publishing},
title = {{Motion blur}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}512-1},
year = {2019}
}
@article{Mendrik2015,
abstract = {Many methods have been proposed for tissue segmentation in brain MRI scans. The multitude of methods proposed complicates the choice of one method above others. We have therefore established the MRBrainS online evaluation framework for evaluating (semi)automatic algorithms that segment gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF) on 3T brain MRI scans of elderly subjects (65-80 y). Participants apply their algorithms to the provided data, after which their results are evaluated and ranked. Full manual segmentations of GM, WM, and CSF are available for all scans and used as the reference standard. Five datasets are provided for training and fifteen for testing. The evaluated methods are ranked based on their overall performance to segment GM, WM, and CSF and evaluated using three evaluation metrics (Dice, H95, and AVD) and the results are published on the MRBrainS13 website. We present the results of eleven segmentation algorithms that participated in the MRBrainS13 challenge workshop at MICCAI, where the framework was launched, and three commonly used freeware packages: FreeSurfer, FSL, and SPM. The MRBrainS evaluation framework provides an objective and direct comparison of all evaluated algorithms and can aid in selecting the best performing method for the segmentation goal at hand.},
author = {Mendrik, Adri{\"{e}}nne M. and Vincken, Koen L. and Kuijf, Hugo J. and Breeuwer, Marcel and Bouvy, Willem H. and {De Bresser}, Jeroen and Alansary, Amir and {De Bruijne}, Marleen and Carass, Aaron and El-Baz, Ayman and Jog, Amod and Katyal, Ranveer and Khan, Ali R. and {Van Der Lijn}, Fedde and Mahmood, Qaiser and Mukherjee, Ryan and {Van Opbroek}, Annegreet and Paneri, Sahil and Pereira, S{\'{e}}rgio and Persson, Mikael and Rajchl, Martin and Sarikaya, Duygu and Smedby, {\"{O}}rjan and Silva, Carlos A. and Vrooman, Henri A. and Vyas, Saurabh and Wang, Chunliang and Zhao, Liang and Biessels, Geert Jan and Viergever, Max A.},
doi = {10.1155/2015/813696},
issn = {16875273},
journal = {Computational Intelligence and Neuroscience},
pages = {1--16},
title = {{MRBrainS Challenge: Online Evaluation Framework for Brain Image Segmentation in 3T MRI Scans}},
url = {http://www.hindawi.com/journals/cin/2015/813696/},
volume = {2015},
year = {2015}
}
@misc{Kuijf2018,
author = {Kuijf, Hugo J. and Bennink, H. Edwin and Biessels, Geert Jan and Viergever, Max A. and Weaver, Nick A. and Vincken, Koen L.},
title = {{MRBrainS18 - Grand Challenge on MR Brain Segmentation 2018}},
url = {https://mrbrains18.isi.uu.nl/},
year = {2018}
}
@online{MRIandAlzheimersKaggle,
author = {Boysen, Jacob},
keywords = {health,health sciences,healthcare,image data,medical facilities and services,neurological conditions,neurology,neuroscience,old age},
title = {{MRI and Alzheimers}},
url = {https://www.kaggle.com/jboysen/mri-and-alzheimers},
urldate = {2020-05-25},
year = {2017}
}
@online{Malekzadeh2019,
author = {Malekzadeh, S.},
keywords = {biology,deep learning,health foundations and medical research,medical facilities and services,neurological conditions,object segmentation,old age},
title = {{MRI Hippocampus Segmentation | Kaggle}},
url = {https://www.kaggle.com/sabermalek/mrihs},
urldate = {2020-05-25},
year = {2019}
}
@article{Varmazyar2020,
author = {Varmazyar, Hadi and Ghareaghaji, Zahra and Malekzadeh, Saber},
title = {{MRI Hippocampus Segmentation using Deep Learning autoencoders}},
year = {2020}
}
@online{MSlesion2008,
author = {Styner, Martin and Warfield, Simon and Lee, Joohwi},
title = {{MS lesion segmentation challenage 2008}},
url = {http://www.ia.unc.edu/MSseg/},
urldate = {2020-05-21},
year = {2008}
}
@online{MSSEG,
author = {FLI-IAM},
title = {{MS segmentation challenge using a data management and processing infrastructure}},
url = {https://portal.fli-iam.irisa.fr/msseg-challenge/overview},
urldate = {2020-05-21},
year = {2016}
}
@online{MSChallengeIACL2015,
author = {Pham, Dzung and Bazin, Pierre-Louis and Carass, Aaron and Calabresi, Peter and Crainiceanu, Ciprian and Ellingsen, Lotta and He, Qing and Prince, Jerry and Reich, Daniel and Roy, Snehashis},
title = {{MSChallenge - IACL}},
url = {http://iacl.ece.jhu.edu/index.php/MSChallenge},
urldate = {2020-05-21},
year = {2015}
}
@incollection{Boyarski2020,
address = {Cham},
author = {Boyarski, Amit and Bronstein, Alex},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_827-1},
pages = {1--14},
publisher = {Springer International Publishing},
title = {{Multidimensional Scaling}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}827-1},
year = {2020}
}
@incollection{Sheller2019,
abstract = {Deep learning models for semantic segmentation of images require large amounts of data. In the medical imaging domain, acquiring sufficient data is a significant challenge. Labeling medical image data requires expert knowledge. Collaboration between institutions could address this challenge, but sharing medical data to a centralized location faces various legal, privacy, technical, and data-ownership challenges, especially among international institutions. In this study, we introduce the first use of federated learning for multi-institutional collaboration, enabling deep learning modeling without sharing patient data. Our quantitative results demonstrate that the performance of federated semantic segmentation models (Dice = 0.852) on multimodal brain scans is similar to that of models trained by sharing data (Dice = 0.862). We compare federated learning with two alternative collaborative learning methods and find that they fail to match the performance of federated learning.},
archivePrefix = {arXiv},
arxivId = {1810.04304},
author = {Sheller, Micah J. and Reina, G. Anthony and Edwards, Brandon and Martin, Jason and Bakas, Spyridon},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-11723-8_9},
eprint = {1810.04304},
isbn = {9783030117221},
issn = {16113349},
keywords = {BraTS,Deep learning,Federated,Glioma,Incremental,Machine learning,Segmentation},
pages = {92--104},
title = {{Multi-institutional deep learning modeling without sharing patient data: A feasibility study on brain tumor segmentation}},
url = {http://link.springer.com/10.1007/978-3-030-11723-8{\_}9},
volume = {11383 LNCS},
year = {2019}
}
@article{Song2019,
abstract = {It is a challenging problem to achieve generalized nuclear segmentation in digital histopathology images. Existing techniques, using either handcrafted features in learning-based models or traditional image analysis-based approaches, do not effectively tackle the challenging cases, such as crowded nuclei, chromatin-sparse, and heavy background clutter. In contrast, deep networks have achieved state-of-the-art performance in modeling various nuclear appearances. However, their success is limited due to the size of the considered networks. We solve these problems by reformulating nuclear segmentation in terms of a cascade 2-class classification problem and propose a multi-layer boosting sparse convolutional (ML-BSC) model. In the proposed ML-BSC model, discriminative probabilistic binary decision trees (PBDTs) are designed as weak learners in each layer to cope with challenging cases. A sparsity-constrained cascade structure enables the ML-BSC model to improve representation learning. Comparing to the existing techniques, our method can accurately separate individual nuclei in complex histopathology images, and it is more robust against chromatin-sparse and heavy background clutter. An evaluation carried out using three disparate datasets demonstrates the superiority of our method over the state-of-the-art supervised approaches in terms of segmentation accuracy.},
author = {Song, Jie and Xiao, Liang and Molaei, Mohsen and Lian, Zhichao},
doi = {https://doi.org/10.1016/j.knosys.2019.03.031},
issn = {0950-7051},
journal = {Knowledge-Based Systems},
keywords = {Cascade classification,Multi-layer boosting sparse convolutional model,Nucleus segmentation,Probabilistic binary decision tree,Representation learning},
pages = {40 -- 53},
title = {{Multi-layer boosting sparse convolutional model for generalized nuclear segmentation from histopathology images}},
url = {http://www.sciencedirect.com/science/article/pii/S095070511930156X},
volume = {176},
year = {2019}
}
@online{Bakas2019,
author = {Bakas, Spyridon and Menze, Bjoern and Davatzikos, Christos and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Freymann, John B. and Kirby, Justin S. and Davatzikos, Christos and Fathallah-Shaykh, Hassan and Wiest, Roland and Jakab, Andras and Colen, Rivka R. and Marcus, Daniel and Weber, Marc-Andre and Mahajan, Abhishek},
title = {{Multimodal Brain Tumor Segmentation Challenge 2019 | CBICA | Perelman School of Medicine at the University of Pennsylvania}},
url = {https://www.med.upenn.edu/cbica/brats-2019/},
year = {2019}
}
@article{Mahmood2018,
abstract = {Humans make accurate decisions by interpreting complex data from multiple sources. Medical diagnostics, in particular, often hinge on human interpretation of multi-modal information. In order for artificial intelligence to make progress in automated, objective, and accurate diagnosis and prognosis, methods to fuse information from multiple medical imaging modalities are required. However, combining information from multiple data sources has several challenges, as current deep learning architectures lack the ability to extract useful representations from multimodal information, and often simple concatenation is used to fuse such information. In this work, we propose Multimodal DenseNet, a novel architecture for fusing multimodal data. Instead of focusing on concatenation or early and late fusion, our proposed architectures fuses information over several layers and gives the model flexibility in how it combines information from multiple sources. We apply this architecture to the challenge of polyp characterization and landmark identification in endoscopy. Features from white light images are fused with features from narrow band imaging or depth maps. This study demonstrates that Multimodal DenseNet outperforms monomodal classification as well as other multimodal fusion techniques by a significant margin on two different datasets.},
annote = {{\_}eprint: 1811.07407},
archivePrefix = {arXiv},
arxivId = {1811.07407},
author = {Mahmood, Faisal and Yang, Ziyun and Ashley, Thomas and Durr, Nicholas J},
eprint = {1811.07407},
journal = {ArXiv e-prints},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning},
title = {{Multimodal Densenet}},
url = {http://arxiv.org/abs/1811.07407},
year = {2018}
}
@article{Trullo2019,
abstract = {Segmentation of organs at risk (OAR) in computed tomography (CT) is of vital importance in radiotherapy treatment. This task is time consuming and for some organs, it is very challenging due to low-intensity contrast in CT. We propose a framework to perform the automatic segmentation of multiple OAR: esophagus, heart, trachea, and aorta. Different from previous works using deep learning techniques, we make use of global localization information, based on an original distance map that yields not only the localization of each organ, but also the spatial relationship between them. Instead of segmenting directly the organs, we first generate the localization map by minimizing a reconstruction error within an adversarial framework. This map that includes localization information of all organs is then used to guide the segmentation task in a fully convolutional setting. Experimental results show encouraging performance on CT scans of 60 patients totaling 11,084 slices in comparison with other state-of-the-art methods.},
author = {Trullo, Roger and Petitjean, Caroline and Dubray, Bernard and Ruan, Su},
doi = {10.1117/1.jmi.6.1.014001},
issn = {2329-4310},
journal = {Journal of Medical Imaging},
title = {{Multiorgan segmentation using distance-aware adversarial networks}},
year = {2019}
}
@inproceedings{Zhuang2016,
abstract = {Cardiac segmentation is commonly a prerequisite for functional analysis of the heart,such as to identify and quantify the infarcts and edema from the normal myocardium using the late-enhanced (LE) and T2-weighted MRI. The automatic delineation of myocardium is however challenging due to the heterogeneous intensity distributions and indistinct boundaries in the images. In this work,we present a multivariate mixture model (MvMM) for text classification,which combines the complementary information from multi-sequence (MS) cardiac MRI and perform the segmentation of them simultaneously. The expectation maximization (EM) method is adopted to estimate the segmentation and model parameters from the log-likelihood (LL) of the mixture model,where a probabilistic atlas is used for initialization. Furthermore,to correct the intra- and inter-image misalignments,we formulate the MvMM with transformations,which are embedded into the LL framework and thus can be optimized by the iterative conditional mode approach. We applied MvMM for segmentation of eighteen subjects with three sequences and obtained promising results. We compared with two conventional methods,and the improvements of segmentation performance on LE and T2 MRI were evident and statistically significant by MvMM.},
author = {Zhuang, Xiahai},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46723-8_67},
isbn = {9783319467221},
issn = {16113349},
title = {{Multivariate mixture model for cardiac segmentation from multi-sequence MRI}},
year = {2016}
}
@article{Zhuang2019,
abstract = {The author proposes a method for simultaneous registration and segmentation of multi-source images, using the multivariate mixture model (MvMM) and maximum of log-likelihood (LL) framework. Specifically, the method is applied to the problem of myocardial segmentation combining the complementary information from multi-sequence (MS) cardiac magnetic resonance (CMR) images. For the image misalignment and incongruent data, the MvMM is formulated with transformations and is further generalized for dealing with the hetero-coverage multi-modality images (HC-MMIs). The segmentation of MvMM is performed in a virtual common space, to which all the images and misaligned slices are simultaneously registered. Furthermore, this common space can be divided into a number of sub-regions, each of which contains congruent data, thus the HC-MMIs can be modeled using a set of conventional MvMMs. Results show that MvMM obtained significantly better performance compared to the conventional approaches and demonstrated good potential for scar quantification as well as myocardial segmentation. The generalized MvMM has also demonstrated better robustness in the incongruent data, where some images may not fully cover the region of interest, and the full coverage can only be reconstructed combining the images from multiple sources.},
author = {Zhuang, Xiahai},
doi = {10.1109/TPAMI.2018.2869576},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Multivariate image,cardiac MRI,medical image analysis,multi-modality,registration,segmentation},
title = {{Multivariate Mixture Model for Myocardial Segmentation Combining Multi-Source Images}},
year = {2019}
}
@inproceedings{Tatarchenko2015a,
abstract = {We present a convolutional network capable of inferring a 3D representation of a previously unseen object given a single image of this object. Concretely, the network can predict an RGB image and a depth map of the object as seen from an arbitrary view. Several of these depth maps fused together give a full point cloud of the object. The point cloud can in turn be transformed into a surface mesh. The network is trained on renderings of synthetic 3D models of cars and chairs. It successfully deals with objects on cluttered background and generates reasonable predictions for real images of cars.},
archivePrefix = {arXiv},
arxivId = {1511.06702},
author = {Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46478-7_20},
eprint = {1511.06702},
isbn = {9783319464770},
issn = {16113349},
keywords = {3D from single image,Convolutional networks,Deep learning},
pages = {322--337},
title = {{Multi-view 3D models from single images with a convolutional network}},
url = {http://arxiv.org/abs/1511.06702},
volume = {9911 LNCS},
year = {2016}
}
@article{Su2015,
annote = {From Duplicate 1 (Multi-view Convolutional Neural Networks for 3D Shape Recognition - Su, Hang; Maji, Subhransu; Kalogerakis, Evangelos; Learned-Miller, Erik G)

{\_}eprint: 1505.00880},
archivePrefix = {arXiv},
arxivId = {1505.00880},
author = {Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik G},
eprint = {1505.00880},
journal = {CoRR},
title = {{Multi-view Convolutional Neural Networks for 3D Shape Recognition}},
url = {http://arxiv.org/abs/1505.00880},
volume = {abs/1505.0},
year = {2015}
}
@article{Tustison2010,
abstract = {A variant of the popular nonparametric nonuniform intensity normalization (N3) algorithm is proposed for bias field correction. Given the superb performance of N3 and its public availability, it has been the subject of several evaluation studies. These studies have demonstrated the importance of certain parameters associated with the B-spline least-squares fitting. We propose the substitution of a recently developed fast and robust B-spline approximation routine and a modified hierarchical optimization scheme for improved bias field correction over the original N3 algorithm. Similar to the N3 algorithm, we also make the source code, testing, and technical documentation of our contribution, which we denote as N4ITK, available to the public through the Insight Toolkit of the National Institutes of Health. Performance assessment is demonstrated using simulated data from the publicly available Brainweb database, hyperpolarized 3He lung image data, and 9.4T postmortem hippocampus data. {\textcopyright} 2006 IEEE.},
author = {Tustison, Nicholas J and Avants, Brian B and Cook, Philip A and Zheng, Yuanjie and Egan, Alexander and Yushkevich, Paul A and Gee, James C},
doi = {10.1109/TMI.2010.2046908},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {B-spline approximation,Bias field,Inhomogeneity,N3},
month = {jun},
number = {6},
pages = {1310--1320},
pmid = {20378467},
title = {{N4ITK: Improved N3 bias correction}},
volume = {29},
year = {2010}
}
@misc{TCIA-NCI-ISBI-2013,
author = {Bloch, Nicholas and Madabhushi, Anant and Huisman, Henkjan and Freymann, John and Kirby, Justin and Grauer, Michael and Enquobahrie, Andinet and Jaffe, Carl and Clarke, Larry and Farahani, Keyvan},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2015.ZF0VLOPV},
publisher = {The Cancer Imaging Archive},
title = {{NCI-ISBI 2013 Challenge: Automated Segmentation of Prostate Structures}},
url = {https://wiki.cancerimagingarchive.net/x/B4NEAQ},
year = {2015}
}
@inproceedings{Chen2015,
abstract = {We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.},
archivePrefix = {arXiv},
arxivId = {1511.05641},
author = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
booktitle = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
eprint = {1511.05641},
title = {{Net2Net: Accelerating learning via knowledge transfer}},
volume = {abs/1511.0},
year = {2016}
}
@inproceedings{Wei2016,
abstract = {We present a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as network morphism in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous nonlinear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.},
archivePrefix = {arXiv},
arxivId = {1603.01670},
author = {Wei, Tao and Wang, Changhu and Rui, Yong and Chen, Chang Wen},
booktitle = {33rd International Conference on Machine Learning, ICML 2016},
eprint = {1603.01670},
isbn = {9781510829008},
pages = {842--850},
title = {{Network morphism}},
volume = {2},
year = {2016}
}
@inproceedings{Zoph2016,
abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
archivePrefix = {arXiv},
arxivId = {1904.09981},
author = {Zoph, Barret and Le, Quoc V},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1904.09981},
title = {{Neural architecture search with reinforcement learning}},
volume = {abs/1611.0},
year = {2019}
}
@book{Haykin2011,
author = {{Simon Haykin}},
publisher = {China Machine Press and Pearson Education},
title = {{Neural Network and Learning Machines 3rd edition}},
translator = {Shen, Furao and Xu, Ye and Zheng, Jun and Chao, Jin},
year = {2009}
}
@article{Lepping2016,
abstract = {Background Anterior cingulate cortex (ACC) and striatum are part of the emotional neural circuitry implicated in major depressive disorder (MDD). Music is often used for emotion regulation, and pleasurable music listening activates the dopaminergic system in the brain, including the ACC. The present study uses functional MRI (fMRI) and an emotional nonmusical and musical stimuli paradigm to examine how neural processing of emotionally provocative auditory stimuli is altered within the ACC and striatum in depression. Method Nineteen MDD and 20 never-depressed (ND) control participants listened to standardized positive and negative emotional musical and nonmusical stimuli during fMRI scanning and gave subjective ratings of valence and arousal following scanning. Results ND participants exhibited greater activation to positive versus negative stimuli in ventral ACC. When compared with ND participants, MDD participants showed a different pattern of activation in ACC. In the rostral part of the ACC, ND participants showed greater activation for positive information, while MDD participants showed greater activation to negative information. In dorsal ACC, the pattern of activation distinguished between the types of stimuli, with ND participants showing greater activation to music compared to nonmusical stimuli, while MDD participants showed greater activation to nonmusical stimuli, with the greatest response to negative nonmusical stimuli. No group differences were found in striatum. Conclusions These results suggest that people with depression may process emotional auditory stimuli differently based on both the type of stimulation and the emotional content of that stimulation. This raises the possibility that music may be useful in retraining ACC function, potentially leading to more effective and targeted treatments.},
author = {Lepping, Rebecca J. and Atchley, Ruth Ann and Chrysikou, Evangelia and Martin, Laura E. and Clair, Alicia A. and Ingram, Rick E. and Simmons, W. Kyle and Savage, Cary R.},
doi = {10.1371/journal.pone.0156859},
editor = {Kotz, Sonja},
issn = {19326203},
journal = {PLoS ONE},
month = {jun},
number = {6},
pages = {e0156859},
title = {{Neural processing of emotional musical and nonmusical stimuli in depression}},
url = {http://dx.plos.org/10.1371/journal.pone.0156859},
volume = {11},
year = {2016}
}
@article{Rumelhart1988,
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
address = {Cambridge, MA, USA},
annote = {Section: Learning Representations by Back-propagating Errors},
author = {Mingolla, Ennio and Bullock, Daniel},
doi = {10.1016/0893-6080(89)90025-7},
editor = {Anderson, James A and Rosenfeld, Edward},
isbn = {0-262-01097-6},
issn = {08936080},
journal = {Neural Networks},
number = {5},
pages = {405--409},
publisher = {MIT Press},
title = {{Neurocomputing: Foundations of Research}},
url = {http://dl.acm.org/citation.cfm?id=65669.104451},
volume = {2},
year = {1989}
}
@article{Wardlaw2013,
abstract = {Cerebral small vessel disease (SVD) is a common accompaniment of ageing. Features seen on neuroimaging include recent small subcortical infarcts, lacunes, white matter hyperintensities, perivascular spaces, microbleeds, and brain atrophy. SVD can present as a stroke or cognitive decline, or can have few or no symptoms. SVD frequently coexists with neurodegenerative disease, and can exacerbate cognitive deficits, physical disabilities, and other symptoms of neurodegeneration. Terminology and definitions for imaging the features of SVD vary widely, which is also true for protocols for image acquisition and image analysis. This lack of consistency hampers progress in identifying the contribution of SVD to the pathophysiology and clinical features of common neurodegenerative diseases. We are an international working group from the Centres of Excellence in Neurodegeneration. We completed a structured process to develop definitions and imaging standards for markers and consequences of SVD. We aimed to achieve the following: first, to provide a common advisory about terms and definitions for features visible on MRI; second, to suggest minimum standards for image acquisition and analysis; third, to agree on standards for scientific reporting of changes related to SVD on neuroimaging; and fourth, to review emerging imaging methods for detection and quantification of preclinical manifestations of SVD. Our findings and recommendations apply to research studies, and can be used in the clinical setting to standardise image interpretation, acquisition, and reporting. This Position Paper summarises the main outcomes of this international effort to provide the STandards for ReportIng Vascular changes on nEuroimaging (STRIVE). {\textcopyright} 2013 Elsevier Ltd.},
author = {Wardlaw, Joanna M. and Smith, Eric E. and Biessels, Geert J. and Cordonnier, Charlotte and Fazekas, Franz and Frayne, Richard and Lindley, Richard I. and O'Brien, John T. and Barkhof, Frederik and Benavente, Oscar R. and Black, Sandra E. and Brayne, Carol and Breteler, Monique and Chabriat, Hugues and DeCarli, Charles and de Leeuw, Frank Erik and Doubal, Fergus and Duering, Marco and Fox, Nick C. and Greenberg, Steven and Hachinski, Vladimir and Kilimann, Ingo and Mok, Vincent and van Oostenbrugge, Robert and Pantoni, Leonardo and Speck, Oliver and Stephan, Blossom C.M. and Teipel, Stefan and Viswanathan, Anand and Werring, David and Chen, Christopher and Smith, Colin and van Buchem, Mark and Norrving, Bo and Gorelick, Philip B. and Dichgans, Martin},
doi = {10.1016/S1474-4422(13)70124-8},
issn = {14744422},
journal = {The Lancet Neurology},
month = {aug},
number = {8},
pages = {822--838},
pmid = {23867200},
title = {{Neuroimaging standards for research into small vessel disease and its contribution to ageing and neurodegeneration}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1474442213701248},
volume = {12},
year = {2013}
}
@article{Rajchl2018,
abstract = {NeuroNet is a deep convolutional neural network mimicking multiple popular and state-of-the-art brain segmentation tools including FSL, SPM, and MALPEM. The network is trained on 5,000 T1-weighted brain MRI scans from the UK Biobank Imaging Study that have been automatically segmented into brain tissue and cortical and sub-cortical structures using the standard neuroimaging pipelines. Training a single model from these complementary and partially overlapping label maps yields a new powerful "all-in-one", multi-output segmentation tool. The processing time for a single subject is reduced by an order of magnitude compared to running each individual software package. We demonstrate very good reproducibility of the original outputs while increasing robustness to variations in the input data. We believe NeuroNet could be an important tool in large-scale population imaging studies and serve as a new standard in neuroscience by reducing the risk of introducing bias when choosing a specific software package.},
annote = {{\_}eprint: 1806.04224},
archivePrefix = {arXiv},
arxivId = {1806.04224},
author = {Rajchl, Martin and Pawlowski, Nick and Rueckert, Daniel and Matthews, Paul M and Glocker, Ben},
eprint = {1806.04224},
title = {{NeuroNet: Fast and Robust Reproduction of Multiple Brain Image Segmentation Pipelines}},
url = {http://arxiv.org/abs/1806.04224},
year = {2018}
}
@article{Gibson2018,
abstract = {Background and objectives: Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. Established deep-learning platforms are flexible but do not provide specific functionality for medical image analysis and adapting them for this domain of application requires substantial implementation effort. Consequently, there has been substantial duplication of effort and incompatible infrastructure developed across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon. Methods: The NiftyNet infrastructure provides a modular deep-learning pipeline for a range of medical imaging applications including segmentation, regression, image generation and representation learning applications. Components of the NiftyNet pipeline including data loading, data augmentation, network architectures, loss functions and evaluation metrics are tailored to, and take advantage of, the idiosyncracies of medical image analysis and computer-assisted intervention. NiftyNet is built on the TensorFlow framework and supports features such as TensorBoard visualization of 2D and 3D images and computational graphs by default. Results: We present three illustrative medical image analysis applications built using NiftyNet infrastructure: (1) segmentation of multiple abdominal organs from computed tomography; (2) image regression to predict computed tomography attenuation maps from brain magnetic resonance images; and (3) generation of simulated ultrasound images for specified anatomical poses. Conclusions: The NiftyNet infrastructure enables researchers to rapidly develop and distribute deep learning solutions for segmentation, regression, image generation and representation learning applications, or extend the platform to new applications.},
archivePrefix = {arXiv},
arxivId = {1709.03485},
author = {Gibson, Eli and Li, Wenqi and Sudre, Carole and Fidon, Lucas and Shakir, Dzhoshkun I and Wang, Guotai and Eaton-Rosen, Zach and Gray, Robert and Doel, Tom and Hu, Yipeng and Whyntie, Tom and Nachev, Parashkev and Modat, Marc and Barratt, Dean C and Ourselin, S{\'{e}}bastien and Cardoso, M Jorge and Vercauteren, Tom},
doi = {10.1016/j.cmpb.2018.01.025},
eprint = {1709.03485},
issn = {18727565},
journal = {Computer Methods and Programs in Biomedicine},
keywords = {Convolutional neural network,Deep learning,Generative adversarial network,Image regression,Medical image analysis,Segmentation},
pages = {113--122},
title = {{NiftyNet: a deep-learning platform for medical imaging}},
url = {http://www.sciencedirect.com/science/article/pii/S0169260717311823},
volume = {158},
year = {2018}
}
@misc{,
title = {{No Title}},
url = {https://mrbrains18.isi.uu.nl/}
}
@misc{Unknown,
doi = {10.18112/openneuro.ds001354.v1.0.0},
title = {{No Titleبرنامج مقترح لمادة التشكيل على المانيكان لطلاب الفرقة الرابعة شعبة الملابس والنسيج}}
}
@article{S.Senthilraja2014,
abstract = {Image processing concept play a important role in the field of Medical to diagnosis of diseases. Noise is introduced in the medical images due to various reasons. In Medical Imaging, Noise degrades the quality of images. This degradation includes suppression of edges, blurring boundaries etc. Edge and preservation details are very important to discover a disease. Noise removal is a very challenging issue in the Medical Image Processing. Denoising can help the physicians to diagnose the diseases. Medical Images include CT, MRI scan, X-ray and ultrasound images etc. This paper we implemented a new filter called WB-Filter for Medical Image denoising. WB-Filter mainly focuses on speckle noise {\&} Gaussian Noise removal especially in the CT scan images. Experimental results are compared with other three filtering concepts. The result images quality is measured by the PSNR, RMSE and MSE. The results demonstrate that the proposed WB-Filter concept obtaining the optimum result quality of the Medical Image.},
author = {Senthilraja, S and Suresh, P and Suganthi, M},
issn = {2229-5518},
journal = {International Journal of Scientific and Engineering Research},
month = {mar},
number = {3},
pages = {243},
title = {{Noise Reduction in Computed Tomography Image Using WB–Filter}},
volume = {5},
year = {2014}
}
@misc{nvidia-docker,
abstract = {Over the last few years there has been a dramatic rise in the use of containers for deploying data center applications at scale. The reason for this is simple: containers encapsulate an application's dependencies to provide reproducible and reliable execution of applications and services without the overhead of a full virtual machine. If you have ever spent a day provisioning a server with a multitude of packages for a scientific or deep learning application, or have put in weeks of effort to ensure your application can be built and deployed in multiple linux environments.},
author = {{Ryan Olson, Jonathan Calmels}, Felix Abecassis and Phil Rogers |},
title = {{NVIDIA Docker: GPU Server Application Deployment Made Easy}},
url = {https://devblogs.nvidia.com/nvidia-docker-gpu-server-application-deployment-made-easy/},
year = {2016}
}
@online{OASIS,
keywords = {MRI,data,longitudinal,neuroimaging,open source},
title = {{OASIS Brains - Open Access Series of Imaging Studies}},
url = {https://www.oasis-brains.org/{\#}data},
year = {2019}
}
@article{LaMontagne2019,
abstract = {OASIS-3 is a compilation of MRI and PET imaging and related clinical data for 1098 participants who were collected across several ongoing studies in the Washington University Knight Alzheimer Disease Research Center over the course of 15 years. Participants include 605 cognitively normal adults and 493 individuals at various stages of cognitive decline ranging in age from 42 to 95 years. The OASIS-3 dataset contains over 2000 MR sessions, including multiple structural and functional sequences. PET metabolic and amyloid imaging includes over 1500 raw imaging scans and the accompanying post-processed files from the PET Unified Pipeline (PUP) are also available in OASIS-3. OASIS-3 also contains post-processed imaging data such as volumetric segmentations and PET analyses. Imaging data is accompanied by dementia and APOE status and longitudinal clinical and cognitive outcomes. OASIS-3 is available as an open access data set to the scientific community to answer questions related to healthy aging and dementia.

{\#}{\#}{\#} Competing Interest Statement

Authors P.J.L., S.K., R.H., E.G., C.X., J.H., K.M., A.G.V., M.E.R., C.C. declare no competing interests. J.C.M. is funded by NIH grants {\#} P50AG005681; P01AG003991; P01AG026276 and UF1AG032438. Neither J.C.M. nor his family owns stock or has equity interest (outside of mutual funds or other externally directed accounts) in any pharmaceutical or biotechnology company. T.L.S.B. Participated in clinical trials sponsored by Eli Lilly, Roche, and Biogen. Avid Radiopharmaceuticals (a wholly owned subsidiary of Eli Lilly) provided T.L.S.B. doses of 18F-florbetapir, partial funding for 18F-florbetapir scanning, precursor for 18F-flortaucipir and technology transfer for manufacturing of 18F-flortaucipir).

{\#}{\#}{\#} Funding Statement

Funding for the Knight ADRC and KARI were provided by NIH P50AG00561, P30NS09857781, P01AG026276, P01AG003991, R01AG043434, R01AG054567, UL1TR000448, and R01EB009352. Florbetapir doses were provided by Avid Radiopharmaceuticals, a wholly owned subsidiary of Eli Lilly. 

{\#}{\#}{\#} Author Declarations

All relevant ethical guidelines have been followed; any necessary IRB and/or ethics committee approvals have been obtained and details of the IRB/oversight body are included in the manuscript.

Yes

All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.

Yes

I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).

Yes

I have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.

Yes

OASIS-3 data is openly available to the scientific community at https://www.oasis-brains.org. Prior to accessing the data, users are required to agree to the OASIS Data Use Terms.

{\textless}http://www.oasis-brains.org/{\textgreater}},
author = {LaMontagne, Pamela J and Benzinger, Tammie L.S. and Morris, John C. and Keefe, Sarah and Hornbeck, Russ and Xiong, Chengjie and Grant, Elizabeth and Hassenstab, Jason and Moulder, Krista and Vlassenko, Andrei and Raichle, Marcus E. and Cruchaga, Carlos and Marcus, Daniel},
doi = {10.1101/2019.12.13.19014902},
journal = {medRxiv},
month = {jan},
pages = {2019.12.13.19014902},
title = {{OASIS-3: Longitudinal Neuroimaging, Clinical, and Cognitive Dataset for Normal Aging and Alzheimer Disease}},
url = {http://medrxiv.org/content/early/2019/12/15/2019.12.13.19014902.abstract},
year = {2019}
}
@incollection{Wu2012,
abstract = {Over the past twenty years, data-driven methods have become a dominant paradigm for computer vision, with numerous practical successes. In difficult computer vision tasks, such as the detection of object categories (for example, the detection of faces of various gender, age, race, and pose, under various illumination and background conditions), researchers generally learn a classifier that can distinguish an image patch that contains the object of interest from all other image patches. Ensemble learning methods have been very successful in learning classifiers for object detection.},
address = {Cham},
author = {Wu, Jianxin and Rehg, James M.},
booktitle = {Ensemble Machine Learning: Methods and Applications},
doi = {10.1007/9781441993267_8},
isbn = {9781441993267},
pages = {225--250},
publisher = {Springer International Publishing},
title = {{Object detection}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}660-1},
year = {2012}
}
@article{Commowick2018,
abstract = {We present a study of multiple sclerosis segmentation algorithms conducted at the international MICCAI 2016 challenge. This challenge was operated using a new open-science computing infrastructure. This allowed for the automatic and independent evaluation of a large range of algorithms in a fair and completely automatic manner. This computing infrastructure was used to evaluate thirteen methods of MS lesions segmentation, exploring a broad range of state-of-theart algorithms, against a high-quality database of 53 MS cases coming from four centers following a common definition of the acquisition protocol. Each case was annotated manually by an unprecedented number of seven different experts. Results of the challenge highlighted that automatic algorithms, including the recent machine learning methods (random forests, deep learning, {\ldots}), are still trailing human expertise on both detection and delineation criteria. In addition, we demonstrate that computing a statistically robust consensus of the algorithms performs closer to human expertise on one score (segmentation) although still trailing on detection scores.},
author = {Commowick, Olivier and Istace, Audrey and Kain, Micha{\"{e}}l and Laurent, Baptiste and Leray, Florent and Simon, Mathieu and Pop, Sorina Camarasu and Girard, Pascal and Am{\'{e}}li, Roxana and Ferr{\'{e}}, Jean Christophe and Kerbrat, Anne and Tourdias, Thomas and Cervenansky, Fr{\'{e}}d{\'{e}}ric and Glatard, Tristan and Beaumont, J{\'{e}}r{\'{e}}my and Doyle, Senan and Forbes, Florence and Knight, Jesse and Khademi, April and Mahbod, Amirreza and Wang, Chunliang and McKinley, Richard and Wagner, Franca and Muschelli, John and Sweeney, Elizabeth and Roura, Eloy and Llad{\'{o}}, Xavier and Santos, Michel M. and Santos, Wellington P. and Silva-Filho, Abel G. and Tomas-Fernandez, Xavier and Urien, H{\'{e}}l{\`{e}}ne and Bloch, Isabelle and Valverde, Sergi and Cabezas, Mariano and Vera-Olmos, Francisco Javier and Malpica, Norberto and Guttmann, Charles and Vukusic, Sandra and Edan, Gilles and Dojat, Michel and Styner, Martin and Warfield, Simon K. and Cotton, Fran{\c{c}}ois and Barillot, Christian},
doi = {10.1038/s41598-018-31911-7},
issn = {20452322},
journal = {Scientific Reports},
month = {dec},
number = {1},
pages = {13650},
title = {{Objective Evaluation of Multiple Sclerosis Lesion Segmentation using a Data Management and Processing Infrastructure}},
url = {http://www.nature.com/articles/s41598-018-31911-7},
volume = {8},
year = {2018}
}
@inproceedings{Wang2017,
abstract = {We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D shape analysis. Built upon the octree representation of 3D shapes, our method takes the average normal vectors of a 3D model sampled in the finest leaf octants as input and performs 3D CNN operations on the octants occupied by the 3D shape surface. We design a novel octree data structure to efficiently store the octant information and CNN features into the graphics memory and execute the entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN structures and works for 3D shapes in different representations. By restraining the computations on the octants occupied by 3D surfaces, the memory and computational costs of the O-CNN grow quadratically as the depth of the octree increases, which makes the 3D CNN feasible for high-resolution 3D models. We compare the performance of the O-CNN with other existing 3D CNN solutions and demonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks, including object classification, shape retrieval, and shape segmentation.},
archivePrefix = {arXiv},
arxivId = {1712.01537},
author = {Wang, Peng Shuai and Liu, Yang and Guo, Yu Xiao and Sun, Chun Yu and Tong, Xin},
booktitle = {ACM Transactions on Graphics},
doi = {10.1145/3072959.3073608},
eprint = {1712.01537},
issn = {15577368},
keywords = {Convolutional neural network,Object classification,Octree,Shape retrieval,Shape segmentation},
number = {4},
pages = {72:1--72:11},
title = {{O-CNN: Octree-based convolutional neural networks for 3D shape analysis}},
url = {http://doi.acm.org/10.1145/3072959.3073608},
volume = {36},
year = {2017}
}
@inproceedings{Riegler2016,
abstract = {We present OctNet, a representation for deep learning with sparse 3D data. In contrast to existing models, our representation enables 3D convolutional networks which are both deep and high resolution. Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each leaf node stores a pooled feature representation. This allows to focus memory allocation and computation to the relevant dense regions and enables deeper networks without compromising resolution. We demonstrate the utility of our OctNet representation by analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling.},
annote = {{\_}eprint: 1611.05009},
archivePrefix = {arXiv},
arxivId = {1611.05009},
author = {Riegler, Gernot and Ulusoy, Ali Osman and Geiger, Andreas},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.701},
eprint = {1611.05009},
isbn = {9781538604571},
pages = {6620--6629},
title = {{OctNet: Learning deep 3D representations at high resolutions}},
url = {http://arxiv.org/abs/1611.05009},
volume = {2017-Janua},
year = {2017}
}
@article{Wilhelms1992,
abstract = {The large size of many volume data sets often prevents visualization algorithms from providing interactive rendering. The use of hierarchical data structures can ameliorate this problem by storing summary information to prevent useless exploration of regions of little or no current interest within the volume. This paper discusses research into the use of the octree hierarchical data structure when the regions of current interest can vary during the application, and are not known a priori. Octrees are well suited to the six-sided cell structure of many volumes. A new space-efficient design is introduced for octree representations of volumes whose resolutions are not conveniently a power of two; octrees following this design are called branch-on-need octrees 1992. Also, a caching method is described that essentially passes information between octree neighbors whose visitation times may be quite different, then discards it when its useful life is over. Using the application of octrees to isosurface generation as a focus, space and time comparisons for octree-based versus more traditional “marching” methods are presented. {\textcopyright} 1992, ACM. All rights reserved.},
annote = {Place: New York, NY, USA
Publisher: ACM},
author = {Wilhelms, Jane and {Van Gelder}, Allen},
doi = {10.1145/130881.130882},
issn = {15577368},
journal = {ACM Transactions on Graphics (TOG)},
keywords = {hierarchical spatial enumeration,isosurface extraction,octree,scientific visualization},
number = {3},
pages = {201--227},
title = {{Octrees for Faster Isosurface Generation}},
url = {http://doi.acm.org/10.1145/130881.130882},
volume = {11},
year = {1992}
}
@incollection{Richardt2020,
address = {Cham},
author = {Richardt, Christian},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_808-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Omnidirectional Stereo}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}808-1},
year = {2020}
}
@article{Cordonnier2019,
abstract = {Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available.},
archivePrefix = {arXiv},
arxivId = {1911.03584},
author = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
eprint = {1911.03584},
month = {nov},
title = {{On the Relationship between Self-Attention and Convolutional Layers}},
url = {http://arxiv.org/abs/1911.03584},
year = {2019}
}
@article{Marcus2007,
abstract = {The Open Access Series of Imaging Studies is a series of magnetic resonance imaging data sets that is publicly available for study and analysis. The initial data set consists of a cross-sectional collection of 416 subjects aged 18 to 96 years. One hundred of the included subjects older than 60 years have been clinically diagnosed with very mild to moderate Alzheimer's disease. The subjects are all right-handed and include both men and women. For each subject, three or four individual T1-weighted magnetic resonance imaging scans obtained in single imaging sessions are included. Multiple within-session acquisitions provide extremely high contrast-to-noise ratio, making the data amenable to a wide range of analytic approaches including automated computational analysis. Additionally, a reliability data set is included containing 20 subjects without dementia imaged on a subsequent visit within 90 days of their initial session. Automated calculation of whole-brain volume and estimated total intracranial volume are presented to demonstrate use of the data for measuring differences associated with normal aging and Alzheimer's disease. {\textcopyright} 2007 Massachusetts Institute of Technology.},
author = {Marcus, Daniel S. and Wang, Tracy H. and Parker, Jamie and Csernansky, John G. and Morris, John C. and Buckner, Randy L.},
doi = {10.1162/jocn.2007.19.9.1498},
issn = {0898929X},
journal = {Journal of Cognitive Neuroscience},
month = {sep},
number = {9},
pages = {1498--1507},
pmid = {17714011},
title = {{Open Access Series of Imaging Studies (OASIS): Cross-sectional MRI data in young, middle aged, nondemented, and demented older adults}},
url = {http://www.mitpressjournals.org/doi/10.1162/jocn.2007.19.9.1498},
volume = {19},
year = {2007}
}
@article{Marcus2010,
abstract = {The Open Access Series of Imaging Studies is a series of neuroimaging data sets that are publicly available for study and analysis. The present MRI data set consists of a longitudinal collection of 150 subjects aged 60 to 96 years all acquired on the same scanner using identical sequences. Each subject was scanned on two or more visits, separated by at least 1 year for a total of 373 imaging sessions. Subjects were characterized using the Clinical Dementia Rating (CDR) as either nondemented or with very mild tomild Alzheimer's disease. Seventy-two of the subjects were characterized as nondemented throughout the study. Sixty-four of the included subjects were characterized as demented at the time of their initial visits and remained so for subsequent scans, including 51 individuals with CDR 0.5 similar level of impairment to individuals elsewhere considered to have "mild cognitive impairment." Another 14 subjects were characterized as nondemented at the time of their initial visit (CDR 0) and were subsequently characterized as demented at a later visit (CDR {\textgreater} 0). The subjects were all right-handed and include bothmen (n=62) and women (n = 88). For each scanning session, three or four individual T1-weighted MRI scans were obtained. Multiple withinsession acquisitions provide extremely high contrast to noise, making the data amenable to a wide range of analytic approaches including automated computational analysis. Automated calculation of whole-brain volume is presented to demonstrate use of the data for measuring differences associated with normal aging and Alzheimer's disease. {\textcopyright} 2010 Massachusetts Institute of Technology.},
author = {Marcus, Daniel S. and Fotenos, Anthony F. and Csernansky, John G. and Morris, John C. and Buckner, Randy L.},
doi = {10.1162/jocn.2009.21407},
issn = {0898929X},
journal = {Journal of Cognitive Neuroscience},
month = {dec},
number = {12},
pages = {2677--2684},
title = {{Open access series of imaging studies: Longitudinal MRI data in nondemented and demented older adults}},
url = {http://www.mitpressjournals.org/doi/10.1162/jocn.2009.21407},
volume = {22},
year = {2010}
}
@article{Goode2013,
abstract = {Although widely touted as a replacement for glass slides and microscopes in pathology, digital slides present major challenges in data storage, transmission, processing and interoperability. Since no universal data format is in widespread use for these images today, each vendor defines its own proprietary data formats, analysis tools, viewers and software libraries. This creates issues not only for pathologists, but also for interoperability. In this paper, we present the design and implementation of OpenSlide, a vendor-neutral C library for reading and manipulating digital slides of diverse vendor formats. The library is extensible and easily interfaced to various programming languages. An application written to the OpenSlide interface can transparently handle multiple vendor formats. OpenSlide is in use today by many academic and industrial organizations world-wide, including many research sites in the United States that are funded by the National Institutes of Health.},
author = {Satyanarayanan, Mahadev and Goode, Adam and Gilbert, Benjamin and Harkes, Jan and Jukic, Drazen},
doi = {10.4103/2153-3539.119005},
issn = {2153-3539},
journal = {Journal of Pathology Informatics},
number = {1},
pages = {27},
title = {{OpenSlide: A vendor-neutral software foundation for digital pathology}},
volume = {4},
year = {2013}
}
@incollection{Brox2020,
address = {Cham},
author = {Brox, Thomas},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_600-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Optical Flow: Traditional Approaches}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}600-1},
year = {2020}
}
@article{Song2013,
abstract = {Positron emission tomography (PET)-computed tomography (CT) images have been widely used in clinical practice for radiotherapy treatment planning of the radiotherapy. Many existing segmentation approaches only work for a single imaging modality, which suffer from the low spatial resolution in PET or low contrast in CT. In this work, we propose a novel method for the co-segmentation of the tumor in both PET and CT images, which makes use of advantages from each modality: the functionality information from PET and the anatomical structure information from CT. The approach formulates the segmentation problem as a minimization problem of a Markov random field model, which encodes the information from both modalities. The optimization is solved using a graph-cut based method. Two sub-graphs are constructed for the segmentation of the PET and the CT images, respectively. To achieve consistent results in two modalities, an adaptive context cost is enforced by adding context arcs between the two sub-graphs. An optimal solution can be obtained by solving a single maximum flow problem, which leads to simultaneous segmentation of the tumor volumes in both modalities. The proposed algorithm was validated in robust delineation of lung tumors on 23 PET-CT datasets and two head-and-neck cancer subjects. Both qualitative and quantitative results show significant improvement compared to the graph cut methods solely using PET or CT. {\textcopyright} 2012 IEEE.},
author = {Song, Qi and Bai, Junjie and Han, Dongfeng and Bhatia, Sudershan and Sun, Wenqing and Rockey, William and Bayouth, John E and Buatti, John M and Wu, Xiaodong},
doi = {10.1109/TMI.2013.2263388},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Context information,Positron emission tomography-computed tomography (,global optimization,graph cut,image segmentation,lung tumor},
month = {sep},
number = {9},
pages = {1685--1697},
title = {{Optimal Co-segmentation of tumor in PET-CT images with context information}},
volume = {32},
year = {2013}
}
@incollection{Kelly2014,
address = {Cham},
author = {Kelly, Alonzo and Kelly, Alonzo},
booktitle = {Mobile Robotics},
doi = {10.1017/cbo9781139381284.006},
pages = {270--369},
publisher = {Springer International Publishing},
title = {{Optimal Estimation}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}714-1},
year = {2014}
}
@book{Dorronsoro2020,
address = {Cham},
author = {Dorronsoro, Bernab{\'{e}} and Ruiz, Patricia and Carlos, Juan and Torre, De and Urda, Daniel and Eds, El-ghazali Talbi},
doi = {10.1007/978-3-030-41913-4},
editor = {Dorronsoro, Bernab{\'{e}} and Ruiz, Patricia and de la Torre, Juan Carlos and Urda, Daniel and Talbi, El-Ghazali},
isbn = {9783030419127},
publisher = {Springer International Publishing},
series = {Communications in Computer and Information Science},
title = {{Optimization and Learning}},
url = {http://link.springer.com/10.1007/978-3-030-41913-4},
volume = {1173},
year = {2020}
}
@book{Bors2020,
address = {Berkeley, CA},
author = {Bors, Luc and Samajdwer, Ardhendu and van Oosterhout, Mascha},
booktitle = {Oracle Digital Assistant},
doi = {10.1007/978-1-4842-5422-6},
isbn = {978-1-4842-5421-9},
publisher = {Apress},
title = {{Oracle Digital Assistant}},
url = {http://link.springer.com/10.1007/978-1-4842-5422-6},
year = {2020}
}
@inproceedings{Sedaghat2016,
abstract = {Recent work has shown good recognition results in 3D object recognition using 3D convolutional networks. In this paper, we show that the object orientation plays an important role in 3D recognition. More specifically, we argue that objects induce different features in the network under rotation. Thus, we approach the category-level classification task as a multi-task problem, in which the network is trained to predict the pose of the object in addition to the class label as a parallel task. We show that this yields significant improvements in the classification results. We test our suggested architecture on several datasets representing various 3D data sources: LiDAR data, CAD models, and RGB-D images. We report state-of-the-art results on classification as well as significant improvements in precision and speed over the baseline on 3D detection.},
annote = {{\_}eprint: 1604.03351},
archivePrefix = {arXiv},
arxivId = {1604.03351},
author = {Sedaghat, Nima and Zolfaghari, Mohammadreza and Amiri, Ehsan and Brox, Thomas},
booktitle = {British Machine Vision Conference 2017, BMVC 2017},
doi = {10.5244/c.31.97},
eprint = {1604.03351},
isbn = {190172560X},
title = {{Orientation-boosted Voxel nets for 3D object recognition}},
url = {http://arxiv.org/abs/1604.03351},
volume = {abs/1604.0},
year = {2017}
}
@data{GrandChallengePALM,
author = {Zhang, Huazhu Fu; Fei Li; Jos{\'{e}} Ignacio Orlando; Hrvoje Bogunovi{\'{c}}; Xu Sun; Jingan Liao; Yanwu Xu; Shaochong Zhang; Xiulan},
booktitle = {IEEE Dataport},
doi = {10.21227/55pk-8z03},
publisher = {IEEE Dataport},
title = {{PALM: PAthoLogic Myopia Challenge}},
url = {http://dx.doi.org/10.21227/55pk-8z03},
volume = {[Online]},
year = {2019}
}
@incollection{Cai2018,
abstract = {Automatic pancreas segmentation in radiology images, e.g., computed tomography (CT), and magnetic resonance imaging (MRI), is frequently required by computer-aided screening, diagnosis, and quantitative assessment. Yet, pancreas is a challenging abdominal organ to segment due to the high inter-patient anatomical variability in both shape and volume metrics. Recently, convolutional neural networks (CNN) have demonstrated promising performance on accurate segmentation of pancreas. However, the CNN-based method often suffers from segmentation discontinuity for reasons such as noisy image quality and blurry pancreatic boundary. In this chapter, we first discuss the CNN configurations and training objectives that lead to the state-of-the-art performance on pancreas segmentation. We then present a recurrent neural network (RNN) to address the problem of segmentation spatial inconsistency across adjacent image slices. The RNN takes outputs of the CNN and refines the segmentation by improving the shape smoothness.},
annote = {{\_}eprint: 1803.11303},
archivePrefix = {arXiv},
arxivId = {1803.11303},
author = {Cai, Jinzheng and Lu, Le and Xing, Fuyong and Yang, Lin},
booktitle = {Advances in Computer Vision and Pattern Recognition},
doi = {10.1007/978-3-030-13969-8_1},
eprint = {1803.11303},
issn = {21916594},
keywords = {Computer Science - Computer Vision and Pattern Rec},
pages = {3--21},
title = {{Pancreas Segmentation in CT and MRI via Task-Specific Network Design and Recurrent Neural Contextual Learning}},
year = {2019}
}
@inproceedings{Cai2017a,
abstract = {Deep neural networks have demonstrated very promising performance on accurate segmentation of challenging organs (e.g., pancreas) in abdominal CT and MRI scans. The current deep learning approaches conduct pancreas segmentation by processing sequences of 2D image slices independently through deep, dense per-pixel masking for each image, without explicitly enforcing spatial consistency constraint on segmentation of successive slices. We propose a new convolutional/recurrent neural network architecture to address the contextual learning and segmentation consistency problem. A deep convolutional sub-network is first designed and pre-trained from scratch. The output layer of this network module is then connected to recurrent layers and can be fine-tuned for contextual learning, in an end-to-end manner. Our recurrent sub-network is a type of Long short-term memory (LSTM) network that performs segmentation on an image by integrating its neighboring slice segmentation predictions, in the form of a dependent sequence processing. Additionally, a novel segmentation-direct loss function (named Jaccard Loss) is proposed and deep networks are trained to optimize Jaccard Index (JI) directly. Extensive experiments are conducted to validate our proposed deep models, on quantitative pancreas segmentation using both CT and MRI scans. Our method outperforms the state-of-the-art work on CT [11] and MRI pancreas segmentation [1], respectively.},
address = {Cham},
author = {Cai, Jinzheng and Lu, Le and Xie, Yuanpu and Xing, Fuyong and Yang, Lin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-66179-7_77},
editor = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D Louis and Duchesne, Simon},
isbn = {9783319661780},
issn = {16113349},
pages = {674--682},
publisher = {Springer International Publishing},
title = {{Pancreas segmentation in MRI using graph-based decision fusion on convolutional neural networks}},
volume = {10435 LNCS},
year = {2017}
}
@incollection{Cochran2011,
abstract = {A partially observable Markov decision process (POMDP) is a generalization of a Markov decision process where the states of the model are not completely observable by the decision maker. Noisy observations provide a belief regarding the underlying state, while the decision maker has some control over the progression of the model through the selection of actions. In this article, we introduce POMDPs and discuss the relationship between Markov models and POMDPs. A general POMDP formulation and a wide range of POMDP applications from the literature are also presented.},
address = {Hoboken, NJ, USA},
author = {Cochran, James J. and Cox, Louis A. and Keskinocak, Pinar and Kharoufeh, Jeffrey P. and Smith, J. Cole and Yaylali, Emine and Ivy, Julie S.},
booktitle = {Wiley Encyclopedia of Operations Research and Management Science},
doi = {10.1002/9780470400531.eorms0646},
month = {feb},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Partially Observable MDPs (POMDPS): Introduction and Examples}},
url = {http://doi.wiley.com/10.1002/9780470400531.eorms0646},
year = {2011}
}
@article{Zhang2019b,
abstract = {Diagnostic pathology is the foundation and gold standard for identifying carcinomas. However, high inter-observer variability substantially affects productivity in routine pathology and is especially ubiquitous in diagnostician-deficient medical centres. Despite rapid growth in computer-aided diagnosis (CAD), the application of whole-slide pathology diagnosis remains impractical. Here, we present a novel pathology whole-slide diagnosis method, powered by artificial intelligence, to address the lack of interpretable diagnosis. The proposed method masters the ability to automate the human-like diagnostic reasoning process and translate gigapixels directly to a series of interpretable predictions, providing second opinions and thereby encouraging consensus in clinics. Moreover, using 913 collected examples of whole-slide data representing patients with bladder cancer, we show that our method matches the performance of 17 pathologists in the diagnosis of urothelial carcinoma. We believe that our method provides an innovative and reliable means for making diagnostic suggestions and can be deployed at low cost as next-generation, artificial intelligence-enhanced CAD technology for use in diagnostic pathology.},
author = {Zhang, Zizhao and Chen, Pingjun and McGough, Mason and Xing, Fuyong and Wang, Chunbao and Bui, Marilyn and Xie, Yuanpu and Sapkota, Manish and Cui, Lei and Dhillon, Jasreman and Ahmad, Nazeel and Khalil, Farah K. and Dickinson, Shohreh I. and Shi, Xiaoshuang and Liu, Fujun and Su, Hai and Cai, Jinzheng and Yang, Lin},
doi = {10.1038/s42256-019-0052-1},
journal = {Nature Machine Intelligence},
number = {5},
pages = {236--245},
title = {{Pathologist-level interpretable whole-slide cancer diagnosis with deep learning}},
volume = {1},
year = {2019}
}
@book{Evans2013,
address = {Rotterdam},
author = {Evans, Kate},
doi = {10.1007/978-94-6209-242-6},
isbn = {978-94-6209-242-6},
publisher = {SensePublishers},
title = {{Pathways Through Writing Blocks in the Academic Environment}},
url = {http://link.springer.com/10.1007/978-94-6209-242-6},
year = {2013}
}
@book{Loveday2013,
address = {Cham},
author = {Loveday, Thomas and Wiggins, Mark},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-3-642-36530-0},
editor = {{De Marsico}, Maria and {Sanniti di Baja}, Gabriella and Fred, Ana},
isbn = {978-3-642-36529-4},
issn = {09252312},
keywords = {combining classifier,electricity theft,optimum path forest,support vector machine,unbalance class problem,ute},
number = {January},
pages = {109--120},
pmid = {3403},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Pattern Recognition - Applications and Methods}},
url = {http://link.springer.com/10.1007/978-3-642-36530-0},
volume = {204},
year = {2013}
}
@misc{Tessa2018,
author = {Tessa, Carlo},
doi = {10.18112/OPENNEURO.DS001354.V1.0.0},
publisher = {Openneuro},
title = {{PD De Novo: Resting State fMRI and Physiological Signals}},
url = {https://openneuro.org/datasets/ds001354/versions/1.0.0},
year = {2018}
}
@online{GrandChallengeODIR-2019,
title = {{Peking University International Competition on Ocular Disease Intelligent Recognition}},
url = {https://odir2019.grand-challenge.org/}
}
@incollection{Panda2020,
address = {Cham},
author = {Panda, Rameswar and Roy-Chowdhury, Amit K.},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_825-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Person Re-identification: Current Approaches and Future Challenges}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}825-1},
year = {2020}
}
@article{Goldberger2000,
abstract = {The newly inaugurated Research Resource for Complex Physiologic Signals, which was created under the auspices of the National Center for Research Resources of the National Institutes of Health, is intended to stimulate current research and new investigations in the study of cardiovascular and other complex biomedical signals. The resource has 3 interdependent components. PhysioBank is a large and growing archive of well-characterized digital recordings of physiological signals and related data for use by the biomedical research community. It currently includes databases of multiparameter cardiopulmonary, neural, and other biomedical signals from healthy subjects and from patients with a variety of conditions with major public health implications, including life-threatening arrhythmias, congestive heart failure, sleep apnea, neurological disorders, and aging. PhysioToolkit is a library of open-source software for physiological signal processing and analysis, the detection of physiologically significant events using both classic techniques and novel methods based on statistical physics and nonlinear dynamics, the interactive display and characterization of signals, the creation of new databases, the simulation of physiological and other signals, the quantitative evaluation and comparison of analysis methods, and the analysis of nonstationary processes. PhysioNet is an on-line forum for the dissemination and exchange of recorded biomedical signals and open-source software for analyzing them. It provides facilities for the cooperative analysis of data and the evaluation of proposed new algorithms. In addition to providing free electronic access to PhysioBank data and PhysioToolkit software via the World Wide Web (http://www.physionet. org), PhysioNet offers services and training via on-line tutorials to assist users with varying levels of expertise.},
author = {Goldberger, A. L. and Amaral, L. A. and Glass, L. and Hausdorff, J. M. and Ivanov, P. C. and Mark, R. G. and Mietus, J. E. and Moody, G. B. and Peng, C. K. and Stanley, H. E.},
doi = {10.1161/01.cir.101.23.e215},
issn = {15244539},
journal = {Circulation},
month = {jun},
number = {23},
pmid = {10851218},
title = {{PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals.}},
url = {https://www.ahajournals.org/doi/10.1161/01.CIR.101.23.e215},
volume = {101},
year = {2000}
}
@article{Rajan2019,
abstract = {Pulmonary embolisms (PE) are known to be one of the leading causes for cardiac-related mortality. Due to inherent variabilities in how PE manifests and the cumbersome nature of manual diagnosis, there is growing interest in leveraging AI tools for detecting PE. In this paper, we build a two-stage detection pipeline that is accurate, computationally efficient, robust to variations in PE types and kernels used for CT reconstruction, and most importantly, does not require dense annotations. Given the challenges in acquiring expert annotations in large-scale datasets, our approach produces state-of-the-art results with very sparse emboli contours (at 10mm slice spacing), while using models with significantly lower number of parameters. We achieve AUC scores of 0.94 on the validation set and 0.85 on the test set of highly severe PEs. Using a large, real-world dataset characterized by complex PE types and patients from multiple hospitals, we present an elaborate empirical study and provide guidelines for designing highly generalizable pipelines.},
annote = {{\_}eprint: 1910.02175},
archivePrefix = {arXiv},
arxivId = {1910.02175},
author = {Rajan, Deepta and Beymer, David and Abedin, Shafiqul and Dehghan, Ehsan},
eprint = {1910.02175},
title = {{Pi-PE: A Pipeline for Pulmonary Embolism Detection using Sparsely Annotated 3D CT Images}},
url = {http://arxiv.org/abs/1910.02175},
year = {2019}
}
@inproceedings{Huang2016a,
abstract = {In this paper, we tackle the labeling problem for 3D point clouds. We introduce a 3D point cloud labeling scheme based on 3D Convolutional Neural Network. Our approach minimizes the prior knowledge of the labeling problem and does not require a segmentation step or hand-crafted features as most previous approaches did. Particularly, we present solutions for large data handling during the training and testing process. Experiments performed on the urban point cloud dataset containing 7 categories of objects show the robustness of our approach.},
author = {Jing, Huang and You, Suya},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2016.7900038},
isbn = {9781509048472},
issn = {10514651},
keywords = {3D convolutional neural network,3D point cloud labeling scheme,Labeling,Neural networks,Testing,Three-dimensional displays,Training,Training data,Two dimensional displays,computer vision,data handling,neural nets,object recognition,testing process,training process,urban point cloud dataset},
pages = {2670--2675},
title = {{Point cloud labeling using 3D Convolutional Neural Network}},
year = {2016}
}
@inproceedings{Li2018,
abstract = {We present a simple and general framework for feature learning from point clouds. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point clouds are irregular and unordered, thus directly convolving kernels against features associated with the points will result in desertion of shape information and variance to point ordering. To address these problems, we propose to learn an X-transformation from the input points to simultaneously promote two causes: the first is the weighting of the input features associated with the points, and the second is the permutation of the points into a latent and potentially canonical order. Element-wise product and sum operations of the typical convolution operator are subsequently applied on the X-transformed features. The proposed method is a generalization of typical CNNs to feature learning from point clouds, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.},
annote = {{\_}eprint: 1801.07791},
archivePrefix = {arXiv},
arxivId = {1801.07791},
author = {Li, Yangyan and Bu, Rui and Sun, Mingchao and Wu, Wei and Di, Xinhan and Chen, Baoquan},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1801.07791},
issn = {10495258},
pages = {820--830},
title = {{PointCNN: Convolution on X-transformed points}},
url = {http://arxiv.org/abs/1801.07791},
volume = {2018-Decem},
year = {2018}
}
@inproceedings{Qi2016,
abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
annote = {{\_}eprint: 1612.00593},
archivePrefix = {arXiv},
arxivId = {1612.00593},
author = {Qi, Charles Ruizhongtai and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.16},
eprint = {1612.00593},
isbn = {9781538604571},
pages = {77--85},
title = {{PointNet: Deep learning on point sets for 3D classification and segmentation}},
url = {http://arxiv.org/abs/1612.00593},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Qi2017,
abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
annote = {{\_}eprint: 1706.02413},
archivePrefix = {arXiv},
arxivId = {1706.02413},
author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1706.02413},
issn = {10495258},
pages = {5100--5109},
title = {{PointNet++: Deep hierarchical feature learning on point sets in a metric space}},
url = {http://arxiv.org/abs/1706.02413},
volume = {2017-Decem},
year = {2017}
}
@book{Shaik2020,
address = {Berkeley, CA},
author = {Shaik, Baji},
booktitle = {PostgreSQL Configuration},
doi = {10.1007/978-1-4842-5663-3},
isbn = {978-1-4842-5662-6},
publisher = {Apress},
title = {{PostgreSQL Configuration}},
url = {http://link.springer.com/10.1007/978-1-4842-5663-3},
year = {2020}
}
@inproceedings{Snoek2012,
abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1206.2944},
isbn = {9781627480031},
issn = {10495258},
pages = {2951--2959},
title = {{Practical Bayesian optimization of machine learning algorithms}},
volume = {4},
year = {2012}
}
@inproceedings{Zhong2017a,
abstract = {Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained sequentially to choose component layers. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it performs competitive results in comparison to the hand-crafted state-of-the-art networks on image classification, additionally, the best network generated by BlockQNN achieves 3.54{\%} top-1 error rate on CIFAR-10 which beats all existing auto-generate networks. (2) in the meanwhile, it offers tremendous reduction of the search space in designing networks which only spends 3 days with 32 GPUs, and (3) moreover, it has strong generalizability that the network built on CIFAR also performs well on a larger-scale ImageNet dataset.},
archivePrefix = {arXiv},
arxivId = {1708.05552},
author = {Zhong, Zhao and Yan, Junjie and Wu, Wei and Shao, Jing and Liu, Cheng Lin},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00257},
eprint = {1708.05552},
isbn = {9781538664209},
issn = {10636919},
pages = {2423--2432},
title = {{Practical Block-Wise Neural Network Architecture Generation}},
year = {2018}
}
@book{BergHansen2020,
address = {Berkeley, CA},
author = {{Berg Hansen}, Kim},
doi = {10.1007/978-1-4842-5617-6},
isbn = {978-1-4842-5616-9},
publisher = {Apress},
title = {{Practical Oracle SQL}},
url = {http://link.springer.com/10.1007/978-1-4842-5617-6},
year = {2020}
}
@article{Veta2019,
abstract = {Tumor proliferation is an important biomarker indicative of the prognosis of breast cancer patients. Assessment of tumor proliferation in a clinical setting is a highly subjective and labor-intensive task. Previous efforts to automate tumor proliferation assessment by image analysis only focused on mitosis detection in predefined tumor regions. However, in a real-world scenario, automatic mitosis detection should be performed in whole-slide images (WSIs) and an automatic method should be able to produce a tumor proliferation score given a WSI as input. To address this, we organized the TUmor Proliferation Assessment Challenge 2016 (TUPAC16) on prediction of tumor proliferation scores from WSIs. The challenge dataset consisted of 500 training and 321 testing breast cancer histopathology WSIs. In order to ensure fair and independent evaluation, only the ground truth for the training dataset was provided to the challenge participants. The first task of the challenge was to predict mitotic scores, i.e., to reproduce the manual method of assessing tumor proliferation by a pathologist. The second task was to predict the gene expression based PAM50 proliferation scores from the WSI. The best performing automatic method for the first task achieved a quadratic-weighted Cohen's kappa score of $\kappa$ = 0.567, 95{\%} CI [0.464, 0.671] between the predicted scores and the ground truth. For the second task, the predictions of the top method had a Spearman's correlation coefficient of r = 0.617, 95{\%} CI [0.581 0.651] with the ground truth. This was the first comparison study that investigated tumor proliferation assessment from WSIs. The achieved results are promising given the difficulty of the tasks and weakly-labeled nature of the ground truth. However, further research is needed to improve the practical utility of image analysis methods for this task.},
archivePrefix = {arXiv},
arxivId = {1807.08284},
author = {Veta, Mitko and Heng, Yujing J. and Stathonikos, Nikolas and Bejnordi, Babak Ehteshami and Beca, Francisco and Wollmann, Thomas and Rohr, Karl and Shah, Manan A. and Wang, Dayong and Rousson, Mikael and Hedlund, Martin and Tellez, David and Ciompi, Francesco and Zerhouni, Erwan and Lanyi, David and Viana, Matheus and Kovalev, Vassili and Liauchuk, Vitali and Phoulady, Hady Ahmady and Qaiser, Talha and Graham, Simon and Rajpoot, Nasir and Sj{\"{o}}blom, Erik and Molin, Jesper and Paeng, Kyunghyun and Hwang, Sangheum and Park, Sunggyun and Jia, Zhipeng and Chang, Eric I.Chao and Xu, Yan and Beck, Andrew H. and van Diest, Paul J. and Pluim, Josien P.W.},
doi = {10.1016/j.media.2019.02.012},
eprint = {1807.08284},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Breast cancer,Cancer prognostication,Deep learning,Tumor proliferation},
month = {jul},
pages = {111--121},
title = {{Predicting breast tumor proliferation from whole-slide images: The TUPAC16 challenge}},
url = {http://arxiv.org/abs/1807.08284 http://dx.doi.org/10.1016/j.media.2019.02.012},
volume = {54},
year = {2019}
}
@article{Akkus2017,
abstract = {Several studies have linked codeletion of chromosome arms 1p/19q in low-grade gliomas (LGG) with positive response to treatment and longer progression-free survival. Hence, predicting 1p/19q status is crucial for effective treatment planning of LGG. In this study, we predict the 1p/19q status from MR images using convolutional neural networks (CNN), which could be a non-invasive alternative to surgical biopsy and histopathological analysis. Our method consists of three main steps: image registration, tumor segmentation, and classification of 1p/19q status using CNN. We included a total of 159 LGG with 3 image slices each who had biopsy-proven 1p/19q status (57 non-deleted and 102 codeleted) and preoperative postcontrast-T1 (T1C) and T2 images. We divided our data into training, validation, and test sets. The training data was balanced for equal class probability and was then augmented with iterations of random translational shift, rotation, and horizontal and vertical flips to increase the size of the training set. We shuffled and augmented the training data to counter overfitting in each epoch. Finally, we evaluated several configurations of a multi-scale CNN architecture until training and validation accuracies became consistent. The results of the best performing configuration on the unseen test set were 93.3{\%} (sensitivity), 82.22{\%} (specificity), and 87.7{\%} (accuracy). Multi-scale CNN with their self-learning capability provides promising results for predicting 1p/19q status non-invasively based on T1C and T2 images. Predicting 1p/19q status non-invasively from MR images would allow selecting effective treatment strategies for LGG patients without the need for surgical biopsy.},
author = {Akkus, Zeynettin and Ali, Issa and Sedl{\'{a}}ř, Jiř{\'{i}} and Agrawal, Jay P. and Parney, Ian F. and Giannini, Caterina and Erickson, Bradley J.},
doi = {10.1007/s10278-017-9984-3},
issn = {1618727X},
journal = {Journal of Digital Imaging},
keywords = {1p/19q codeletion,Convolutional neural networks,Low grade gliomas,Therapy response},
month = {aug},
number = {4},
pages = {469--476},
pmid = {28600641},
title = {{Predicting Deletion of Chromosomal Arms 1p/19q in Low-Grade Gliomas from MR Images Using Machine Intelligence}},
url = {http://link.springer.com/10.1007/s10278-017-9984-3},
volume = {30},
year = {2017}
}
@book{Kainz2017,
address = {Cham},
author = {Kainz, Bernhard and Bhatia, Kanwal and Vaillant, Ghislain and Zuluaga, Maria A.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-52280-7},
editor = {Zuluaga, Maria A. and Bhatia, Kanwal and Kainz, Bernhard and Moghari, Mehdi H. and Pace, Danielle F.},
isbn = {9783319522791},
issn = {16113349},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Preface}},
url = {http://link.springer.com/10.1007/978-3-319-52280-7},
volume = {10129 LNCS},
year = {2017}
}
@article{Ogiela2008,
abstract = {This chapter briefly discusses the main stages of image preprocessing. The introduction to this book mentioned that the preprocessing of medical image is subject to certain restrictions and is generally more complex than the processing of other image types [26, 52]. This is why, of the many different techniques and methods for image filtering, we have decided to discuss here only selected ones, most frequently applied to medical images and which have been proven to be suitable for that purpose in numerous practical cases. Their operation will be illustrated with examples of simple procedures aimed at improving the quality of imaging and allowing significant information to be generated for its use at the stages of image interpretation. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
address = {Berlin, Heidelberg},
author = {Ogiela, Marek R and Tadeusiewicz, Ryszard},
doi = {10.1007/978-3-540-75402-2_4},
isbn = {9783540753995},
issn = {1860949X},
journal = {Studies in Computational Intelligence},
pages = {65--97},
publisher = {Springer Berlin Heidelberg},
title = {{Preprocessing medical images and their overall enhancement}},
url = {https://doi.org/10.1007/978-3-540-75402-2{\_}4},
volume = {84},
year = {2008}
}
@incollection{Tefas2016,
address = {Cham},
author = {Tefas, Anastasios and Pitas, Ioannis},
booktitle = {Intelligent Systems},
doi = {10.1201/b17700-1},
isbn = {9781439802847},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Principal component analysis}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}649-1},
year = {2016}
}
@article{Tofighi2019,
abstract = {Cell nuclei detection is a challenging research topic because of limitations in cellular image quality and diversity of nuclear morphology, i.e., varying nuclei shapes, sizes, and overlaps between multiple cell nuclei. This has been a topic of enduring interest with promising recent success shown by deep learning methods. These methods train convolutional neural networks (CNNs) with a training set of input images and known, labeled nuclei locations. Many such methods are supplemented by spatial or morphological processing. Using a set of canonical cell nuclei shapes, prepared with the help of a domain expert, we develop a new approach that we call shape priors (SPs) with CNNs (SPs-CNN). We further extend the network to introduce an SP layer and then allowing it to become trainable (i.e., optimizable). We call this network as tunable SP-CNN (TSP-CNN). In summary, we present new network structures that can incorporate "expected behavior" of nucleus shapes via two components: learnable layers that perform the nucleus detection and a fixed processing part that guides the learning with prior information. Analytically, we formulate two new regularization terms that are targeted at: 1) learning the shapes and 2) reducing false positives while simultaneously encouraging detection inside the cell nucleus boundary. Experimental results on two challenging datasets reveal that the proposed SP-CNN and TSP-CNN can outperform the state-of-the-art alternatives.},
archivePrefix = {arXiv},
arxivId = {1901.07061},
author = {Tofighi, Mohammad and Guo, Tiantong and Vanamala, Jairam K.P. and Monga, Vishal},
doi = {10.1109/TMI.2019.2895318},
eprint = {1901.07061},
issn = {1558254X},
journal = {IEEE transactions on medical imaging},
keywords = {Biomedical imaging,Computer architecture,Deep learning,Image edge detection,Image segmentation,Microprocessors,Nucleus detection,Shape,TSP-CNN,biology computing,canonical cell nuclei shapes,cell nuclei detection,cell nucleus boundary,cell nucleus detection,cellular biophysics,cellular image quality,convolutional neural nets,convolutional neural networks,deep learning,deep learning methods,domain expert,fixed processing part,input images,labeled nuclei locations,learnable layers,learnable shapes,learning (artificial intelligence),medical image processing,morphological processing,multiple cell nuclei,network structures,nuclear morphology,nucleus shapes,regularization terms,shape priors,spatial processing,training set,tunable SP-CNN},
month = {sep},
number = {9},
pages = {2047--2058},
title = {{Prior Information Guided Regularized Deep Learning for Cell Nucleus Detection}},
volume = {38},
year = {2019}
}
@book{Reinkemeyer2020,
address = {Cham},
booktitle = {Process Mining in Action},
doi = {10.1007/978-3-030-40172-6},
editor = {Reinkemeyer, Lars},
isbn = {978-3-030-40171-9},
publisher = {Springer International Publishing},
title = {{Process Mining in Action}},
url = {http://link.springer.com/10.1007/978-3-030-40172-6},
year = {2020}
}
@misc{k8s,
author = {{The Linux Foundation}},
booktitle = {Kubernetes.Io},
title = {{Production-Grade Container Orchestration - Kubernetes}},
url = {https://kubernetes.io/},
year = {2020}
}
@misc{neuromorphometrics2012,
title = {{Products | Neuromorphometrics, Inc.}},
url = {http://www.neuromorphometrics.com/?page{\_}id=23}
}
@book{Skalka2005,
address = {Cham},
author = {Skalka, Christian},
booktitle = {IEEE Security and Privacy},
doi = {10.1109/MSP.2005.77},
editor = {M{\"{u}}ller, Peter},
isbn = {978-3-030-44913-1},
issn = {15407993},
number = {3},
pages = {80--83},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Programming languages and systems security}},
url = {http://link.springer.com/10.1007/978-3-030-44914-8},
volume = {3},
year = {2005}
}
@inproceedings{Liu2017a,
abstract = {We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.},
archivePrefix = {arXiv},
arxivId = {1808.00391},
author = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01246-5_2},
eprint = {1808.00391},
isbn = {9783030012458},
issn = {16113349},
pages = {19--35},
title = {{Progressive Neural Architecture Search}},
volume = {11205 LNCS},
year = {2018}
}
@book{Baines2014,
address = {Rotterdam},
author = {Baines, Lawrence},
doi = {10.1007/978-94-6209-671-4},
isbn = {978-94-6209-671-4},
publisher = {SensePublishers},
title = {{Project-Based Writing in Science}},
url = {http://link.springer.com/10.1007/978-94-6209-671-4},
year = {2014}
}
@article{Oliphant2007,
annote = {{\_}eprint: https://aip.scitation.org/doi/pdf/10.1109/MCSE.2007.58},
author = {Oliphant, Travis E},
doi = {10.1109/MCSE.2007.58},
journal = {Computing in Science {\&} Engineering},
number = {3},
pages = {10--20},
title = {{Python for Scientific Computing}},
url = {https://aip.scitation.org/doi/abs/10.1109/MCSE.2007.58},
volume = {9},
year = {2007}
}
@book{Milliken2020,
abstract = {A Ten-Week Bootcamp Approach to Python Programming},
address = {Berkeley, CA},
author = {Milliken, Connor P.},
booktitle = {Python Projects for Beginners},
doi = {10.1007/978-1-4842-5355-7},
isbn = {978-1-4842-5354-0},
publisher = {Apress},
title = {{Python Projects for Beginners}},
url = {http://link.springer.com/10.1007/978-1-4842-5355-7},
year = {2020}
}
@article{Steiner2019,
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
archivePrefix = {arXiv},
arxivId = {1912.01703},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"{o}}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
eprint = {1912.01703},
title = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
url = {http://arxiv.org/abs/1912.01703},
year = {2019}
}
@article{Ruifrok2001,
abstract = {OBJECTIVE: To develop a flexible method of separation and quantification of immunohistochemical staining by means of color image analysis. STUDY DESIGN: An algorithm was developed to deconvolve the color information acquired with red-green-blue (RGB) cameras and to calculate the contribution of each of the applied stains based on stain-specific RGB absorption. The algorithm was tested using different combinations of diaminobenzidine, hematoxylin and eosin at different staining levels. RESULTS: Quantification of the different stains was not significantly influenced by the combination of multiple stains in a single sample. The color deconvolution algorithm resulted in comparable quantification independent of the stain combinations as long as the histochemical procedures did not influence the amount of stain in the sample due to bleaching because of stain solubility and saturation of staining was prevented. CONCLUSION: This image analysis algorithm provides a robust and flexible method for objective immunohistochemical analysis of samples stained with up to three different stains using a laboratory microscope, standard RGB camera setup and the public domain program NIH Image.},
author = {Ruifrok, A. C. and Johnston, Dennis A},
issn = {08846812},
journal = {Analytical and Quantitative Cytology and Histology},
keywords = {Color deconvolution,Color separation,Computer-assisted,Image processing,Immunohistochemistry},
number = {4},
pages = {291--299},
pmid = {11531144},
title = {{Quantification of histochemical staining by color deconvolution}},
volume = {23},
year = {2001}
}
@article{Daducci2014a,
abstract = {Validation is arguably the bottleneck in the diffusion magnetic resonance imaging (MRI) community. This paper evaluates and compares 20 algorithms for recovering the local intra-voxel fiber structure from diffusion MRI data and is based on the results of the 'HARDI reconstruction challenge' organized in the context of the 'ISBI 2012' conference. Evaluated methods encompass a mixture of classical techniques well known in the literature such as diffusion tensor, Q-Ball and diffusion spectrum imaging, algorithms inspired by the recent theory of compressed sensing and also brand new approaches proposed for the first time at this contest. To quantitatively compare the methods under controlled conditions, two datasets with known ground-truth were synthetically generated and two main criteria were used to evaluate the quality of the reconstructions in every voxel: correct assessment of the number of fiber populations and angular accuracy in their orientation. This comparative study investigates the behavior of every algorithm with varying experimental conditions and highlights strengths and weaknesses of each approach. This information can be useful not only for enhancing current algorithms and develop the next generation of reconstruction methods, but also to assist physicians in the choice of the most adequate technique for their studies. {\textcopyright} 1982-2012 IEEE.},
author = {Daducci, Alessandro and Canales-Rodriguez, Erick Jorge and Descoteaux, Maxime and Garyfallidis, Eleftherios and Gur, Yaniv and Lin, Ying Chia and Mani, Merry and Merlet, Sylvain and Paquette, Michael and Ramirez-Manzanares, Alonso and Reisert, Marco and Rodrigues, Paulo Reis and Sepehrband, Farshid and Caruyer, Emmanuel and Choupan, Jeiran and Deriche, Rachid and Jacob, Mathews and Menegaz, Gloria and Prckovska, Vesna and Rivera, Mariano and Wiaux, Yves and Thiran, Jean Philippe},
doi = {10.1109/TMI.2013.2285500},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Diffusion magnetic resonance imaging (dMRI),local reconstruction,quantitative comparison,synthetic data,validation},
month = {feb},
number = {2},
pages = {384--399},
pmid = {24132007},
title = {{Quantitative comparison of reconstruction methods for intra-voxel fiber recovery from diffusion MRI}},
url = {http://ieeexplore.ieee.org/document/6630106/},
volume = {33},
year = {2014}
}
@misc{CPTAC2018,
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/k9/tcia.2018.3rje41q1},
publisher = {The Cancer Imaging Archive},
title = {{Radiology Data from the Clinical Proteomic Tumor Analysis Consortium Glioblastoma Multiforme [CPTAC-GBM] collection [Data set]. The Cancer Imaging Archive.}},
url = {https://wiki.cancerimagingarchive.net/x/gAHUAQ},
year = {2018}
}
@article{Vallieres2017,
abstract = {Quantitative extraction of high-dimensional mineable data from medical images is a process known as radiomics. Radiomics is foreseen as an essential prognostic tool for cancer risk assessment and the quantification of intratumoural heterogeneity. In this work, 1615 radiomic features (quantifying tumour image intensity, shape, texture) extracted from pre-treatment FDG-PET and CT images of 300 patients from four different cohorts were analyzed for the risk assessment of locoregional recurrences (LR) and distant metastases (DM) in head-and-neck cancer. Prediction models combining radiomic and clinical variables were constructed via random forests and imbalance-adjustment strategies using two of the four cohorts. Independent validation of the prediction and prognostic performance of the models was carried out on the other two cohorts (LR: AUC = 0.69 and CI = 0.67; DM: AUC = 0.86 and CI = 0.88). Furthermore, the results obtained via Kaplan-Meier analysis demonstrated the potential of radiomics for assessing the risk of specific tumour outcomes using multiple stratification groups. This could have important clinical impact, notably by allowing for a better personalization of chemo-radiation treatments for head-and-neck cancer patients from different risk groups.},
archivePrefix = {arXiv},
arxivId = {1703.08516},
author = {Valli{\`{e}}res, Martin and Kay-Rivest, Emily and Perrin, L{\'{e}}o Jean and Liem, Xavier and Furstoss, Christophe and Aerts, Hugo J.W.L. and Khaouam, Nader and Nguyen-Tan, Phuc Felix and Wang, Chang Shu and Sultanem, Khalil and Seuntjens, Jan and {El Naqa}, Issam},
doi = {10.1038/s41598-017-10371-5},
eprint = {1703.08516},
issn = {20452322},
journal = {Scientific Reports},
month = {dec},
number = {1},
pages = {10117},
pmid = {28860628},
title = {{Radiomics strategies for risk assessment of tumour failure in head-and-neck cancer}},
url = {http://www.nature.com/articles/s41598-017-10371-5},
volume = {7},
year = {2017}
}
@article{Zhong2017,
abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
annote = {{\_}eprint: 1708.04896},
archivePrefix = {arXiv},
arxivId = {1708.04896},
author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
eprint = {1708.04896},
title = {{Random Erasing Data Augmentation}},
url = {http://arxiv.org/abs/1708.04896},
year = {2017}
}
@article{Bergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms. {\textcopyright} 2012 James Bergstra and Yoshua Bengio.},
author = {Bergstra, James and Bengio, Yoshua},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Deep learning,Global optimization,Model selection,Neural networks,Response surface modeling},
number = {1},
pages = {281--305},
title = {{Random search for hyper-parameter optimization}},
volume = {13},
year = {2012}
}
@article{Ju2015,
abstract = {Accurate lung tumor delineation plays an important role in radiotherapy treatment planning. Since the lung tumor has poor boundary in positron emission tomography (PET) images and low contrast in computed tomography (CT) images, segmentation of tumor in the PET and CT images is a challenging task. In this paper, we effectively integrate the two modalities by making fully use of the superior contrast of PET images and superior spatial resolution of CT images. Random walk and graph cut method is integrated to solve the segmentation problem, in which random walk is utilized as an initialization tool to provide object seeds for graph cut segmentation on the PET and CT images. The co-segmentation problem is formulated as an energy minimization problem which is solved by max-flow/min-cut method. A graph, including two sub-graphs and a special link, is constructed, in which one sub-graph is for the PET and another is for CT, and the special link encodes a context term which penalizes the difference of the tumor segmentation on the two modalities. To fully utilize the characteristics of PET and CT images, a novel energy representation is devised. For the PET, a downhill cost and a 3D derivative cost are proposed. For the CT, a shape penalty cost is integrated into the energy function which helps to constrain the tumor region during the segmentation. We validate our algorithm on a data set which consists of 18 PET-CT images. The experimental results indicate that the proposed method is superior to the graph cut method solely using the PET or CT is more accurate compared with the random walk method, random walk co-segmentation method, and non-improved graph cut method.},
author = {Ju, Wei and Xiang, Deihui and Zhang, Bin and Wang, Lirong and Kopriva, Ivica and Chen, Xinjian},
doi = {10.1109/TIP.2015.2488902},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Computed Tomography (CT),Positron Emission Tomography (PET),graph cut,image segmentation,interactive segmentation,lung tumor,prior information,random walk},
number = {12},
pages = {5854--5867},
title = {{Random Walk and Graph Cut for Co-Segmentation of Lung Tumor on PET-CT Images}},
volume = {24},
year = {2015}
}
@book{Daepp2011,
address = {New York, NY},
author = {Daepp, Ulrich and Gorkin, Pamela},
doi = {10.1007/978-1-4419-9479-0},
isbn = {978-1-4419-9478-3},
publisher = {Springer New York},
series = {Undergraduate Texts in Mathematics},
title = {{Reading, Writing, and Proving}},
url = {http://link.springer.com/10.1007/978-1-4419-9479-0},
year = {2011}
}
@book{Roos2019,
address = {Singapore},
author = {Roos, Cathryn and Roos, Gregory},
doi = {10.1007/978-981-13-7820-1},
isbn = {978-981-13-7819-5},
publisher = {Springer Singapore},
series = {SpringerBriefs in Education},
title = {{Real Science in Clear English}},
url = {http://link.springer.com/10.1007/978-981-13-7820-1},
year = {2019}
}
@book{Magnor2020,
address = {Cham},
doi = {10.1007/978-3-030-41816-8},
editor = {Magnor, Marcus and Sorkine-Hornung, Alexander},
isbn = {978-3-030-41815-1},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Real VR – Immersive Digital Reality}},
url = {http://link.springer.com/10.1007/978-3-030-41816-8},
volume = {11900},
year = {2020}
}
@article{VanRullen2018,
abstract = {Although distinct categories are reliably decoded from fMRI brain responses, it has proved more difficult to distinguish visually similar inputs, such as different faces. Here, we apply a recently developed deep learning system to reconstruct face images from human fMRI. We trained a variational auto-encoder (VAE) neural network using a GAN (Generative Adversarial Network) unsupervised procedure over a large data set of celebrity faces. The auto-encoder latent space provides a meaningful, topologically organized 1024-dimensional description of each image. We then presented several thousand faces to human subjects, and learned a simple linear mapping between the multi-voxel fMRI activation patterns and the 1024 latent dimensions. Finally, we applied this mapping to novel test images, translating fMRI patterns into VAE latent codes, and codes into face reconstructions. The system not only performed robust pairwise decoding ({\textgreater}95{\%} correct), but also accurate gender classification, and even decoded which face was imagined, rather than seen.},
archivePrefix = {arXiv},
arxivId = {1810.03856},
author = {VanRullen, Rufin and Reddy, Leila},
doi = {10.1038/s42003-019-0438-y},
eprint = {1810.03856},
issn = {23993642},
journal = {Communications Biology},
month = {oct},
number = {1},
pmid = {31925027},
title = {{Reconstructing faces from fMRI patterns using deep generative neural networks}},
url = {http://arxiv.org/abs/1810.03856},
volume = {2},
year = {2019}
}
@article{Shillcock2016,
abstract = {Large-scale brain initiatives such as the US BRAIN initiative and the European Human Brain Project aim to marshall a vast amount of data and tools for the purpose of furthering our understanding of brains. Fundamental to this goal is that neuronal morphologies must be seamlessly reconstructed and aggregated on scales up to the whole rodent brain. The experimental labor needed to manually produce this number of digital morphologies is prohibitively large. The BigNeuron initiative is assembling community-generated, open-source, automated reconstruction algorithms into an open platform, and is beginning to generate an increasing flow of high-quality reconstructed neurons. We propose a novel extension of this workflow to use this data stream to generate an unlimited number of statistically equivalent, yet distinct, digital morphologies. This will bring automated processing of reconstructed cells into digital neurons to the wider neuroscience community, and enable a range of morphologically accurate computational models.},
author = {Shillcock, Julian C. and Hawrylycz, Michael and Hill, Sean and Peng, Hanchuan},
doi = {10.1007/s40708-016-0041-7},
issn = {21984026},
journal = {Brain Informatics},
keywords = {Automated reconstruction,BigNeuron,Morphometric analysis,Neuron,Synthesis},
month = {dec},
number = {4},
pages = {205--209},
title = {{Reconstructing the brain: from image stacks to neuron synthesis}},
url = {http://link.springer.com/10.1007/s40708-016-0041-7},
volume = {3},
year = {2016}
}
@incollection{Rusinkiewicz2020,
address = {Cham},
author = {Rusinkiewicz, Szymon},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_537-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Reflectance Models}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}537-1},
year = {2020}
}
@misc{Orlando2020a,
abstract = {Glaucoma is one of the leading causes of irreversible but preventable blindness in working age populations. Color fundus photography (CFP) is the most cost-effective imaging modality to screen for retinal disorders. However, its application to glaucoma has been limited to the computation of a few related biomarkers such as the vertical cup-to-disc ratio. Deep learning approaches, although widely applied for medical image analysis, have not been extensively used for glaucoma assessment due to the limited size of the available data sets. Furthermore, the lack of a standardize benchmark strategy makes difficult to compare existing methods in a uniform way. In order to overcome these issues we set up the Retinal Fundus Glaucoma Challenge, REFUGE (https://refuge.grand-challenge.org), held in conjunction with MICCAI 2018. The challenge consisted of two primary tasks, namely optic disc/cup segmentation and glaucoma classification. As part of REFUGE, we have publicly released a data set of 1200 fundus images with ground truth segmentations and clinical glaucoma labels, currently the largest existing one. We have also built an evaluation framework to ease and ensure fairness in the comparison of different models, encouraging the development of novel techniques in the field. 12 teams qualified and participated in the online challenge. This paper summarizes their methods and analyzes their corresponding results. In particular, we observed that two of the top-ranked teams outperformed two human experts in the glaucoma classification task. Furthermore, the segmentation results were in general consistent with the ground truth annotations, with complementary outcomes that can be further exploited by ensembling the results.},
archivePrefix = {arXiv},
arxivId = {1910.03667},
author = {Orlando, Jos{\'{e}} Ignacio and Fu, Huazhu and {Barbossa Breda}, Jo{\~{a}}o and van Keer, Karel and Bathula, Deepti R. and Diaz-Pinto, Andr{\'{e}}s and Fang, Ruogu and Heng, Pheng Ann and Kim, Jeyoung and Lee, Joon Ho and Lee, Joonseok and Li, Xiaoxiao and Liu, Peng and Lu, Shuai and Murugesan, Balamurali and Naranjo, Valery and Phaye, Sai Samarth R. and Shankaranarayana, Sharath M. and Sikka, Apoorva and Son, Jaemin and van den Hengel, Anton and Wang, Shujun and Wu, Junyan and Wu, Zifeng and Xu, Guanghui and Xu, Yongli and Yin, Pengshuai and Li, Fei and Zhang, Xiulan and Xu, Yanwu and Bogunovi{\'{c}}, Hrvoje},
booktitle = {Medical Image Analysis},
doi = {10.1016/j.media.2019.101570},
eprint = {1910.03667},
issn = {13618423},
keywords = {Deep learning,Fundus photography,Glaucoma,Image classification,Image segmentation},
pmid = {31630011},
title = {{REFUGE Challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs}},
year = {2020}
}
@misc{Orlando2020,
abstract = {Glaucoma is one of the leading causes of irreversible but preventable blindness in working age populations. Color fundus photography (CFP) is the most cost-effective imaging modality to screen for retinal disorders. However, its application to glaucoma has been limited to the computation of a few related biomarkers such as the vertical cup-to-disc ratio. Deep learning approaches, although widely applied for medical image analysis, have not been extensively used for glaucoma assessment due to the limited size of the available data sets. Furthermore, the lack of a standardize benchmark strategy makes difficult to compare existing methods in a uniform way. In order to overcome these issues we set up the Retinal Fundus Glaucoma Challenge, REFUGE (https://refuge.grand-challenge.org), held in conjunction with MICCAI 2018. The challenge consisted of two primary tasks, namely optic disc/cup segmentation and glaucoma classification. As part of REFUGE, we have publicly released a data set of 1200 fundus images with ground truth segmentations and clinical glaucoma labels, currently the largest existing one. We have also built an evaluation framework to ease and ensure fairness in the comparison of different models, encouraging the development of novel techniques in the field. 12 teams qualified and participated in the online challenge. This paper summarizes their methods and analyzes their corresponding results. In particular, we observed that two of the top-ranked teams outperformed two human experts in the glaucoma classification task. Furthermore, the segmentation results were in general consistent with the ground truth annotations, with complementary outcomes that can be further exploited by ensembling the results.},
archivePrefix = {arXiv},
arxivId = {1910.03667},
author = {Orlando, Jos{\'{e}} Ignacio and Fu, Huazhu and {Barbossa Breda}, Jo{\~{a}}o and van Keer, Karel and Bathula, Deepti R. and Diaz-Pinto, Andr{\'{e}}s and Fang, Ruogu and Heng, Pheng Ann and Kim, Jeyoung and Lee, Joon Ho and Lee, Joonseok and Li, Xiaoxiao and Liu, Peng and Lu, Shuai and Murugesan, Balamurali and Naranjo, Valery and Phaye, Sai Samarth R. and Shankaranarayana, Sharath M. and Sikka, Apoorva and Son, Jaemin and van den Hengel, Anton and Wang, Shujun and Wu, Junyan and Wu, Zifeng and Xu, Guanghui and Xu, Yongli and Yin, Pengshuai and Li, Fei and Zhang, Xiulan and Xu, Yanwu and Bogunovi{\'{c}}, Hrvoje},
booktitle = {Medical Image Analysis},
doi = {10.1016/j.media.2019.101570},
eprint = {1910.03667},
issn = {13618423},
keywords = {Deep learning,Fundus photography,Glaucoma,Image classification,Image segmentation},
pmid = {31630011},
title = {{REFUGE Challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs}},
volume = {59},
year = {2020}
}
@incollection{Deguchi2020,
address = {Cham},
author = {Deguchi, Koichiro},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_828-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Regularization}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}828-1},
year = {2020}
}
@article{Real2018,
abstract = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier— AmoebaNet-A—that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9{\%} top-1 / 96.6{\%} top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
archivePrefix = {arXiv},
arxivId = {1802.01548},
author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
doi = {10.1609/aaai.v33i01.33014780},
eprint = {1802.01548},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
pages = {4780--4789},
title = {{Regularized Evolution for Image Classifier Architecture Search}},
volume = {33},
year = {2019}
}
@book{Koelsch2016,
address = {Berkeley, CA},
author = {Koelsch, George},
doi = {10.1007/978-1-4842-2099-3},
isbn = {978-1-4842-2098-6},
publisher = {Apress},
title = {{Requirements Writing for System Engineering}},
url = {http://link.springer.com/10.1007/978-1-4842-2099-3},
year = {2016}
}
@online{GrandChallengeREFUGE,
title = {{Retinal Fundus Glaucoma Challenge}},
url = {https://refuge.grand-challenge.org/}
}
@article{Niemeijer2010,
abstract = {The detection of microaneurysms in digital color fundus photographs is a critical first step in automated screening for diabetic retinopathy (DR), a common complication of diabetes. To accomplish this detection numerous methods have been published in the past but none of these was compared with each other on the same data. In this work we present the results of the first international microaneurysm detection competition, organized in the context of the Retinopathy Online Challenge (ROC), a multiyear online competition for various aspects of DR detection. For this competition, we compare the results of five different methods, produced by five different teams of researchers on the same set of data. The evaluation was performed in a uniform manner using an algorithm presented in this work. The set of data used for the competition consisted of 50 training images with available reference standard and 50 test images where the reference standard was witheld by the organizers (M. Niemeijer, B. van Ginneken, and M. D. Abrmoff). The results obtained on the test data was submitted through a website after which standardized evaluation software was used to determine the performance of each of the methods. A human expert detected microaneurysms in the test set to allow comparison with the performance of the automatic methods. The overall results show that microaneurysm detection is a challenging task for both the automatic methods as well as the human expert. There is room for improvement as the best performing system does not reach the performance of the human expert. The data associated with the ROC microaneurysm detection competition will remain publicly available and the website will continue accepting submissions. {\textcopyright} 2006 IEEE.},
author = {Niemeijer, Meindert and {Van Ginneken}, Bram and Cree, Michael J. and Mizutani, Atsushi and Quellec, Gw{\'{e}}nol{\'{e}} and Sanchez, Clara I. and Zhang, Bob and Hornero, Roberto and Lamard, Mathieu and Muramatsu, Chisako and Wu, Xiangqian and Cazuguel, Guy and You, Jane and Mayo, Agust{\'{i}}n and Li, Qin and Hatanaka, Yuji and Cochener, B{\'{e}}atrice and Roux, Christian and Karray, Fakhri and Garcia, Mar{\'{i}}a and Fujita, Hiroshi and Abramoff, Michael D.},
doi = {10.1109/TMI.2009.2033909},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Computer aided detection,Computer aided diagnosis,Diabetic retinopathy,Fundus photographs,Retina,Retinopathy Online Challenge (ROC) competition},
number = {1},
pages = {185--195},
pmid = {19822469},
title = {{Retinopathy online challenge: Automatic detection of microaneurysms in digital color fundus photographs}},
volume = {29},
year = {2010}
}
@article{Bogunovic2019,
abstract = {Retinal swelling due to the accumulation of fluid is associated with the most vision-threatening retinal diseases. Optical coherence tomography (OCT) is the current standard of care in assessing the presence and quantity of retinal fluid and image-guided treatment management. Deep learning methods have made their impact across medical imaging, and many retinal OCT analysis methods have been proposed. However, it is currently not clear how successful they are in interpreting the retinal fluid on OCT, which is due to the lack of standardized benchmarks. To address this, we organized a challenge RETOUCH in conjunction with MICCAI 2017, with eight teams participating. The challenge consisted of two tasks: fluid detection and fluid segmentation. It featured for the first time: all three retinal fluid types, with annotated images provided by two clinical centers, which were acquired with the three most common OCT device vendors from patients with two different retinal diseases. The analysis revealed that in the detection task, the performance on the automated fluid detection was within the inter-grader variability. However, in the segmentation task, fusing the automated methods produced segmentations that were superior to all individual methods, indicating the need for further improvements in the segmentation performance.},
author = {Bogunovic, Hrvoje and Venhuizen, Freerk and Klimscha, Sophie and Apostolopoulos, Stefanos and Bab-Hadiashar, Alireza and Bagci, Ulas and Beg, Mirza Faisal and Bekalo, Loza and Chen, Qiang and Ciller, Carlos and Gopinath, Karthik and Gostar, Amirali K. and Jeon, Kiwan and Ji, Zexuan and Kang, Sung Ho and Koozekanani, Dara D. and Lu, Donghuan and Morley, Dustin and Parhi, Keshab K. and Park, Hyoung Suk and Rashno, Abdolreza and Sarunic, Marinko and Shaikh, Saad and Sivaswamy, Jayanthi and Tennakoon, Ruwan and Yadav, Shivin and {De Zanet}, Sandro and Waldstein, Sebastian M. and Gerendas, Bianca S. and Klaver, Caroline and Sanchez, Clara I. and Schmidt-Erfurth, Ursula},
doi = {10.1109/TMI.2019.2901398},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Evaluation,image classification,image segmentation,optical coherence tomography,retina},
month = {aug},
number = {8},
pages = {1858--1874},
pmid = {30835214},
title = {{RETOUCH: The Retinal OCT Fluid Detection and Segmentation Benchmark and Challenge}},
url = {https://ieeexplore.ieee.org/document/8653407/},
volume = {38},
year = {2019}
}
@inproceedings{Chatfield2014,
abstract = {The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.},
annote = {{\_}eprint: 1405.3531},
archivePrefix = {arXiv},
arxivId = {1405.3531},
author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
booktitle = {BMVC 2014 - Proceedings of the British Machine Vision Conference 2014},
doi = {10.5244/c.28.6},
eprint = {1405.3531},
title = {{Return of the devil in the details: Delving deep into convolutional nets}},
year = {2014}
}
@article{Jenkinson2012,
abstract = {FSL (the FMRIB Software Library) is a comprehensive library of analysis tools for functional, structural and diffusion MRI brain imaging data, written mainly by members of the Analysis Group, FMRIB, Oxford. For this NeuroImage special issue on“20years offMRI”we have beenaskedtowrite about the history, developments and current status of FSL.We also include some descriptions of parts of FSL that are not well covered in the existing literature.We hope that some of this contentmight be of interest to users of FSL, and alsomaybe to newresearch groups considering creating, releasing and supporting newsoftware packages for brain image analysis.},
author = {Jenkinson, Mark and Beckmann, Christian F and Behrens, Timothy E J and Woolrich, Mark W and Smith, Stephen M},
doi = {10.1016/j.neuroimage.2011.09.015},
issn = {1053-8119},
journal = {NeuroImage},
keywords = {FSL,Software},
number = {2},
pages = {782--790},
title = {{Review FSL}},
url = {http://www.sciencedirect.com/science/article/pii/S1053811911010603},
volume = {62},
year = {2012}
}
@misc{Minati2009,
abstract = {This comprehensive, pedagogically-oriented review is aimed at a heterogeneous audience representative of the allied disciplines involved in research and patient care. After a foreword on epidemiology, genetics, and risk factors, the amyloid cascade model is introduced and the main neuropathological hallmarks are discussed. The progression of memory, language, visual processing, executive, attentional, and praxis deficits, and of behavioral symptoms is presented. After a summary on neuropsychological assessment, emerging biomarkers from cerebrospinal fluid assays, magnetic resonance imaging, nuclear medicine, and electrophysiology are discussed. Existing treatments are briefly reviewed, followed by an introduction to emerging disease-modifying therapies such as secretase modulators, inhibitors of Abeta aggregation, immunotherapy, inhibitors of tau protein phosphorylation, and delivery of nerve growth factor. {\textcopyright} 2009 Sage Publications.},
author = {Minati, Ludovico and Edginton, Trudi and {Grazia Bruzzone}, Maria and Giaccone, Giorgio},
booktitle = {American Journal of Alzheimer's Disease and other Dementias},
doi = {10.1177/1533317508328602},
issn = {15333175},
keywords = {Alzheimer's disease,Neuroimaging,Neuropathology,Neuropsychological testing,Pharmacotherapy},
number = {2},
pages = {95--121},
title = {{Reviews: Current concepts in alzheimer's disease: A multidisciplinary review}},
volume = {24},
year = {2009}
}
@incollection{Lin2020,
address = {Cham},
author = {Lin, Tong and Zha, Hongbin},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_801-1},
pages = {1--6},
publisher = {Springer International Publishing},
title = {{Riemannian Manifold}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}801-1},
year = {2020}
}
@incollection{Meer2020,
address = {Cham},
author = {Meer, Peter},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_882-1},
pages = {1--8},
publisher = {Springer International Publishing},
title = {{Robust Estimation Techniques}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}882-1},
year = {2020}
}
@online{GrandChallengeROCC,
author = {Rabbani, Hossein and Rasti, Reza and Kafieh, Rahele},
title = {{ROCC - Retinal OCT Classification Challenge (ROCC)}},
url = {https://rocc.grand-challenge.org/},
year = {2017}
}
@article{Munos2016,
abstract = {In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace($\lambda$), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of "off-policyness"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to Q∗ without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q($\lambda$), which was an open problem since 1989. We illustrate the benefits of Retrace($\lambda$) on a standard suite of Atari 2600 games.},
archivePrefix = {arXiv},
arxivId = {1606.02647},
author = {Munos, R{\'{e}}mi and Stepleton, Thomas and Harutyunyan, Anna and Bellemare, Marc G.},
eprint = {1606.02647},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
month = {jun},
pages = {1054--1062},
title = {{Safe and efficient off-policy reinforcement learning}},
url = {http://arxiv.org/abs/1606.02647},
year = {2016}
}
@article{Kasthuri2015,
abstract = {We describe automated technologies to probe the structure of neural tissue at nanometer resolution and use them to generate a saturated reconstruction of a sub-volume of mouse neocortex in which all cellular objects (axons, dendrites, and glia) and many sub-cellular components (synapses, synaptic vesicles, spines, spine apparati, postsynaptic densities, and mitochondria) are rendered and itemized in a database. We explore these data to study physical properties of brain tissue. For example, by tracing the trajectories of all excitatory axons and noting their juxtapositions, both synaptic and non-synaptic, with every dendritic spine we refute the idea that physical proximity is sufficient to predict synaptic connectivity (the so-called Peters' rule). This online minable database provides general access to the intrinsic complexity of the neocortex and enables further data-driven inquiries. Video Abstract},
author = {Kasthuri, Narayanan and Hayworth, Kenneth Jeffrey and Berger, Daniel Raimund and Schalek, Richard Lee and Conchello, Jos{\'{e}} Angel and Knowles-Barley, Seymour and Lee, Dongil and V{\'{a}}zquez-Reina, Amelio and Kaynig, Verena and Jones, Thouis Raymond and Roberts, Mike and Morgan, Josh Lyskowski and Tapia, Juan Carlos and Seung, H. Sebastian and Roncal, William Gray and Vogelstein, Joshua Tzvi and Burns, Randal and Sussman, Daniel Lewis and Priebe, Carey Eldin and Pfister, Hanspeter and Lichtman, Jeff William},
doi = {10.1016/j.cell.2015.06.054},
issn = {10974172},
journal = {Cell},
month = {jul},
number = {3},
pages = {648--661},
pmid = {26232230},
title = {{Saturated Reconstruction of a Volume of Neocortex}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0092867415008247},
volume = {162},
year = {2015}
}
@online{Mascalchi2018a,
author = {Mascalchi, Mario},
title = {{SCA2 Diffusion Tensor Imaging}},
url = {https://openneuro.org/datasets/ds001378/versions/00003},
year = {2018}
}
@incollection{Zhou2020,
abstract = {{\textcopyright} Springer International Publishing Switzerland 2015. There are a large number of small and inexpensive single-board computers with Linux operating systems available on the market today. Most of these aim for the consumer and enthusiast market, but can also be used in research and commercial applications. This paper builds on several years of experience with using such computers in student projects, as well as the development of cyber-physical and embedded control systems. A summary of the properties that are key for dependability for selected boards is given in tabulated form. These boards have interesting properties for many embedded and cyber-physical systems, e.g. high-performance, small size and low cost. The use of Linux for operating system means a development environment that is familiar to many developers, and the availability of many libraries and applications. While not suitable for applications were formally proven dependability is necessary, we argue that by actively mitigating some of the potential problems identified in this paper such computers can be used in many applications where high dependability is desirable, especially in combination with low-cost. A solution with redundant single-board computers is presented as a strategy for achieving high dependability. Due to the low cost and small size, this is feasible for applications were redundancy traditionally would be prohibitively too large or costly.},
address = {Cham},
author = {Zhou, Bolei},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_799-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Scene Classification}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}799-1},
year = {2020}
}
@inproceedings{Tchapmi2017,
abstract = {3D semantic scene labeling is fundamental to agents operating in the real world. In particular, labeling raw 3D point sets from sensors provides fine-grained semantics. Recent works leverage the capabilities of Neural Networks(NNs), but are limited to coarse voxel predictions and do not explicitly enforce global consistency. We present SEGCloud, an end-To-end framework to obtain 3D point-level segmentation that combines the advantages of NNs, trilinear interpolation(TI) and fully connected Conditional Random Fields (FC-CRF). Coarse voxel predictions from a 3D Fully Convolutional NN are transferred back to the raw 3D points via trilinear interpolation. Then the FC-CRF enforces global consistency and provides fine-grained semantics on the points. We implement the latter as a differentiable Recurrent NN to allow joint optimization. We evaluate the framework on two indoor and two outdoor 3D datasets (NYU V2, S3DIS, KITTI, Semantic3D.net), and show performance comparable or superior to the state-of-The-Art on all datasets.},
annote = {{\_}eprint: 1710.07563},
archivePrefix = {arXiv},
arxivId = {1710.07563},
author = {Tchapmi, Lyne and Choy, Christopher and Armeni, Iro and Gwak, JunYoung and Savarese, Silvio},
booktitle = {Proceedings - 2017 International Conference on 3D Vision, 3DV 2017},
doi = {10.1109/3DV.2017.00067},
eprint = {1710.07563},
isbn = {9781538626108},
keywords = {3D-Conditional-Random-Fields,3D-Convolutional-Neural-Networks,3D-Point-Clouds,3D-Semantic-Segmentation,RGB-D},
pages = {537--547},
title = {{SEGCloud: Semantic segmentation of 3D point clouds}},
url = {http://arxiv.org/abs/1710.07563},
volume = {abs/1710.0},
year = {2018}
}
@article{Naylor2019,
abstract = {The advent of digital pathology provides us with the challenging opportunity to automatically analyze whole slides of diseased tissue in order to derive quantitative profiles that can be used for diagnosis and prognosis tasks. In particular, for the development of interpretable models, the detection and segmentation of cell nuclei is of the utmost importance. In this paper, we describe a new method to automatically segment nuclei from Haematoxylin and Eosin (HE) stained histopathology data with fully convolutional networks. In particular, we address the problem of segmenting touching nuclei by formulating the segmentation problem as a regression task of the distance map. We demonstrate superior performance of this approach as compared to other approaches using Convolutional Neural Networks.},
author = {Naylor, Peter and La{\'{e}}, Marick and Reyal, Fabien and Walter, Thomas},
doi = {10.1109/TMI.2018.2865709},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Cancer research,deep learning,digital pathology,histopathology,nuclei segmentation},
number = {2},
pages = {448--459},
title = {{Segmentation of Nuclei in Histopathology Images by Deep Regression of the Distance Map}},
volume = {38},
year = {2019}
}
@inproceedings{Trullo2017,
abstract = {Cancer is one of the leading causes of death worldwide. Radiotherapy is a standard treatment for this condition and the first step of the radiotherapy process is to identify the target volumes to be targeted and the healthy organs at risk (OAR) to be protected. Unlike previous methods for automatic segmentation of OAR that typically use local information and individually segment each OAR, in this paper, we propose a deep learning framework for the joint segmentation of OAR in CT images of the thorax, specifically the heart, esophagus, trachea and the aorta. Making use of Fully Convolutional Networks (FCN), we present several extensions that improve the performance, including a new architecture that allows to use low level features with high level information, effectively combining local and global information for improving the localization accuracy. Finally, by using Conditional Random Fields (specifically the CRF as Recurrent Neural Network model), we are able to account for relationships between the organs to further improve the segmentation results. Experiments demonstrate competitive performance on a dataset of 30 CT scans.},
author = {Trullo, R and Petitjean, Caroline and Ruan, Su and Dubray, Bernard and Nie, D and Shen, D},
booktitle = {Proceedings - International Symposium on Biomedical Imaging},
doi = {10.1109/ISBI.2017.7950685},
isbn = {9781509011711},
issn = {19458452},
keywords = {CRF,CRFasRNN,CT Segmentation,Fully Convolutional Networks (FCN)},
pages = {1003--1006},
title = {{Segmentation of Organs at Risk in thoracic CT images using a SharpMask architecture and Conditional Random Fields}},
volume = {2017},
year = {2017}
}
@incollection{Shotton2020,
address = {Cham},
author = {Shotton, Jamie and Kohli, Pushmeet},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_251-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Semantic Image Segmentation: Traditional Approach}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}251-1},
year = {2020}
}
@inproceedings{Hackel2017,
abstract = {This paper presents a new 3D point cloud classification benchmark data set with over four billion manually labelled points, meant as input for data-hungry (deep) learning methods. We also discuss first submissions to the benchmark that use deep convolutional neural networks (CNNs) as a work horse, which already show remarkable performance improvements over state-of-the-art. CNNs have become the de-facto standard for many tasks in computer vision and machine learning like semantic segmentation or object detection in images, but have no yet led to a true breakthrough for 3D point cloud labelling tasks due to lack of training data. With the massive data set presented in this paper, we aim at closing this data gap to help unleash the full potential of deep learning methods for 3D labelling tasks. Our semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains 8 semantic classes and covers a wide range of urban outdoor scenes: churches, streets, railroad tracks, squares, villages, soccer fields and castles. We describe our labelling interface and show that our data set provides more dense and complete point clouds with much higher overall number of labelled points compared to those already available to the research community. We further provide baseline method descriptions and comparison between methods submitted to our online system. We hope semantic3D.net will pave the way for deep learning methods in 3D point cloud labelling to learn richer, more general 3D representations, and first submissions after only a few months indicate that this might indeed be the case.},
archivePrefix = {arXiv},
arxivId = {1704.03847},
author = {Hackel, Timo and Savinov, N and Ladicky, L and Wegner, Jan D and Schindler, K and Pollefeys, M},
booktitle = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
doi = {10.5194/isprs-annals-IV-1-W1-91-2017},
eprint = {1704.03847},
issn = {21949050},
number = {1W1},
pages = {91--98},
title = {{SEMANTIC3D.NET: A NEW LARGE-SCALE POINT CLOUD CLASSIFICATION BENCHMARK}},
volume = {4},
year = {2017}
}
@inproceedings{Kipf2016,
abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
annote = {{\_}eprint: 1609.02907},
archivePrefix = {arXiv},
arxivId = {1609.02907},
author = {Kipf, Thomas N and Welling, Max},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1609.02907},
title = {{Semi-supervised classification with graph convolutional networks}},
url = {http://arxiv.org/abs/1609.02907},
volume = {abs/1609.0},
year = {2019}
}
@article{Wang2020,
abstract = {The availability of a large amount of annotated data is critical for many medical image analysis applications, in particular for those relying on deep learning methods which are known to be data-hungry. However, annotated medical data, especially multimodal data, is often scarce and costly to obtain. In this paper, we address the problem of synthesizing multi-parameter magnetic resonance imaging data (i.e. mp-MRI), which typically consists of Apparent Diffusion Coefficient (ADC) and T2-weighted (T2w) images, containing clinically significant (CS) prostate cancer (PCa) via semi-supervised learning and adversarial learning. Specifically, our synthesizer generates mp-MRI data in a sequential manner: first utilizing a decoder to generate an ADC map from a 128-d latent vector, followed by translating the ADC to the T2w image via U-Net. The synthesizer is trained in a semi-supervised manner. In the supervised training process, a limited amount of paired ADC-T2w images and the corresponding ADC encodings are provided and the synthesizer learns the paired relationship by explicitly minimizing the reconstruction losses between synthetic and real images. To avoid overfitting limited ADC encodings, an unlimited amount of random latent vectors and unpaired ADC-T2w Images are utilized in the unsupervised training process for learning the marginal image distributions of real images. To improve the robustness for training the synthesizer, we decompose the difficult task of generating full-size images into several simpler tasks which generate sub-images only. A StitchLayer is then employed to seamlessly fuse sub-images together in an interlaced manner into a full-size image. In addition, to enforce the synthetic images to indeed contain distinguishable CS PCa lesions, we propose to also maximize an auxiliary distance of Jensen-Shannon divergence (JSD) between CS and nonCS images. Experimental results show that our method can effectively synthesize a large variety of mp-MRI images which contain meaningful CS PCa lesions, display a good visual quality and have the correct paired relationship between the two modalities of a pair. Compared to the state-of-the-art methods based on adversarial learning (Liu and Tuzel, 2016; Costa et al., 2017), our method achieves a significant improvement in terms of both visual quality and several popular quantitative evaluation metrics.},
author = {Wang, Zhiwei and Lin, Yi and Cheng, Kwang-Ting (Tim) and Yang, Xin},
doi = {https://doi.org/10.1016/j.media.2019.101565},
issn = {1361-8415},
journal = {Medical Image Analysis},
keywords = {Deep learning,GAN,Generative models,Multimodal image synthesis},
pages = {101565},
title = {{Semi-supervised mp-MRI data synthesis with StitchLayer and auxiliary distance maximization}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841519301057},
volume = {59},
year = {2020}
}
@inproceedings{Hutter2011,
abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach. {\textcopyright} Springer-Verlag Berlin Heidelberg 2011.},
author = {Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-25566-3_40},
isbn = {9783642255656},
issn = {03029743},
pages = {507--523},
title = {{Sequential model-based optimization for general algorithm configuration}},
volume = {6683 LNCS},
year = {2011}
}
@incollection{Matsushita2020,
address = {Cham},
author = {Matsushita, Yasuyuki},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_829-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Shape from Shading}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}829-1},
year = {2020}
}
@article{Chang2015,
abstract = {We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.},
annote = {{\_}eprint: 1512.03012},
archivePrefix = {arXiv},
arxivId = {1512.03012},
author = {Chang, Angel X and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qi-Xing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},
eprint = {1512.03012},
journal = {CoRR},
title = {{ShapeNet: An Information-Rich 3D Model Repository}},
url = {http://arxiv.org/abs/1512.03012},
volume = {abs/1512.0},
year = {2015}
}
@incollection{Kang2020,
address = {Cham},
author = {Kang, Sing Bing},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_795-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Shiftable Windows for Stereo}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}795-1},
year = {2020}
}
@misc{Zheng2018,
abstract = {In the early days, content-based image retrieval (CBIR) was studied with global features. Since 2003, image retrieval based on local descriptors (de facto SIFT) has been extensively studied for over a decade due to the advantage of SIFT in dealing with image transformations. Recently, image representations based on the convolutional neural network (CNN) have attracted increasing interest in the community and demonstrated impressive performance. Given this time of rapid evolution, this article provides a comprehensive survey of instance retrieval over the last decade. Two broad categories, SIFT-based and CNN-based methods, are presented. For the former, according to the codebook size, we organize the literature into using large/medium-sized/small codebooks. For the latter, we discuss three lines of methods, i.e., using pre-trained or fine-tuned CNN models, and hybrid methods. The first two perform a single-pass of an image to the network, while the last category employs a patch-based feature extraction scheme. This survey presents milestones in modern instance retrieval, reviews a broad selection of previous works in different categories, and provides insights on the connection between SIFT and CNN-based methods. After analyzing and comparing retrieval performance of different categories on several datasets, we discuss promising directions towards generic and specialized instance retrieval.},
archivePrefix = {arXiv},
arxivId = {1608.01807},
author = {Zheng, Liang and Yang, Yi and Tian, Qi},
booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
doi = {10.1109/TPAMI.2017.2709749},
eprint = {1608.01807},
issn = {01628828},
keywords = {Instance retrieval,SIFT,convolutional neural network,literature survey},
month = {may},
number = {5},
pages = {1224--1244},
publisher = {IEEE Computer Society},
title = {{SIFT Meets CNN: A Decade Survey of Instance Retrieval}},
volume = {40},
year = {2018}
}
@article{Ferdouse2011,
abstract = {Noise problems in signals have gained huge attention due to the need of noise-free output signal in numerous communication systems. The principal of adaptive noise cancellation is to acquire an estimation of the unwanted interfering signal and subtract it from the corrupted signal. Noise cancellation operation is controlled adaptively with the target of achieving improved signal to noise ratio. This paper concentrates upon the analysis of adaptive noise canceller using Recursive Least Square (RLS), Fast Transversal Recursive Least Square (FTRLS) and Gradient Adaptive Lattice (GAL) algorithms. The performance analysis of the algorithms is done based on convergence behavior, convergence time, correlation coefficients and signal to noise ratio. After comparing all the simulated results we observed that GAL performs the best in noise cancellation in terms of Correlation Coefficient, SNR and Convergence Time. RLS, FTRLS and GAL were never evaluated and compared before on their performance in noise cancellation in terms of the criteria we considered here.},
annote = {{\_}eprint: 1104.1962},
author = {Ferdouse, Lilatul and Akhter, Nasrin and Nipa, Tamanna Haque and Jaigirdar, Fariha Tasmin},
journal = {CoRR},
keywords = {Adaptive Filter,Computer Science - Other Computer Science,Convergence,DOAJ:Computer Science,DOAJ:Technology and Engineering,Electronic computers. Computer science,FTRLS,GAL,IJCSI,Instruments and machines,Mathematics,Mean Square Error,Noise,Q,QA1-939,QA71-90,QA75.5-76.95,RLS,Science},
title = {{Simulation and Performance Analysis of Adaptive Filtering Algorithms in Noise Cancellation}},
volume = {abs/1104.1},
year = {2011}
}
@article{Prastawa2009,
abstract = {Obtaining validation data and comparison metrics for segmentation of magnetic resonance images (MRI) are difficult tasks due to the lack of reliable ground truth. This problem is even more evident for images presenting pathology, which can both alter tissue appearance through infiltration and cause geometric distortions. Systems for generating synthetic images with user-defined degradation by noise and intensity inhomogeneity offer the possibility for testing and comparison of segmentation methods. Such systems do not yet offer simulation of sufficiently realistic looking pathology. This paper presents a system that combines physical and statistical modeling to generate synthetic multi-modal 3D brain MRI with tumor and edema, along with the underlying anatomical ground truth, Main emphasis is placed on simulation of the major effects known for tumor MRI, such as contrast enhancement, local distortion of healthy tissue, infiltrating edema adjacent to tumors, destruction and deformation of fiber tracts, and multi-modal MRI contrast of healthy tissue and pathology. The new method synthesizes pathology in multi-modal MRI and diffusion tensor imaging (DTI) by simulating mass effect, warping and destruction of white matter fibers, and infiltration of brain tissues by tumor cells. We generate synthetic contrast enhanced MR images by simulating the accumulation of contrast agent within the brain. The appearance of the the brain tissue and tumor in MRI is simulated by synthesizing texture images from real MR images. The proposed method is able to generate synthetic ground truth and synthesized MR images with tumor and edema that exhibit comparable segmentation challenges to real tumor MRI. Such image data sets will find use in segmentation reliability studies, comparison and validation of different segmentation methods, training and teaching, or even in evaluating standards for tumor size like the RECIST criteria (response evaluation criteria in solid tumors). {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Prastawa, Marcel and Bullitt, Elizabeth and Gerig, Guido},
doi = {10.1016/j.media.2008.11.002},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Brain MRI,Diffusion tensor imaging,Gold standard,Ground truth,Segmentation validation,Simulation of tumor infiltration,Tumor simulation},
month = {apr},
number = {2},
pages = {297--311},
title = {{Simulation of brain tumors in MR images for evaluation of segmentation efficacy}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841508001357},
volume = {13},
year = {2009}
}
@article{Wu2016,
annote = {From Duplicate 2 (Single Image 3D Interpreter Network - Wu, Jiajun; Xue, Tianfan; Lim, Joseph J; Tian, Yuandong; Tenenbaum, Joshua B; Torralba, Antonio; Freeman, William T)

{\_}eprint: 1604.08685},
archivePrefix = {arXiv},
arxivId = {1604.08685},
author = {Wu, Jiajun and Xue, Tianfan and Lim, Joseph J and Tian, Yuandong and Tenenbaum, Joshua B and Torralba, Antonio and Freeman, William T},
eprint = {1604.08685},
journal = {CoRR},
title = {{Single Image 3D Interpreter Network}},
url = {http://arxiv.org/abs/1604.08685},
volume = {abs/1604.0},
year = {2016}
}
@misc{TCIA-Breast-MRI-NACT-Pilot,
author = {Newitt, David and Hylton, Nola},
doi = {10.7937/K9/TCIA.2016.QHsyhJKy},
pages = {The Cancer Imaging Archive},
publisher = {The Cancer Imaging Archive},
title = {{Single site breast DCE-MRI data and segmentations from patients undergoing neoadjuvant chemotherapy}},
url = {https://wiki.cancerimagingarchive.net/x/ZIhXAQ},
year = {2016}
}
@article{Tatarchenko2015,
abstract = {We present a convolutional network capable of inferring a 3D representation of a previously unseen object given a single image of this object. Concretely, the network can predict an RGB image and a depth map of the object as seen from an arbitrary view. Several of these depth maps fused together give a full point cloud of the object. The point cloud can in turn be transformed into a surface mesh. The network is trained on renderings of synthetic 3D models of cars and chairs. It successfully deals with objects on cluttered background and generates reasonable predictions for real images of cars.},
annote = {{\_}eprint: 1511.06702},
archivePrefix = {arXiv},
arxivId = {arXiv:1511.06702v1},
author = {Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},
doi = {10.1007/978-3-319-46478-7_20},
eprint = {arXiv:1511.06702v1},
isbn = {9783319464770},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {3D from single image,Convolutional networks,Deep learning},
pages = {322--337},
title = {{Single-view to Multi-view: Reconstructing Unseen Views with a Convolutional Network}},
url = {http://arxiv.org/abs/1511.06702},
volume = {9911 LNCS},
year = {2016}
}
@incollection{Zhang2016,
abstract = {Photoacoustic imaging is a non-ionizing imaging modality that provides contrast consistent with optical imaging techniques while the resolution and penetration depth is similar to ultrasound techniques. In a previous publication Opt. Express 18, 11406 (2010), a technique was introduced to experimentally acquire the imaging operator for a photoacoustic imaging system. While this was an important foundation for future work, we have recently improved the experimental procedure allowing for a more densely populated imaging operator to be acquired. Subsets of the imaging operator were produced by varying the transducer count as well as the measurement space temporal sampling rate. Examination of the matrix rank and the effect of contributing object space singular vectors to image reconstruction were performed. For a PAI system collecting only limited data projections, matrix rank increased linearly with transducer count and measurement space temporal sampling rate. Image reconstruction using a regularized pseudoinverse of the imaging operator was performed on photoacoustic signals from a point source, line source, and an array of point sources derived from the imaging operator. As expected, image quality increased for each object with increasing transducer count and measurement space temporal sampling rate. Using the same approach, but on experimentally sampled photoacoustic signals from a moving point-like source, acquisition, data transfer, reconstruction and image display took 1.4 s using one laser pulse per 3D frame. With relatively simple hardware improvements to data transfer and computation speed, our current imaging results imply that acquisition and display of 3D photoacoustic images at laser repetition rates of 10Hz is easily achieved.},
address = {Cham},
author = {Zhang, Yanchun and Xu, Guandong},
booktitle = {Encyclopedia of Database Systems},
doi = {10.1007/978-1-4899-7993-3_538-2},
pages = {1--3},
publisher = {Springer International Publishing},
title = {{Singular Value Decomposition}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}802-1},
year = {2016}
}
@inproceedings{Brock2017,
abstract = {Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks.},
archivePrefix = {arXiv},
arxivId = {1708.05344},
author = {Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
booktitle = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
eprint = {1708.05344},
title = {{SmaSH: One-shot model architecture search through hypernetworks}},
volume = {abs/1708.0},
year = {2018}
}
@article{Berrada2018,
abstract = {The top-k error is a common measure of performance in machine learning and computer vision. In practice, top-k classification is typically performed with deep neural networks trained with the cross-entropy loss. Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data. In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-k classification can bring significant improvements. Our empirical evidence suggests that the loss function must be smooth and have non-sparse gradients in order to work well with deep neural networks. Consequently, we introduce a family of smoothed loss functions that are suited to top-k optimization via deep learning. The widely used cross-entropy is a special case of our family. Evaluating our smooth loss functions is computationally challenging: a na{\"{i}}ve algorithm would require O(nk) operations, where n is the number of classes. Thanks to a connection to polynomial algebra and a divide- and-conquer approach, we provide an algorithm with a time complexity of O(kn). Furthermore, we present a novel approximation to obtain fast and stable algorithms on GPUs with single floating point precision. We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size, for the predominant use case of k = 5. Our investigation reveals that our loss is more robust to noise and overfitting than cross-entropy.},
archivePrefix = {arXiv},
arxivId = {1802.07595},
author = {Berrada, Leonard and Zisserman, Andrew and Kumar, M. Pawan},
eprint = {1802.07595},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
month = {feb},
title = {{Smooth loss functions for deep top-k classification}},
url = {http://arxiv.org/abs/1802.07595},
year = {2018}
}
@article{Ning2015,
abstract = {Diffusion magnetic resonance imaging (dMRI) is the modality of choice for investigating in-vivo white matter connectivity and neural tissue architecture of the brain. The diffusion-weighted signal in dMRI reflects the diffusivity of water molecules in brain tissue and can be utilized to produce image-based biomarkers for clinical research. Due to the constraints on scanning time, a limited number of measurements can be acquired within a clinically feasible scan time. In order to reconstruct the dMRI signal from a discrete set of measurements, a large number of algorithms have been proposed in recent years in conjunction with varying sampling schemes, i.e., with varying b-values and gradient directions. Thus, it is imperative to compare the performance of these reconstruction methods on a single data set to provide appropriate guidelines to neuroscientists on making an informed decision while designing their acquisition protocols. For this purpose, the SPArse Reconstruction Challenge (SPARC) was held along with the workshop on Computational Diffusion MRI (at MICCAI 2014) to validate the performance of multiple reconstruction methods using data acquired from a physical phantom. A total of 16 reconstruction algorithms (9 teams) participated in this community challenge. The goal was to reconstruct single b-value and/or multiple b-value data from a sparse set of measurements. In particular, the aim was to determine an appropriate acquisition protocol (in terms of the number of measurements, b-values) and the analysis method to use for a neuroimaging study. The challenge did not delve on the accuracy of these methods in estimating model specific measures such as fractional anisotropy (FA) or mean diffusivity, but on the accuracy of these methods to fit the data. This paper presents several quantitative results pertaining to each reconstruction algorithm. The conclusions in this paper provide a valuable guideline for choosing a suitable algorithm and the corresponding data-sampling scheme for clinical neuroscience applications.},
author = {Ning, Lipeng and Laun, Frederik and Gur, Yaniv and DiBella, Edward V.R. and Deslauriers-Gauthier, Samuel and Megherbi, Thinhinane and Ghosh, Aurobrata and Zucchelli, Mauro and Menegaz, Gloria and Fick, Rutger and St-Jean, Samuel and Paquette, Michael and Aranda, Ramon and Descoteaux, Maxime and Deriche, Rachid and O'Donnell, Lauren and Rathi, Yogesh},
doi = {10.1016/j.media.2015.10.012},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Angular error,Diffusion MRI,Normalized mean square error,Physical phantom},
month = {dec},
number = {1},
pages = {316--331},
title = {{Sparse Reconstruction Challenge for diffusion MRI: Validation on a physical phantom to determine which acquisition scheme and analysis method to use?}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841515001541},
volume = {26},
year = {2015}
}
@article{S.Rameshkumar2016,
author = {Rameshkumar, S and Thilak, J Anish Jafrin and Suresh, P and Sathishkumar, S and Subramani, N},
doi = {10.15680/IJIRSET.2016.0512161},
issn = {2319-8753},
journal = {International Journal of Innovative Research in Science Engineering and Technology},
keywords = {filter,image denoising,mri,mse,psnr,rmse,wb},
month = {dec},
number = {12},
pages = {21079--21083},
title = {{Speckle Noise Removal in MRI Scan Image Using WB – Filter}},
volume = {5},
year = {2016}
}
@inproceedings{Rippel2015,
abstract = {Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs). We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality. This representation also enables a new form of stochastic regularization by randomized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling. Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training.},
annote = {{\_}eprint: 1506.03767},
archivePrefix = {arXiv},
arxivId = {1506.03767},
author = {Rippel, Oren and Snoek, Jasper and Adams, Ryan P},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1506.03767},
issn = {10495258},
keywords = {Computer Science - Learning,Statistics - Machine Learning},
pages = {2449--2457},
title = {{Spectral representations for convolutional neural networks}},
volume = {2015-Janua},
year = {2015}
}
@article{Jamaludin2017,
abstract = {The objective of this work is to automatically produce radiological gradings of spinal lumbar MRIs and also localize the predicted pathologies. We show that this can be achieved via a Convolutional Neural Network (CNN) framework that takes intervertebral disc volumes as inputs and is trained only on disc-specific class labels. Our contributions are: (i) a CNN architecture that predicts multiple gradings at once, and we propose variants of the architecture including using 3D convolutions; (ii) showing that this architecture can be trained using a multi-task loss function without requiring segmentation level annotation; and (iii) a localization method that clearly shows pathological regions in the disc volumes. We compare three visualization methods for the localization. The network is applied to a large corpus of MRI T2 sagittal spinal MRIs (using a standard clinical scan protocol) acquired from multiple machines, and is used to automatically compute disk and vertebra gradings for each MRI. These are: Pfirrmann grading, disc narrowing, upper/lower endplate defects, upper/lower marrow changes, spondylolisthesis, and central canal stenosis. We report near human performances across the eight gradings, and also visualize the evidence for these gradings localized on the original scans.},
author = {Jamaludin, Amir and Kadir, Timor and Zisserman, Andrew},
doi = {10.1016/j.media.2017.07.002},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {MRI analysis,Radiological classification,Spinal MRI},
pages = {63--73},
title = {{SpineNet: Automated classification and evidence visualization in spinal MRIs}},
url = {http://www.sciencedirect.com/science/article/pii/S136184151730110X},
volume = {41},
year = {2017}
}
@inproceedings{Su2018,
abstract = {We present a network architecture for processing point clouds that directly operates on a collection of points represented as a sparse set of samples in a high-dimensional lattice. Na{\~{A}}ely applying convolutions on this lattice scales poorly, both in terms of memory and computational cost, as the size of the lattice increases. Instead, our network uses sparse bilateral convolutional layers as building blocks. These layers maintain efficiency by using indexing structures to apply convolutions only on occupied parts of the lattice, and allow flexible specifications of the lattice structure enabling hierarchical and spatially-aware feature learning, as well as joint 2D-3D reasoning. Both point-based and image-based representations can be easily incorporated in a network with such layers and the resulting model can be trained in an end-to-end manner. We present results on 3D segmentation tasks where our approach outperforms existing state-of-the-art techniques.},
annote = {{\_}eprint: 1802.08275},
archivePrefix = {arXiv},
arxivId = {1802.08275},
author = {Su, Hang and Jampani, Varun and Sun, Deqing and Maji, Subhransu and Kalogerakis, Evangelos and Yang, Ming Hsuan and Kautz, Jan},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00268},
eprint = {1802.08275},
isbn = {9781538664209},
issn = {10636919},
pages = {2530--2539},
title = {{SPLATNet: Sparse Lattice Networks for Point Cloud Processing}},
url = {http://arxiv.org/abs/1802.08275},
volume = {abs/1802.0},
year = {2018}
}
@article{Kuijf2019,
abstract = {Quantification of cerebral white matter hyperintensities (WMH) of presumed vascular origin is of key importance in many neurological research studies. Currently, measurements are often still obtained from manual segmentations on brain MR images, which is a laborious procedure. The automatic WMH segmentation methods exist, but a standardized comparison of the performance of such methods is lacking. We organized a scientific challenge, in which developers could evaluate their methods on a standardized multi-center/-scanner image dataset, giving an objective comparison: the WMH Segmentation Challenge. Sixty T1 + FLAIR images from three MR scanners were released with the manual WMH segmentations for training. A test set of 110 images from five MR scanners was used for evaluation. The segmentation methods had to be containerized and submitted to the challenge organizers. Five evaluation metrics were used to rank the methods: 1) Dice similarity coefficient; 2) modified Hausdorff distance (95th percentile); 3) absolute log-transformed volume difference; 4) sensitivity for detecting individual lesions; and 5) F1-score for individual lesions. In addition, the methods were ranked on their inter-scanner robustness; 20 participants submitted their methods for evaluation. This paper provides a detailed analysis of the results. In brief, there is a cluster of four methods that rank significantly better than the other methods, with one clear winner. The inter-scanner robustness ranking shows that not all the methods generalize to unseen scanners. The challenge remains open for future submissions and provides a public platform for method evaluation.},
archivePrefix = {arXiv},
arxivId = {1904.00682},
author = {Kuijf, Hugo J. and Casamitjana, Adri{\`{a}} and Collins, D. Louis and Dadar, Mahsa and Georgiou, Achilleas and Ghafoorian, Mohsen and Jin, Dakai and Khademi, April and Knight, Jesse and Li, Hongwei and Llad{\'{o}}, Xavier and Biesbroek, J. Matthijs and Luna, Miguel and Mahmood, Qaiser and Mckinley, Richard and Mehrtash, Alireza and Ourselin, Sebastien and Park, Bo Yong and Park, Hyunjin and Park, Sang Hyun and Pezold, Simon and Puybareau, Elodie and {De Bresser}, Jeroen and Rittner, Leticia and Sudre, Carole H. and Valverde, Sergi and Vilaplana, Veronica and Wiest, Roland and Xu, Yongchao and Xu, Ziyue and Zeng, Guodong and Zhang, Jianguo and Zheng, Guoyan and Heinen, Rutger and Chen, Christopher and {Van Der Flier}, Wiesje and Barkhof, Frederik and Viergever, Max A. and Biessels, Geert Jan and Andermatt, Simon and Bento, Mariana and Berseth, Matt and Belyaev, Mikhail and Cardoso, M. Jorge},
doi = {10.1109/TMI.2019.2905770},
eprint = {1904.00682},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Magnetic resonance imaging (MRI),brain,evaluation and performance,segmentation},
month = {nov},
number = {11},
pages = {2556--2568},
title = {{Standardized Assessment of Automatic Segmentation of White Matter Hyperintensities and Results of the WMH Segmentation Challenge}},
url = {https://ieeexplore.ieee.org/document/8669968/},
volume = {38},
year = {2019}
}
@article{Balocco2014,
abstract = {This paper describes an evaluation framework that allows a standardized and quantitative comparison of IVUS lumen and media segmentation algorithms. This framework has been introduced at the MICCAI 2011 Computing and Visualization for (Intra)Vascular Imaging (CVII) workshop, comparing the results of eight teams that participated.We describe the available data-base comprising of multi-center, multi-vendor and multi-frequency IVUS datasets, their acquisition, the creation of the reference standard and the evaluation measures. The approaches address segmentation of the lumen, the media, or both borders; semi- or fully-automatic operation; and 2-D vs. 3-D methodology. Three performance measures for quantitative analysis have been proposed. The results of the evaluation indicate that segmentation of the vessel lumen and media is possible with an accuracy that is comparable to manual annotation when semi-automatic methods are used, as well as encouraging results can be obtained also in case of fully-automatic segmentation. The analysis performed in this paper also highlights the challenges in IVUS segmentation that remains to be solved. {\textcopyright} 2013 Elsevier Ltd.},
author = {Balocco, Simone and Gatta, Carlo and Ciompi, Francesco and Wahle, Andreas and Radeva, Petia and Carlier, Stephane and Unal, Gozde and Sanidas, Elias and Mauri, Josepa and Carillo, Xavier and Kovarnik, Tomas and Wang, Ching Wei and Chen, Hsiang Chou and Exarchos, Themis P. and Fotiadis, Dimitrios I. and Destrempes, Fran{\c{c}}ois and Cloutier, Guy and Pujol, Oriol and Alberti, Marina and Mendizabal-Ruiz, E. Gerardo and Rivera, Mariano and Aksoy, Timur and Downe, Richard W. and Kakadiaris, Ioannis A.},
doi = {10.1016/j.compmedimag.2013.07.001},
issn = {08956111},
journal = {Computerized Medical Imaging and Graphics},
keywords = {Algorithm comparison,Evaluation framework,IVUS (intravascular ultrasound),Image segmentation},
month = {mar},
number = {2},
pages = {70--90},
title = {{Standardized evaluation methodology and reference database for evaluating IVUS image segmentation}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0895611113001298},
volume = {38},
year = {2014}
}
@article{Bron2015,
abstract = {Algorithms for computer-aided diagnosis of dementia based on structural MRI have demonstrated high performance in the literature, but are difficult to compare as different data sets and methodology were used for evaluation. In addition, it is unclear how the algorithms would perform on previously unseen data, and thus, how they would perform in clinical practice when there is no real opportunity to adapt the algorithm to the data at hand. To address these comparability, generalizability and clinical applicability issues, we organized a grand challenge that aimed to objectively compare algorithms based on a clinically representative multi-center data set. Using clinical practice as the starting point, the goal was to reproduce the clinical diagnosis. Therefore, we evaluated algorithms for multi-class classification of three diagnostic groups: patients with probable Alzheimer's disease, patients with mild cognitive impairment and healthy controls. The diagnosis based on clinical criteria was used as reference standard, as it was the best available reference despite its known limitations. For evaluation, a previously unseen test set was used consisting of 354 T1-weighted MRI scans with the diagnoses blinded. Fifteen research teams participated with a total of 29 algorithms. The algorithms were trained on a small training set (n. =. 30) and optionally on data from other sources (e.g., the Alzheimer's Disease Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle flagship study of aging). The best performing algorithm yielded an accuracy of 63.0{\%} and an area under the receiver-operating-characteristic curve (AUC) of 78.8{\%}. In general, the best performances were achieved using feature extraction based on voxel-based morphometry or a combination of features that included volume, cortical thickness, shape and intensity. The challenge is open for new submissions via the web-based framework: http://caddementia.grand-challenge.org.},
author = {Bron, Esther E. and Smits, Marion and van der Flier, Wiesje M. and Vrenken, Hugo and Barkhof, Frederik and Scheltens, Philip and Papma, Janne M. and Steketee, Rebecca M.E. and {M{\'{e}}ndez Orellana}, Carolina and Meijboom, Rozanna and Pinto, Madalena and Meireles, Joana R. and Garrett, Carolina and Bastos-Leite, Ant{\'{o}}nio J. and Abdulkadir, Ahmed and Ronneberger, Olaf and Amoroso, Nicola and Bellotti, Roberto and C{\'{a}}rdenas-Pe{\~{n}}a, David and {\'{A}}lvarez-Meza, Andr{\'{e}}s M. and Dolph, Chester V. and Iftekharuddin, Khan M. and Eskildsen, Simon F. and Coup{\'{e}}, Pierrick and Fonov, Vladimir S. and Franke, Katja and Gaser, Christian and Ledig, Christian and Guerrero, Ricardo and Tong, Tong and Gray, Katherine R. and Moradi, Elaheh and Tohka, Jussi and Routier, Alexandre and Durrleman, Stanley and Sarica, Alessia and {Di Fatta}, Giuseppe and Sensi, Francesco and Chincarini, Andrea and Smith, Garry M. and Stoyanov, Zhivko V. and S{\o}rensen, Lauge and Nielsen, Mads and Tangaro, Sabina and Inglese, Paolo and Wachinger, Christian and Reuter, Martin and van Swieten, John C. and Niessen, Wiro J. and Klein, Stefan},
doi = {10.1016/j.neuroimage.2015.01.048},
issn = {10959572},
journal = {NeuroImage},
keywords = {Alzheimer's disease,Challenge,Classification,Computer-aided diagnosis,Mild cognitive impairment,Structural MRI},
month = {may},
pages = {562--579},
title = {{Standardized evaluation of algorithms for computer-aided diagnosis of dementia based on structural MRI: The CADDementia challenge}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811915000737},
volume = {111},
year = {2015}
}
@incollection{Theodoridis2015,
abstract = {http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/},
author = {Theodoridis, Sergios},
booktitle = {Machine Learning},
doi = {10.1016/b978-0-12-801522-3.00005-7},
month = {aug},
pages = {161--231},
title = {{Stochastic Gradient Descent}},
url = {https://en.wikipedia.org/wiki/Stochastic{\_}gradient{\_}descent{\#}Extensions{\_}and{\_}variants},
year = {2015}
}
@incollection{Bottou2012,
abstract = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.},
author = {Bottou, L{\'{e}}on},
booktitle = {Neural Networks: Tricks of the Trade - Second Edition},
doi = {10.1007/978-3-642-35289-8_25},
pages = {421--436},
title = {{Stochastic Gradient Descent Tricks}},
url = {https://doi.org/10.1007/978-3-642-35289-8{\_}25},
year = {2012}
}
@article{Vahadane2016,
abstract = {Staining and scanning of tissue samples for microscopic examination is fraught with undesirable color variations arising from differences in raw materials and manufacturing techniques of stain vendors, staining protocols of labs, and color responses of digital scanners. When comparing tissue samples, color normalization and stain separation of the tissue images can be helpful for both pathologists and software. Techniques that are used for natural images fail to utilize structural properties of stained tissue samples and produce undesirable color distortions. The stain concentration cannot be negative. Tissue samples are stained with only a few stains and most tissue regions are characterized by at most one effective stain. We model these physical phenomena that define the tissue structure by first decomposing images in an unsupervised manner into stain density maps that are sparse and non-negative. For a given image, we combine its stain density maps with stain color basis of a pathologist-preferred target image, thus altering only its color while preserving its structure described by the maps. Stain density correlation with ground truth and preference by pathologists were higher for images normalized using our method when compared to other alternatives. We also propose a computationally faster extension of this technique for large whole-slide images that selects an appropriate patch sample instead of using the entire image to compute the stain color basis.},
author = {Vahadane, Abhishek and Peng, Tingying and Sethi, Amit and Albarqouni, Shadi and Wang, Lichao and Baust, Maximilian and Steiger, Katja and Schlitter, Anna Melissa and Esposito, Irene and Navab, Nassir},
doi = {10.1109/TMI.2016.2529665},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Color normalization,Unsupervised stain separation,histopathological images,non-negative matrix factorization,sparse regularization},
number = {8},
pages = {1962--1971},
title = {{Structure-Preserving Color Normalization and Sparse Stain Separation for Histological Images}},
volume = {35},
year = {2016}
}
@incollection{Fisher2020,
abstract = {Sub-pixel estimation is the process of estimating the value of a geometric quantity to better than pixel accuracy, even though the data was originally sampled on an integer pixel quantized space.},
address = {Cham},
author = {Fisher, Robert B.},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_189-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Subpixel Estimation}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}189-1},
year = {2020}
}
@incollection{Fukui2020,
address = {Cham},
author = {Fukui, Kazuhiro},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_708-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Subspace Methods}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}708-1},
year = {2020}
}
@article{Cortes1995,
abstract = {In this paper, the optimal margin algorithm is generalized$\backslash$nto non-separable problems by the introduction of slack$\backslash$nvariables in the statement of the optimization problem.},
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1007/bf00994018},
issn = {0885-6125},
journal = {Machine Learning},
number = {3},
pages = {273--297},
title = {{Support-vector networks}},
volume = {20},
year = {1995}
}
@article{Corneanu2016,
abstract = {Facial expressions are an important way through which humans interact socially. Building a system capable of automatically recognizing facial expressions from images and video has been an intense field of study in recent years. Interpreting such expressions remains challenging and much research is needed about the way they relate to human affect. This paper presents a general overview of automatic RGB, 3D, thermal and multimodal facial expression analysis. We define a new taxonomy for the field, encompassing all steps from face detection to facial expression recognition, and describe and classify the state of the art methods accordingly. We also present the important datasets and the bench-marking of most influential methods. We conclude with a general discussion about trends, important questions and future lines of research.},
archivePrefix = {arXiv},
arxivId = {1606.03237},
author = {Corneanu, Ciprian Adrian and Sim{\'{o}}n, Marc Oliu and Cohn, Jeffrey F. and Guerrero, Sergio Escalera},
doi = {10.1109/TPAMI.2016.2515606},
eprint = {1606.03237},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {3D,Facial expression,RGB,affect,emotion recognition,multimodal,thermal},
month = {aug},
number = {8},
pages = {1548--1568},
publisher = {IEEE Computer Society},
title = {{Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-Related Applications}},
volume = {38},
year = {2016}
}
@inproceedings{Yi2016,
abstract = {In this paper, we study the problem of semantic annotation on 3D models that are represented as shape graphs. A functional view is taken to represent localized information on graphs, so that annotations such as part segment or keypoint are nothing but 0-1 indicator vertex functions. Compared with images that are 2D grids, shape graphs are irregular and non-isomorphic data structures. To enable the prediction of vertex functions on them by convolutional neural networks, we resort to spectral CNN method that enables weight sharing by parametrizing kernels in the spectral domain spanned by graph Laplacian eigenbases. Under this setting, our network, named SyncSpecCNN, strives to overcome two key challenges: how to share coefficients and conduct multi-scale analysis in different parts of the graph for a single shape, and how to share information across related but different shapes that may be represented by very different graphs. Towards these goals, we introduce a spectral parametrization of dilated convolutional kernels and a spectral transformer network. Experimentally we tested SyncSpecCNN on various tasks, including 3D shape part segmentation and keypoint prediction. State-of-the-art performance has been achieved on all benchmark datasets.},
annote = {{\_}eprint: 1612.00606},
archivePrefix = {arXiv},
arxivId = {1612.00606},
author = {Yi, Li and Su, Hao and Guo, Xingwen and Guibas, Leonidas},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.697},
eprint = {1612.00606},
isbn = {9781538604571},
pages = {6584--6592},
title = {{SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation}},
url = {http://arxiv.org/abs/1612.00606},
volume = {2017-Janua},
year = {2017}
}
@book{Ramage2020,
abstract = {In our profoundly complex and interconnected world, there is a pressing need for systems thinking, to consider environmental, societal and organisational issues as interconnected wholes rather than separating them into parts and looking at each in isolation. This book presents a biographical history of systems thinking, by examining the life and work of thirty of its major thinkers. Systems Thinkers discusses each thinker's key contributions, the way this contribution was expressed in practice and the relationship between their life and ideas. This discussion is supported by an extract from the thinker's own writing, to give a flavour of their work and to give readers a sense of which thinkers are most relevant to their own interests. Systems thinking is highly interdisciplinary, so the thinkers selected come from a wide range of areas, including biology, management, physiology, anthropology, chemistry, public policy, sociology and environmental studies. Some are core innovators in systems ideas; some have been primarily practitioners who also advanced and popularised systems ideas; others are well-known figures who drew heavily upon systems thinking although it was not their primary discipline. The book provides an appetising ‘taster' of the writings of each of the thirty thinkers, to encourage the reader to explore the published works of the thinkers themselves. This second edition has been updated to reflect continuing scholarship in the academic community about the thirty thinkers, and in some cases new writing by them, bringing fresh insights about these inspiring and deeply relevant figures who challenged accepted ways of thinking and seeing, and continue to do so.},
address = {London},
author = {Ramage, Magnus and Shipp, Karen},
doi = {10.1007/978-1-4471-7475-2},
isbn = {978-1-4471-7474-5},
pages = {1--324},
publisher = {Springer London},
title = {{Systems Thinkers 2nd ed}},
url = {http://link.springer.com/10.1007/978-1-4471-7475-2},
year = {2020}
}
@online{TADPOLE,
title = {{TADPOLE - Home}},
url = {https://tadpole.grand-challenge.org/},
urldate = {2020-05-25}
}
@inproceedings{Abadi2016,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, generalpurpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that Tensor-Flow achieves for several real-world applications.},
annote = {{\_}eprint: 1605.08695},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek Gordon and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
booktitle = {Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016},
eprint = {1605.08695},
isbn = {9781931971331},
pages = {265--283},
title = {{TensorFlow: A system for large-scale machine learning}},
url = {http://arxiv.org/abs/1605.08695},
volume = {abs/1605.0},
year = {2016}
}
@article{Vallet2015,
abstract = {The objective of the TerraMobilita/iQmulus 3D urban analysis benchmark is to evaluate the current state of the art in urban scene analysis from mobile laser scanning (MLS) at large scale. A very detailed semantic tree for urban scenes is proposed. We call analysis the capacity of a method to separate the points of the scene into these categories (classification), and to separate the different objects of the same type for object classes (detection). A very large ground truth is produced manually in two steps using advanced editing tools developed especially for this benchmark. Based on this ground truth, the benchmark aims at evaluating the classification, detection and segmentation quality of the submitted results.},
author = {Vallet, Bruno and Br{\'{e}}dif, Mathieu and Serna, Andres and Marcotegui, Beatriz and Paparoditis, Nicolas},
doi = {10.1016/j.cag.2015.03.004},
issn = {00978493},
journal = {Computers and Graphics (Pergamon)},
keywords = {Benchmark,Classification,Laser scanning,Mobile mapping,Segmentation,Urban scene},
pages = {126--133},
title = {{TerraMobilita/iQmulus urban point cloud analysis benchmark}},
volume = {49},
year = {2015}
}
@article{Ye2015,
abstract = {This paper analyzes, compares, and contrasts technical challenges, methods, and the performance of text detection and recognition research in color imagery. It summarizes the fundamental problems and enumerates factors that should be considered when addressing these problems. Existing techniques are categorized as either stepwise or integrated and sub-problems are highlighted including text localization, verification, segmentation and recognition. Special issues associated with the enhancement of degraded text and the processing of video text, multi-oriented, perspectively distorted and multilingual text are also addressed. The categories and sub-categories of text are illustrated, benchmark datasets are enumerated, and the performance of the most representative approaches is compared. This review provides a fundamental comparison and analysis of the remaining problems in the field.},
author = {Ye, Qixiang and Doermann, David},
doi = {10.1109/TPAMI.2014.2366765},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Survey,Text Detection,Text Localization,Text Recognition},
month = {jul},
number = {7},
pages = {1480--1500},
publisher = {IEEE Computer Society},
title = {{Text Detection and Recognition in Imagery: A Survey}},
volume = {37},
year = {2015}
}
@article{Mueller2005,
abstract = {With increasing life expectancy in developed countries, the incidence of Alzheimer's disease (AD) and its socioeconomic impact are growing. Increasing knowledge of the mechanisms of AD facilitates the development of treatment strategies aimed at slowing down or preventing neuronal death. AD treatment trials using clinical outcome measures require long observation times and large patient samples. There is increasing evidence that neuroimaging and cerebrospinal fluid and blood biomarkers may provide information that may reduce sample sizes and observation periods. The Alzheimer's Disease Neuroimaging Initiative will help identify clinical, neuroimaging, and biomarker outcome measures that provide the highest power for measurement of longitudinal changes and for prediction of transitions. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {Mueller, Susanne G. and Weiner, Michael W. and Thal, Leon J. and Petersen, Ronald C. and Jack, Clifford and Jagust, William and Trojanowski, John Q. and Toga, Arthur W. and Beckett, Laurel},
doi = {10.1016/j.nic.2005.09.008},
issn = {10525149},
journal = {Neuroimaging Clinics of North America},
number = {4},
pages = {869--877},
pmid = {16443497},
title = {{The Alzheimer's disease neuroimaging initiative}},
volume = {15},
year = {2005}
}
@article{Weiner2017,
abstract = {Introduction The overall goal of the Alzheimer's Disease Neuroimaging Initiative (ADNI) is to validate biomarkers for Alzheimer's disease (AD) clinical trials. ADNI-3, which began on August 1, 2016, is a 5-year renewal of the current ADNI-2 study. Methods ADNI-3 will follow current and additional subjects with normal cognition, mild cognitive impairment, and AD using innovative technologies such as tau imaging, magnetic resonance imaging sequences for connectivity analyses, and a highly automated immunoassay platform and mass spectroscopy approach for cerebrospinal fluid biomarker analysis. A Systems Biology/pathway approach will be used to identify genetic factors for subject selection/enrichment. Amyloid positron emission tomography scanning will be standardized using the Centiloid method. The Brain Health Registry will help recruit subjects and monitor subject cognition. Results Multimodal analyses will provide insight into AD pathophysiology and disease progression. Discussion ADNI-3 will aim to inform AD treatment trials and facilitate development of AD disease-modifying treatments.},
author = {Weiner, Michael W. and Veitch, Dallas P. and Aisen, Paul S. and Beckett, Laurel A. and Cairns, Nigel J. and Green, Robert C. and Harvey, Danielle and Jack, Clifford R. and Jagust, William and Morris, John C. and Petersen, Ronald C. and Salazar, Jennifer and Saykin, Andrew J. and Shaw, Leslie M. and Toga, Arthur W. and Trojanowski, John Q.},
doi = {10.1016/j.jalz.2016.10.006},
issn = {15525279},
journal = {Alzheimer's and Dementia},
keywords = {Alzheimer's disease,Amyloid phenotyping,Brain Health Registry,Centiloid method,Clinical trial biomarkers,Functional connectivity,Tau imaging},
month = {may},
number = {5},
pages = {561--571},
pmid = {27931796},
title = {{The Alzheimer's Disease Neuroimaging Initiative 3: Continued innovation for clinical trial improvement}},
url = {http://doi.wiley.com/10.1016/j.jalz.2016.10.006},
volume = {13},
year = {2017}
}
@article{Marinescu2020,
abstract = {We present the findings of "The Alzheimer's Disease Prediction Of Longitudinal Evolution" (TADPOLE) Challenge, which compared the performance of 92 algorithms from 33 international teams at predicting the future trajectory of 219 individuals at risk of Alzheimer's disease. Challenge participants were required to make a prediction, for each month of a 5-year future time period, of three key outcomes: clinical diagnosis, Alzheimer's Disease Assessment Scale Cognitive Subdomain (ADAS-Cog13), and total volume of the ventricles. No single submission was best at predicting all three outcomes. For clinical diagnosis and ventricle volume prediction, the best algorithms strongly outperform simple baselines in predictive ability. However, for ADAS-Cog13 no single submitted prediction method was significantly better than random guessing. Two ensemble methods based on taking the mean and median over all predictions, obtained top scores on almost all tasks. Better than average performance at diagnosis prediction was generally associated with the additional inclusion of features from cerebrospinal fluid (CSF) samples and diffusion tensor imaging (DTI). On the other hand, better performance at ventricle volume prediction was associated with inclusion of summary statistics, such as patient-specific biomarker trends. The submission system remains open via the website https://tadpole.grand-challenge.org, while code for submissions is being collated by TADPOLE SHARE: https://tadpole-share.github.io/. Our work suggests that current prediction algorithms are accurate for biomarkers related to clinical diagnosis and ventricle volume, opening up the possibility of cohort refinement in clinical trials for Alzheimer's disease.},
archivePrefix = {arXiv},
arxivId = {2002.03419},
author = {Marinescu, Razvan V. and Oxtoby, Neil P. and Young, Alexandra L. and Bron, Esther E. and Toga, Arthur W. and Weiner, Michael W. and Barkhof, Frederik and Fox, Nick C. and Eshaghi, Arman and Toni, Tina and Salaterski, Marcin and Lunina, Veronika and Ansart, Manon and Durrleman, Stanley and Lu, Pascal and Iddi, Samuel and Li, Dan and Thompson, Wesley K. and Donohue, Michael C. and Nahon, Aviv and Levy, Yarden and Halbersberg, Dan and Cohen, Mariya and Liao, Huiling and Li, Tengfei and Yu, Kaixian and Zhu, Hongtu and Tamez-Pena, Jose G. and Ismail, Aya and Wood, Timothy and Bravo, Hector Corrada and Nguyen, Minh and Sun, Nanbo and Feng, Jiashi and Yeo, B. T. Thomas and Chen, Gang and Qi, Ke and Chen, Shiyang and Qiu, Deqiang and Buciuman, Ionut and Kelner, Alex and Pop, Raluca and Rimocea, Denisa and Ghazi, Mostafa M. and Nielsen, Mads and Ourselin, Sebastien and Sorensen, Lauge and Venkatraghavan, Vikram and Liu, Keli and Rabe, Christina and Manser, Paul and Hill, Steven M. and Howlett, James and Huang, Zhiyue and Kiddle, Steven and Mukherjee, Sach and Rouanet, Anais and Taschler, Bernd and Tom, Brian D. M. and White, Simon R. and Faux, Noel and Sedai, Suman and Oriol, Javier de Velasco and Clemente, Edgar E. V. and Estrada, Karol and Aksman, Leon and Altmann, Andre and Stonnington, Cynthia M. and Wang, Yalin and Wu, Jianfeng and Devadas, Vivek and Fourrier, Clementine and Raket, Lars Lau and Sotiras, Aristeidis and Erus, Guray and Doshi, Jimit and Davatzikos, Christos and Vogel, Jacob and Doyle, Andrew and Tam, Angela and Diaz-Papkovich, Alex and Jammeh, Emmanuel and Koval, Igor and Moore, Paul and Lyons, Terry J. and Gallacher, John and Tohka, Jussi and Ciszek, Robert and Jedynak, Bruno and Pandya, Kruti and Bilgel, Murat and Engels, William and Cole, Joseph and Golland, Polina and Klein, Stefan and Alexander, Daniel C.},
eprint = {2002.03419},
month = {feb},
title = {{The Alzheimer's Disease Prediction Of Longitudinal Evolution (TADPOLE) Challenge: Results after 1 Year Follow-up}},
url = {http://arxiv.org/abs/2002.03419},
year = {2020}
}
@book{Lantsoght2018,
address = {Cham},
author = {Lantsoght, Eva O. L.},
doi = {10.1007/978-3-319-77425-1},
isbn = {978-3-319-77424-4},
publisher = {Springer International Publishing},
series = {Springer Texts in Education},
title = {{The A-Z of the PhD Trajectory}},
url = {http://link.springer.com/10.1007/978-3-319-77425-1},
year = {2018}
}
@article{Clark2013,
abstract = {The National Institutes of Health have placed significant emphasis on sharing of research data to support secondary research. Investigators have been encouraged to publish their clinical and imaging data as part of fulfilling their grant obligations. Realizing it was not sufficient to merely ask investigators to publish their collection of imaging and clinical data, the National Cancer Institute (NCI) created the open source National Biomedical Image Archive software package as a mechanism for centralized hosting of cancer related imaging. NCI has contracted with Washington University in Saint Louis to create The Cancer Imaging Archive (TCIA) - an open-source, open-access information resource to support research, development, and educational initiatives utilizing advanced medical imaging of cancer. In its first year of operation, TCIA accumulated 23 collections (3.3 million images). Operating and maintaining a high-availability image archive is a complex challenge involving varied archive-specific resources and driven by the needs of both image submitters and image consumers. Quality archives of any type (traditional library, PubMed, refereed journals) require management and customer service. This paper describes the management tasks and user support model for TCIA. {\textcopyright} 2013 Society for Imaging Informatics in Medicine.},
author = {Clark, Kenneth and Vendt, Bruce and Smith, Kirk and Freymann, John and Kirby, Justin and Koppel, Paul and Moore, Stephen and Phillips, Stanley and Maffitt, David and Pringle, Michael and Tarbox, Lawrence and Prior, Fred},
doi = {10.1007/s10278-013-9622-7},
issn = {08971889},
journal = {Journal of Digital Imaging},
keywords = {Biomedical image analysis,Cancer detection,Cancer imaging,Image archive,NBIA,TCIA},
month = {dec},
number = {6},
pages = {1045--1057},
pmid = {23884657},
title = {{The cancer imaging archive (TCIA): Maintaining and operating a public information repository}},
url = {https://doi.org/10.1007/s10278-013-9622-7 http://link.springer.com/10.1007/s10278-013-9622-7},
volume = {26},
year = {2013}
}
@article{Clark2013,
abstract = {The National Institutes of Health have placed significant emphasis on sharing of research data to support secondary research. Investigators have been encouraged to publish their clinical and imaging data as part of fulfilling their grant obligations. Realizing it was not sufficient to merely ask investigators to publish their collection of imaging and clinical data, the National Cancer Institute (NCI) created the open source National Biomedical Image Archive software package as a mechanism for centralized hosting of cancer related imaging. NCI has contracted with Washington University in Saint Louis to create The Cancer Imaging Archive (TCIA) - an open-source, open-access information resource to support research, development, and educational initiatives utilizing advanced medical imaging of cancer. In its first year of operation, TCIA accumulated 23 collections (3.3 million images). Operating and maintaining a high-availability image archive is a complex challenge involving varied archive-specific resources and driven by the needs of both image submitters and image consumers. Quality archives of any type (traditional library, PubMed, refereed journals) require management and customer service. This paper describes the management tasks and user support model for TCIA. {\textcopyright} 2013 Society for Imaging Informatics in Medicine.},
author = {Clark, Kenneth and Vendt, Bruce and Smith, Kirk and Freymann, John and Kirby, Justin and Koppel, Paul and Moore, Stephen and Phillips, Stanley and Maffitt, David and Pringle, Michael and Tarbox, Lawrence and Prior, Fred},
doi = {10.1007/s10278-013-9622-7},
issn = {08971889},
journal = {Journal of Digital Imaging},
keywords = {Biomedical image analysis,Cancer detection,Cancer imaging,Image archive,NBIA,TCIA},
number = {6},
pages = {1045--1057},
pmid = {23884657},
title = {{The cancer imaging archive (TCIA): Maintaining and operating a public information repository}},
url = {https://doi.org/10.1007/s10278-013-9622-7},
volume = {26},
year = {2013}
}
@book{Alley2018,
address = {New York, NY},
author = {Alley, Michael},
doi = {10.1007/978-1-4419-8288-9},
isbn = {978-1-4419-8287-2},
publisher = {Springer New York},
title = {{The Craft of Scientific Writing}},
url = {http://link.springer.com/10.1007/978-1-4419-8288-9},
year = {2018}
}
@article{Lowekamp2013,
abstract = {SimpleITK is a new interface to the Insight Segmentation and Registration Toolkit (ITK) designed to facilitate rapid prototyping, education and scientific activities via high level programming languages. ITK is a templated C++ library of image processing algorithms and frameworks for biomedical and other applications, and it was designed to be generic, flexible and extensible. Initially, ITK provided a direct wrapping interface to languages such as Python and Tcl through the WrapITK system. Unlike WrapITK, which exposed ITK's complex templated interface, SimpleITK was designed to provide an easy to use and simplified interface to ITK's algorithms. It includes procedural methods, hides ITK's demand driven pipeline, and provides a template-less layer. Also SimpleITK provides practical conveniences such as binary distribution packages and overloaded operators. Our user-friendly design goals dictated a departure from the direct interface wrapping approach of WrapITK, toward a new facade class structure that only exposes the required functionality, hiding ITK's extensive template use. Internally SimpleITK utilizes a manual description of each filter with code-generation and advanced C++ meta-programming to provide the higher-level interface, bringing the capabilities of ITK to a wider audience. SimpleITK is licensed as open source software library under the Apache License Version 2.0 and more information about downloading it can be found at http://www.simpleitk.org. {\textcopyright} 2013 Lowek amp, Chen, Ib{\'{a}}{\~{n}}ez and Blezek.},
author = {Lowekamp, Bradley C. and Chen, David T. and Ib{\'{a}}{\~{n}}ez, Luis and Blezek, Daniel},
doi = {10.3389/fninf.2013.00045},
issn = {16625196},
journal = {Frontiers in Neuroinformatics},
keywords = {Image processing and analysis,Image processing software,Insight toolkit,Segmentation,Software design,Software development},
number = {DEC},
pages = {45},
title = {{The design of simpleITK}},
volume = {7},
year = {2013}
}
@misc{LeCun1998b,
address = {Cambridge, MA, USA},
annote = {Section: Convolutional Networks for Images, Speech, and Time Series},
author = {Andrew, Alex M.},
booktitle = {Kybernetes},
doi = {10.1108/k.1999.28.9.1084.1},
editor = {Arbib, Michael A},
isbn = {0-262-51102-9},
issn = {0368492X},
keywords = {Artificial intelligence,Brain,Cybernetics,Neural networks,Publication},
number = {9},
pages = {1084--1094},
publisher = {MIT Press},
title = {{The Handbook of Brain Theory and Neural Networks}},
url = {http://dl.acm.org/citation.cfm?id=303568.303704},
volume = {28},
year = {1999}
}
@article{Shackman2011,
abstract = {It has been argued that emotion, pain and cognitive control are functionally segregated in distinct subdivisions of the cingulate cortex. However, recent observations encourage a fundamentally different view. Imaging studies demonstrate that negative affect, pain and cognitive control activate an overlapping region of the dorsal cingulate - the anterior midcingulate cortex (aMCC). Anatomical studies reveal that the aMCC constitutes a hub where information about reinforcers can be linked to motor centres responsible for expressing affect and executing goal-directed behaviour. Computational modelling and other kinds of evidence suggest that this intimacy reflects control processes that are common to all three domains. These observations compel a reconsideration of the dorsal cingulate's contribution to negative affect and pain. {\textcopyright} 2011 Macmillan Publishers Limited. All rights reserved.},
author = {Shackman, Alexander J. and Salomons, Tim V. and Slagter, Heleen A. and Fox, Andrew S. and Winter, Jameel J. and Davidson, Richard J.},
doi = {10.1038/nrn2994},
issn = {1471003X},
journal = {Nature Reviews Neuroscience},
month = {mar},
number = {3},
pages = {154--167},
pmid = {21331082},
title = {{The integration of negative affect, pain and cognitive control in the cingulate cortex}},
url = {http://www.nature.com/articles/nrn2994},
volume = {12},
year = {2011}
}
@misc{Ibanez2003,
abstract = {Everything you need to install, use, and extend the Insight Segmentation and Registration Toolikit ITK. Includes detailed examples, installation procedures, and system overview for ITK version 2.4. (The included examples are taken directly from the ITK source code repository and are designed to demonstrate the essential features of the software.) The book comes with a CD-ROM that contains a complete hyperlinked version of the book plus ITK source code, data, Windows binaries, and extensive class documentation. Also includes CMake binaries for managing the ITK build process on a variety of compiler and operating system configurations.},
author = {Ibanez, L and Schroeder, W and Ng, L and Cates, J},
booktitle = {The ITK Software Guide},
doi = {1�930934-15�7},
edition = {First},
isbn = {1930934157},
issn = {10445323},
number = {May},
pages = {804},
pmid = {1000070720},
publisher = {Kitware, Inc.},
title = {{The ITK Software Guide}},
url = {http://www.itk.org/ItkSoftwareGuide.pdf},
volume = {Second},
year = {2005}
}
@misc{Trikha2013,
abstract = {Femtosecond laser-assisted cataract surgery (FLACS) represents a potential paradigm shift in cataract surgery, but it is not without controversy. Advocates of the technology herald FLACS as a revolution that promises superior outcomes and an improved safety profile for patients. Conversely, detractors point to the large financial costs involved and claim that similar results are achievable with conventional small-incision phacoemulsification. This review provides a balanced and comprehensive account of the development of FLACS since its inception. It explains the physiology and mechanics underlying the technology, and critically reviews the outcomes and implications of initial studies. The benefits and limitations of using femtosecond laser accuracy to create corneal incisions, anterior capsulotomy, and lens fragmentation are explored, with reference to the main platforms, which currently offer FLACS. Economic considerations are discussed, in addition to the practicalities associated with the implementation of FLACS in a healthcare setting. The influence on surgical training and skills is considered and possible future applications of the technology introduced. While in its infancy, FLACS sets out the exciting possibility of a new level of precision in cataract surgery. However, further work in the form of large scale, phase 3 randomised controlled trials are required to demonstrate whether its theoretical benefits are significant in practice and worthy of the necessary huge financial investment and system overhaul. Whether it gains widespread acceptance is likely to be influenced by a complex interplay of scientific and socio-economic factors in years to come. {\textcopyright} 2013 Macmillan Publishers Limited All rights reserved 0950-222X/13.},
author = {Trikha, S. and Turnbull, A. M.J. and Morris, R. J. and Anderson, D. F. and Hossain, P.},
booktitle = {Eye (Basingstoke)},
doi = {10.1038/eye.2012.293},
issn = {14765454},
keywords = {capsulorhexis,capsulotomy,cataract surgery,femtosecond,laser,phacoemulsfication},
title = {{The journey to femtosecond laser-assisted cataract surgery: New beginnings or a false dawn?}},
year = {2013}
}
@misc{Trikha2013a,
abstract = {Femtosecond laser-assisted cataract surgery (FLACS) represents a potential paradigm shift in cataract surgery, but it is not without controversy. Advocates of the technology herald FLACS as a revolution that promises superior outcomes and an improved safety profile for patients. Conversely, detractors point to the large financial costs involved and claim that similar results are achievable with conventional small-incision phacoemulsification. This review provides a balanced and comprehensive account of the development of FLACS since its inception. It explains the physiology and mechanics underlying the technology, and critically reviews the outcomes and implications of initial studies. The benefits and limitations of using femtosecond laser accuracy to create corneal incisions, anterior capsulotomy, and lens fragmentation are explored, with reference to the main platforms, which currently offer FLACS. Economic considerations are discussed, in addition to the practicalities associated with the implementation of FLACS in a healthcare setting. The influence on surgical training and skills is considered and possible future applications of the technology introduced. While in its infancy, FLACS sets out the exciting possibility of a new level of precision in cataract surgery. However, further work in the form of large scale, phase 3 randomised controlled trials are required to demonstrate whether its theoretical benefits are significant in practice and worthy of the necessary huge financial investment and system overhaul. Whether it gains widespread acceptance is likely to be influenced by a complex interplay of scientific and socio-economic factors in years to come. {\textcopyright} 2013 Macmillan Publishers Limited All rights reserved 0950-222X/13.},
author = {Trikha, S. and Turnbull, A. M.J. and Morris, R. J. and Anderson, D. F. and Hossain, P.},
booktitle = {Eye (Basingstoke)},
doi = {10.1038/eye.2012.293},
issn = {14765454},
keywords = {capsulorhexis,capsulotomy,cataract surgery,femtosecond,laser,phacoemulsfication},
title = {{The journey to femtosecond laser-assisted cataract surgery: New beginnings or a false dawn?}},
year = {2013}
}
@article{Heller2019a,
abstract = {The morphometry of a kidney tumor revealed by contrast-enhanced Computed Tomography (CT) imaging is an important factor in clinical decision making surrounding the lesion's diagnosis and treatment. Quantitative study of the relationship between kidney tumor morphology and clinical outcomes is difficult due to data scarcity and the laborious nature of manually quantifying imaging predictors. Automatic semantic segmentation of kidneys and kidney tumors is a promising tool towards automatically quantifying a wide array of morphometric features, but no sizeable annotated dataset is currently available to train models for this task. We present the KiTS19 challenge dataset: A collection of multi-phase CT imaging, segmentation masks, and comprehensive clinical outcomes for 300 patients who underwent nephrectomy for kidney tumors at our center between 2010 and 2018. 210 (70{\%}) of these patients were selected at random as the training set for the 2019 MICCAI KiTS Kidney Tumor Segmentation Challenge and have been released publicly. With the presence of clinical context and surgical outcomes, this data can serve not only for benchmarking semantic segmentation models, but also for developing and studying biomarkers which make use of the imaging and semantic segmentation masks.},
archivePrefix = {arXiv},
arxivId = {1904.00445},
author = {Heller, Nicholas and Sathianathen, Niranjan and Kalapara, Arveen and Walczak, Edward and Moore, Keenan and Kaluzniak, Heather and Rosenberg, Joel and Blake, Paul and Rengel, Zachary and Oestreich, Makinna and Dean, Joshua and Tradewell, Michael and Shah, Aneri and Tejpaul, Resha and Edgerton, Zachary and Peterson, Matthew and Raza, Shaneabbas and Regmi, Subodh and Papanikolopoulos, Nikolaos and Weight, Christopher},
eprint = {1904.00445},
month = {mar},
title = {{The KiTS19 Challenge Data: 300 Kidney Tumor Cases with Clinical Context, CT Semantic Segmentations, and Surgical Outcomes}},
url = {http://arxiv.org/abs/1904.00445},
year = {2019}
}
@article{Heller2019,
abstract = {The morphometry of a kidney tumor revealed by contrast-enhanced Computed Tomography (CT) imaging is an important factor in clinical decision making surrounding the lesion's diagnosis and treatment. Quantitative study of the relationship between kidney tumor morphology and clinical outcomes is difficult due to data scarcity and the laborious nature of manually quantifying imaging predictors. Automatic semantic segmentation of kidneys and kidney tumors is a promising tool towards automatically quantifying a wide array of morphometric features, but no sizeable annotated dataset is currently available to train models for this task. We present the KiTS19 challenge dataset: A collection of multi-phase CT imaging, segmentation masks, and comprehensive clinical outcomes for 300 patients who underwent nephrectomy for kidney tumors at our center between 2010 and 2018. 210 (70{\%}) of these patients were selected at random as the training set for the 2019 MICCAI KiTS Kidney Tumor Segmentation Challenge and have been released publicly. With the presence of clinical context and surgical outcomes, this data can serve not only for benchmarking semantic segmentation models, but also for developing and studying biomarkers which make use of the imaging and semantic segmentation masks.},
annote = {{\_}eprint: 1904.00445},
archivePrefix = {arXiv},
arxivId = {1904.00445},
author = {Heller, Nicholas and Sathianathen, Niranjan and Kalapara, Arveen and Walczak, Edward and Moore, Keenan and Kaluzniak, Heather and Rosenberg, Joel and Blake, Paul and Rengel, Zachary and Oestreich, Makinna and Dean, Joshua and Tradewell, Michael and Shah, Aneri and Tejpaul, Resha and Edgerton, Zachary and Peterson, Matthew and Raza, Shaneabbas and Regmi, Subodh and Papanikolopoulos, Nikolaos and Weight, Christopher},
eprint = {1904.00445},
title = {{The KiTS19 Challenge Data: 300 Kidney Tumor Cases with Clinical Context, CT Semantic Segmentations, and Surgical Outcomes}},
url = {http://arxiv.org/abs/1904.00445},
year = {2019}
}
@article{Bilic2019a,
abstract = {In this work, we report the set-up and results of the Liver Tumor Segmentation Benchmark (LITS) organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI) 2016 and International Conference On Medical Image Computing Computer Assisted Intervention (MICCAI) 2017. Twenty four valid state-of-the-art liver and liver tumor segmentation algorithms were applied to a set of 131 computed tomography (CT) volumes with different types of tumor contrast levels (hyper-/hypo-intense), abnormalities in tissues (metastasectomie) size and varying amount of lesions. The submitted algorithms have been tested on 70 undisclosed volumes. The dataset is created in collaboration with seven hospitals and research institutions and manually reviewed by independent three radiologists. We found that not a single algorithm performed best for liver and tumors. The best liver segmentation algorithm achieved a Dice score of 0.96(MICCAI) whereas for tumor segmentation the best algorithm evaluated at 0.67(ISBI) and 0.70(MICCAI). The LITS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
archivePrefix = {arXiv},
arxivId = {1901.04056},
author = {Bilic, Patrick and Christ, Patrick Ferdinand and Vorontsov, Eugene and Chlebus, Grzegorz and Chen, Hao and Dou, Qi and Fu, Chi-Wing and Han, Xiao and Heng, Pheng-Ann and Hesser, J{\"{u}}rgen and Kadoury, Samuel and Konopczynski, Tomasz and Le, Miao and Li, Chunming and Li, Xiaomeng and Lipkov{\`{a}}, Jana and Lowengrub, John and Meine, Hans and Moltz, Jan Hendrik and Pal, Chris and Piraud, Marie and Qi, Xiaojuan and Qi, Jin and Rempfler, Markus and Roth, Karsten and Schenk, Andrea and Sekuboyina, Anjany and Vorontsov, Eugene and Zhou, Ping and H{\"{u}}lsemeyer, Christian and Beetz, Marcel and Ettlinger, Florian and Gruen, Felix and Kaissis, Georgios and Loh{\"{o}}fer, Fabian and Braren, Rickmer and Holch, Julian and Hofmann, Felix and Sommer, Wieland and Heinemann, Volker and Jacobs, Colin and Mamani, Gabriel Efrain Humpire and van Ginneken, Bram and Chartrand, Gabriel and Tang, An and Drozdzal, Michal and Ben-Cohen, Avi and Klang, Eyal and Amitai, Marianne M. and Konen, Eli and Greenspan, Hayit and Moreau, Johan and Hostettler, Alexandre and Soler, Luc and Vivanti, Refael and Szeskin, Adi and Lev-Cohain, Naama and Sosna, Jacob and Joskowicz, Leo and Menze, Bjoern H.},
eprint = {1901.04056},
month = {jan},
title = {{The Liver Tumor Segmentation Benchmark (LiTS)}},
url = {http://arxiv.org/abs/1901.04056},
year = {2019}
}
@article{Bilic2019,
abstract = {In this work, we report the set-up and results of the Liver Tumor Segmentation Benchmark (LITS) organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI) 2016 and International Conference On Medical Image Computing Computer Assisted Intervention (MICCAI) 2017. Twenty four valid state-of-the-art liver and liver tumor segmentation algorithms were applied to a set of 131 computed tomography (CT) volumes with different types of tumor contrast levels (hyper-/hypo-intense), abnormalities in tissues (metastasectomie) size and varying amount of lesions. The submitted algorithms have been tested on 70 undisclosed volumes. The dataset is created in collaboration with seven hospitals and research institutions and manually reviewed by independent three radiologists. We found that not a single algorithm performed best for liver and tumors. The best liver segmentation algorithm achieved a Dice score of 0.96(MICCAI) whereas for tumor segmentation the best algorithm evaluated at 0.67(ISBI) and 0.70(MICCAI). The LITS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
annote = {{\_}eprint: 1901.04056},
archivePrefix = {arXiv},
arxivId = {1901.04056},
author = {Bilic, Patrick and Christ, Patrick Ferdinand and Vorontsov, Eugene and Chlebus, Grzegorz and Chen, Hao and Dou, Qi and Fu, Chi-Wing and Han, Xiao and Heng, Pheng-Ann and Hesser, J{\"{u}}rgen and Kadoury, Samuel and Konopczynski, Tomasz and Le, Miao and Li, Chunming and Li, Xiaomeng and Lipkov{\`{a}}, Jana and Lowengrub, John and Meine, Hans and Moltz, Jan Hendrik and Pal, Chris and Piraud, Marie and Qi, Xiaojuan and Qi, Jin and Rempfler, Markus and Roth, Karsten and Schenk, Andrea and Sekuboyina, Anjany and Vorontsov, Eugene and Zhou, Ping and H{\"{u}}lsemeyer, Christian and Beetz, Marcel and Ettlinger, Florian and Gruen, Felix and Kaissis, Georgios and Loh{\"{o}}fer, Fabian and Braren, Rickmer and Holch, Julian and Hofmann, Felix and Sommer, Wieland and Heinemann, Volker and Jacobs, Colin and Mamani, Gabriel Efrain Humpire and van Ginneken, Bram and Chartrand, Gabriel and Tang, An and Drozdzal, Michal and Ben-Cohen, Avi and Klang, Eyal and Amitai, Marianne M. and Konen, Eli and Greenspan, Hayit and Moreau, Johan and Hostettler, Alexandre and Soler, Luc and Vivanti, Refael and Szeskin, Adi and Lev-Cohain, Naama and Sosna, Jacob and Joskowicz, Leo and Menze, Bjoern H.},
eprint = {1901.04056},
journal = {CoRR},
title = {{The Liver Tumor Segmentation Benchmark (LiTS)}},
url = {http://arxiv.org/abs/1901.04056},
volume = {abs/1901.0},
year = {2019}
}
@inproceedings{Berman2017,
abstract = {The Jaccard index, also referred to as the intersection-over-union score, is commonly employed in the evaluation of image segmentation results given its perceptual qualities, scale invariance - which lends appropriate relevance to small objects, and appropriate counting of false negatives, in comparison to per-pixel losses. We present a method for direct optimization of the mean intersection-over-union loss in neural networks, in the context of semantic image segmentation, based on the convex Lov{\~{A}}¡sz extension of submodular losses. The loss is shown to perform better with respect to the Jaccard index measure than the traditionally used cross-entropy loss. We show quantitative and qualitative differences between optimizing the Jaccard index per image versus optimizing the Jaccard index taken over an entire dataset. We evaluate the impact of our method in a semantic segmentation pipeline and show substantially improved intersection-over-union segmentation scores on the Pascal VOC and Cityscapes datasets using state-of-the-art deep learning segmentation architectures.},
annote = {{\_}eprint: 1705.08790},
archivePrefix = {arXiv},
arxivId = {1705.08790},
author = {Berman, Maxim and Triki, Amal Rannen and Blaschko, Matthew B},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00464},
eprint = {1705.08790},
isbn = {9781538664209},
issn = {10636919},
pages = {4413--4421},
title = {{The Lovasz-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks}},
year = {2018}
}
@article{Menze2015b,
abstract = {In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients - manually annotated by up to four raters - and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74{\%}-85{\%}), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
author = {Menze, Bjoern H. and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Yuliya and Porz, Nicole and Slotboom, Johannes and Wiest, Roland and Lanczi, Levente and Gerstner, Elizabeth and Weber, Marc Andr{\'{e}} and Arbel, Tal and Avants, Brian B. and Ayache, Nicholas and Buendia, Patricia and Collins, D. Louis and Cordier, Nicolas and Corso, Jason J. and Criminisi, Antonio and Das, Tilak and Delingette, Herv{\'{e}} and Demiralp, {\c{C}}ağatay and Durst, Christopher R. and Dojat, Michel and Doyle, Senan and Festa, Joana and Forbes, Florence and Geremia, Ezequiel and Glocker, Ben and Golland, Polina and Guo, Xiaotao and Hamamci, Andac and Iftekharuddin, Khan M. and Jena, Raj and John, Nigel M. and Konukoglu, Ender and Lashkari, Danial and Mariz, Jos{\'{e}} Ant{\'{o}}nio and Meier, Raphael and Pereira, S{\'{e}}rgio and Precup, Doina and Price, Stephen J. and Raviv, Tammy Riklin and Reza, Syed M.S. and Ryan, Michael and Sarikaya, Duygu and Schwartz, Lawrence and Shin, Hoo Chang and Shotton, Jamie and Silva, Carlos A. and Sousa, Nuno and Subbanna, Nagesh K. and Szekely, Gabor and Taylor, Thomas J. and Thomas, Owen M. and Tustison, Nicholas J. and Unal, Gozde and Vasseur, Flor and Wintermark, Max and Ye, Dong Hye and Zhao, Liang and Zhao, Binsheng and Zikic, Darko and Prastawa, Marcel and Reyes, Mauricio and {Van Leemput}, Koen},
doi = {10.1109/TMI.2014.2377694},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Benchmark,Brain,Image segmentation,MRI,Oncology/tumor},
month = {oct},
number = {10},
pages = {1993--2024},
pmid = {25494501},
title = {{The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)}},
url = {http://ieeexplore.ieee.org/document/6975210/},
volume = {34},
year = {2015}
}
@article{VanDerWalt2011,
abstract = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts. {\textcopyright} 2011 IEEE.},
archivePrefix = {arXiv},
arxivId = {1102.1523},
author = {{Van Der Walt}, St{\'{e}}fan and Colbert, S Chris and Varoquaux, Ga{\"{e}}l},
doi = {10.1109/MCSE.2011.37},
eprint = {1102.1523},
issn = {15219615},
journal = {Computing in Science and Engineering},
keywords = {NumPy,Python,numerical computations,programming libraries,scientific programming},
month = {mar},
number = {2},
pages = {22--30},
title = {{The NumPy array: A structure for efficient numerical computation}},
volume = {13},
year = {2011}
}
@book{Taulli2020,
address = {Berkeley, CA},
author = {Taulli, Tom},
doi = {10.1007/978-1-4842-5729-6},
isbn = {9781484257289},
pages = {2--3},
publisher = {Apress},
title = {{The Robotic Process Automation Handbook A Guide to Implementing RPA Systems}},
url = {https://doi.org/10.1007/978-1-4842-5729-6},
year = {2020}
}
@article{Maloney2010,
author = {Maloney, John and Resnick, Mitchel and Rusk, Natalie and Silverman, Brian and Eastmond, Evelyn},
doi = {10.1145/1868358.1868363},
journal = {ACM Transactions on Computing Education (TOCE)},
pages = {16},
title = {{The Scratch Programming Language and Environment}},
volume = {10},
year = {2010}
}
@book{Parija2018,
address = {Singapore},
doi = {10.1007/978-981-13-0890-1},
editor = {Parija, Subhash Chandra and Kate, Vikram},
isbn = {978-981-13-0889-5},
publisher = {Springer Singapore},
title = {{Thesis Writing for Master's and Ph.D. Program}},
url = {http://link.springer.com/10.1007/978-981-13-0890-1},
year = {2018}
}
@article{Zela2018,
abstract = {While existing work on neural architecture search (NAS) tunes hyperparameters in a separate post-processing step, we demonstrate that architectural choices and other hyperparameter settings interact in a way that can render this separation suboptimal. Likewise, we demonstrate that the common practice of using very few epochs during the main NAS and much larger numbers of epochs during a post-processing step is inefficient due to little correlation in the relative rankings for these two training regimes. To combat both of these problems, we propose to use a recent combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search.},
archivePrefix = {arXiv},
arxivId = {1807.06906},
author = {Zela, Arber and Klein, Aaron and Falkner, Stefan and Hutter, Frank},
eprint = {1807.06906},
journal = {ArXiv},
title = {{Towards Automated Deep Learning: Efficient Joint Neural Architecture and Hyperparameter Search}},
url = {http://arxiv.org/abs/1807.06906},
volume = {abs/1807.0},
year = {2018}
}
@incollection{Mendoza2016,
abstract = {Recent advances in AutoML have led to automated tools that can compete with machine learning experts on supervised learning tasks. However, current AutoML tools do not yet support modern neural networks effectively. In this work, we present a first version of Auto-Net, which provides automatically-tuned feed-forward neural networks without any human intervention. We report results on datasets from the recent AutoML challenge showing that ensembling Auto-Net with Auto-sklearn can perform better than either alone and report the first results on winning competition datasets against human experts with automatically-tuned neural networks.},
author = {Mendoza, Hector and Klein, Aaron and Feurer, Matthias and Springenberg, Jost Tobias and Urban, Matthias and Burkart, Michael and Dippel, Maximilian and Lindauer, Marius and Hutter, Frank},
booktitle = {AutoML@ICML},
doi = {10.1007/978-3-030-05318-5_7},
pages = {135--149},
title = {{Towards Automatically-Tuned Deep Neural Networks}},
year = {2019}
}
@article{Cote2013,
abstract = {We have developed the Tractometer: an online evaluation and validation system for tractography processing pipelines. One can now evaluate the results of more than 57,000 fiber tracking outputs using different acquisition settings (b-value, averaging), different local estimation techniques (tensor, q-ball, spherical deconvolution) and different tracking parameters (masking, seeding, maximum curvature, step size). At this stage, the system is solely based on a revised FiberCup analysis, but we hope that the community will get involved and provide us with new phantoms, new algorithms, third party libraries and new geometrical metrics, to name a few. We believe that the new connectivity analysis and tractography characteristics proposed can highlight limits of the algorithms and contribute in solving open questions in fiber tracking: from raw data to connectivity analysis. Overall, we show that (i) averaging improves quality of tractography, (ii) sharp angular ODF profiles helps tractography, (iii) seeding and multi-seeding has a large impact on tractography outputs and must be used with care, and (iv) deterministic tractography produces less invalid tracts which leads to better connectivity results than probabilistic tractography. {\textcopyright} 2013 Elsevier B.V.},
author = {C{\^{o}}t{\'{e}}, Marc Alexandre and Girard, Gabriel and Bor{\'{e}}, Arnaud and Garyfallidis, Eleftherios and Houde, Jean Christophe and Descoteaux, Maxime},
doi = {10.1016/j.media.2013.03.009},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Connectivity analysis,Diffusion MRI,Tractography,Validation},
month = {oct},
number = {7},
pages = {844--857},
pmid = {23706753},
title = {{Tractometer: Towards validation of tractography pipelines}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23706753},
volume = {17},
year = {2013}
}
@inproceedings{Boser1992,
abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
author = {Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
booktitle = {Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory},
doi = {10.1145/130385.130401},
isbn = {089791497X},
pages = {144--152},
publisher = {ACM Press},
title = {{Training algorithm for optimal margin classifiers}},
year = {1992}
}
@incollection{Yang2020,
abstract = {Transfer learning deals with how systems can quickly adapt themselves to new situations, tasks and environments. It gives machine learning systems the ability to leverage auxiliary data and models to help solve target problems when there is only a small amount of data available. This makes such systems more reliable and robust, keeping the machine learning model faced with unforeseeable changes from deviating too much from expected performance. At an enterprise level, transfer learning allows knowledge to be reused so experience gained once can be repeatedly applied to the real world. For example, a pre-trained model that takes account of user privacy can be downloaded and adapted at the edge of a computer network. This self-contained, comprehensive reference text describes the standard algorithms and demonstrates how these are used in different transfer learning paradigms. It offers a solid grounding for newcomers as well as new insights for seasoned researchers and developers.},
address = {Cham},
author = {Yang, Qiang and Zhang, Yu and Dai, Wenyuan and Pan, Sinno Jialin},
booktitle = {Transfer Learning},
doi = {10.1017/9781139061773},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Transfer Learning}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}837-1},
year = {2020}
}
@article{Mogadala2019,
abstract = {Integration of vision and language tasks has seen a significant growth in the recent times due to surge of interest from multi-disciplinary communities such as deep learning, computer vision, and natural language processing. In this survey, we focus on ten different vision and language integration tasks in terms of their problem formulation, methods, existing datasets, evaluation measures, and comparison of results achieved with the corresponding state-of-the-art methods. This goes beyond earlier surveys which are either task-specific or concentrate only on one type of visual content i.e., image or video. We then conclude the survey by discussing some possible future directions for integration of vision and language research.},
archivePrefix = {arXiv},
arxivId = {1907.09358},
author = {Mogadala, Aditya and Kalimuthu, Marimuthu and Klakow, Dietrich},
eprint = {1907.09358},
month = {jul},
title = {{Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods}},
url = {http://arxiv.org/abs/1907.09358},
year = {2019}
}
@article{Kuo2016,
abstract = {This work attempts to address two fundamental questions about the structure of the convolutional neural networks (CNN): (1) why a nonlinear activation function is essential at the filter output of all intermediate layers? (2) what is the advantage of the two-layer cascade system over the one-layer system? A mathematical model called the “REctified-COrrelations on a Sphere” (RECOS) is proposed to answer these two questions. After the CNN training process, the converged filter weights define a set of anchor vectors in the RECOS model. Anchor vectors represent the frequently occurring patterns (or the spectral components). The necessity of rectification is explained using the RECOS model. Then, the behavior of a two-layer RECOS system is analyzed and compared with its one-layer counterpart. The LeNet-5 and the MNIST dataset are used to illustrate discussion points. Finally, the RECOS model is generalized to a multilayer system with the AlexNet as an example.},
annote = {{\_}eprint: 1609.04112},
archivePrefix = {arXiv},
arxivId = {1609.04112},
author = {Kuo, C.-C. C.Jay},
doi = {10.1016/j.jvcir.2016.11.003},
eprint = {1609.04112},
issn = {10959076},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Convolutional neural network (CNN),MNIST dataset,Nonlinear activation,RECOS model,Rectified linear unit (ReLU)},
pages = {406--413},
title = {{Understanding convolutional neural networks with a mathematical model}},
url = {http://arxiv.org/abs/1609.04112},
volume = {41},
year = {2016}
}
@article{Zhanga,
author = {Zhang, Liang and Fan, Zhonghao and Li, Yuehan and Zhu, Guangming and Li, Ping and Lu, Xiaoyuan and Shen, Peiyi and Afaq, Syed},
title = {{U-net based analysis of MRI for Alzheimer ' s disease diagnosis}},
year = {2019}
}
@inproceedings{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
annote = {{\_}eprint: 1505.04597},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
isbn = {9783319245737},
issn = {16113349},
pages = {234--241},
title = {{U-net: Convolutional networks for biomedical image segmentation}},
volume = {9351},
year = {2015}
}
@unpublished{Zhang,
author = {Zhang, Liang and Kong, Xiangwen},
title = {unknow}
}
@inproceedings{Boulch2017,
abstract = {In this work, we describe a new, general, and efficient method for unstructured point cloud labeling. As the question of efficiently using deep Convolutional Neural Networks (CNNs) on 3D data is still a pending issue, we propose a framework which applies CNNs on multiple 2D image views (or snapshots) of the point cloud. The approach consists in three core ideas. (i) We pick many suitable snapshots of the point cloud. We generate two types of images: a Red-Green-Blue (RGB) view and a depth composite view containing geometric features. (ii) We then perform a pixel-wise labeling of each pair of 2D snapshots using fully convolutional networks. Different architectures are tested to achieve a profitable fusion of our heterogeneous inputs. (iii) Finally, we perform fast back-projection of the label predictions in the 3D space using efficient buffering to label every 3D point. Experiments show that our method is suitable for various types of point clouds such as Lidar or photogrammetric data.},
annote = {ISSN: 1997-0471},
author = {Boulch, Alexandre and {Le Saux}, B. and Audebert, Nicolas},
booktitle = {Eurographics Workshop on 3D Object Retrieval, EG 3DOR},
doi = {10.2312/3dor.20171047},
editor = {Pratikakis, Ioannis and Dupont, Florent and Ovsjanikov, Maks},
isbn = {9783038680307},
issn = {19970471},
pages = {17--24},
publisher = {The Eurographics Association},
title = {{Unstructured point cloud semantic labeling using deep segmentation networks}},
volume = {2017-April},
year = {2017}
}
@book{Leordeanu2020,
address = {Cham},
author = {Leordeanu, Marius},
doi = {10.1007/978-3-030-42128-1},
isbn = {978-3-030-42127-4},
publisher = {Springer International Publishing},
series = {Advances in Computer Vision and Pattern Recognition},
title = {{Unsupervised Learning in Space and Time}},
url = {http://link.springer.com/10.1007/978-3-030-42128-1},
year = {2020}
}
@article{Setio2016,
abstract = {Automatic detection of pulmonary nodules in thoracic computed tomography (CT) scans has been an active area of research for the last two decades. However, there have only been few studies that provide a comparative performance evaluation of different systems on a common database. We have therefore set up the LUNA16 challenge, an objective evaluation framework for automatic nodule detection algorithms using the largest publicly available reference database of chest CT scans, the LIDC-IDRI data set. In LUNA16, participants develop their algorithm and upload their predictions on 888 CT scans in one of the two tracks: 1) the complete nodule detection track where a complete CAD system should be developed, or 2) the false positive reduction track where a provided set of nodule candidates should be classified. This paper describes the setup of LUNA16 and presents the results of the challenge so far. Moreover, the impact of combining individual systems on the detection performance was also investigated. It was observed that the leading solutions employed convolutional networks and used the provided set of nodule candidates. The combination of these solutions achieved an excellent sensitivity of over 95{\%} at fewer than 1.0 false positives per scan. This highlights the potential of combining algorithms to improve the detection performance. Our observer study with four expert readers has shown that the best system detects nodules that were missed by expert readers who originally annotated the LIDC-IDRI data. We released this set of additional nodules for further development of CAD systems.},
archivePrefix = {arXiv},
arxivId = {1612.08012},
author = {Setio, Arnaud Arindra Adiyoso and Traverso, Alberto and de Bel, Thomas and Berens, Moira S.N. and van den Bogaard, Cas and Cerello, Piergiorgio and Chen, Hao and Dou, Qi and Fantacci, Maria Evelina and Geurts, Bram and van der Gugten, Robbert and Heng, Pheng Ann and Jansen, Bart and de Kaste, Michael M.J. and Kotov, Valentin and Lin, Jack Yu Hung and Manders, Jeroen T.M.C. and S{\'{o}}{\~{n}}ora-Mengana, Alexander and Garc{\'{i}}a-Naranjo, Juan Carlos and Papavasileiou, Evgenia and Prokop, Mathias and Saletta, Marco and Schaefer-Prokop, Cornelia M. and Scholten, Ernst T. and Scholten, Luuk and Snoeren, Miranda M. and Torres, Ernesto Lopez and Vandemeulebroucke, Jef and Walasek, Nicole and Zuidhof, Guido C.A. and van Ginneken, Bram and Jacobs, Colin},
doi = {10.1016/j.media.2017.06.015},
eprint = {1612.08012},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Computed tomography,Computer-aided detection,Convolutional networks,Deep learning,Medical image challenges,Pulmonary nodules},
pages = {1--13},
pmid = {28732268},
title = {{Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: The LUNA16 challenge}},
volume = {42},
year = {2017}
}
@inproceedings{Sharma2016,
abstract = {With the advent of affordable depth sensors, 3D capture becomes more and more ubiquitous and already has made its way into commercial products. Yet, capturing the geometry or complete shapes of everyday objects using scanning devices (e.g. Kinect) still comes with several challenges that result in noise or even incomplete shapes. Recent success in deep learning has shown how to learn complex shape distributions in a data-driven way from large scale 3D CAD Model collections and to utilize them for 3D processing on volumetric representations and thereby circumventing problems of topology and tessellation. Prior work has shown encouraging results on problems ranging from shape completion to recognition.We provide an analysis of such approaches and discover that training as well as the resulting representation are strongly and unnecessarily tied to the notion of object labels. Thus, we propose a full convolutional volumetric auto encoder that learns volumetric representation from noisy data by estimating the voxel occupancy grids. The proposed method outperforms prior work on challenging tasks like denoising and shape completion. We also show that the obtained deep embedding gives competitive performance when used for classification and promising results for shape interpolation.},
annote = {{\_}eprint: 1604.03755},
archivePrefix = {arXiv},
arxivId = {1604.03755},
author = {Sharma, Abhishek and Grau, Oliver and Fritz, Mario},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-49409-8_20},
eprint = {1604.03755},
isbn = {9783319494081},
issn = {16113349},
keywords = {3D deep learning,Denoising auto-encoder,Shape blending,Shape completion},
pages = {236--250},
title = {{VConv-DAE: Deep volumetric shape learning without object labels}},
url = {http://arxiv.org/abs/1604.03755},
volume = {9915 LNCS},
year = {2016}
}
@inproceedings{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
annote = {{\_}eprint: 1409.1556},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1409.1556},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2015}
}
@article{Geiger2013,
abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide. {\textcopyright} The Author(s) 2013.},
author = {Geiger, Andreas and Lenz, Philip and Stiller, Christoph and Urtasun, Raquel},
doi = {10.1177/0278364913491297},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Dataset,GPS,KITTI,SLAM,autonomous driving,benchmarks,cameras,computer vision,field robotics,laser,mobile robotics,object detection,optical flow,stereo,tracking},
number = {11},
pages = {1231--1237},
title = {{Vision meets robotics: The KITTI dataset}},
volume = {32},
year = {2013}
}
@misc{Bouget2017,
abstract = {In recent years, tremendous progress has been made in surgical practice for example with Minimally Invasive Surgery (MIS). To overcome challenges coming from deported eye-to-hand manipulation, robotic and computer-assisted systems have been developed. Having real-time knowledge of the pose of surgical tools with respect to the surgical camera and underlying anatomy is a key ingredient for such systems. In this paper, we present a review of the literature dealing with vision-based and marker-less surgical tool detection. This paper includes three primary contributions: (1) identification and analysis of data-sets used for developing and testing detection algorithms, (2) in-depth comparison of surgical tool detection methods from the feature extraction process to the model learning strategy and highlight existing shortcomings, and (3) analysis of validation techniques employed to obtain detection performance results and establish comparison between surgical tool detectors. The papers included in the review were selected through PubMed and Google Scholar searches using the keywords: “surgical tool detection”, “surgical tool tracking”, “surgical instrument detection” and “surgical instrument tracking” limiting results to the year range 2000 2015. Our study shows that despite significant progress over the years, the lack of established surgical tool data-sets, and reference format for performance assessment and method ranking is preventing faster improvement.},
author = {Bouget, David and Allan, Max and Stoyanov, Danail and Jannin, Pierre},
booktitle = {Medical Image Analysis},
doi = {10.1016/j.media.2016.09.003},
issn = {13618423},
keywords = {Data-set,Endoscopic/microscopic images,Object detection,Tool detection,Validation},
title = {{Vision-based and marker-less surgical tool detection and tracking: a review of the literature}},
year = {2017}
}
@article{Hohman2019,
abstract = {Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W's and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.},
archivePrefix = {arXiv},
arxivId = {1801.06889},
author = {Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
doi = {10.1109/TVCG.2018.2843369},
eprint = {1801.06889},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Deep learning,information visualization,neural networks,visual analytics},
number = {8},
pages = {2674--2693},
title = {{Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841519300829},
volume = {25},
year = {2019}
}
@article{Miyawaki2008,
abstract = {Perceptual experience consists of an enormous number of possible states. Previous fMRI studies have predicted a perceptual state by classifying brain activity into prespecified categories. Constraint-free visual image reconstruction is more challenging, as it is impractical to specify brain activity for all possible images. In this study, we reconstructed visual images by combining local image bases of multiple scales, whose contrasts were independently decoded from fMRI activity by automatically selecting relevant voxels and exploiting their correlated patterns. Binary-contrast, 10 × 10-patch images (2100 possible states) were accurately reconstructed without any image prior on a single trial or volume basis by measuring brain activity only for several hundred random images. Reconstruction was also used to identify the presented image among millions of candidates. The results suggest that our approach provides an effective means to read out complex perceptual states from brain activity while discovering information representation in multivoxel patterns. {\textcopyright} 2008 Elsevier Inc. All rights reserved.},
author = {Miyawaki, Yoichi and Uchida, Hajime and Yamashita, Okito and aki Sato, Masa and Morito, Yusuke and Tanabe, Hiroki C. and Sadato, Norihiro and Kamitani, Yukiyasu},
doi = {10.1016/j.neuron.2008.11.004},
issn = {08966273},
journal = {Neuron},
keywords = {SYSNEURO},
month = {dec},
number = {5},
pages = {915--929},
title = {{Visual Image Reconstruction from Human Brain Activity using a Combination of Multiscale Local Image Decoders}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627308009586},
volume = {60},
year = {2008}
}
@misc{Zhang2018,
abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
annote = {{\_}eprint: 1802.00614},
archivePrefix = {arXiv},
arxivId = {1802.00614},
author = {shi Zhang, Quan and chun Zhu, Song},
booktitle = {Frontiers of Information Technology and Electronic Engineering},
doi = {10.1631/FITEE.1700808},
eprint = {1802.00614},
issn = {20959230},
keywords = {Artificial intelligence,Deep learning,Interpretable model},
number = {1},
pages = {27--39},
title = {{Visual interpretability for deep learning: a survey}},
volume = {19},
year = {2018}
}
@article{Wu2016,
abstract = {Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. Given an image and a question in natural language, it requires reasoning over visual elements of the image and general knowledge to infer the correct answer. In the first part of this survey, we examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to connect the visual and textual modalities. In particular, we examine the common approach of combining convolutional and recurrent neural networks to map images and questions to a common feature space. We also discuss memory-augmented and modular architectures that interface with structured knowledge bases. In the second part of this survey, we review the datasets available for training and evaluating VQA systems. The various datatsets contain questions at different levels of complexity, which require different capabilities and types of reasoning. We examine in depth the question/answer pairs from the Visual Genome project, and evaluate the relevance of the structured annotations of images with scene graphs for VQA. Finally, we discuss promising future directions for the field, in particular the connection to structured knowledge bases and the use of natural language processing models.},
archivePrefix = {arXiv},
arxivId = {1607.05910},
author = {Wu, Qi and Teney, Damien and Wang, Peng and Shen, Chunhua and Dick, Anthony and van den Hengel, Anton},
doi = {10.1016/j.cviu.2017.05.001},
eprint = {1607.05910},
issn = {1090235X},
journal = {Computer Vision and Image Understanding},
keywords = {Knowledge bases,Natural language processing,Recurrent neural networks,Visual question answering},
month = {jul},
pages = {21--40},
title = {{Visual question answering: A survey of methods and datasets}},
url = {http://arxiv.org/abs/1607.05910},
volume = {163},
year = {2017}
}
@article{Smeulders2014,
abstract = {There is a large variety of trackers, which have been proposed in the literature during the last two decades with some mixed success. Object tracking in realistic scenarios is a difficult problem, therefore, it remains a most active area of research in computer vision. A good tracker should perform well in a large number of videos involving illumination changes, occlusion, clutter, camera motion, low contrast, specularities, and at least six more aspects. However, the performance of proposed trackers have been evaluated typically on less than ten videos, or on the special purpose datasets. In this paper, we aim to evaluate trackers systematically and experimentally on 315 video fragments covering above aspects. We selected a set of nineteen trackers to include a wide variety of algorithms often cited in literature, supplemented with trackers appearing in 2010 and 2011 for which the code was publicly available. We demonstrate that trackers can be evaluated objectively by survival curves, Kaplan Meier statistics, and Grubs testing. We find that in the evaluation practice the F-score is as effective as the object tracking accuracy (OTA) score. The analysis under a large variety of circumstances provides objective insight into the strengths and weaknesses of trackers. {\textcopyright} 2014 IEEE.},
author = {Smeulders, Arnold W.M. and Chu, Dung M. and Cucchiara, Rita and Calderara, Simone and Dehghan, Afshin and Shah, Mubarak},
doi = {10.1109/TPAMI.2013.230},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Camera surveillance,Computer vision,Image processing,Object tracking,Tracking dataset,Tracking evaluation,Video understanding},
number = {7},
pages = {1442--1468},
publisher = {IEEE Computer Society},
title = {{Visual tracking: An experimental survey}},
volume = {36},
year = {2014}
}
@inproceedings{Milletari2016,
abstract = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
annote = {{\_}eprint: 1606.04797},
archivePrefix = {arXiv},
arxivId = {1606.04797},
author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed Ahmad},
booktitle = {Proceedings - 2016 4th International Conference on 3D Vision, 3DV 2016},
doi = {10.1109/3DV.2016.79},
eprint = {1606.04797},
isbn = {9781509054077},
keywords = {Deep learning,convolutional neural networks,machine learning,prostate,segmentation},
pages = {565--571},
title = {{V-Net: Fully convolutional neural networks for volumetric medical image segmentation}},
year = {2016}
}
@inproceedings{Engelcke2017,
abstract = {This paper proposes a computationally efficient approach to detecting objects natively in 3D point clouds using convolutional neural networks (CNNs). In particular, this is achieved by leveraging a feature-centric voting scheme to implement novel convolutional layers which explicitly exploit the sparsity encountered in the input. To this end, we examine the trade-off between accuracy and speed for different architectures and additionally propose to use an L1 penalty on the filter activations to further encourage sparsity in the intermediate representations. To the best of our knowledge, this is the first work to propose sparse convolutional layers and L1 regularisation for efficient large-scale processing of 3D data. We demonstrate the efficacy of our approach on the KITTI object detection benchmark and show that VoteSDeep models with as few as three layers outperform the previous state of the art in both laser and laser-vision based approaches by margins of up to 40{\%} while remaining highly competitive in terms of processing time.},
archivePrefix = {arXiv},
arxivId = {1609.06666},
author = {Engelcke, Martin and Rao, Dushyant and Wang, Dominic Zeng and Tong, Chi Hay and Posner, Ingmar},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2017.7989161},
eprint = {1609.06666},
isbn = {9781509046331},
issn = {10504729},
pages = {1355--1361},
title = {{Vote3Deep: Fast object detection in 3D point clouds using efficient convolutional neural networks}},
url = {https://doi.org/10.1109/ICRA.2017.7989161},
year = {2017}
}
@inproceedings{Maturana2015,
abstract = {Robust object recognition is a crucial skill for robots operating autonomously in real world environments. Range sensors such as LiDAR and RGBD cameras are increasingly found in modern robotic systems, providing a rich source of 3D information that can aid in this task. However, many current systems do not fully utilize this information and have trouble efficiently dealing with large amounts of point cloud data. In this paper, we propose VoxNet, an architecture to tackle this problem by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN). We evaluate our approach on publicly available benchmarks using LiDAR, RGBD, and CAD data. VoxNet achieves accuracy beyond the state of the art while labeling hundreds of instances per second.},
author = {Maturana, Daniel and Scherer, Sebastian},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2015.7353481},
isbn = {9781479999941},
issn = {21530866},
pages = {922--928},
title = {{VoxNet: A 3D Convolutional Neural Network for real-time object recognition}},
url = {https://doi.org/10.1109/IROS.2015.7353481},
volume = {2015-Decem},
year = {2015}
}
@article{Li2019,
abstract = {Developing new deep learning methods for medical image analysis is a prevalent research topic in machine learning. In this paper, we propose a deep learning scheme with a novel loss function for weakly supervised breast cancer diagnosis. According to the Nottingham Grading System, mitotic count plays an important role in breast cancer diagnosis and grading. To determine the cancer grade, pathologists usually need to manually count mitosis from a great deal of histopathology images, which is a very tedious and time-consuming task. This paper proposes an automatic method for detecting mitosis. We regard the mitosis detection task as a semantic segmentation problem and use a deep fully convolutional network to address it. Different from conventional training data used in semantic segmentation system, the training label of mitosis data is usually in the format of centroid pixel, rather than all the pixels belonging to a mitosis. The centroid label is a kind of weak label, which is much easier to annotate and can save the effort of pathologists a lot. However, technically this weak label is not sufficient for training a mitosis segmentation model. To tackle this problem, we expand the single-pixel label to a novel label with concentric circles, where the inside circle is a mitotic region and the ring around the inside circle is a “middle ground”. During the training stage, we do not compute the loss of the ring region because it may have the presence of both mitotic and non-mitotic pixels. This new loss termed as “concentric loss” is able to make the semantic segmentation network be trained with the weakly annotated mitosis data. On the generated segmentation map from the segmentation model, we filter out low confidence and obtain mitotic cells. On the challenging ICPR 2014 MITOSIS dataset and AMIDA13 dataset, we achieve a 0.562 F-score and 0.673 F-score respectively, outperforming all previous approaches significantly. On the latest TUPAC16 dataset, we obtain a F-score of 0.669, which is also the state-of-the-art result. The excellent results quantitatively demonstrate the effectiveness of the proposed mitosis segmentation network with the concentric loss. All of our code has been made publicly available at https://github.com/ChaoLi977/SegMitos{\_}mitosis{\_}detection.},
author = {Li, Chao and Wang, Xinggang and Liu, Wenyu and Latecki, Longin Jan and Wang, Bo and Huang, Junzhou},
doi = {10.1016/j.media.2019.01.013},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Breast cancer grading,Fully convolutional network,Mitosis detection,Weakly supervised learning},
pages = {165--178},
pmid = {30798116},
title = {{Weakly supervised mitosis detection in breast histopathology images using concentric loss}},
volume = {53},
year = {2019}
}
@book{Mardan2019,
address = {Berkeley, CA},
author = {Mardan, Azat},
doi = {10.1007/978-1-4842-3970-4},
isbn = {978-1-4842-3969-8},
publisher = {Apress},
title = {{Write Your Way To Success}},
url = {http://link.springer.com/10.1007/978-1-4842-3970-4},
year = {2019}
}
@book{Blair2016,
address = {Rotterdam},
author = {Blair, Lorrie},
doi = {10.1007/978-94-6300-426-8},
isbn = {978-94-6300-426-8},
publisher = {SensePublishers},
title = {{Writing a Graduate Thesis or Dissertation}},
url = {http://link.springer.com/10.1007/978-94-6300-426-8},
year = {2016}
}
@book{Parija2017,
address = {Singapore},
doi = {10.1007/978-981-10-4720-6},
editor = {Parija, Subhash Chandra and Kate, Vikram},
isbn = {978-981-10-4719-0},
publisher = {Springer Singapore},
title = {{Writing and Publishing a Scientific Research Paper}},
url = {http://link.springer.com/10.1007/978-981-10-4720-6},
year = {2017}
}
@book{Englander2014,
address = {Dordrecht},
author = {Englander, Karen},
doi = {10.1007/978-94-007-7714-9},
isbn = {978-94-007-7713-2},
publisher = {Springer Netherlands},
series = {SpringerBriefs in Education},
title = {{Writing and Publishing Science Research Papers in English}},
url = {http://link.springer.com/10.1007/978-94-007-7714-9},
year = {2014}
}
@book{Zobel2014,
address = {London},
author = {Zobel, Justin},
doi = {10.1007/978-1-4471-6639-9},
isbn = {978-1-4471-6638-2},
publisher = {Springer London},
title = {{Writing for Computer Science}},
url = {http://link.springer.com/10.1007/978-1-4471-6639-9},
year = {2014}
}
@book{RenckJalongo2016,
address = {Cham},
author = {{Renck Jalongo}, Mary and Saracho, Olivia N.},
doi = {10.1007/978-3-319-31650-5},
isbn = {978-3-319-31648-2},
publisher = {Springer International Publishing},
series = {Springer Texts in Education},
title = {{Writing for Publication}},
url = {http://link.springer.com/10.1007/978-3-319-31650-5},
year = {2016}
}
@book{Myatt2017,
address = {New York},
doi = {10.1057/978-1-137-59932-2},
editor = {Myatt, Alice Johnston and Gaillet, Lyn{\'{e}}e Lewis},
isbn = {978-1-137-59931-5},
publisher = {Palgrave Macmillan US},
title = {{Writing Program and Writing Center Collaborations}},
url = {http://link.springer.com/10.1057/978-1-137-59932-2},
year = {2017}
}
@book{Greenshields2017,
address = {Cham},
author = {Greenshields, Will},
doi = {10.1007/978-3-319-47533-2},
isbn = {978-3-319-47532-5},
publisher = {Springer International Publishing},
title = {{Writing the Structures of the Subject}},
url = {http://link.springer.com/10.1007/978-3-319-47533-2},
year = {2017}
}
@article{Yang2019,
abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
annote = {{\_}eprint: 1906.08237},
archivePrefix = {arXiv},
arxivId = {1906.08237},
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V},
eprint = {1906.08237},
title = {{XLNet: Generalized Autoregressive Pretraining for Language Understanding}},
url = {http://arxiv.org/abs/1906.08237},
year = {2019}
}
@article{Graham2018,
abstract = {Nuclear segmentation within Haematoxylin {\&} Eosin stained histology images is a fundamental prerequisite in the digital pathology work-flow, due to the ability for nuclear features to act as key diagnostic markers. The development of automated methods for nuclear segmentation enables the quantitative analysis of tens of thousands of nuclei within a whole-slide pathology image, opening up possibilities of further analysis of large-scale nuclear morphometry. However, automated nuclear segmentation is faced with a major challenge in that there are several different types of nuclei, some of them exhibiting large intra-class variability such as the tumour cells. Additionally, some of the nuclei are often clustered together. To address these challenges, we present a novel convolutional neural network for automated nuclear segmentation that leverages the instance-rich information encoded within the vertical and horizontal distances of nuclear pixels to their centres of mass. These distances are then utilised to separate clustered nuclei, resulting in an accurate segmentation, particularly in areas with overlapping instances. We demonstrate state-of-the-art performance compared to other methods on four independent multi-tissue histology image datasets. Furthermore, we propose an interpretable and reliable evaluation framework that effectively quantifies nuclear segmentation performance and overcomes the limitations of existing performance measures.},
annote = {{\_}eprint: 1812.06499},
archivePrefix = {arXiv},
arxivId = {1812.06499},
author = {Graham, Simon and Vu, Quoc Dang and Raza, Shan e Ahmed and Kwak, Jin Tae and Rajpoot, Nasir},
eprint = {1812.06499},
journal = {CoRR},
pages = {1--11},
title = {{XY Network for Nuclear Segmentation in Multi-Tissue Histology Images}},
url = {http://arxiv.org/abs/1812.06499},
volume = {abs/1812.0},
year = {2018}
}
@incollection{Lampert2020,
address = {Cham},
author = {Lampert, Christoph H.},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_874-1},
pages = {1--3},
publisher = {Springer International Publishing},
title = {{Zero-Shot Learning}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}874-1},
year = {2020}
}
@incollection{Gitchat2018,
address = {Cham},
author = {Gitchat, Iteye},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_299-1},
pages = {1--10},
publisher = {Springer International Publishing},
title = {{特征选择 ( Feature Selection ) 特征选择 Feature Selection}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}299-1},
year = {2018}
}
