@misc{ADNI,
abstract = {The Alzheimer's Disease Neuroimaging Initiative (ADNI) unites researchers with study data as they work to define the progression of Alzheimer's disease (AD). ADNI researchers collect, validate and utilize data, including MRI and PET images, genetics, cognitive tests, CSF and blood biomarkers as predictors of the disease. Study resources and data from the North American ADNI study are available through this website, including Alzheimer's disease patients, mild cognitive impairment subjects, and elderly controls.},
author = {Iwatsubo, Takeshi},
booktitle = {Nihon rinsho. Japanese journal of clinical medicine},
issn = {00471852},
keywords = {[1] “Alzheimer's Disease Neuroimaging Initiative.”},
pages = {570--574},
pmid = {22787853},
title = {{[Alzheimer's disease Neuroimaging Initiative (ADNI)].}},
url = {http://adni.loni.usc.edu/},
urldate = {2020-05-25},
volume = {69 Suppl 8},
year = {2011}
}
@article{Klein2012,
abstract = {We introduce the Mindboggle-101 dataset, the largest and most complete set of free, publicly accessible, manually labeled human brain images. To manually label the macroscopic anatomy in magnetic resonance images of 101 healthy participants, we created a new cortical labeling protocol that relies on robust anatomical landmarks and minimal manual edits after initialization with automated labels. The "Desikan-Killiany-Tourville" (DKT) protocol is intended to improve the ease, consistency, and accuracy of labeling human cortical areas. Given how difficult it is to label brains, the Mindboggle-101 dataset is intended to serve as brain atlases for use in labeling other brains, as a normative dataset to establish mor-phometric variation in a healthy population for comparison against clinical populations, and contribute to the development, training, testing, and evaluation of automated registration and labeling algorithms. To this end, we also introduce benchmarks for the evaluation of such algorithms by comparing our manual labels with labels automatically generated by probabilistic and multi-atlas registration-based approaches. All data and related software and updated information are available on the http://mindboggle.info/data website. Copy; 2012 Kleinand Tourville.},
author = {Klein, Arno and Tourville, Jason},
doi = {10.3389/fnins.2012.00171},
issn = {16624548},
journal = {Frontiers in Neuroscience},
keywords = {Anatomy,Cerebral cortex,Human brain,Labeling,Mri,Parcellation,Segmentation},
number = {DEC},
title = {{101 Labeled Brain Images and a Consistent Human Cortical Labeling Protocol}},
url = {http://journal.frontiersin.org/article/10.3389/fnins.2012.00171/abstract},
volume = {6},
year = {2012}
}
@article{Litjens2018,
abstract = {Background: The presence of lymph node metastases is one of the most important factors in breast cancer prognosis. The most common way to assess regional lymph node status is the sentinel lymph node procedure. The sentinel lymph node is the most likely lymph node to contain metastasized cancer cells and is excised, histopathologically processed, and examined by a pathologist. This tedious examination process is time-consuming and can lead to small metastases being missed. However, recent advances in whole-slide imaging and machine learning have opened an avenue for analysis of digitized lymph node sections with computer algorithms. For example, convolutional neural networks, a type of machine-learning algorithm, can be used to automatically detect cancer metastases in lymph nodes with high accuracy. To train machine-learning models, large, well-curated datasets are needed. Results: We released a dataset of 1,399 annotated whole-slide images (WSIs) of lymph nodes, both with and without metastases, in 3 terabytes of data in the context of the CAMELYON16 and CAMELYON17 Grand Challenges. Slides were collected from five medical centers to cover a broad range of image appearance and staining variations. Each WSI has a slide-level label indicating whether it contains no metastases, macro-metastases, micro-metastases, or isolated tumor cells. Furthermore, for 209 WSIs, detailed hand-drawn contours for all metastases are provided. Last, open-source software tools to visualize and interact with the data have been made available. Conclusions: A unique dataset of annotated, whole-slide digital histopathology images has been provided with high potential for re-use.},
author = {Litjens, Geert and Bandi, Peter and Bejnordi, Babak Ehteshami and Geessink, Oscar and Balkenhol, Maschenka and Bult, Peter and Halilovic, Altuna and Hermsen, Meyke and van de Loo, Rob and Vogels, Rob and Manson, Quirine F. and Stathonikos, Nikolas and Baidoshvili, Alexi and van Diest, Paul and Wauters, Carla and van Dijk, Marcory and van der Laak, Jeroen},
doi = {10.1093/gigascience/giy065},
issn = {2047217X},
journal = {GigaScience},
keywords = {Breast cancer,Grand challenge,Lymph node metastases,Sentinel node,Whole-slide images},
month = {jun},
number = {6},
pmid = {29860392},
title = {{1399 H{\&}E-stained sentinel lymph node sections of breast cancer patients: The CAMELYON dataset}},
url = {https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giy065/5026175},
volume = {7},
year = {2018}
}
@online{2018AtrialSegmentationChallenge,
author = {Zhao, Jichao and Xiong, Zhaohan},
title = {{2018 Atrial Segmentation Challenge – Atrial Segmentation Challenge}},
url = {http://atriaseg2018.cardiacatlas.org/}
}
@article{2020ADFact,
abstract = {This article describes the public health impact of Alzheimer's disease (AD), including incidence and prevalence, mortality and morbidity, use and costs of care, and the overall impact on caregivers and society. The Special Report discusses the future challenges of meeting care demands for the growing number of people living with Alzheimer's dementia in the United States with a particular emphasis on primary care. By mid-century, the number of Americans age 65 and older with Alzheimer's dementia may grow to 13.8 million. This represents a steep increase from the estimated 5.8 million Americans age 65 and older who have Alzheimer's dementia today. Official death certificates recorded 122,019 deaths from AD in 2018, the latest year for which data are available, making Alzheimer's the sixth leading cause of death in the United States and the fifth leading cause of death among Americans age 65 and older. Between 2000 and 2018, deaths resulting from stroke, HIV and heart disease decreased, whereas reported deaths from Alzheimer's increased 146.2{\%}. In 2019, more than 16 million family members and other unpaid caregivers provided an estimated 18.6 billion hours of care to people with Alzheimer's or other dementias. This care is valued at nearly {\$}244 billion, but its costs extend to family caregivers' increased risk for emotional distress and negative mental and physical health outcomes. Average per-person Medicare payments for services to beneficiaries age 65 and older with AD or other dementias are more than three times as great as payments for beneficiaries without these conditions, and Medicaid payments are more than 23 times as great. Total payments in 2020 for health care, long-term care and hospice services for people age 65 and older with dementia are estimated to be {\$}305 billion. As the population of Americans living with Alzheimer's dementia increases, the burden of caring for that population also increases. These challenges are exacerbated by a shortage of dementia care specialists, which places an increasing burden on primary care physicians (PCPs) to provide care for people living with dementia. Many PCPs feel underprepared and inadequately trained to handle dementia care responsibilities effectively. This report includes recommendations for maximizing quality care in the face of the shortage of specialists and training challenges in primary care.},
doi = {10.1002/alz.12068},
issn = {15525279},
journal = {Alzheimer's and Dementia},
keywords = {Alzheimer's dementia,Alzheimer's disease,Alzheimer's disease continuum,Biomarkers,Caregivers,Dementia,Dementia care training,Family caregiver,Geriatrician,Health care costs,Health care expenditures,Health care professional,Incidence,Long-term care costs,Medicaid spending,Medicare spending,Morbidity,Mortality,Prevalence,Primary care physician,Risk factors,Spouse caregiver},
month = {mar},
number = {3},
pages = {391--460},
pmid = {32157811},
title = {{2020 Alzheimer's disease facts and figures}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/alz.12068},
volume = {16},
year = {2020}
}
@inproceedings{Seff2014,
abstract = {Enlarged lymph nodes (LNs) can provide important information for cancer diagnosis, staging, and measuring treatment reactions, making automated detection a highly sought goal. In this paper, we propose a new algorithm representation of decomposing the LN detection problem into a set of 2D object detection subtasks on sampled CT slices, largely alleviating the curse of dimensionality issue. Our 2D detection can be effectively formulated as linear classification on a single image feature type of Histogram of Oriented Gradients (HOG), covering a moderate field-of-view of 45 by 45 voxels. We exploit both max-pooling and sparse linear fusion schemes to aggregate these 2D detection scores for the final 3D LN detection. In this manner, detection is more tractable and does not need to perform perfectly at instance level (as weak hypotheses) since our aggregation process will robustly harness collective information for LN detection. Two datasets (90 patients with 389 mediastinal LNs and 86 patients with 595 abdominal LNs) are used for validation. Cross-validation demonstrates 78.0{\%} sensitivity at 6 false positives/volume (FP/vol.) (86.1{\%} at 10 FP/vol.) and 73.1{\%} sensitivity at 6 FP/vol. (87.2{\%} at 10 FP/vol.), for the mediastinal and abdominal datasets respectively. Our results compare favorably to previous state-of-the-art methods. {\textcopyright} 2014 Springer International Publishing.},
author = {Seff, Ari and Lu, Le and Cherry, Kevin M. and Roth, Holger R. and Liu, Jiamin and Wang, Shijun and Hoffman, Joanne and Turkbey, Evrim B. and Summers, Ronald M.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10404-1_68},
isbn = {9783319104034},
issn = {16113349},
title = {{2D view aggregation for lymph node detection using a shallow hierarchy of linear classifiers}},
year = {2014}
}
@inproceedings{Zhong2017b,
abstract = {Positron emission tomography – computed tomography (PET-CT) has been widely used in modern cancer imaging. Accurate tumor delineation from PET and CT plays an important role in radiation therapy. The PET-CT co-segmentation technique, which makes use of advantages of both modalities, has achieved impressive performance for tumor delineation. In this work, we propose a novel 3D image matting based semi-automated co-segmentation method for tumor delineation on dual PET-CT scans. The “matte” values generated by 3D image matting are employed to compute the region costs for the graph based co-segmentation. Compared to previous PET-CT co-segmentation methods, our method is completely data-driven in the design of cost functions, thus using much less hyper-parameters in our segmentation model. Comparative experiments on 54 PET-CT scans of lung cancer patients demonstrated the effectiveness of our method.},
author = {Zhong, Zisha and Kim, Yusung and Buatti, John and Wu, Xiaodong},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-67564-0_4},
isbn = {9783319675633},
issn = {16113349},
keywords = {Co-segmentation,Image matting,Image segmentation,Interactive segmentation,Lung tumor segmentation},
pages = {31--42},
title = {{3D alpha matting based co-segmentation of tumors on PET-CT images}},
volume = {10555 LNCS},
year = {2017}
}
@article{Khvostikov2018,
abstract = {Computer-aided early diagnosis of Alzheimers Disease (AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the subject of extensive research in recent years. Some recent studies have shown promising results in the AD and MCI determination using structural and functional Magnetic Resonance Imaging (sMRI, fMRI), Positron Emission Tomography (PET) and Diffusion Tensor Imaging (DTI) modalities. Furthermore, fusion of imaging modalities in a supervised machine learning framework has shown promising direction of research. In this paper we first review major trends in automatic classification methods such as feature extraction based methods as well as deep learning approaches in medical image analysis applied to the field of Alzheimer's Disease diagnostics. Then we propose our own algorithm for Alzheimer's Disease diagnostics based on a convolutional neural network and sMRI and DTI modalities fusion on hippocampal ROI using data from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (http://adni.loni.usc.edu). Comparison with a single modality approach shows promising results. We also propose our own method of data augmentation for balancing classes of different size and analyze the impact of the ROI size on the classification results as well.},
annote = {{\_}eprint: 1801.05968},
archivePrefix = {arXiv},
arxivId = {1801.05968},
author = {Khvostikov, Alexander and Aderghal, Karim and Benois-Pineau, Jenny and Krylov, Andrey and Catheline, Gwenaelle},
eprint = {1801.05968},
journal = {arXiv},
keywords = {Alzheimers Disease,Convolutional Neural Networks,Deep learning,Image Fusion,Machine Learning,Medical Imaging,Mild Cognitive Impairment},
month = {jan},
title = {{3D CNN-based classification using sMRI and MD-DTI images for Alzheimer disease studies}},
url = {http://arxiv.org/abs/1801.05968},
year = {2018}
}
@inproceedings{Dou2016,
abstract = {Automatic liver segmentation from CT volumes is a crucial prerequisite yet challenging task for computer-aided hepatic disease diagnosis and treatment. In this paper,we present a novel 3D deeply supervised network (3D DSN) to address this challenging task. The proposed 3D DSN takes advantage of a fully convolutional architecture which performs efficient end-to-end learning and inference. More importantly,we introduce a deep supervision mechanism during the learning process to combat potential optimization difficulties,and thus the model can acquire a much faster convergence rate and more powerful discrimination capability. On top of the high-quality score map produced by the 3D DSN,a conditional random field model is further employed to obtain refined segmentation results. We evaluated our framework on the public MICCAI-SLiver07 dataset. Extensive experiments demonstrated that our method achieves competitive segmentation results to state-of-the-art approaches with a much faster processing speed.},
address = {Cham},
archivePrefix = {arXiv},
arxivId = {1607.00582},
author = {Dou, Qi and Chen, Hao and Jin, Yueming and Yu, Lequan and Qin, Jing and Heng, Pheng Ann},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46723-8_18},
editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R and Unal, Gozde and Wells, William},
eprint = {1607.00582},
isbn = {9783319467221},
issn = {16113349},
pages = {149--157},
publisher = {Springer International Publishing},
title = {{3D deeply supervised network for automatic liver segmentation from CT volumes}},
volume = {9901 LNCS},
year = {2016}
}
@inproceedings{Zhong2018,
annote = {ISSN: 1945-8452},
author = {Zhong, Z and Kim, Y and Zhou, L and Plichta, K and Allen, B and Buatti, J and Wu, X},
booktitle = {2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)},
doi = {10.1109/ISBI.2018.8363561},
keywords = {3D fully convolutional networks,Biomedical imaging,Computed tomography,Image segmentation,Lung,PET-CT images,PET-CT scans,Three-dimensional displays,Tumors,automated accurate tumor delineation,biomedical MRI,cancer,cancer diagnosis,co-segmentation,co-segmentation model,computed tomography,computerised tomography,critical diagnostic information,deep learning,dual-modality imaging,final tumor segmentation results,fully convolutional networks,graph cut,image classification,image segmentation,learning (artificial intelligence),lung,lung cancer patients,lung tumor segmentation,medical image processing,positron emission tomography,probability maps,semantic segmentation framework,tumor reading,tumours},
month = {apr},
pages = {228--231},
title = {{3D fully convolutional networks for co-segmentation of tumors on PET-CT images}},
year = {2018}
}
@inproceedings{Kim2013,
abstract = {Scene understanding is an important yet very challenging problem in computer vision. In the past few years, researchers have taken advantage of the recent diffusion of depth-RGB (RGB-D) cameras to help simplify the problem of inferring scene semantics. However, while the added 3D geometry is certainly useful to segment out objects with different depth values, it also adds complications in that the 3D geometry is often incorrect because of noisy depth measurements and the actual 3D extent of the objects is usually unknown because of occlusions. In this paper we propose a new method that allows us to jointly refine the 3D reconstruction of the scene (raw depth values) while accurately segmenting out the objects or scene elements from the 3D reconstruction. This is achieved by introducing a new model which we called Voxel-CRF. The Voxel-CRF model is based on the idea of constructing a conditional random field over a 3D volume of interest which captures the semantic and 3D geometric relationships among different elements (voxels) of the scene. Such model allows to jointly estimate (1) a dense voxel-based 3D reconstruction and (2) the semantic labels associated with each voxel even in presence of partial occlusions using an approximate yet efficient inference strategy. We evaluated our method on the challenging NYU Depth dataset (Version 1 and 2). Experimental results show that our method achieves competitive accuracy in inferring scene semantics and visually appealing results in improving the quality of the 3D reconstruction. We also demonstrate an interesting application of object removal and scene completion from RGB-D images. {\textcopyright} 2013 IEEE.},
author = {Kim, Byung Soo and Kohli, Pushmeet and Savarese, Silvio},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2013.180},
isbn = {9781479928392},
keywords = {3D reconstruction,RGB-D,Scene understanding},
pages = {1425--1432},
title = {{3D scene understanding by voxel-CRF}},
url = {https://doi.org/10.1109/ICCV.2013.180},
year = {2013}
}
@article{VanGinneken2007,
abstract = {This paper describes the setup of a segmentation competition for the automatic extraction of Multiple Sclerosis (MS) lesions from brain Magnetic Resonance Imaging (MRI) data. This competition is one of three competitions that make up a comparison workshop at the 2008 Medical Image Computing and Computer Assisted Intervention (MICCAI) conference and was modeled after the successful comparison workshop on liver and caudate segmentation at the 2007 MICCAI conference. In this paper, the rationale for organizing the competition is discussed, the training and test data sets for both segmentation tasks are described and the scoring system used to evaluate the segmentation is presented.},
author = {van Ginneken, Bram and Heimann, Tobias and Styner, Martin},
journal = {International Conference on Medical Image Computing and Computer Assisted Intervention},
pages = {7--15},
title = {{3D segmentation in the clinic: A grand challeng}},
url = {http://grand-challenge2008.bigr.nl/proceedings/pdfs/msls08/Styner.pdf},
volume = {10},
year = {2007}
}
@inproceedings{Wu2014,
abstract = {3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet - a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.},
annote = {{\_}eprint: 1406.5670},
archivePrefix = {arXiv},
arxivId = {1406.5670},
author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298801},
eprint = {1406.5670},
isbn = {9781467369640},
issn = {10636919},
keywords = {Computer Science - Computer Vision and Pattern Rec},
pages = {1912--1920},
title = {{3D ShapeNets: A deep representation for volumetric shapes}},
url = {http://arxiv.org/abs/1406.5670},
volume = {07-12-June},
year = {2015}
}
@ONLINE{Landman,
author = {Landman, Bennett A and Anderson, Adam W and Schilling, Kurt and Alexander, Simon and Kerins, Fergal and Westin, C-F and Rathi, Yogesh and Dyrby, Tim B. and Descoteaux, Maxime and Houde, Jean-Christophe and Verma, Ragini and Pierpaoli, Carlo and Irfanoglu, Okan and Thomas, Cibu},
title = {{3-D Validation of Tractography with Experimental MRI (3D VoTEM)}},
url = {https://my.vanderbilt.edu/votem/}
}
@article{Dettmers2016,
abstract = {The creation of practical deep learning data-products often requires parallelization across processors and computers to make deep learning feasible on large data sets, but bottlenecks in communication bandwidth make it difficult to attain good speedups through parallelism. Here we develop and test 8-bit approximation algorithms which make better use of the available bandwidth by compressing 32-bit gradients and nonlinear activations to 8-bit approximations. We show that these approximations do not decrease predictive performance on MNIST, CIFAR10, and ImageNet for both model and data parallelism and provide a data transfer speedup of 2x relative to 32-bit parallelism. We build a predictive model for speedups based on our experimental data, verify its validity on known speedup data, and show that we can obtain a speedup of 50x and more on a system of 96 GPUs compared to a speedup of 23x for 32-bit. We compare our data types with other methods and show that 8-bit approximations achieve state-of-the-art speedups for model parallelism. Thus 8-bit approximation is an efficient method to parallelize convolutional networks on very large systems of GPUs.},
archivePrefix = {arXiv},
arxivId = {1511.04561},
author = {Dettmers, Tim},
eprint = {1511.04561},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
title = {{8-Bit Approximations for Parallelism in Deep Learning}},
year = {2016}
}
@article{Nadkarni2019a,
abstract = {The gray mouse lemur (Microcebus murinus) is a small prosimian of growing interest for studies of primate biology and evolution, and notably as a model organism of brain aging. As brain atlases are essential tools for brain investigation, the objective of the current work was to create the first 3D digital atlas of the mouse lemur brain. For this, a template image was constructed from in vivo magnetic resonance imaging (MRI) data of 34 animals. This template was then manually segmented into 40 cortical, 74 subcortical and 6 cerebro-spinal fluid (CSF) regions. Additionally, we generated probability maps of gray matter, white matter and CSF. The template, manual segmentation and probability maps, as well as imaging tools used to create and manipulate the template, can all be freely downloaded. The atlas was first used to automatically assess regional age-associated cerebral atrophy in a cohort of mouse lemurs previously studied by voxel based morphometry (VBM). Results based on the atlas were in good agreement with the VBM ones, showing age-associated atrophy in the same brain regions such as the insular, parietal or occipital cortices as well as the thalamus or hypothalamus. The atlas was also used as a tool for comparative neuroanatomy. To begin with, we compared measurements of brain regions in our MRI data with histology-based measures from a reference article largely used in previous comparative neuroanatomy studies. We found large discrepancies between our MRI-based data and those of the reference histology-based article. Next, regional brain volumes were compared amongst the mouse lemur and several other mammalian species where high quality volumetric MRI brain atlases were available, including rodents (mouse, rat) and primates (marmoset, macaque, and human). Unlike those based on histological atlases, measures from MRI atlases indicated similar cortical to cerebral volume indices in all primates, including in mouse lemurs, and lower values in mice. On the other hand, white matter to cerebral volume index increased from rodents to small primates (mouse lemurs and marmosets) to macaque, reaching their highest values in humans.},
author = {Nadkarni, Nachiket A. and Bougacha, Salma and Garin, Cl{\'{e}}ment and Dhenain, Marc and Picq, Jean Luc},
doi = {10.1016/j.neuroimage.2018.10.010},
issn = {10959572},
journal = {NeuroImage},
keywords = {Atlas,Cerebral atrophy,Comparative anatomy,MRI,Mouse lemur,Template},
month = {jan},
pages = {85--95},
pmid = {30326295},
title = {{A 3D population-based brain atlas of the mouse lemur primate with examples of applications in aging studies and comparative anatomy}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811918319694},
volume = {185},
year = {2019}
}
@article{Li2020,
abstract = {In this study, we present a baseline approach for AutoImplant (https://autoimplant.grand-challenge.org/) – the cranial implant design challenge, which can be formulated as a volumetric shape learning task. In this task, the defective skull, the complete skull and the cranial implant are represented as binary voxel grids. To accomplish this task, the implant can be either reconstructed directly from the defective skull or obtained by taking the difference between a defective skull and a complete skull. In the latter case, a complete skull has to be reconstructed given a defective skull, which defines a volumetric shape completion problem. Our baseline approach for this task is based on the former formulation, i.e., a deep neural network is trained to predict the implants directly from the defective skulls. The approach generates high-quality implants in two steps: First, an encoder-decoder network learns a coarse representation of the implant from downsampled, defective skulls; The coarse implant is only used to generate the bounding box of the defected region in the original high-resolution skull. Second, another encoder-decoder network is trained to generate a fine implant from the bounded area. On the test set, the proposed approach achieves an average dice similarity score (DSC) of 0.8555 and Hausdorff distance (HD) of 5.1825 mm. The codes are available at https://github.com/Jianningli/autoimplant.},
archivePrefix = {arXiv},
arxivId = {2006.12449},
author = {Li, Jianning and Pepe, Antonio and Gsaxner, Christina and Campe, Gord von and Egger, Jan},
doi = {10.1007/978-3-030-60946-7_8},
eprint = {2006.12449},
isbn = {9783030609450},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Cranial implant design,Cranioplasty,Deep learning,Shape learning,Skull reconstruction,Volumetric shape completion},
month = {jun},
pages = {75--84},
title = {{A Baseline Approach for AutoImplant: The MICCAI 2020 Cranial Implant Design Challenge}},
url = {http://arxiv.org/abs/2006.12449},
volume = {12445 LNCS},
year = {2020}
}
@inproceedings{Jurio2010,
abstract = {In this work we carry out a comparison study between different color spaces in clustering-based image segmentation. We use two similar clustering algorithms, one based on the entropy and the other on the ignorance. The study involves four color spaces and, in all cases, each pixel is represented by the values of the color channels in that space. Our purpose is to identify the best color representation, if there is any, when using this kind of clustering algorithms. {\textcopyright} Springer-Verlag Berlin Heidelberg 2010.},
address = {Berlin, Heidelberg},
author = {Jurio, Aranzazu and Pagola, Miguel and Galar, Mikel and Lopez-Molina, Carlos and Paternain, Daniel},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-642-14058-7_55},
editor = {H{\"{u}}llermeier, Eyke and Kruse, Rudolf and Hoffmann, Frank},
isbn = {9783642140570},
issn = {18650929},
keywords = {CMY,Clustering,Color space,HSV,Image segmentation,RGB,YUV},
pages = {532--541},
publisher = {Springer Berlin Heidelberg},
title = {{A comparison study of different color spaces in clustering based image segmentation}},
volume = {81 PART 2},
year = {2010}
}
@misc{Kalsotra2019,
abstract = {Background subtraction is an effective method of choice when it comes to detection of moving objects in videos and has been recognized as a breakthrough for the wide range of applications of intelligent video analytics (IVA). In recent years, a number of video datasets intended for background subtraction have been created to address the problem of large realistic datasets with accurate ground truth. The use of these datasets enables qualitative as well as quantitative comparisons and allows benchmarking of different algorithms. Finding the appropriate dataset is generally a cumbersome task for an exhaustive evaluation of algorithms. Therefore, we systematically survey standard video datasets and list their applicability for different applications. This paper presents a comprehensive account of public video datasets for background subtraction and attempts to cover the lack of a detailed description of each dataset. The video datasets are presented in chronological order of their appearance. Current trends of deep learning in background subtraction along with top-ranked background subtraction methods are also discussed in this paper. The survey introduced in this paper will assist researchers of the computer vision community in the selection of appropriate video dataset to evaluate their algorithms on the basis of challenging scenarios that exist in both indoor and outdoor environments.},
author = {Kalsotra, Rudrika and Arora, Sakshi},
booktitle = {IEEE Access},
doi = {10.1109/ACCESS.2019.2914961},
issn = {21693536},
keywords = {Background model,background subtraction,challenges,datasets,deep neural networks,foreground,intelligent video analytics (IVA),video frames},
pages = {59143--59171},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A Comprehensive Survey of Video Datasets for Background Subtraction}},
volume = {7},
year = {2019}
}
@article{Zhang2017,
abstract = {Automated cardiac image interpretation has the potential to transform clinical practice in multiple ways including enabling low-cost serial assessment of cardiac function by non-experts in the primary care and rural setting. We hypothesized that advances in computer vision could enable building a fully automated, scalable analysis pipeline for echocardiogram (echo) interpretation. Our approach entailed: 1) preprocessing of complete echo studies; 2) convolutional neural networks (CNN) for view identification, image segmentation, and phasing of the cardiac cycle; 3) quantification of chamber volumes and left ventricular mass; 4) particle tracking to compute longitudinal strain; and 5) targeted disease detection. CNNs accurately identified views (e.g. 99{\%} for apical 4-chamber) and segmented individual cardiac chambers. The resulting cardiac structure measurements agreed with study report values [e.g. median absolute deviations (MAD) of 11.8 g/kg/m2 for left ventricular mass index and 7.7 mL/kg/m2 for left ventricular diastolic volume index, derived from 1319 and 2918 studies, respectively]. In terms of cardiac function, we computed automated ejection fraction and longitudinal strain measurements (within 2 cohorts), which agreed with commercial software-derived values [for ejection fraction, MAD=5.3{\%}, N=3101 studies; for strain, MAD=1.5{\%} (n=197) and 1.6{\%} (n=110)], and demonstrated applicability to serial monitoring of breast cancer patients for trastuzumab cardiotoxicity. Overall, we found that, compared to manual measurements, automated measurements had superior performance across seven internal consistency metrics (e.g. the correlation of left ventricular diastolic volumes with left atrial volumes) with an average increase in the absolute Spearman correlation coefficient of 0.05 (p=0.02). Finally, we used CNNs to develop disease detection algorithms for hypertrophic cardiomyopathy and cardiac amyloidosis, with C-statistics of 0.93 and 0.84, respectively. Our pipeline lays the groundwork for using automated interpretation to support point-of-care handheld cardiac ultrasound and large-scale analysis of the millions of echos archived within healthcare systems.},
annote = {{\_}eprint: 1706.07342},
archivePrefix = {arXiv},
arxivId = {1706.07342},
author = {Zhang, Jeffrey and Gajjala, Sravani and Agrawal, Pulkit and Tison, Geoffrey H. and Hallock, Laura A. and Beussink-Nelson, Lauren and Lassen, Mats H. and Fan, Eugene and Aras, Mandar A. and Jordan, Cha Randle and Fleischmann, Kirsten E. and Melisko, Michelle and Qasim, Atif and Efros, Alexei and Shah, Sanjiv J. and Bajcsy, Ruzena and Deo, Rahul C.},
eprint = {1706.07342},
journal = {arXiv},
title = {{A computer vision pipeline for automated determination of cardiac structure and function and detection of disease by two-dimensional echocardiography}},
url = {http://arxiv.org/abs/1706.07342},
year = {2017}
}
@article{Maguolo2020,
abstract = {In this paper, we compare and evaluate different testing protocols used for automatic COVID-19 diagnosis from X-Ray images in the recent literature. We show that similar results can be obtained using X-Ray images that do not contain most of the lungs. We are able to remove the lungs from the images by turning to black the center of the X-Ray scan and training our classifiers only on the outer part of the images. Hence, we deduce that several testing protocols for the recognition are not fair and that the neural networks are learning patterns in the dataset that are not correlated to the presence of COVID-19. Finally, we show that creating a fair testing protocol is a challenging task, and we provide a method to measure how fair a specific testing protocol is. In the future research we suggest to check the fairness of a testing protocol using our tools and we encourage researchers to look for better techniques than the ones that we propose.},
archivePrefix = {arXiv},
arxivId = {2004.12823},
author = {Maguolo, Gianluca and Nanni, Loris},
eprint = {2004.12823},
journal = {arXiv},
keywords = {Convolutional Neural Networks,Covid-19,Covid-19 Diagnosis,X-Ray Images},
month = {apr},
title = {{A Critic Evaluation of Methods for COVID-19 Automatic Detection from X-Ray Images}},
url = {http://arxiv.org/abs/2004.12823},
year = {2020}
}
@article{Kumar2017,
abstract = {Nuclear segmentation in digital microscopic tissue images can enable extraction of high-quality features for nuclear morphometrics and other analysis in computational pathology. Conventional image processing techniques, such as Otsu thresholding and watershed segmentation, do not work effectively on challenging cases, such as chromatin-sparse and crowded nuclei. In contrast, machine learning-based segmentation can generalize across various nuclear appearances. However, training machine learning algorithms requires data sets of images, in which a vast number of nuclei have been annotated. Publicly accessible and annotated data sets, along with widely agreed upon metrics to compare techniques, have catalyzed tremendous innovation and progress on other image classification problems, particularly in object recognition. Inspired by their success, we introduce a large publicly accessible data set of hematoxylin and eosin (HE)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries, whose quality was validated by a medical doctor. Because our data set is taken from multiple hospitals and includes a diversity of nuclear appearances from several patients, disease states, and organs, techniques trained on it are likely to generalize well and work right out-of-the-box on other HE-stained images. We also propose a new metric to evaluate nuclear segmentation results that penalizes object- and pixel-level errors in a unified manner, unlike previous metrics that penalize only one type of error. We also propose a segmentation technique based on deep learning that lays a special emphasis on identifying the nuclear boundaries, including those between the touching or overlapping nuclei, and works well on a diverse set of test images.},
author = {Kumar, Neeraj and Verma, Ruchika and Sharma, Sanuj and Bhargava, Surabhi and Vahadane, Abhishek and Sethi, Amit},
doi = {10.1109/TMI.2017.2677499},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Annotation,boundaries,dataset,deep learning,nuclear segmentation,nuclei},
month = {jul},
number = {7},
pages = {1550--1560},
pmid = {28287963},
title = {{A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology}},
volume = {36},
year = {2017}
}
@article{Krishnamurthy2018,
abstract = {Automatic deception detection is an important task that has gained momentum in computational linguistics due to its potential applications. In this paper, we propose a simple yet tough to beat multi-modal neural model for deception detection. By combining features from different modalities such as video, audio, and text along with Micro-Expression features, we show that detecting deception in real life videos can be more accurate. Experimental results on a dataset of real-life deception videos show that our model outperforms existing techniques for deception detection with an accuracy of 96.14{\%} and ROC-AUC of 0.9799.},
archivePrefix = {arXiv},
arxivId = {1803.00344},
author = {Krishnamurthy, Gangeshwar and Majumder, Navonil and Poria, Soujanya and Cambria, Erik},
eprint = {1803.00344},
journal = {arXiv},
month = {mar},
title = {{A deep learning approach for multimodal deception detection}},
url = {http://arxiv.org/abs/1803.00344},
year = {2018}
}
@article{Burlutskiy2018ADL,
abstract = {We developed a deep learning framework that helps to automatically identify and segment lung cancer areas in patients' tissue specimens. The study was based on a cohort of lung cancer patients operated at the Uppsala University Hospital. The tissues were reviewed by lung pathologists and then the cores were compiled to tissue micro-arrays (TMAs). For experiments, hematoxylin-eosin stained slides from 712 patients were scanned and then manually annotated. Then these scans and annotations were used to train segmentation models of the developed framework. The performance of the developed deep learning framework was evaluated on fully annotated TMA cores from 178 patients reaching pixel-wise precision of 0.80 and recall of 0.86. Finally, publicly available Stanford TMA cores were used to demonstrate high performance of the framework qualitatively.},
archivePrefix = {arXiv},
arxivId = {1807.10466},
author = {Burlutskiy, Nikolay and Gu, Feng and Wilen, Lena Kajland and Backman, Max and Micke, Patrick},
eprint = {1807.10466},
issn = {23318422},
journal = {arXiv},
month = {jul},
title = {{A deep learning framework for automatic diagnosis in lung cancer}},
url = {http://arxiv.org/abs/1807.10466},
volume = {abs/1807.1},
year = {2018}
}
@article{Zhao2018,
abstract = {Accurate and reliable brain tumor segmentation is a critical component in cancer diagnosis, treatment planning, and treatment outcome evaluation. Build upon successful deep learning techniques, a novel brain tumor segmentation method is developed by integrating fully convolutional neural networks (FCNNs) and Conditional Random Fields (CRFs) in a unified framework to obtain segmentation results with appearance and spatial consistency. We train a deep learning based segmentation model using 2D image patches and image slices in following steps: 1) training FCNNs using image patches; 2) training CRFs as Recurrent Neural Networks (CRF-RNN) using image slices with parameters of FCNNs fixed; and 3) fine-tuning the FCNNs and the CRF-RNN using image slices. Particularly, we train 3 segmentation models using 2D image patches and slices obtained in axial, coronal and sagittal views respectively, and combine them to segment brain tumors using a voting based fusion strategy. Our method could segment brain images slice-by-slice, much faster than those based on image patches. We have evaluated our method based on imaging data provided by the Multimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015 and BRATS 2016. The experimental results have demonstrated that our method could build a segmentation model with Flair, T1c, and T2 scans and achieve competitive performance as those built with Flair, T1, T1c, and T2 scans.},
author = {Zhao, Xiaomei and Wu, Yihong and Song, Guidong and Li, Zhenye and Zhang, Yazhuo and Fan, Yong},
doi = {10.1016/j.media.2017.10.002},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Brain tumor segmentation,Conditional random fields,Deep learning,Fully convolutional neural networks},
pages = {98--111},
pmid = {29040911},
title = {{A deep learning model integrating FCNNs and CRFs for brain tumor segmentation}},
url = {http://www.sciencedirect.com/science/article/pii/S136184151730141X},
volume = {43},
year = {2018}
}
@inproceedings{Zhou2017,
abstract = {Deep neural networks have been widely adopted for automatic organ segmentation from abdominal CT scans. However, the segmentation accuracy of some small organs (e.g., the pancreas) is sometimes below satisfaction, arguably because deep networks are easily disrupted by the complex and variable background regions which occupies a large fraction of the input volume. In this paper, we formulate this problem into a fixed-point model which uses a predicted segmentation mask to shrink the input region. This is motivated by the fact that a smaller input region often leads to more accurate segmentation. In the training process, we use the ground-truth annotation to generate accurate input regions and optimize network weights. On the testing stage, we fix the network parameters and update the segmentation results in an iterative manner. We evaluate our approach on the NIH pancreas segmentation dataset, and outperform the state-of-the-art by more than 4{\%}, measured by the average Dice-S{\o}rensen Coefficient (DSC). In addition, we report 62.43{\%} DSC in the worst case, which guarantees the reliability of our approach in clinical applications.},
address = {Cham},
archivePrefix = {arXiv},
arxivId = {1612.08230},
author = {Zhou, Yuyin and Xie, Lingxi and Shen, Wei and Wang, Yan and Fishman, Elliot K. and Yuille, Alan L.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-66182-7_79},
editor = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D Louis and Duchesne, Simon},
eprint = {1612.08230},
isbn = {9783319661810},
issn = {16113349},
pages = {693--701},
publisher = {Springer International Publishing},
title = {{A fixed-point model for pancreas segmentation in abdominal CT scans}},
volume = {10433 LNCS},
year = {2017}
}
@article{Jimenez-Carretero2019,
abstract = {Lung vessel segmentation has been widely explored by the biomedical image processing community; however, the differentiation of arterial from venous irrigation is still a challenge. Pulmonary artery–vein (AV) segmentation using computed tomography (CT) is growing in importance owing to its undeniable utility in multiple cardiopulmonary pathological states, especially those implying vascular remodelling, allowing the study of both flow systems separately. We present a new framework to approach the separation of tree-like structures using local information and a specifically designed graph-cut methodology that ensures connectivity as well as the spatial and directional consistency of the derived subtrees. This framework has been applied to the pulmonary AV classification using a random forest (RF) pre-classifier to exploit the local anatomical differences of arteries and veins. The evaluation of the system was performed using 192 bronchopulmonary segment phantoms, 48 anthropomorphic pulmonary CT phantoms, and 26 lungs from noncontrast CT images with precise voxel-based reference standards obtained by manually labelling the vessel trees. The experiments reveal a relevant improvement in the accuracy (∼ 20{\%}) of the vessel particle classification with the proposed framework with respect to using only the pre-classification based on local information applied to the whole area of the lung under study. The results demonstrated the accurate differentiation between arteries and veins in both clinical and synthetic cases, specifically when the image quality can guarantee a good airway segmentation, which opens a huge range of possibilities in the clinical study of cardiopulmonary diseases.},
author = {Jimenez-Carretero, Daniel and Bermejo-Pel{\'{a}}ez, David and Nardelli, Pietro and Fraga, Patricia and Fraile, Eduardo and {San Jos{\'{e}} Est{\'{e}}par}, Ra{\'{u}}l and Ledesma-Carbayo, Maria J.},
doi = {10.1016/j.media.2018.11.011},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Arteries,Artery-vein segmentation,Graph-cuts,Lung,Noncontrast CT,Phantoms,Random forest,Veins},
pages = {144--159},
pmid = {30579223},
title = {{A graph-cut approach for pulmonary artery-vein segmentation in noncontrast CT images}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841518308740},
volume = {52},
year = {2019}
}
@article{Simpson2019,
abstract = {Semantic segmentation of medical images aims to associate a pixel with a label in a medical image without human initialization. The success of semantic segmentation algorithms is contingent on the availability of high-quality imaging data with corresponding labels provided by experts. We sought to create a large collection of annotated medical image datasets of various clinically relevant anatomies available under open source license to facilitate the development of semantic segmentation algorithms. Such a resource would allow: 1) objective assessment of general-purpose segmentation methods through comprehensive benchmarking and 2) open and free access to medical image data for any researcher interested in the problem domain. Through a multi-institutional effort, we generated a large, curated dataset representative of several highly variable segmentation tasks that was used in a crowd-sourced challenge - the Medical Segmentation Decathlon held during the 2018 Medical Image Computing and Computer Aided Interventions Conference in Granada, Spain. Here, we describe these ten labeled image datasets so that these data may be effectively reused by the research community.},
archivePrefix = {arXiv},
arxivId = {1902.09063},
author = {Simpson, Amber L. and Antonelli, Michela and Bakas, Spyridon and Bilello, Michel and Farahani, Keyvan and {Van Ginneken}, Bram and Kopp-Schneider, Annette and Landman, Bennett A. and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M. and Bilic, Patrick and Christ, Patrick F. and Do, Richard K.G. and Gollub, Marc and Golia-Pernicka, Jennifer and Heckers, Stephan H. and Jarnagin, William R. and McHugo, Maureen K. and Napel, Sandy and Vorontsov, Eugene and Maier-Hein, Lena and {Jorge Cardoso}, M.},
eprint = {1902.09063},
journal = {arXiv},
month = {feb},
title = {{A large annotated medical image dataset for the development and evaluation of segmentation algorithms}},
url = {http://arxiv.org/abs/1902.09063},
year = {2019}
}
@article{Liew2017,
abstract = {Stroke is the leading cause of adult disability worldwide, with up to two-thirds of individuals experiencing long-term disabilities. Large-scale neuroimaging studies have shown promise in identifying robust biomarkers (e.g., measures of brain structure) of long-term stroke recovery following rehabilitation. However, analyzing large rehabilitation-related datasets is problematic due to barriers in accurate stroke lesion segmentation. Manually-traced lesions are currently the gold standard for lesion segmentation on T1-weighted MRIs, but are labor intensive and require anatomical expertise. While algorithms have been developed to automate this process, the results often lack accuracy. Newer algorithms that employ machine-learning techniques are promising, yet these require large training datasets to optimize performance. Here we present ATLAS (Anatomical Tracings of Lesions After Stroke), an open-source dataset of 304 T1-weighted MRIs with manually segmented lesions and metadata. This large, diverse dataset can be used to train and test lesion segmentation algorithms and provides a standardized dataset for comparing the performance of different segmentation methods. We hope ATLAS release 1.1 will be a useful resource to assess and improve the accuracy of current lesion segmentation methods.},
author = {Liew, Sook Lei and Anglin, Julia M. and Banks, Nick W. and Sondag, Matt and Ito, Kaori L. and Kim, Hosung and Chan, Jennifer and Ito, Joyce and Jung, Connie and Khoshab, Nima and Lefebvre, Stephanie and Nakamura, William and Saldana, David and Schmiesing, Allie and Tran, Cathy and Vo, Danny and Ard, Tyler and Heydari, Panthea and Kim, Bokkyu and Aziz-Zadeh, Lisa and Cramer, Steven C. and Liu, Jingchun and Soekadar, Surjo and Nordvik, Jan Egil and Westlye, Lars T. and Wang, Junping and Winstein, Carolee and Yu, Chunshui and Ai, Lei and Koo, Bonhwang and Craddock, R. Cameron and Milham, Michael and Lakich, Matthew and Pienta, Amy and Stroud, Alison},
doi = {10.1101/179614},
journal = {bioRxiv},
title = {{A large, open source dataset of stroke anatomical brain images and manual lesion segmentations}},
year = {2017}
}
@article{Liew2018,
abstract = {Stroke is the leading cause of adult disability worldwide, with up to two-thirds of individuals experiencing long-term disabilities. Large-scale neuroimaging studies have shown promise in identifying robust biomarkers (e.g., measures of brain structure) of long-term stroke recovery following rehabilitation. However, analyzing large rehabilitation-related datasets is problematic due to barriers in accurate stroke lesion segmentation. Manually-traced lesions are currently the gold standard for lesion segmentation on T1-weighted MRIs, but are labor intensive and require anatomical expertise. While algorithms have been developed to automate this process, the results often lack accuracy. Newer algorithms that employ machine-learning techniques are promising, yet these require large training datasets to optimize performance. Here we present ATLAS (Anatomical Tracings of Lesions After Stroke), an open-source dataset of 304 T1-weighted MRIs with manually segmented lesions and metadata. This large, diverse dataset can be used to train and test lesion segmentation algorithms and provides a standardized dataset for comparing the performance of different segmentation methods. We hope ATLAS release 1.1 will be a useful resource to assess and improve the accuracy of current lesion segmentation methods.},
author = {Liew, Sook Lei and Anglin, Julia M. and Banks, Nick W. and Sondag, Matt and Ito, Kaori L. and Kim, Hosung and Chan, Jennifer and Ito, Joyce and Jung, Connie and Khoshab, Nima and Lefebvre, Stephanie and Nakamura, William and Saldana, David and Schmiesing, Allie and Tran, Cathy and Vo, Danny and Ard, Tyler and Heydari, Panthea and Kim, Bokkyu and Aziz-Zadeh, Lisa and Cramer, Steven C. and Liu, Jingchun and Soekadar, Surjo and Nordvik, Jan Egil and Westlye, Lars T. and Wang, Junping and Winstein, Carolee and Yu, Chunshui and Ai, Lei and Koo, Bonhwang and Craddock, R. Cameron and Milham, Michael and Lakich, Matthew and Pienta, Amy and Stroud, Alison},
doi = {10.1038/sdata.2018.11},
issn = {20524463},
journal = {Scientific Data},
month = {dec},
number = {1},
pages = {180011},
pmid = {29461514},
title = {{A large, open source dataset of stroke anatomical brain images and manual lesion segmentations}},
url = {http://www.nature.com/articles/sdata201811},
volume = {5},
year = {2018}
}
@misc{TCIA-NNC2-0461,
author = {{Li, P., Wang, S., Li, T., Lu, J., HuangFu, Y., {\&} Wang}, D},
doi = {10.7937/TCIA.2020.NNC2-0461},
publisher = {The Cancer Imaging Archive},
title = {{A Large-Scale CT and PET/CT Dataset for Lung Cancer Diagnosis [Data set]}},
url = {https://wiki.cancerimagingarchive.net/x/WIkvB},
year = {2020}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed. {\textcopyright} 1943 The University of Chicago Press.},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
number = {4},
pages = {115--133},
title = {{A logical calculus of the ideas immanent in nervous activity}},
url = {https://doi.org/10.1007/BF02478259},
volume = {5},
year = {1943}
}
@book{Henderson2020,
address = {Cham},
author = {Henderson, Brad},
booktitle = {A Math-Based Writing System for Engineers},
doi = {10.1007/978-3-030-10756-7},
isbn = {978-3-030-10754-3},
publisher = {Springer International Publishing},
title = {{A Math-Based Writing System for Engineers}},
url = {http://link.springer.com/10.1007/978-3-030-10756-7},
year = {2020}
}
@book{Goodfellow2016DeepLearning,
abstract = {We study a family of 'classical' orthogonal polynomials which satisfy (apart from a three-term recurrence relation) an eigenvalue problem with a differential operator of Dunkl type. These polynomials can be obtained from the little q-Jacobi polynomials in the limit q = -1. We also show that these polynomials provide a nontrivial realization of the Askey-Wilson algebra for q = -1. {\textcopyright} 2011 IOP Publishing Ltd.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Vinet, Luc and Zhedanov, Alexei},
booktitle = {Journal of Physics A: Mathematical and Theoretical},
doi = {10.1088/1751-8113/44/8/085201},
eprint = {1011.1669},
isbn = {9788578110796},
issn = {17518113},
keywords = {icle},
number = {8},
pages = {1689--1699},
pmid = {25246403},
publisher = {The MIT Press},
title = {{A 'missing' family of classical orthogonal polynomials}},
url = {https://www.xarg.org/ref/a/0262035618/},
volume = {44},
year = {2011}
}
@article{Kumar2019,
abstract = {Generalized nucleus segmentation techniques can contribute greatly to reducing the time to develop and validate visual biomarkers for new digital pathology datasets. We summarize the results of MoNuSeg 2018 Challenge whose objective was to develop generalizable nuclei segmentation techniques in digital pathology. The challenge was an official satellite event of the MICCAI 2018 conference in which 32 teams with more than 80 participants from geographically diverse institutes participated. Contestants were given a training set with 30 images from seven organs with annotations of 21,623 individual nuclei. A test dataset with 14 images taken from seven organs, including two organs that did not appear in the training set was released without annotations. Entries were evaluated based on average aggregated Jaccard index (AJI) on the test set to prioritize accurate instance segmentation as opposed to mere semantic segmentation. More than half the teams that completed the challenge outperformed a previous baseline. Among the trends observed that contributed to increased accuracy were the use of color normalization as well as heavy data augmentation. Additionally, fully convolutional networks inspired by variants of U-Net, FCN, and Mask-RCNN were popularly used, typically based on ResNet or VGG base architectures. Watershed segmentation on predicted semantic segmentation maps was a popular post-processing strategy. Several of the top techniques compared favorably to an individual human annotator and can be used with confidence for nuclear morphometrics.},
author = {Kumar, Neeraj and Verma, Ruchika and Anand, Deepak and Zhou, Yanning and Onder, Omer Fahri and Tsougenis, Efstratios and Chen, Hao and Heng, Pheng Ann and Li, Jiahui and Hu, Zhiqiang and Wang, Yuqin Yunzhi and Koohbanani, Navid Alemi and Jahanifar, Mostafa and Tajeddin, Neda Zamani and Gooya, Ali and Rajpoot, Nasir and Ren, Xuhua and Zhou, Sihang and Wang, Qian and Shen, Dinggang and Yang, Cheng Kun and Weng, Chi Hung and Yu, Wei Hsiang and Yeh, Chao Yuan and Yang, Shuang and Xu, Shuoyu and Yeung, Pak Hei and Sun, Peng and Mahbod, Amirreza and Schaefer, Gerald and Ellinger, Isabella and Ecker, Rupert and Smedby, Orjan and Wang, Chunliang and Chidester, Benjamin and Ton, That Vinh and Tran, Minh Triet and Ma, Jun Jian and Do, Minh N. and Graham, Simon and Vu, Quoc Dang and Kwak, Jin Tae and Gunda, Akshaykumar and Chunduri, Raviteja and Hu, Corey and Zhou, Xiaoyang and Lotfi, Dariush and Safdari, Reza and Kascenas, Antanas and O'Neil, Alison and Eschweiler, Dennis and Stegmaier, Johannes and Cui, Yanping and Yin, Baocai and Chen, Kailin and Tian, Xinmei and Gruening, Philipp and Barth, Erhardt and Arbel, Elad and Remer, Itay and Ben-Dor, Amir and Sirazitdinova, Ekaterina and Kohl, Matthias and Braunewell, Stefan and Li, Yuexiang and Xie, Xinpeng and Shen, Linlin and Ma, Jun Jian and Baksi, Krishanu Das and Khan, Mohammad Azam and Choo, Jaegul and Colomer, Adrian and Naranjo, Valery and Pei, Linmin and Iftekharuddin, Khan M. and Roy, Kaushiki and Bhattacharjee, Debotosh and Pedraza, Anibal and Bueno, Maria Gloria and Devanathan, Sabarinathan and Radhakrishnan, Saravanan and Koduganty, Praveen and Wu, Zihan and Cai, Guanyu and Liu, Xiaojie and Wang, Yuqin Yunzhi and Sethi, Amit},
doi = {10.1109/TMI.2019.2947628},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Multi-organ,aggregated Jaccard index,digital pathology,instance segmentation,nucleus segmentation},
month = {may},
number = {5},
pages = {1380--1391},
pmid = {31647422},
title = {{A Multi-Organ Nucleus Segmentation Challenge}},
url = {https://ieeexplore.ieee.org/document/8880654/},
volume = {39},
year = {2019}
}
@article{Chang2011,
abstract = {Thyroid region segmentation and volume estimation is a prerequisite step to diagnosing the pathology of the thyroid gland. In this study, a progressive learning vector quantization neural network (PLVQNN) combined with a preprocessing procedure is proposed for automatic thyroid segmentation and volume estimation using computerized tomography (CT) images. The preprocessing procedure is used to extract the region of interest (ROI) of thyroid glands and exclude non-thyroid glands based on thyroid anatomy. The PLVQNN contains several learning vector quantization neural networks (LVQNNs), each responsible for segmenting one slice of a thyroid CT image. The training of the PLVQNN is conducted starting from the LVQNN of most reliable (middle) slices, where the thyroid has the largest region. The training then propagates upwards and downwards to adjacent LVQNNs using the results of the middle slice as the initialization values and constraints. Experimental results show that the proposed method can effectively segment thyroid glands and estimate thyroid volume. {\textcopyright} 2011 IEEE.},
author = {Chang, Chuan Yu and Chung, Pau Choo and Hong, Yong Cheng and Tseng, Chin Hsiao},
doi = {10.1109/MCI.2011.942756},
issn = {1556603X},
journal = {IEEE Computational Intelligence Magazine},
month = {nov},
number = {4},
pages = {43--55},
title = {{A neural network for thyroid segmentation and volume estimation in CT images}},
url = {http://ieeexplore.ieee.org/document/6052365/},
volume = {6},
year = {2011}
}
@misc{TCIA-CT-Lymph-Nodes,
author = {Roth, H R and Lu, L and Seff, A and Cherry, K M and Hoffman, J and Wang, S and Summers, R M},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2015.AQIIDCNM},
publisher = {The Cancer Imaging Archive},
title = {{A new 2.5 d representation for lymph node detection in CT}},
url = {https://wiki.cancerimagingarchive.net/x/0gAtAQ},
year = {2015}
}
@inproceedings{Roth2014,
abstract = {Automated Lymph Node (LN) detection is an important clinical diagnostic task but very challenging due to the low contrast of surrounding structures in Computed Tomography (CT) and to their varying sizes, poses, shapes and sparsely distributed locations. State-of-the-art studies show the performance range of 52.9{\%} sensitivity at 3.1 false-positives per volume (FP/vol.), or 60.9{\%} at 6.1 FP/vol. for mediastinal LN, by one-shot boosting on 3D HAAR features. In this paper, we first operate a preliminary candidate generation stage, towards ∼100{\%} sensitivity at the cost of high FP levels (∼40 per patient), to harvest volumes of interest (VOI). Our 2.5D approach consequently decomposes any 3D VOI by resampling 2D reformatted orthogonal views N times, via scale, random translations, and rotations with respect to the VOI centroid coordinates. These random views are then used to train a deep Convolutional Neural Network (CNN) classifier. In testing, the CNN is employed to assign LN probabilities for all N random views that can be simply averaged (as a set) to compute the final classification probability per VOI. We validate the approach on two datasets: 90 CT volumes with 388 mediastinal LNs and 86 patients with 595 abdominal LNs. We achieve sensitivities of 70{\%}/83{\%} at 3 FP/vol. and 84{\%}/90{\%} at 6 FP/vol. in mediastinum and abdomen respectively, which drastically improves over the previous state-of-the-art work. {\textcopyright} 2014 Springer International Publishing.},
author = {Roth, Holger R. and Lu, Le and Seff, Ari and Cherry, Kevin M. and Hoffman, Joanne and Wang, Shijun and Liu, Jiamin and Turkbey, Evrim and Summers, Ronald M.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10404-1_65},
isbn = {9783319104034},
issn = {16113349},
pmid = {25333158},
title = {{A new 2.5D representation for lymph node detection using random sets of deep convolutional neural network observations}},
year = {2014}
}
@article{Kostakoglu2015,
abstract = {Our objective was to determine whether early change in standardized uptake values (SUVs) of 3′deoxy-3′-18F-fluorothymidine (18F-FLT) using PET with CT could predict pathologic complete response (pCR) of primary breast cancer to neoadjuvant chemotherapy (NAC). The key secondary objective was to correlate SUV with the proliferation marker Ki-67 at baseline and after NAC Methods: This prospective, multicenter phase II study did not specify the therapeutic regimen, thus, NAC varied among centers. Al evaluable patients underwent 18F-FLT PET/CT at baseline (FLT1) and after 1 cycle of NAC (FLT2); 43 patients were imaged at FLT1, FLT2, and after NAC completion (FLT3). The percentage change in maximum SUV ({\%}DSUVmax) between FLT1 and FLT2 and FLT3 was calculated for the primary tumors. The predictive value of DSUVmax for pCR was determined using receiver-operating-characteristic curve analysis. The correlation between SUVmax and Ki-67 was also assessed. Results: Fifty-one of 90 recruited patients (median age, 54 y; stage IIA-IIIC) met the eligibility criteria for the primary objective analysis, with an additional 22 patients totaling 73 patients for secondary analyses. A pCR in the primary breast cancer was achieved in 9 of 51 patients. NAC resulted in a significant reduction in {\%}SUVmax (mean D, 39{\%}; 95{\%} confidence interval, 31-46). There was a marginal difference in {\%}DSUVmax-FLT1-FLT2 between pCR and no-pCR patient groups (Wilcoxon 1 -sided P = 0.050). The area under the curve for DSUVmax in the prediction of pCR was 0.68 (90{\%} confidence interval, 0.50-0.83; Delong 1-sided P = 0.05), with slightly better predictive value for percentage mean SUV (P = 0.02) and similar prediction for peak SUV (P = 0.04). There was a weak correlation with pretherapy SUVmax and Ki-67 (r = 0.29, P = 0.04), but the correlation between SUVmax and Ki-67 after completion of NAC was stronger (r = 0.68, P {\textless} 0.0001). Conclusion: 18F-FLT PET imaging of breast cancer after 1 cycle of NAC weakly predicted pCR in the setting of variable NAC regimens. Posttherapy 18F-FLT uptake correlated with Ki-67 on surgical specimens. These results suggest some efficacy of 18F-FLT as an indicator of early therapeutic response of breast cancer to NAC and support future multicenter studies to test 18F-FLT PET in a more uniformly treated patient population.},
author = {Kostakoglu, Lale and Duan, Fenghai and Idowu, Michael O. and Jolles, Paul R. and Bear, Harry D. and Muzi, Mark and Cormack, Jean and Muzi, John P. and Pryma, Daniel A. and Specht, Jennifer M. and Hovanessian-Larsen, Linda and Miliziano, John and Mallett, Sharon and Shields, Anthony F. and Mankoff, David A.},
doi = {10.2967/jnumed.115.160663},
issn = {2159662X},
journal = {Journal of Nuclear Medicine},
keywords = {18 F-FLT PET,Breast cancer,Early treatment response,Neoadjuvant therapy},
month = {nov},
number = {11},
pages = {1681--1689},
title = {{A phase II study of 3′-Deoxy-3′-18F-fluorothymidine PET in the assessment of early response of breast cancer to neoadjuvant chemotherapy: Results from ACRIN 6688}},
url = {http://jnm.snmjournals.org/cgi/doi/10.2967/jnumed.115.160663},
volume = {56},
year = {2015}
}
@article{Vallieres2015,
abstract = {This study aims at developing a joint FDG-PET and MRI texture-based model for the early evaluation of lung metastasis risk in soft-tissue sarcomas (STSs). We investigate if the creation of new composite textures from the combination of FDG-PET and MR imaging information could better identify aggressive tumours. Towards this goal, a cohort of 51 patients with histologically proven STSs of the extremities was retrospectively evaluated. All patients had pre-treatment FDG-PET and MRI scans comprised of T1-weighted and T2-weighted fat-suppression sequences (T2FS). Nine non-texture features (SUV metrics and shape features) and forty-one texture features were extracted from the tumour region of separate (FDG-PET, T1 and T2FS) and fused (FDG-PET/T1 and FDG-PET/T2FS) scans. Volume fusion of the FDG-PET and MRI scans was implemented using the wavelet transform. The influence of six different extraction parameters on the predictive value of textures was investigated. The incorporation of features into multivariable models was performed using logistic regression. The multivariable modeling strategy involved imbalance-adjusted bootstrap resampling in the following four steps leading to final prediction model construction: (1) feature set reduction; (2) feature selection; (3) prediction performance estimation; and (4) computation of model coefficients. Univariate analysis showed that the isotropic voxel size at which texture features were extracted had the most impact on predictive value. In multivariable analysis, texture features extracted from fused scans significantly outperformed those from separate scans in terms of lung metastases prediction estimates. The best performance was obtained using a combination of four texture features extracted from FDG-PET/T1 and FDG-PET/T2FS scans. This model reached an area under the receiver-operating characteristic curve of 0.984 ± 0.002, a sensitivity of 0.955 ± 0.006, and a specificity of 0.926 ± 0.004 in bootstrapping evaluations. Ultimately, lung metastasis risk assessment at diagnosis of STSs could improve patient outcomes by allowing better treatment adaptation.},
author = {Valli{\`{e}}res, M. and Freeman, C. R. and Skamene, S. R. and {El Naqa}, I.},
doi = {10.1088/0031-9155/60/14/5471},
issn = {13616560},
journal = {Physics in Medicine and Biology},
keywords = {FDG-PET,MRI,lung metastases,outcome prediction,radiomics,soft-tissue sarcoma,texture analysis},
month = {jul},
number = {14},
pages = {5471--5496},
title = {{A radiomics model from joint FDG-PET and MRI texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities}},
url = {https://iopscience.iop.org/article/10.1088/0031-9155/60/14/5471},
volume = {60},
year = {2015}
}
@article{Avants2011,
abstract = {The United States National Institutes of Health (NIH) commit significant support to open-source data and software resources in order to foment reproducibility in the biomedical imaging sciences. Here, we report and evaluate a recent product of this commitment: Advanced Neuroimaging Tools (ANTs), which is approaching its 2.0 release. The ANTs open source software library consists of a suite of state-of-the-art image registration, segmentation and template building tools for quantitative morphometric analysis. In this work, we use ANTs to quantify, for the first time, the impact of similarity metrics on the affine and deformable components of a template-based normalization study. We detail the ANTs implementation of three similarity metrics: squared intensity difference, a new and faster cross-correlation, and voxel-wise mutual information. We then use two-fold cross-validation to compare their performance on openly available, manually labeled, T1-weighted MRI brain image data of 40 subjects (UCLA's LPBA40 dataset). We report evaluation results on cortical and whole brain labels for both the affine and deformable components of the registration. Results indicate that the best ANTs methods are competitive with existing brain extraction results (Jaccard = 0.958) and cortical labeling approaches. Mutual information affine mapping combined with cross-correlation diffeomorphic mapping gave the best cortical labeling results (Jaccard = 0.669. ±. 0.022). Furthermore, our two-fold cross-validation allows us to quantify the similarity of templates derived from different subgroups. Our open code, data and evaluation scripts set performance benchmark parameters for this state-of-the-art toolkit. This is the first study to use a consistent transformation framework to provide a reproducible evaluation of the isolated effect of the similarity metric on optimal template construction and brain labeling. {\textcopyright} 2010 Elsevier Inc.},
author = {Avants, Brian B. and Tustison, Nicholas J. and Song, Gang and Cook, Philip A. and Klein, Arno and Gee, James C.},
doi = {10.1016/j.neuroimage.2010.09.025},
issn = {10538119},
journal = {NeuroImage},
number = {3},
pages = {2033--2044},
pmid = {20851191},
title = {{A reproducible evaluation of ANTs similarity metric performance in brain image registration}},
url = {http://www.sciencedirect.com/science/article/pii/S1053811910012061},
volume = {54},
year = {2011}
}
@article{Gavrielides2010,
abstract = {A number of interrelated factors can affect the precision and accuracy of lung nodule size estimation. To quantify the effect of these factors, we have been conducting phantom CT studies using an anthropomorphic thoracic phantom containing a vasculature insert to which synthetic nodules were inserted or attached. Ten repeat scans were acquired on different multi-detector scanners, using several sets of acquisition and reconstruction protocols and various nodule characteristics (size, shape, density, location). This study design enables both bias and variance analysis for the nodule size estimation task. The resulting database is in the process of becoming publicly available as a resource to facilitate the assessment of lung nodule size estimation methodologies and to enable comparisons between different methods regarding measurement error. This resource complements public databases of clinical data and will contribute towards the development of procedures that will maximize the utility of CT imaging for lung cancer screening and tumor therapy evaluation.},
author = {Gavrielides, Marios A. and Kinnard, Lisa M. and Myers, Kyle J. and Peregoy, Jennifer and Pritchard, William F. and Zeng, Rongping and Esparza, Juan and Karanian, John and Petrick, Nicholas},
doi = {10.1364/oe.18.015244},
issn = {1094-4087},
journal = {Optics Express},
month = {jul},
number = {14},
pages = {15244},
title = {{A resource for the assessment of lung nodule size estimation methods: database of thoracic CT scans of an anthropomorphic phantom}},
url = {https://www.osapublishing.org/oe/abstract.cfm?uri=oe-18-14-15244},
volume = {18},
year = {2010}
}
@article{Gui2020,
abstract = {Generative adversarial networks (GANs) are a hot research topic recently. GANs have been widely studied since 2014, and a large number of algorithms have been proposed. However, there is few comprehensive study explaining the connections among different GANs variants, and how they have evolved. In this paper, we attempt to provide a review on various GANs methods from the perspectives of algorithms, theory, and applications. Firstly, the motivations, mathematical representations, and structure of most GANs algorithms are introduced in details. Furthermore, GANs have been combined with other machine learning algorithms for specific applications, such as semi-supervised learning, transfer learning, and reinforcement learning. This paper compares the commonalities and differences of these GANs methods. Secondly, theoretical issues related to GANs are investigated. Thirdly, typical applications of GANs in image processing and computer vision, natural language processing, music, speech and audio, medical field, and data science are illustrated. Finally, the future open research problems for GANs are pointed out.},
archivePrefix = {arXiv},
arxivId = {2001.06937},
author = {Gui, Jie and Sun, Zhenan and Wen, Yonggang and Tao, Dacheng and Ye, Jieping},
eprint = {2001.06937},
journal = {arXiv},
keywords = {Algorithm,Applications,Deep Learning,Generative Adversarial Networks,Theory},
month = {jan},
title = {{A review on generative adversarial networks: Algorithms, theory, and applications}},
url = {http://arxiv.org/abs/2001.06937},
year = {2020}
}
@book{Blackwell2011,
abstract = {In the modern world, every scientist who wants to publish findings in an interna- tional, peer-reviewed journal must write in English. This can be very challenging for people who are not native speakers of English. Indeed, it can be challenging for people who are native speakers. However, whether you are writing papers in your first or any other language, the process can be greatly facilitated by approaching it in a logical, systematic manner. Writing a paper is not easy, especially if you are not writing in your first language, but approaching the task methodically can ease the process, by: Delimiting the study. Writing brief (subsequently expanded) statements describing the rationale and specific objective(s) of the study, what was done, the main findings, novel aspects and limitations (focus) of the analyses, and finally the implications of the findings. Using these statements to compose each section of the paper, which should be written clearly and simply, following the target journals Instructions for authors, providing clear figures and tables, where appropriate. This procedure will help to maximize the chances of your paper being accepted. However, do not be too dismayed if it is rejected this does not necessarily mean that your work has no merit.},
address = {New York, NY},
author = {Blackwell, John and Martin, Jan},
booktitle = {A Scientific Approach to Scientific Writing},
doi = {10.1007/978-1-4419-9788-3},
isbn = {978-1-4419-9787-6},
publisher = {Springer New York},
title = {{A Scientific Approach to Scientific Writing}},
url = {http://link.springer.com/10.1007/978-1-4419-9788-3},
year = {2011}
}
@misc{Matek2019a,
author = {Matek, Christian and Schwarz, Simone and Marr, Carsten and Spiekermann, Karsten},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/tcia.2019.36f5o9ld},
publisher = {The Cancer Imaging Archive},
title = {{A Single-cell Morphological Dataset of Leukocytes from AML Patients and Non-malignant Controls [Data set]}},
url = {https://wiki.cancerimagingarchive.net/x/fgWkAw},
year = {2019}
}
@article{Hsu2016,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Fowler, Bridget},
doi = {10.1177/02632760022050997},
eprint = {0-387-31073-8},
institution = {Department of Computer Science National Taiwan University, Taipei 106, Taiwan},
isbn = {013805326X},
issn = {02632764},
journal = {Theory, Culture and Society},
month = {may},
number = {1},
pages = {39--61},
pmid = {18190633},
title = {{A sociological analysis of the satanic verses affair}},
url = {http://www.csie.ntu.edu.tw/{\%}7B{~}{\%}7Dcjlin/papers/guide/guide.pdf},
volume = {17},
year = {2000}
}
@article{Jeyavathana2016,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 305502844 A : Analysis -processing Segmentation Article CITATIONS 0 READS 171 3 , including : Balasubramanian Manonmaniam 17 SEE Anbarasa Manonmaniam 12 SEE All . The . Abstract : Pre - Processing and Segmentation Techniques are used in the application of medical images . Image segmentation is a tediousprocess due to restrictions on Image acquisitions . The most important goal of medical image segmentation is to perform operations on images to detect patterns and to retrieve information from it . In this paper , first medical image processing is discussed . Then we have been proposed approaches to segment CT and CXR images . The comparative study of various image processing techniques has been given in tabular form . This survey provides details of automated segmentation methods , specifically discussed in the context of CT images . The motive is to discuss the problems encountered in the segmentation of CT images , and the relative merits and limitations of methods currently available for segmentation of medical images .},
author = {{Beaulah Jeyavathana}, R and Balasubramanian, R and Pandian, A Anbarasa},
journal = {International Journal of Research and Scientific Innovation},
keywords = {CT,CXR,Pre - processing,Segmentation},
number = {June},
pages = {2321--2705},
title = {{A Survey : Analysis on Pre - processing and Segmentation Techniques for Medical Images}},
volume = {III},
year = {2016}
}
@inproceedings{Lee2015,
abstract = {A precise analysis of medical image is an important stage in the contouring phase throughout radiotherapy preparation. Medical images are mostly used as radiographic techniques in diagnosis, clinical studies and treatment planning Medical image processing tool are also similarly as important. With a medical image processing tool, it is possible to speed up and enhance the operation of the analysis of the medical image. This paper describes medical image processing software tool which attempts to secure the same kind of programmability advantage for exploring applications of the pipelined processors. These tools simulate complete systems consisting of several of the proposed processing components, in a configuration described by a graphical schematic diagram. In this paper, fifteen different medical image processing tools will be compared in several aspects. The main objective of the comparison is to gather and analysis on the tool in order to recommend users of different operating systems on what type of medical image tools to be used when analysing different types of imaging. A result table was attached and discussed in the paper.},
author = {Lee, Lay Khoon and Liew, Siau Chuin},
booktitle = {2015 4th International Conference on Software Engineering and Computer Systems, ICSECS 2015: Virtuous Software Solutions for Big Data},
doi = {10.1109/ICSECS.2015.7333105},
isbn = {9781467367226},
issn = {2289-8522},
keywords = {computer vision,image processing,tools component},
pages = {171--176},
title = {{A survey of medical image processing tools}},
year = {2015}
}
@article{Yong2012,
abstract = {More than 30 students from university campus participated in the Development of Biomedical Image Processing Software Package for New Learners Survey investigating the use of software package for processing and editing image. The survey was available online for six months. Facts and opinions were sought to learn the general information, interactive image processing tool, non-interactive (automatic) tool, current status and future of image processing package tool. Composed of 19 questions, the survey built a comprehensive picture of the software package, programming language, workflow of the tool and captured the attitudes of the respondents. Result shows that MATLAB was difficult to use but it was viewed in high regard however. The result of this study is expected to be beneficial and able to assist users on effective image processing and analysis in a newly developed software package.},
author = {Yong, Ching Yee and Chew, Kim Mey and Mahmood, Nasrul Humaimi and Ariffin, Ismail},
doi = {10.1016/j.sbspro.2012.09.654},
issn = {18770428},
journal = {Procedia - Social and Behavioral Sciences},
keywords = {Image editting,Image processing,Medical imaging,Software package,Visualisation tools},
pages = {265--271},
title = {{A Survey of Visualization Tools in Medical Imaging}},
url = {http://www.sciencedirect.com/science/article/pii/S187704281204116X},
volume = {56},
year = {2012}
}
@misc{Litjens2017,
abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
archivePrefix = {arXiv},
arxivId = {1702.05747},
author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A.W.M. and van Ginneken, Bram and S{\'{a}}nchez, Clara I.},
booktitle = {Medical Image Analysis},
doi = {10.1016/j.media.2017.07.005},
eprint = {1702.05747},
issn = {13618423},
keywords = {Convolutional neural networks,Deep learning,Medical imaging,Survey},
pages = {60--88},
pmid = {28778026},
title = {{A survey on deep learning in medical image analysis}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841517301135},
volume = {42},
year = {2017}
}
@article{Shorten2019,
abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
doi = {10.1186/s40537-019-0197-0},
issn = {21961115},
journal = {Journal of Big Data},
keywords = {Big data,Data Augmentation,Deep Learning,GANs,Image data},
number = {1},
pages = {60},
title = {{A survey on Image Data Augmentation for Deep Learning}},
url = {https://doi.org/10.1186/s40537-019-0197-0},
volume = {6},
year = {2019}
}
@article{Deisenroth2011,
abstract = {Policy search is a subfield in reinforcement learning which focuses on finding good parameters for a given policy parametrization. It is well suited for robotics as it can cope with high-dimensional state and action spaces, one of the main challenges in robot learning. We review recent successes of both model-free and model-based policy search in robot learning. Model-free policy search is a general approach to learn policies based on sampled trajectories. We classify model-free methods based on their policy evaluation strategy, policy update strategy, and exploration strategy and present a unified view on existing algorithms. Learning a policy is often easier than learning an accurate forward model, and, hence, model-free methods are more frequently used in practice. However, for each sampled trajectory, it is necessary to interact with the robot, which can be time consuming and challenging in practice. Model-based policy search addresses this problem by first learning a simulator of the robot's dynamics from data. Subsequently, the simulator generates trajectories that are used for policy learning. For both model-free and model-based policy search methods, we review their respective properties and their applicability to robotic systems.},
author = {Deisenroth, Marc Peter},
doi = {10.1561/2300000021},
issn = {1935-8253},
journal = {Foundations and Trends in Robotics},
number = {1-2},
pages = {1--142},
title = {{A Survey on Policy Search for Robotics}},
volume = {2},
year = {2011}
}
@article{Yao2017,
abstract = {Pre-processing is an important step in digital image matting, which aims to classify more accurate foreground and background pixels from the unknown region of the input three-region mask (Trimap). This step has no relation with the well-known matting equation and only compares color differences between the current unknown pixel and those known pixels. These newly classified pure pixels are then fed to the matting process as samples to improve the quality of the final matte. However, in the research field of image matting, the importance of pre-processing step is still blurry. Moreover, there are no corresponding review articles for this step, and the quantitative comparison of Trimap and alpha mattes after this step still remains unsolved. In this paper, the necessity and the importance of pre-processing step in image matting are firstly discussed in details. Next, current pre-processing methods are introduced by using the following two categories: static thresholding methods and dynamic thresholding methods. Analyses and experimental results show that static thresholding methods, especially the most popular iterative method, can make accurate pixel classifications in those general Trimaps with relatively fewer unknown pixels. However, in a much larger Trimap, there methods are limited by the conservative color and spatial thresholds. In contrast, dynamic thresholding methods can make much aggressive classifications on much difficult cases, but still strongly suffer from noises and false classifications. In addition, the sharp boundary detector is further discussed as a prior of pure pixels. Finally, summaries and a more effective approach are presented for pre-processing compared with the existing methods.},
author = {Yao, Gui Lin},
doi = {10.1007/s11390-017-1709-z},
issn = {10009000},
journal = {Journal of Computer Science and Technology},
keywords = {Trimap expansion,image matting,pixel classification,pre-processing},
number = {1},
pages = {122--138},
title = {{A Survey on Pre-Processing in Image Matting}},
url = {https://doi.org/10.1007/s11390-017-1709-z},
volume = {32},
year = {2017}
}
@article{Hindy2018,
abstract = {As the world moves towards being increasingly dependent on computers and automation, building secure applications, systems and networks are some of the main challenges faced in the current decade. The number of threats that individuals and businesses face is rising exponentially due to the increasing complexity of networks and services of modern networks. To alleviate the impact of these threats, researchers have proposed numerous solutions for anomaly detection; however, current tools often fail to adapt to ever-changing architectures, associated threats and zero-day attacks. This manuscript aims to pinpoint research gaps and shortcomings of current datasets, their impact on building Network Intrusion Detection Systems (NIDS) and the growing number of sophisticated threats. To this end, this manuscript provides researchers with two key pieces of information; a survey of prominent datasets, analyzing their use and impact on the development of the past decade's Intrusion Detection Systems (IDS) and a taxonomy of network threats and associated tools to carry out these attacks. The manuscript highlights that current IDS research covers only 33.3{\%} of our threat taxonomy. Current datasets demonstrate a clear lack of real-network threats, attack representation and include a large number of deprecated threats, which together limit the detection accuracy of current machine learning IDS approaches. The unique combination of the taxonomy and the analysis of the datasets provided in this manuscript aims to improve the creation of datasets and the collection of real-world data. As a result, this will improve the efficiency of the next generation IDS and reflect network threats more accurately within new datasets.},
archivePrefix = {arXiv},
arxivId = {1806.03517},
author = {Hindy, Hanan and Brosset, David and Bayne, Ethan and Seeam, Amar Kumar and Tachtatzis, Christos and Atkinson, Robert and Bellekens, Xavier},
doi = {10.1109/ACCESS.2020.3000179},
eprint = {1806.03517},
issn = {21693536},
journal = {IEEE Access},
keywords = {Anomaly detection,datasets,intrusion detection systems,network attacks,network security,security threats,survey,taxonomy},
month = {jun},
pages = {104650--104675},
title = {{A Taxonomy of Network Threats and the Effect of Current Datasets on Intrusion Detection Systems}},
url = {http://arxiv.org/abs/1806.03517},
volume = {8},
year = {2020}
}
@article{Ali2020a,
abstract = {The Endoscopy Computer Vision Challenge (EndoCV) is a crowd-sourcing initiative to address eminent problems in developing reliable computer aided detection and diagnosis endoscopy systems and suggest a pathway for clinical translation of technologies. Whilst endoscopy is a widely used diagnostic and treatment tool for hollow-organs, there are several core challenges often faced by endoscopists, mainly: 1) presence of multi-class artefacts that hinder their visual interpretation, and 2) difficulty in identifying subtle precancerous precursors and cancer abnormalities. Artefacts often affect the robustness of deep learning methods applied to the gastrointestinal tract organs as they can be confused with tissue of interest. EndoCV2020 challenges are designed to address research questions in these remits. In this paper, we present a summary of methods developed by the top 17 teams and provide an objective comparison of state-of-the-art methods and methods designed by the participants for two sub-challenges: i) artefact detection and segmentation (EAD2020), and ii) disease detection and segmentation (EDD2020). Multi-center, multi-organ, multi-class, and multi-modal clinical endoscopy datasets were compiled for both EAD2020 and EDD2020 sub-challenges. An out-of-sample generalisation ability of detection algorithms was also evaluated. Whilst most teams focused on accuracy improvements, only a few methods hold credibility for clinical usability. The best performing teams provided solutions to tackle class imbalance, and variabilities in size, origin, modality and occurrences by exploring data augmentation, data fusion, and optimal class thresholding techniques.},
archivePrefix = {arXiv},
arxivId = {2010.06034},
author = {Ali, Sharib and Dmitrieva, Mariia and Ghatwary, Noha and Bano, Sophia and Polat, Gorkem and Temizel, Alptekin and Krenzer, Adrian and Hekalo, Amar and Guo, Yun Bo and Matuszewski, Bogdan and Gridach, Mourad and Voiculescu, Irina and Yoganand, Vishnusai and Chavan, Arnav and Raj, Aryan and Nguyen, Nhan T. and Tran, Dat Q. and Huynh, Le Duy and Boutry, Nicolas and Rezvy, Shahadate and Chen, Haijian and Choi, Yoon Ho and Subramanian, Anand and Balasubramanian, Velmurugan and Gao, Xiaohong W. and Hu, Hongyu and Liao, Yusheng and Stoyanov, Danail and Daul, Christian and Realdon, Stefano and Cannizzaro, Renato and Lamarque, Dominique and Tran-Nguyen, Terry and Bailey, Adam and Braden, Barbara and East, James and Rittscher, Jens},
eprint = {2010.06034},
title = {{A translational pathway of deep learning methods in GastroIntestinal Endoscopy}},
url = {http://arxiv.org/abs/2010.06034},
year = {2020}
}
@article{Loffler2020,
abstract = {Published under a CC BY 4.0 license. Supplemental material is available for this article.},
author = {L{\"{o}}ffler, Maximilian T. and Sekuboyina, Anjany and Jacob, Alina and Grau, Anna-Lena and Scharr, Andreas and {El Husseini}, Malek and Kallweit, Mareike and Zimmer, Claus and Baum, Thomas and Kirschke, Jan S.},
doi = {10.1148/ryai.2020190138},
issn = {2638-6100},
journal = {Radiology: Artificial Intelligence},
title = {{A Vertebral Segmentation Dataset with Fracture Grading}},
year = {2020}
}
@article{Born2020,
abstract = {Controlling the COVID-19 pandemic largely hinges upon the existence of fast, safe, and highly-available diagnostic tools. Ultrasound, in contrast to CT or X-Ray, has many practical advantages and can serve as a globally-applicable first-line examination technique. We provide the largest publicly available lung ultrasound (US) dataset for COVID-19 consisting of 106 videos from three classes (COVID-19, bacterial pneumonia, and healthy controls); curated and approved by medical experts. On this dataset, we perform an in-depth study of the value of deep learning methods for differential diagnosis of COVID-19. We propose a frame-based convolutional neural network that correctly classifies COVID-19 US videos with a sensitivity of 0.98+-0.04 and a specificity of 0.91+-08 (frame-based sensitivity 0.93+-0.05, specificity 0.87+-0.07). We further employ class activation maps for the spatio-temporal localization of pulmonary biomarkers, which we subsequently validate for human-in-the-loop scenarios in a blindfolded study with medical experts. Aiming for scalability and robustness, we perform ablation studies comparing mobile-friendly, frame- and video-based architectures and show reliability of the best model by aleatoric and epistemic uncertainty estimates. We hope to pave the road for a community effort toward an accessible, efficient and interpretable screening method and we have started to work on a clinical validation of the proposed method. Data and code are publicly available.},
archivePrefix = {arXiv},
arxivId = {2009.06116},
author = {Born, Jannis and Wiedemann, Nina and Br{\"{a}}ndle, Gabriel and Buhre, Charlotte and Rieck, Bastian and Borgwardt, Karsten},
eprint = {2009.06116},
month = {sep},
title = {{Accelerating COVID-19 Differential Diagnosis with Explainable Ultrasound Image Analysis}},
url = {http://arxiv.org/abs/2009.06116},
year = {2020}
}
@article{Johnson2008,
abstract = {BACKGROUND: Computed tomographic (CT) colonography is a noninvasive option in screening for colorectal cancer. However, its accuracy as a screening tool in asymptomatic adults has not been well defined. METHODS: We recruited 2600 asymptomatic study participants, 50 years of age or older, at 15 study centers. CT colonographic images were acquired with the use of standard bowel preparation, stool and fluid tagging, mechanical insufflation, and multidetector-row CT scanners (with 16 or more rows). Radiologists trained in CT colonography reported all lesions measuring 5 mm or more in diameter. Optical colonoscopy and histologic review were performed according to established clinical protocols at each center and served as the reference standard. The primary end point was detection by CT colonography of histologically confirmed large adenomas and adenocarcinomas (10 mm in diameter or larger) that had been detected by colonoscopy; detection of smaller colorectal lesions (6 to 9 mm in diameter) was also evaluated. RESULTS: Complete data were available for 2531 participants (97{\%}). For large adenomas and cancers, the mean (±SE) per-patient estimates of the sensitivity, specificity, positive and negative predictive values, and area under the receiver-operating-characteristic curve for CT colonography were 0.90±0.03, 0.86±0.02, 0.23±0.02, 0.99±{\textless}0.01, and 0.89±0.02, respectively. The sensitivity of 0.90 (i.e., 90{\%}) indicates that CT colonography failed to detect a lesion measuring 10 mm or more in diameter in 10{\%} of patients. The per-polyp sensitivity for large adenomas or cancers was 0.84±0.04. The per-patient sensitivity for detecting adenomas that were 6 mm or more in diameter was 0.78. CONCLUSIONS: In this study of asymptomatic adults, CT colonographic screening identified 90{\%} of subjects with adenomas or cancers measuring 10 mm or more in diameter. These findings augment published data on the role of CT colonography in screening patients with an average risk of colorectal cancer. (ClinicalTrials.gov number, NCT00084929; American College of Radiology Imaging Network [ACRIN] number, 6664.) Copyright {\textcopyright} 2008 Massachusetts Medical Society. All rights reserved.},
author = {Johnson, C. Daniel and Chen, Mei-Hsiu and Toledano, Alicia Y. and Heiken, Jay P. and Dachman, Abraham and Kuo, Mark D. and Menias, Christine O. and Siewert, Betina and Cheema, Jugesh I. and Obregon, Richard G. and Fidler, Jeff L. and Zimmerman, Peter and Horton, Karen M. and Coakley, Kevin and Iyer, Revathy B. and Hara, Amy K. and Halvorsen, Robert A. and Casola, Giovanna and Yee, Judy and Herman, Benjamin A. and Burgart, Lawrence J. and Limburg, Paul J.},
doi = {10.1056/nejmoa0800996},
issn = {0028-4793},
journal = {New England Journal of Medicine},
month = {sep},
number = {12},
pages = {1207--1217},
pmid = {18799557},
title = {{Accuracy of CT Colonography for Detection of Large Adenomas and Cancers}},
url = {http://www.nejm.org/doi/abs/10.1056/NEJMoa0800996},
volume = {359},
year = {2008}
}
@book{Staron2020,
address = {Cham},
author = {Staron, Miroslaw},
booktitle = {Action Research in Software Engineering},
doi = {10.1007/978-3-030-32610-4},
isbn = {978-3-030-32609-8},
publisher = {Springer International Publishing},
title = {{Action Research in Software Engineering}},
url = {http://link.springer.com/10.1007/978-3-030-32610-4},
year = {2020}
}
@data{GrandChallengeADAM,
author = {Fu, Huazhu and Li, Fei and Orlando, Jos{\'{e}} Ignacio and Bogunovi{\'{c}}, Hrvoje and Sun, Xu and Liao, Jingan and Xu, Yanwu and Zhang, Shaochong and Zhang, Xiulan},
doi = {10.21227/dt4f-rt59},
publisher = {IEEE Dataport},
title = {{ADAM: Automatic Detection challenge on Age-related Macular degeneration}},
url = {http://dx.doi.org/10.21227/dt4f-rt59},
year = {2020}
}
@article{Liu2016,
abstract = {In this paper, we propose an adaptive spatial pooling method for enhancing the discriminability of feature representation for image classification. The core idea is to adopt a spatial distribution matrix to define how the image patches are pooled together. By formulating the pooling distribution learning and classifier training jointly, our method can extract multiple spatial layouts of arbitrary shapes rather than regular rectangular regions. By proper mathematical transformation, the distributions can be learned via a boosting-like algorithm, which improves the efficiency of learning especially for large distribution matrices. Further, our method allows category-specific pooling operations to take advantage of the different spatial layouts of different categories. Experimental results on three benchmark datasets UIUC-Sports, 21-Land-Use and Scene 15 demonstrate the effectiveness of our method.},
author = {Liu, Yinglu and Zhang, Yan Ming and Zhang, Xu Yao and Liu, Cheng Lin},
doi = {10.1016/j.patcog.2016.01.030},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Distribution matrix,Image classification,Spatial layout,Weighted pooling},
month = {jul},
pages = {58--67},
title = {{Adaptive spatial pooling for image classification}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320316000510},
volume = {55},
year = {2016}
}
@article{Carlin2017,
abstract = {The perceptual representation of individual faces is often explained with reference to a norm-based face space. In such spaces, individuals are encoded as vectors where identity is primarily conveyed by direction and distinctiveness by eccentricity. Here we measured human fMRI responses and psychophysical similarity judgments of individual face exemplars, which were generated as realistic 3D animations using a computer-graphics model. We developed and evaluated multiple neurobiologically plausible computational models, each of which predicts a representational distance matrix and a regional-mean activation profile for 24 face stimuli. In the fusiform face area, a face-space coding model with sigmoidal ramp tuning provided a better account of the data than one based on exemplar tuning. However, an image-processing model with weighted banks of Gabor filters performed similarly. Accounting for the data required the inclusion of a measurement-level population averaging mechanism that approximates how fMRI voxels locally average distinct neuronal tunings. Our study demonstrates the importance of comparing multiple models and of modeling the measurement process in computational neuroimaging.},
author = {Carlin, Johan D. and Kriegeskorte, Nikolaus},
doi = {10.1371/journal.pcbi.1005604},
editor = {Daunizeau, Jean},
issn = {15537358},
journal = {PLoS Computational Biology},
month = {jul},
number = {7},
pages = {e1005604},
pmid = {28746335},
title = {{Adjudicating between face-coding models with individual-face fMRI responses}},
url = {https://dx.plos.org/10.1371/journal.pcbi.1005604},
volume = {13},
year = {2017}
}
@article{Bakas2017,
abstract = {Gliomas belong to a group of central nervous system tumors, and consist of various sub-regions. Gold standard labeling of these sub-regions in radiographic imaging is essential for both clinical and computational studies, including radiomic and radiogenomic analyses. Towards this end, we release segmentation labels and radiomic features for all pre-operative multimodal magnetic resonance imaging (MRI) (n=243) of the multi-institutional glioma collections of The Cancer Genome Atlas (TCGA), publicly available in The Cancer Imaging Archive (TCIA). Pre-operative scans were identified in both glioblastoma (TCGA-GBM, n=135) and low-grade-glioma (TCGA-LGG, n=108) collections via radiological assessment. The glioma sub-region labels were produced by an automated state-of-the-art method and manually revised by an expert board-certified neuroradiologist. An extensive panel of radiomic features was extracted based on the manually-revised labels. This set of labels and features should enable i) direct utilization of the TCGA/TCIA glioma collections towards repeatable, reproducible and comparative quantitative studies leading to new predictive, prognostic, and diagnostic assessments, as well as ii) performance evaluation of computer-aided segmentation methods, and comparison to our state-of-the-art method.},
author = {Bakas, Spyridon and Akbari, Hamed and Sotiras, Aristeidis and Bilello, Michel and Rozycki, Martin and Kirby, Justin S. and Freymann, John B. and Farahani, Keyvan and Davatzikos, Christos},
doi = {10.1038/sdata.2017.117},
issn = {20524463},
journal = {Scientific Data},
month = {dec},
number = {1},
pages = {170117},
pmid = {28872634},
title = {{Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features}},
url = {http://www.nature.com/articles/sdata2017117},
volume = {4},
year = {2017}
}
@article{Finlayson2018,
abstract = {The discovery of adversarial examples has raised concerns about the practical deployment of deep learning systems. In this paper, we demonstrate that adversarial examples are capable of manipulating deep learning systems across three clinical domains. For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful. Our models are representative of the current state of the art in medical computer vision and, in some cases, directly reflect architectures already seeing deployment in real world clinical settings. In addition to the technical contribution of our paper, we synthesize a large body of knowledge about the healthcare system to argue that medicine may be uniquely susceptible to adversarial attacks, both in terms of monetary incentives and technical vulnerability. To this end, we outline the healthcare economy and the incentives it creates for fraud and provide concrete examples of how and why such attacks could be realistically carried out. We urge practitioners to be aware of current vulnerabilities when deploying deep learning systems in clinical settings, and encourage the machine learning community to further investigate the domain-specific characteristics of medical learning systems.},
archivePrefix = {arXiv},
arxivId = {1804.05296},
author = {Finlayson, Samuel G. and Chung, Hyung Won and Kohane, Isaac S. and Beam, Andrew L.},
eprint = {1804.05296},
journal = {arXiv},
keywords = {Adversarial examples,Deep learning,Healthcare,Neural networks,Security},
month = {apr},
title = {{Adversarial attacks against medical deep learning systems}},
url = {http://arxiv.org/abs/1804.05296},
year = {2018}
}
@article{Zhu2016,
abstract = {Mass segmentation is an important task in mammogram analysis, providing effective morphological features and regions of interest (ROI) for mass detection and classification. Inspired by the success of using deep convolutional features for natural image analysis and conditional random fields (CRF) for structural learning, we propose an end-to-end network for mammographic mass segmentation. The network employs a fully convolutional network (FCN) to model potential function, followed by a CRF to perform structural learning. Because the mass distribution varies greatly with pixel position, the FCN is combined with position priori for the task. Due to the small size of mammogram datasets, we use adversarial training to control over-fitting. Four models with different convolutional kernels are further fused to improve the segmentation results. Experimental results on two public datasets, INbreast and DDSM-BCRP, show that our end-to-end network combined with adversarial training achieves the-state-of-the-art results.},
archivePrefix = {arXiv},
arxivId = {1612.05970},
author = {Zhu, Wentao and Xiang, Xiang and Tran, Trac D. and Xie, Xiaohui},
doi = {10.1101/095786},
eprint = {1612.05970},
month = {dec},
title = {{Adversarial Deep Structural Networks for Mammographic Mass Segmentation}},
url = {http://arxiv.org/abs/1612.05970},
year = {2016}
}
@inproceedings{Song2017,
abstract = {We propose a method to reconstruct, complete and semantically label a 3D scene from a single input depth image. We improve the accuracy of the regressed semantic 3D maps by a novel architecture based on adversarial learning. In particular, we suggest using multiple adversarial loss terms that not only enforce realistic outputs with respect to the ground truth, but also an effective embedding of the internal features. This is done by correlating the latent features of the encoder working on partial 2.5D data with the latent features extracted from a variational 3D auto-encoder trained to reconstruct the complete semantic scene. In addition, differently from other approaches that operate entirely through 3D convolutions, at test time we retain the original 2.5D structure of the input during downsampling to improve the effectiveness of the internal representation of our model. We test our approach on the main benchmark datasets for semantic scene completion to qualitatively and quantitatively assess the effectiveness of our proposal.},
archivePrefix = {arXiv},
arxivId = {1810.10901},
author = {Wang, Yida and Tan, David Joseph and Navab, Nassir and Tombari, Federico},
booktitle = {Proceedings - 2018 International Conference on 3D Vision, 3DV 2018},
doi = {10.1109/3DV.2018.00056},
eprint = {1810.10901},
isbn = {9781538684252},
keywords = {Adversarial training,Depth image,Latent space,Scene completion},
pages = {426--434},
title = {{Adversarial semantic scene completion from a single depth image}},
year = {2018}
}
@article{Moeskops2017,
abstract = {Convolutional neural networks (CNNs) have been applied to various automatic image segmentation tasks in medical image analysis, including brain MRI segmentation. Generative adversarial networks have recently gained popularity because of their power in generating images that are difficult to distinguish from real images. In this study we use an adversarial training approach to improve CNN-based brain MRI segmentation. To this end, we include an additional loss function that motivates the network to generate segmentations that are difficult to distinguish from manual segmentations. During training, this loss function is optimised together with the conventional average per-voxel cross entropy loss. The results show improved segmentation performance using this adversarial training procedure for segmentation of two different sets of images and using two different network architectures, both visually and in terms of Dice coefficients.},
archivePrefix = {arXiv},
arxivId = {1707.03195},
author = {Moeskops, Pim and Veta, Mitko and Lafarge, Maxime W. and Eppenhof, Koen A.J. and Pluim, Josien P.W.},
doi = {10.1007/978-3-319-67558-9_7},
eprint = {1707.03195},
isbn = {9783319675572},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Adversarial networks,Brain MRI,Convolutional neural networks,Deep learning,Dilated convolution,Medical image segmentation},
month = {jul},
pages = {56--64},
title = {{Adversarial training and dilated convolutions for brain MRI segmentation}},
url = {http://arxiv.org/abs/1707.03195},
volume = {10553 LNCS},
year = {2017}
}
@article{Fu2020,
abstract = {Angle closure glaucoma (ACG) is a more aggressive disease than open-angle glaucoma, where the abnormal anatomical structures of the anterior chamber angle (ACA) may cause an elevated intraocular pressure and gradually leads to glaucomatous optic neuropathy and eventually to visual impairment and blindness. Anterior Segment Optical Coherence Tomography (AS-OCT) imaging provides a fast and contactless way to discriminate angle closure from open angle. Although many medical image analysis algorithms have been developed for glaucoma diagnosis, only a few studies have focused on AS-OCT imaging. In particular, there is no public AS-OCT dataset available for evaluating the existing methods in a uniform way, which limits the progress in the development of automated techniques for angle closure detection and assessment. To address this, we organized the Angle closure Glaucoma Evaluation challenge (AGE), held in conjunction with MICCAI 2019. The AGE challenge consisted of two tasks: scleral spur localization and angle closure classification. For this challenge, we released a large data of 4800 annotated AS-OCT images from 199 patients, and also proposed an evaluation framework to benchmark and compare different models. During the AGE challenge, over 200 teams registered online, and more than 1100 results were submitted for online evaluation. Finally, eight teams participated in the onsite challenge. In this paper, we summarize these eight onsite challenge methods and analyze their corresponding results in the two tasks. We further discuss limitations and future directions. In the AGE challenge, the top-performing approach had an average Euclidean Distance of 10 pixel (10µm) in scleral spur localization, while in the task of angle closure classification, all the algorithms achieved the satisfactory performances, especially, 100{\%} accuracy rate for top-two performances. These artificial intelligence techniques were shown to have the potential to enable new developments in AS-OCT image analysis and image-based angle closure glaucoma assessment in particular.},
archivePrefix = {arXiv},
arxivId = {2005.02258},
author = {Fu, Huazhu and Li, Fei and Sun, Xu and Cao, Xingxing and Liao, Jingan and Orlando, Jos{\'{e}} Ignacio and Tao, Xing and Li, Yuexiang and Zhang, Shihao and Tan, Mingkui and Yuan, Chenglang and Bian, Cheng and Xie, Ruitao and Li, Jiongcheng and Li, Xiaomeng and Wang, Jing and Geng, Le and Li, Panming and Hao, Huaying and Liu, Jiang and Kong, Yan and Ren, Yongyong and Bogunovi{\'{c}}, Hrvoje and Zhang, Xiulan and Xu, Yanwu},
eprint = {2005.02258},
journal = {arXiv},
title = {{AGE Challenge: Angle Closure Glaucoma Evaluation in Anterior Segment Optical Coherence Tomography}},
url = {http://arxiv.org/abs/2005.02258},
year = {2020}
}
@data{Zhang2019a,
author = {Zhang, Huazhu Fu; Fei Li; Jos{\'{e}} Ignacio Orlando; Hrvoje Bogunovi{\'{c}}; Xu Sun; Jingan Liao; Yanwu Xu; Shaochong Zhang; Xiulan},
doi = {10.21227/petb-fy10},
publisher = {IEEE Dataport},
title = {{AGE: Angle closure Glaucoma Evaluation Challenge}},
url = {http://dx.doi.org/10.21227/petb-fy10},
year = {2019}
}
@article{Karim2018,
abstract = {Structural changes to the wall of the left atrium are known to occur with conditions that predispose to Atrial fibrillation. Imaging studies have demonstrated that these changes may be detected non-invasively. An important indicator of this structural change is the wall's thickness. Present studies have commonly measured the wall thickness at few discrete locations. Dense measurements with computer algorithms may be possible on cardiac scans of Computed Tomography (CT) and Magnetic Resonance Imaging (MRI). The task is challenging as the atrial wall is a thin tissue and the imaging resolution is a limiting factor. It is unclear how accurate algorithms may get and how they compare in this new emerging area. We approached this problem of comparability with the Segmentation of Left Atrial Wall for Thickness (SLAWT) challenge organised in conjunction with MICCAI 2016 conference. This manuscript presents the algorithms that had participated and evaluation strategies for comparing them on the challenge image database that is now open-source. The image database consisted of cardiac CT (n=10) and MRI (n=10) of healthy and diseased subjects. A total of 6 algorithms were evaluated with different metrics, with 3 algorithms in each modality. Segmentation of the wall with algorithms was found to be feasible in both modalities. There was generally a lack of accuracy in the algorithms and inter-rater differences showed that algorithms could do better. Benchmarks were determined and algorithms were ranked to allow future algorithms to be ranked alongside the state-of-the-art techniques presented in this work. A mean atlas was also constructed from both modalities to illustrate the variation in thickness within this small cohort.},
author = {Karim, Rashed and Blake, Lauren Emma and Inoue, Jiro and Tao, Qian and Jia, Shuman and Housden, R. James and Bhagirath, Pranav and Duval, Jean Luc and Varela, Marta and Behar, Jonathan and Cadour, Loic and van der Geest, Rob J. and Cochet, Hubert and Drangova, Maria and Sermesant, Maxime and Razavi, Reza and Aslanidi, Oleg and Rajani, Ronak and Rhode, Kawal},
doi = {10.1016/j.media.2018.08.004},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Atrial fibrillation,Left atrial wall thickness,Left atrium,Myocardium},
month = {dec},
pages = {36--53},
pmid = {30208355},
title = {{Algorithms for left atrial wall segmentation and thickness – Evaluation on an open-source CT and MRI image database}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841518306431},
volume = {50},
year = {2018}
}
@misc{Mourya2019,
author = {Mourya, Simmi and Kant, Sonaal and Kumar, Pulkit and Gupta, Anubha and Gupta, Rita},
doi = {10.7937/TCIA.2019.DC64I46R},
publisher = {The Cancer Imaging Archive},
title = {{ALL Challenge dataset of ISBI 2019}},
url = {https://wiki.cancerimagingarchive.net/x/zwYlAw},
year = {2019}
}
@inproceedings{Khagi2019,
abstract = {Various Convolutional Neural Network (CNN) architecture has been proposed for image classification and Object recognition. For the image based classification, it is a complex task for CNN to deal with hundreds of MRI Image slices, each of almost identical nature in a single patient. So, classifying a number of patients as an AD, MCI or NC based on 3D MRI becomes vague technique using 2D CNN architecture. Hence, to address this issue, we have simplified the idea of classifying patients on basis of 3D MRI but acknowledging the 2D features generated from the CNN framework. We present our idea regarding how to obtain 2D features from MRI and transform it to be applicable to classify using machine learning algorithm. Our experiment shows the result of classifying 3 class subjects patients. We employed scratched trained CNN or pretrained Alexnet CNN as generic feature extractor of 2D image which dimensions were reduced using PCA+TSNE, and finally classifying using simple Machine learning algorithm like KNN, Navies Bayes Classifier. Although the result is not so impressive but it definitely shows that this can be better than scratch trained CNN softmax classification based on probability score. The generated feature can be well manipulated and refined for better accuracy, sensitivity, and specificity.},
author = {Khagi, Bijen and Lee, Chung Ghiu and Kwon, Goo Rak},
booktitle = {BMEiCON 2018 - 11th Biomedical Engineering International Conference},
doi = {10.1109/BMEiCON.2018.8609974},
isbn = {9781538657249},
keywords = {CNN,Classifier,Generic feature,MRI,PCA,TSNE},
title = {{Alzheimer's disease Classification from Brain MRI based on transfer learning from CNN}},
year = {2019}
}
@article{Aisen2015,
abstract = {Introduction This article reviews the current status of the Clinical Core of the Alzheimer's Disease Neuroimaging Initiative (ADNI), and summarizes planning for the next stage of the project. Methods Clinical Core activities and plans were synthesized based on discussions among the Core leaders and external advisors. Results The longitudinal data in ADNI-2 provide natural history data on a clinical trials population and continue to inform refinement and standardization of assessments, models of trajectories, and clinical trial methods that have been extended into sporadic preclinical Alzheimer's disease (AD). Discussion Plans for the next phase of the ADNI project include maintaining longitudinal follow-up of the normal and mild cognitive impairment cohorts, augmenting specific clinical cohorts, and incorporating novel computerized cognitive assessments and patient-reported outcomes. A major hypothesis is that AD represents a gradually progressive disease that can be identified precisely in its long presymptomatic phase, during which intervention with potentially disease-modifying agents may be most useful.},
author = {Aisen, Paul S. and Petersen, Ronald C. and Donohue, Michael and Weiner, Michael W.},
doi = {10.1016/j.jalz.2015.05.005},
issn = {15525279},
journal = {Alzheimer's and Dementia},
keywords = {Alzheimer's disease,Amyloid,Cognitive assessment},
month = {jul},
number = {7},
pages = {734--739},
title = {{Alzheimer's Disease Neuroimaging Initiative 2 Clinical Core: Progress and plans}},
url = {http://doi.wiley.com/10.1016/j.jalz.2015.05.005},
volume = {11},
year = {2015}
}
@article{Desikan2006,
abstract = {In this study, we have assessed the validity and reliability of an automated labeling system that we have developed for subdividing the human cerebral cortex on magnetic resonance images into gyral based regions of interest (ROIs). Using a dataset of 40 MRI scans we manually identified 34 cortical ROIs in each of the individual hemispheres. This information was then encoded in the form of an atlas that was utilized to automatically label ROIs. To examine the validity, as well as the intra- and inter-rater reliability of the automated system, we used both intraclass correlation coefficients (ICC), and a new method known as mean distance maps, to assess the degree of mismatch between the manual and the automated sets of ROIs. When compared with the manual ROIs, the automated ROIs were highly accurate, with an average ICC of 0.835 across all of the ROIs, and a mean distance error of less than 1 mm. Intra- and inter-rater comparisons yielded little to no difference between the sets of ROIs. These findings suggest that the automated method we have developed for subdividing the human cerebral cortex into standard gyral-based neuroanatomical regions is both anatomically valid and reliable. This method may be useful for both morphometric and functional studies of the cerebral cortex as well as for clinical investigations aimed at tracking the evolution of disease-induced changes over time, including clinical trials in which MRI-based measures are used to examine response to treatment. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {Desikan, Rahul S. and S{\'{e}}gonne, Florent and Fischl, Bruce and Quinn, Brian T. and Dickerson, Bradford C. and Blacker, Deborah and Buckner, Randy L. and Dale, Anders M. and Maguire, R. Paul and Hyman, Bradley T. and Albert, Marilyn S. and Killiany, Ronald J.},
doi = {10.1016/j.neuroimage.2006.01.021},
issn = {10538119},
journal = {NeuroImage},
month = {jul},
number = {3},
pages = {968--980},
pmid = {16530430},
title = {{An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811906000437},
volume = {31},
year = {2006}
}
@article{Chen2018,
abstract = {Fully-connected Conditional Random Field (CRF) is often used as post-processing to refine voxel classification results by encouraging spatial coherence. In this paper, we propose a new end-to-end training method called Posterior-CRF. In contrast with previous approaches which use the original image intensity in the CRF, our approach applies 3D, fully connected CRF to the posterior probabilities from a CNN and optimizes both CNN and CRF together. The experiments on white matter hyperintensities segmentation demonstrate that our method outperforms CNN, post-processing CRF and different end-to-end training CRF approaches.},
annote = {{\_}eprint: 1811.03549},
archivePrefix = {arXiv},
arxivId = {1811.03549},
author = {Chen, Shuai and Bruijne, Marleen De},
eprint = {1811.03549},
journal = {arXiv},
title = {{An End-to-end Approach to Semantic Segmentation with 3D CNN and Posterior-CRF in Medical Images}},
url = {http://arxiv.org/abs/1811.03549},
year = {2018}
}
@article{Dosovitskiy2020,
abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
archivePrefix = {arXiv},
arxivId = {2010.11929},
author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
eprint = {2010.11929},
month = {oct},
title = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
url = {http://arxiv.org/abs/2010.11929},
year = {2020}
}
@article{Cardona2010,
abstract = {The analysis of microcircuitry (the connectivity at the level of individual neuronal processes and synapses), which is indispensable for our understanding of brain function, is based on serial transmission electron microscopy (TEM) or one of its modern variants. Due to technical limitations, most previous studies that used serial TEM recorded relatively small stacks of individual neurons. As a result, our knowledge of microcircuitry in any nervous system is very limited. We applied the software package TrakEM2 to reconstruct neuronal microcircuitry from TEM sections of a small brain, the early larval brain of Drosophila melanogaster. TrakEM2 enables us to embed the analysis of the TEM image volumes at the microcircuit level into a light microscopically derived neuro-anatomical framework, by registering confocal stacks containing sparsely labeled neural structures with the TEM image volume. We imaged two sets of serial TEM sections of the Drosophila first instar larval brain neuropile and one ventral nerve cord segment, and here report our first results pertaining to Drosophila brain microcircuitry. Terminal neurites fall into a small number of generic classes termed globular, varicose, axiform, and dendritiform. Globular and varicose neurites have large diameter segments that carry almost exclusively presynaptic sites. Dendritiform neurites are thin, highly branched processes that are almost exclusively postsynaptic. Due to the high branching density of dendritiform fibers and the fact that synapses are polyadic, neurites are highly interconnected even within small neuropile volumes. We describe the network motifs most frequently encountered in the Drosophila neuropile. Our study introduces an approach towards a comprehensive anatomical reconstruction of neuronal microcircuitry and delivers microcircuitry comparisons between vertebrate and insect neuropile. {\textcopyright} 2010 Cardona et al.},
author = {Cardona, Albert and Hartenstein, Volker and Saalfeld, Stephan and Preibisch, Stephan and Schmid, Benjamin and Cheng, Anchi and Pulokas, Jim and Tomancak, Pavel},
doi = {10.1371/journal.pbio.1000502},
editor = {Harris, Kristen M.},
issn = {15457885},
journal = {PLoS Biology},
month = {oct},
number = {10},
pages = {e1000502},
title = {{An integrated micro- and macroarchitectural analysis of the Drosophila brain by computer-assisted serial section electron microscopy}},
url = {https://dx.plos.org/10.1371/journal.pbio.1000502},
volume = {8},
year = {2010}
}
@article{Ulman2017,
abstract = {We present a combined report on the results of three editions of the Cell Tracking Challenge, an ongoing initiative aimed at promoting the development and objective evaluation of cell segmentation and tracking algorithms. With 21 participating algorithms and a data repository consisting of 13 data sets from various microscopy modalities, the challenge displays today's state-of-the-art methodology in the field. We analyzed the challenge results using performance measures for segmentation and tracking that rank all participating methods. We also analyzed the performance of all of the algorithms in terms of biological measures and practical usability. Although some methods scored high in all technical aspects, none obtained fully correct solutions. We found that methods that either take prior information into account using learning strategies or analyze cells in a global spatiotemporal video context performed better than other methods under the segmentation and tracking scenarios included in the challenge.},
author = {Ulman, Vladim{\'{i}}r and Ma{\v{s}}ka, Martin and Magnusson, Klas E.G. and Ronneberger, Olaf and Haubold, Carsten and Harder, Nathalie and Matula, Pavel and Matula, Petr and Svoboda, David and Radojevic, Miroslav and Smal, Ihor and Rohr, Karl and Jald{\'{e}}n, Joakim and Blau, Helen M. and Dzyubachyk, Oleh and Lelieveldt, Boudewijn and Xiao, Pengdong and Li, Yuexiang and Cho, Siu Yeung and Dufour, Alexandre C. and Olivo-Marin, Jean Christophe and Reyes-Aldasoro, Constantino C. and Solis-Lemus, Jose A. and Bensch, Robert and Brox, Thomas and Stegmaier, Johannes and Mikut, Ralf and Wolf, Steffen and Hamprecht, Fred A. and Esteves, Tiago and Quelhas, Pedro and Demirel, {\"{O}}mer and Malmstr{\"{o}}m, Lars and Jug, Florian and Tomancak, Pavel and Meijering, Erik and Mu{\~{n}}oz-Barrutia, Arrate and Kozubek, Michal and Ortiz-De-Solorzano, Carlos},
doi = {10.1038/nmeth.4473},
issn = {15487105},
journal = {Nature Methods},
month = {dec},
number = {12},
pages = {1141--1152},
pmid = {29083403},
title = {{An objective comparison of cell-tracking algorithms}},
url = {http://www.nature.com/articles/nmeth.4473},
volume = {14},
year = {2017}
}
@article{Ali2020,
abstract = {We present a comprehensive analysis of the submissions to the first edition of the Endoscopy Artefact Detection challenge (EAD). Using crowd-sourcing, this initiative is a step towards understanding the limitations of existing state-of-the-art computer vision methods applied to endoscopy and promoting the development of new approaches suitable for clinical translation. Endoscopy is a routine imaging technique for the detection, diagnosis and treatment of diseases in hollow-organs; the esophagus, stomach, colon, uterus and the bladder. However the nature of these organs prevent imaged tissues to be free of imaging artefacts such as bubbles, pixel saturation, organ specularity and debris, all of which pose substantial challenges for any quantitative analysis. Consequently, the potential for improved clinical outcomes through quantitative assessment of abnormal mucosal surface observed in endoscopy videos is presently not realized accurately. The EAD challenge promotes awareness of and addresses this key bottleneck problem by investigating methods that can accurately classify, localize and segment artefacts in endoscopy frames as critical prerequisite tasks. Using a diverse curated multi-institutional, multi-modality, multi-organ dataset of video frames, the accuracy and performance of 23 algorithms were objectively ranked for artefact detection and segmentation. The ability of methods to generalize to unseen datasets was also evaluated. The best performing methods (top 15{\%}) propose deep learning strategies to reconcile variabilities in artefact appearance with respect to size, modality, occurrence and organ type. However, no single method outperformed across all tasks. Detailed analyses reveal the shortcomings of current training strategies and highlight the need for developing new optimal metrics to accurately quantify the clinical applicability of methods.},
author = {Ali, Sharib and Zhou, Felix and Braden, Barbara and Bailey, Adam and Yang, Suhui and Cheng, Guanju and Zhang, Pengyi and Li, Xiaoqiong and Kayser, Maxime and Soberanis-Mukul, Roger D. and Albarqouni, Shadi and Wang, Xiaokang and Wang, Chunqing and Watanabe, Seiryo and Oksuz, Ilkay and Ning, Qingtian and Yang, Shufan and Khan, Mohammad Azam and Gao, Xiaohong W. and Realdon, Stefano and Loshchenov, Maxim and Schnabel, Julia A. and East, James E. and Wagnieres, Georges and Loschenov, Victor B. and Grisan, Enrico and Daul, Christian and Blondel, Walter and Rittscher, Jens},
doi = {10.1038/s41598-020-59413-5},
issn = {20452322},
journal = {Scientific Reports},
month = {dec},
number = {1},
pages = {2748},
pmid = {32066744},
title = {{An objective comparison of detection and segmentation algorithms for artefacts in clinical endoscopy}},
url = {http://www.nature.com/articles/s41598-020-59413-5},
volume = {10},
year = {2020}
}
@article{Souza2018,
abstract = {This paper presents an open, multi-vendor, multi-field strength magnetic resonance (MR) T1-weighted volumetric brain imaging dataset, named Calgary-Campinas-359 (CC-359). The dataset is composed of images of older healthy adults (29–80 years) acquired on scanners from three vendors (Siemens, Philips and General Electric) at both 1.5 T and 3 T. CC-359 is comprised of 359 datasets, approximately 60 subjects per vendor and magnetic field strength. The dataset is approximately age and gender balanced, subject to the constraints of the available images. It provides consensus brain extraction masks for all volumes generated using supervised classification. Manual segmentation results for twelve randomly selected subjects performed by an expert are also provided. The CC-359 dataset allows investigation of 1) the influences of both vendor and magnetic field strength on quantitative analysis of brain MR; 2) parameter optimization for automatic segmentation methods; and potentially 3) machine learning classifiers with big data, specifically those based on deep learning methods, as these approaches require a large amount of data. To illustrate the utility of this dataset, we compared to the results of a supervised classifier, the results of eight publicly available skull stripping methods and one publicly available consensus algorithm. A linear mixed effects model analysis indicated that vendor (p−value{\textless}0.001) and magnetic field strength (p−value{\textless}0.001) have statistically significant impacts on skull stripping results.},
author = {Souza, Roberto and Lucena, Oeslle and Garrafa, Julia and Gobbi, David and Saluzzi, Marina and Appenzeller, Simone and Rittner, Let{\'{i}}cia and Frayne, Richard and Lotufo, Roberto},
doi = {10.1016/j.neuroimage.2017.08.021},
issn = {10959572},
journal = {NeuroImage},
keywords = {Brain MR image analysis,Brain extraction,Brain segmentation,MP-RAGE,Public database,Skull stripping},
month = {apr},
pages = {482--494},
pmid = {28807870},
title = {{An open, multi-vendor, multi-field-strength brain MR dataset and analysis of publicly available skull stripping methods agreement}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811917306687},
volume = {170},
year = {2018}
}
@article{Ruder2016,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
annote = {{\_}eprint: 1609.04747},
archivePrefix = {arXiv},
arxivId = {1609.04747},
author = {Ruder, Sebastian},
eprint = {1609.04747},
journal = {CoRR},
title = {{An overview of gradient descent optimization algorithms}},
url = {http://arxiv.org/abs/1609.04747},
volume = {abs/1609.0},
year = {2016}
}
@book{Langer2020,
address = {Cham},
author = {Langer, Arthur M.},
booktitle = {Analysis and Design of Next-Generation Software Architectures},
doi = {10.1007/978-3-030-36899-9},
isbn = {978-3-030-36898-2},
publisher = {Springer International Publishing},
title = {{Analysis and Design of Next-Generation Software Architectures}},
url = {http://link.springer.com/10.1007/978-3-030-36899-9},
year = {2020}
}
@article{Thomas2014,
abstract = {Tractography based on diffusion-weighted MRI (DWI) is widely used for mapping the structural connections of the human brain. Its accuracy is known to be limited by technical factors affecting in vivo data acquisition, such as noise, artifacts, and data undersampling resulting from scan time constraints. It generally is assumed that improvements in data quality and implementation of sophisticated tractography methods will lead to increasingly accurate maps of human anatomical connections. However, assessing the anatomical accuracy of DWI tractography is difficult because of the lack of independent knowledge of the true anatomical connections in humans. Here we investigate the future prospects of DWI-based connectional imaging by applying advanced tractography methods to an ex vivo DWI dataset of the macaque brain. The results of different tractography methods were compared with maps of known axonal projections from previous tracer studies in the macaque. Despite the exceptional quality of the DWI data, none of the methods demonstrated high anatomical accuracy. The methods that showed the highest sensitivity showed the lowest specificity, and vice versa. Additionally, anatomical accuracy was highly dependent upon parameters of the tractography algorithm, with different optimal values for mapping different pathways. These results suggest that there is an inherent limitation in determining long-range anatomical projections based on voxelaveraged estimates of local fiber orientation obtained from DWI data that is unlikely to be overcome by improvements in data acquisition and analysis alone.},
author = {Thomas, Cibu and Ye, Frank Q. and Irfanoglu, M. Okan and Modi, Pooja and Saleem, Kadharbatcha S. and Leopold, David A. and Pierpaoli, Carlo},
doi = {10.1073/pnas.1405672111},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = {nov},
number = {46},
pages = {16574--16579},
pmid = {25368179},
title = {{Anatomical accuracy of brain connections derived from diffusion MRI tractography is inherently limited}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1405672111},
volume = {111},
year = {2014}
}
@misc{Munoz-Gil2020,
abstract = {The deviation from pure Brownian motion, generally referred to as anomalous diffusion, has received large attention in the scientific literature to describe many physical scenarios. Several methods, based on classical statistics and machine learning approaches, have been developed to characterize anomalous diffusion from experimental data, which are usually acquired as particle trajectories. With the aim to assess and compare the available methods to characterize anomalous diffusion, we have organized the Anomalous Diffusion (AnDi) Challenge (http://www.andi-challenge.org/). Specifically, the AnDi Challenge will address three different aspects of anomalous diffusion characterization, namely: (i) Inference of the anomalous diffusion exponent. (ii) Identification of the underlying diffusion model. (iii) Segmentation of trajectories. Each problem includes sub-tasks for different number of dimensions (1D, 2D and 3D). In order to compare the various methods, we have developed a dedicated open-source framework for the simulation of the anomalous diffusion trajectories that are used for the training and test datasets. The challenge was launched on March 1, 2020, and consists of three phases. Currently, the participation to the first phase is open. Submissions will be automatically evaluated and the performance of the top-scoring methods will be thoroughly analyzed and compared in an upcoming article.},
archivePrefix = {arXiv},
arxivId = {2003.12036},
author = {Mu{\~{n}}oz-Gil, Gorka and Volpe, Giovanni and Garcia-March, Miguel Angel and Metzler, Ralf and Lewenstein, Maciej and Manzo, Carlo},
booktitle = {arXiv},
doi = {10.1117/12.2567914},
eprint = {2003.12036},
keywords = {ANDI challenge,anomalous diffusion,physics simulations},
publisher = {Zenodo},
title = {{AnDi: The anomalous diffusion challenge}},
url = {https://zenodo.org/record/3707702},
year = {2020}
}
@article{Borovec2020,
abstract = {Automatic Non-rigid Histological Image Registration (ANHIR) challenge was organized to compare the performance of image registration algorithms on several kinds of microscopy histology images in a fair and independent manner. We have assembled 8 datasets, containing 355 images with 18 different stains, resulting in 481 image pairs to be registered. Registration accuracy was evaluated using manually placed landmarks. In total, 256 teams registered for the challenge, 10 submitted the results, and 6 participated in the workshop. Here, we present the results of 7 well-performing methods from the challenge together with 6 well-known existing methods. The best methods used coarse but robust initial alignment, followed by non-rigid registration, used multiresolution, and were carefully tuned for the data at hand. They outperformed off-the-shelf methods, mostly by being more robust. The best methods could successfully register over 98{\%} of all landmarks and their mean landmark registration accuracy (TRE) was 0.44{\%} of the image diagonal. The challenge remains open to submissions and all images are available for download.},
author = {Borovec, Jiri and Kybic, Jan and Arganda-Carreras, Ignacio and Sorokin, Dmitry V. and Bueno, Gloria and Khvostikov, Alexander V. and Bakas, Spyridon and Chang, Eric I.Chao and Heldmann, Stefan and Kartasalo, Kimmo and Latonen, Leena and Lotz, Johannes and Noga, Michelle and Pati, Sarthak and Punithakumar, Kumaradevan and Ruusuvuori, Pekka and Skalski, Andrzej and Tahmasebi, Nazanin and Valkonen, Masi and Venet, Ludovic and Wang, Yizhe and Weiss, Nick and Wodzinski, Marek and Xiang, Yu and Xu, Yan and Yan, Yan and Yushkevich, Paul and Zhao, Shengyu and Munoz-Barrutia, Arrate},
doi = {10.1109/TMI.2020.2986331},
issn = {1558254X},
journal = {IEEE transactions on medical imaging},
number = {10},
pages = {3042--3052},
pmid = {32275587},
title = {{ANHIR: Automatic Non-Rigid Histological Image Registration Challenge}},
volume = {39},
year = {2020}
}
@online{ANT-Day2019,
author = {Day, Trevor K. M. and Madyastha, Tara M. and Boord, Peter and Askren, Mary K. and Montine, Thomas J. and Grabowski, Thomas J.},
title = {{ANT: Healthy aging and Parkinson's disease}},
url = {https://openneuro.org/datasets/ds001907/versions/2.0.3},
urldate = {2020-05-27},
year = {2019}
}
@book{Khalaf2019,
abstract = {In the recent years, the number of web logs, and the amount of opinionated data on the World Wide Web, have been grown substantially. The ability to determine the political orientation of an article automatically can be beneficial in many areas from academia to security. However, the sentiment classification of web log posts (political web log posts in particular), is apparently more complex than the sentiment classification of conventional text. In this paper, a supervised machine learning with two feature extraction techniques Term Frequency (TF) and Term Frequency-Inverse Document Frequency (TF-IDF) are used for the classification process. For investigation, SVM with four kernels for supervised machine learning have been employed. Subsequent to testing, the results reveal that the linear with TF achieved the results in accuracy of 91.935{\%} also with TF-IDF achieved the 95.161{\%}. The linear kernel was deemed the most suitable for our model.},
address = {Cham},
author = {Alloghani, Mohamed and Al-Jumeily, Dhiya and Aljaaf, Ahmed J and Khalaf, Mohammed and Mustafina, Jamila and Tan, Sin Y},
booktitle = {Applied Computing to Support Industry: Innovation and Technology},
doi = {10.1007/978-3-030-38752-5},
editor = {Khalaf, Mohammed I. and Al-Jumeily, Dhiya and Lisitsa, Alexei},
isbn = {9783030387518},
keywords = {AI medical research,Deep learning,artificial intelligence,machine learning},
pages = {248--261},
publisher = {Springer International Publishing},
series = {Communications in Computer and Information Science},
title = {{Applied Computing to Support Industry}},
url = {http://dx.doi.org/10.1007/978-3-030-38752-5{\_}7},
volume = {1},
year = {2020}
}
@online{KaggleAPTOS2019,
abstract = {Detect diabetic retinopathy to stop blindness before it's too late},
author = {Kaggle},
booktitle = {-},
title = {{APTOS 2019 blindness detection}},
url = {https://www.kaggle.com/c/aptos2019-blindness-detection/data},
year = {2019}
}
@article{Harmon2020,
abstract = {Chest CT is emerging as a valuable diagnostic tool for clinical management of COVID-19 associated lung disease. Artificial intelligence (AI) has the potential to aid in rapid evaluation of CT scans for differentiation of COVID-19 findings from other clinical entities. Here we show that a series of deep learning algorithms, trained in a diverse multinational cohort of 1280 patients to localize parietal pleura/lung parenchyma followed by classification of COVID-19 pneumonia, can achieve up to 90.8{\%} accuracy, with 84{\%} sensitivity and 93{\%} specificity, as evaluated in an independent test set (not included in training and validation) of 1337 patients. Normal controls included chest CTs from oncology, emergency, and pneumonia-related indications. The false positive rate in 140 patients with laboratory confirmed other (non COVID-19) pneumonias was 10{\%}. AI-based algorithms can readily identify CT scans with COVID-19 associated pneumonia, as well as distinguish non-COVID related pneumonias with high specificity in diverse patient populations.},
author = {Harmon, Stephanie A. and Sanford, Thomas H. and Xu, Sheng and Turkbey, Evrim B. and Roth, Holger and Xu, Ziyue and Yang, Dong and Myronenko, Andriy and Anderson, Victoria and Amalou, Amel and Blain, Maxime and Kassin, Michael and Long, Dilara and Varble, Nicole and Walker, Stephanie M. and Bagci, Ulas and Ierardi, Anna Maria and Stellato, Elvira and Plensich, Guido Giovanni and Franceschelli, Giuseppe and Girlando, Cristiano and Irmici, Giovanni and Labella, Dominic and Hammoud, Dima and Malayeri, Ashkan and Jones, Elizabeth and Summers, Ronald M. and Choyke, Peter L. and Xu, Daguang and Flores, Mona and Tamura, Kaku and Obinata, Hirofumi and Mori, Hitoshi and Patella, Francesca and Cariati, Maurizio and Carrafiello, Gianpaolo and An, Peng and Wood, Bradford J. and Turkbey, Baris},
doi = {10.1038/s41467-020-17971-2},
issn = {20411723},
journal = {Nature Communications},
month = {dec},
number = {1},
pages = {4080},
pmid = {32796848},
title = {{Artificial intelligence for the detection of COVID-19 pneumonia on chest CT using multinational datasets}},
url = {http://www.nature.com/articles/s41467-020-17971-2},
volume = {11},
year = {2020}
}
@misc{Martel2019,
abstract = {Breast cancer (BC) is the second most commonly diagnosed cancer in the U.S. with more than 250,000 new cases of invasive breast cancers reported in 2017. The majority of women with locally advanced and a subset of patients with operable breast cancer will undergo systemic therapy prior to their surgery (neoadjuvant therapy/ NAT) to reduce the size of tumor(s) and possibly further undergo breast conserving surgery. The Post-NAT-BRCA dataset is a collection of representative sections from breast resections in patients with residual invasive BC following NAT. Histologic sections were prepared and digitized to produce high resolution, microscopic images of treated BC tumors. Also included, are clinical features and expert pathology annotations of tumor cellularity and cell types. The Residual Cancer Burden Index (RCBi), is a clinically validated tool for assessment of response to NAT associated with prognosis. Tumor cellularity is one of the parameters used for calculating the RCBi. In this dataset, tumor cellularity refers to a measure of residual disease after NAT, in the form of proportion of malignant tumor inside the tumor bed region; also annotated. (See MD Anderson RCB Calculator for a detailed description of tumor cellularity.) Malignant, healthy, lymphocyte and other labels were also provided for individual cells to aid development of cell segmentation algorithms.},
author = {Martel, Anne L. and Salama, Sherine and Nofech-Mozes, Sharon and Akbar, Shazia and Peikari, Mohammad},
doi = {10.7937/TCIA.2019.4YIBTJNO},
publisher = {The Cancer Imaging Archive},
title = {{Assessment of Residual Breast Cancer Cellularity after Neoadjuvant Chemotherapy using Digital Pathology [Data set]}},
url = {https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=52758117},
year = {2019}
}
@article{attentionunet,
abstract = {We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The source code for the proposed architecture is publicly available.},
archivePrefix = {arXiv},
arxivId = {1804.03999},
author = {Oktay, Ozan and Schlemper, Jo and {Le Folgoc}, Loic and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y. and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
eprint = {1804.03999},
issn = {23318422},
journal = {arXiv},
month = {apr},
title = {{Attention U-Net: Learning where to look for the pancreas}},
url = {http://arxiv.org/abs/1804.03999},
volume = {abs/1804.0},
year = {2018}
}
@incollection{Lempitsky2020,
address = {Cham},
author = {Lempitsky, Victor},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_862-1},
pages = {1--6},
publisher = {Springer International Publishing},
title = {{Autoencoder}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}862-1},
year = {2020}
}
@inproceedings{Jin2018,
abstract = {Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet [51], PNAS [29], usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Extensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The code and documentation are available at https://autokeras.com. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.},
archivePrefix = {arXiv},
arxivId = {1806.10282},
author = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/3292500.3330648},
eprint = {1806.10282},
isbn = {9781450362016},
keywords = {AutoML,Automated Machine Learning,Bayesian Optimization,Network Morphism,Neural Architecture Search},
pages = {1946--1956},
title = {{Auto-keras: An efficient neural architecture search system}},
year = {2019}
}
@article{Yue2006,
abstract = {Craniofacial landmark localization and anatomical structure tracing on cephalograms are two important ways to obtain the cephalometric analysis. In order to computerize them in parallel, a model-based approach is proposed to locate 262 craniofacial feature points, including 90 landmarks and 172 auxiliary points. In model training, 12 landmarks are selected as reference points and used to divide every training shape to 10 regions according to the anatomical knowledge; principle components analysis is employed to characterize the region shape variations and the statistical grey profile of every feature point. Locating feature points on an input image is a two-stage procedure. First, we identify the reference landmarks by image processing and pattern matching techniques, so that the shape partition is performed on the input image. Then, for each region, its feature points are located by a modified active shape model. All craniofacial anatomical structures can be traced out by connecting the located points with subdivision curves according to the prior knowledge. Users are permitted to modify the results interactively in many different ways. Experimental results show the advantage and reliability of the proposed method. {\textcopyright} 2006 IEEE.},
author = {Yue, Weining and Yin, Dali and Li, Chengjun and Wang, Guoping and Xu, Tianmin},
doi = {10.1109/TBME.2006.876638},
issn = {00189294},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {ASM,Cephalometry,Edge detection,Landmarks,PCA,Pattern matching,Structure tracing,Superimposition},
month = {aug},
number = {8},
pages = {1615--1623},
title = {{Automated 2-D cephalometric analysis on X-ray images by a model-based approach}},
url = {http://ieeexplore.ieee.org/document/1658156/},
volume = {53},
year = {2006}
}
@article{Liu2019,
abstract = {Accurate diagnosis of thyroid nodules using ultrasonography is a valuable but tough task even for experienced radiologists, considering both benign and malignant nodules have heterogeneous appearances. Computer-aided diagnosis (CAD) methods could potentially provide objective suggestions to assist radiologists. However, the performance of existing learning-based approaches is still limited, for direct application of general learning models often ignores critical domain knowledge related to the specific nodule diagnosis. In this study, we propose a novel deep-learning-based CAD system, guided by task-specific prior knowledge, for automated nodule detection and classification in ultrasound images. Our proposed CAD system consists of two stages. First, a multi-scale region-based detection network is designed to learn pyramidal features for detecting nodules at different feature scales. The region proposals are constrained by the prior knowledge about size and shape distributions of real nodules. Then, a multi-branch classification network is proposed to integrate multi-view diagnosis-oriented features, in which each network branch captures and enhances one specific group of characteristics that were generally used by radiologists. We evaluated and compared our method with the state-of-the-art CAD methods and experienced radiologists on two datasets, i.e. Dataset I and Dataset II. The detection and diagnostic accuracy on Dataset I were 97.5{\%} and 97.1{\%}, respectively. Besides, our CAD system also achieved better performance than experienced radiologists on Dataset II, with improvements of accuracy for 8{\%}. The experimental results demonstrate that our proposed method is effective in the discrimination of thyroid nodules.},
author = {Liu, Tianjiao and Guo, Qianqian and Lian, Chunfeng and Ren, Xuhua and Liang, Shujun and Yu, Jing and Niu, Lijuan and Sun, Weidong and Shen, Dinggang},
doi = {10.1016/j.media.2019.101555},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Clinical knowledge,Convolutional neural networks,Thyroid nodule,Ultrasound image},
pages = {101555},
pmid = {31520984},
title = {{Automated detection and classification of thyroid nodules in ultrasound images using clinical-knowledge-guided convolutional neural networks}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841519300970},
volume = {58},
year = {2019}
}
@misc{Heuvel2018,
abstract = {In this paper we present a computer aided detection (CAD) system for automated measurement of the fetal head circumference (HC) in 2D ultrasound images for all trimesters of the pregnancy. The HC can be used to estimate the gestational age and monitor growth of the fetus. Automated HC assessment could be valuable in developing countries, where there is a severe shortage of trained sonographers. The CAD system consists of two steps: First, Haar-like features were computed from the ultrasound images to train a random forest classifier to locate the fetal skull. Secondly, the HC was extracted using Hough transform, dynamic programming and an ellipse fit. The CAD system was trained on 999 images and validated on an independent test set of 335 images from all trimesters. The test set was manually annotated by an experienced sonographer and a medical researcher. The reference gestational age (GA) was estimated using the crown-rump length measurement (CRL). The mean difference between the reference GA and the GA estimated by the experienced sonographer was 0.8 ± 2.6, −0.0 ± 4.6 and 1.9 ± 11.0 days for the first, second and third trimester, respectively. The mean difference between the reference GA and the GA estimated by the medical researcher was 1.6 ± 2.7, 2.0 ± 4.8 and 3.9 ± 13.7 days. The mean difference between the reference GA and the GA estimated by the CAD system was 0.6 ± 4.3, 0.4 ± 4.7 and 2.5 ± 12.4 days. The results show that the CAD system performs comparable to an experienced sonographer. The presented system shows similar or superior results compared to systems published in literature. This is the first automated system for HC assessment evaluated on a large test set which contained data of all trimesters of the pregnancy.},
author = {van den Heuvel, Thomas L.A. and de Bruijn, Dagmar and de Korte, Chris L. and van Ginneken, Bram},
booktitle = {PLoS ONE},
doi = {10.1371/journal.pone.0200412},
isbn = {1111111111},
issn = {19326203},
number = {8},
pages = {1--20},
pmid = {30138319},
publisher = {Zenodo},
title = {{Automated measurement of fetal head circumference using 2D ultrasound images}},
url = {https://zenodo.org/record/1322001},
volume = {13},
year = {2018}
}
@article{VandenHeuvel2018,
abstract = {In this paper we present a computer aided detection (CAD) system for automated measurement of the fetal head circumference (HC) in 2D ultrasound images for all trimesters of the pregnancy. The HC can be used to estimate the gestational age and monitor growth of the fetus. Automated HC assessment could be valuable in developing countries, where there is a severe shortage of trained sonographers. The CAD system consists of two steps: First, Haar-like features were computed from the ultrasound images to train a random forest classifier to locate the fetal skull. Secondly, the HC was extracted using Hough transform, dynamic programming and an ellipse fit. The CAD system was trained on 999 images and validated on an independent test set of 335 images from all trimesters. The test set was manually annotated by an experienced sonographer and a medical researcher. The reference gestational age (GA) was estimated using the crown-rump length measurement (CRL). The mean difference between the reference GA and the GA estimated by the experienced sonographer was 0.8 ± 2.6, −0.0 ± 4.6 and 1.9 ± 11.0 days for the first, second and third trimester, respectively. The mean difference between the reference GA and the GA estimated by the medical researcher was 1.6 ± 2.7, 2.0 ± 4.8 and 3.9 ± 13.7 days. The mean difference between the reference GA and the GA estimated by the CAD system was 0.6 ± 4.3, 0.4 ± 4.7 and 2.5 ± 12.4 days. The results show that the CAD system performs comparable to an experienced sonographer. The presented system shows similar or superior results compared to systems published in literature. This is the first automated system for HC assessment evaluated on a large test set which contained data of all trimesters of the pregnancy.},
author = {van den Heuvel, Thomas L.A. and de Bruijn, Dagmar and de Korte, Chris L. and van Ginneken, Bram},
doi = {10.1371/journal.pone.0200412},
issn = {19326203},
journal = {PLoS ONE},
number = {8},
pmid = {30138319},
title = {{Automated measurement of fetal head circumference using 2D ultrasound images}},
volume = {13},
year = {2018}
}
@article{Yang2014,
abstract = {Purpose To develop an automated magnetic resonance imaging (MRI) parotid segmentation method to monitor radiation-induced parotid gland changes in patients after head and neck radiation therapy (RT).
Methods and Materials The proposed method combines the atlas registration method, which captures the global variation of anatomy, with a machine learning technology, which captures the local statistical features, to automatically segment the parotid glands from the MRIs. The segmentation method consists of 3 major steps. First, an atlas (pre-RT MRI and manually contoured parotid gland mask) is built for each patient. A hybrid deformable image registration is used to map the pre-RT MRI to the post-RT MRI, and the transformation is applied to the pre-RT parotid volume. Second, the kernel support vector machine (SVM) is trained with the subject-specific atlas pair consisting of multiple features (intensity, gradient, and others) from the aligned pre-RT MRI and the transformed parotid volume. Third, the well-trained kernel SVM is used to differentiate the parotid from surrounding tissues in the post-RT MRIs by statistically matching multiple texture features. A longitudinal study of 15 patients undergoing head and neck RT was conducted: baseline MRI was acquired prior to RT, and the post-RT MRIs were acquired at 3-, 6-, and 12-month follow-up examinations. The resulting segmentations were compared with the physicians' manual contours.
Results Successful parotid segmentation was achieved for all 15 patients (42 post-RT MRIs). The average percentage of volume differences between the automated segmentations and those of the physicians' manual contours were 7.98{\%} for the left parotid and 8.12{\%} for the right parotid. The average volume overlap was 91.1{\%} ± 1.6{\%} for the left parotid and 90.5{\%} ± 2.4{\%} for the right parotid. The parotid gland volume reduction at follow-up was 25{\%} at 3 months, 27{\%} at 6 months, and 16{\%} at 12 months.
Conclusions We have validated our automated parotid segmentation algorithm in a longitudinal study. This segmentation method may be useful in future studies to address radiation-induced xerostomia in head and neck radiation therapy.},
author = {Yang, Xiaofeng and Wu, Ning and Cheng, Guanghui and Zhou, Zhengyang and Yu, David S. and Beitler, Jonathan J. and Curran, Walter J. and Liu, Tian},
doi = {10.1016/j.ijrobp.2014.08.350},
issn = {1879355X},
journal = {International Journal of Radiation Oncology Biology Physics},
month = {dec},
number = {5},
pages = {1225--1233},
pmid = {25442347},
title = {{Automated segmentation of the parotid gland based on atlas registration and machine learning: A longitudinal mri study in head-and-neck radiation therapy}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0360301614040577},
volume = {90},
year = {2014}
}
@article{Lu2017,
abstract = {Purpose: Segmentation of the liver from abdominal computed tomography (CT) images is an essential step in some computer-assisted clinical interventions, such as surgery planning for living donor liver transplant, radiotherapy and volume measurement. In this work, we develop a deep learning algorithm with graph cut refinement to automatically segment the liver in CT scans. Methods: The proposed method consists of two main steps: (i) simultaneously liver detection and probabilistic segmentation using 3D convolutional neural network; (ii) accuracy refinement of the initial segmentation with graph cut and the previously learned probability map. Results: The proposed approach was validated on forty CT volumes taken from two public databases MICCAI-Sliver07 and 3Dircadb1. For the MICCAI-Sliver07 test dataset, the calculated mean ratios of volumetric overlap error (VOE), relative volume difference (RVD), average symmetric surface distance (ASD), root-mean-square symmetric surface distance (RMSD) and maximum symmetric surface distance (MSD) are 5.9, 2.7 {\%}, 0.91, 1.88 and 18.94 mm, respectively. For the 3Dircadb1 dataset, the calculated mean ratios of VOE, RVD, ASD, RMSD and MSD are 9.36, 0.97 {\%}, 1.89, 4.15 and 33.14 mm, respectively. Conclusions: The proposed method is fully automatic without any user interaction. Quantitative results reveal that the proposed approach is efficient and accurate for hepatic volume estimation in a clinical setup. The high correlation between the automatic and manual references shows that the proposed method can be good enough to replace the time-consuming and nonreproducible manual segmentation method.},
archivePrefix = {arXiv},
arxivId = {1605.03012},
author = {Lu, Fang and Wu, Fa and Hu, Peijun and Peng, Zhiyi and Kong, Dexing},
doi = {10.1007/s11548-016-1467-3},
eprint = {1605.03012},
issn = {18616429},
journal = {International Journal of Computer Assisted Radiology and Surgery},
keywords = {3D convolution neural network,CT images,Graph cut,Liver segmentation},
number = {2},
pages = {171--182},
pmid = {27604760},
title = {{Automatic 3D liver location and segmentation via convolutional neural network and graph cut}},
url = {https://doi.org/10.1007/s11548-016-1467-3},
volume = {12},
year = {2017}
}
@article{Peikari2017,
abstract = {Neoadjuvant treatment (NAT) of breast cancer (BCa) is an option for patients with the locally advanced disease. It has been compared with standard adjuvant therapy with the aim of improving prognosis and surgical outcome. Moreover, the response of the tumor to the therapy provides useful information for patient management. The pathological examination of the tissue sections after surgery is the gold-standard to estimate the residual tumor and the assessment of cellularity is an important component of tumor burden assessment. In the current clinical practice, tumor cellularity is manually estimated by pathologists on hematoxylin and eosin (H{\&}E) stained slides, the quality, and reliability of which might be impaired by inter-observer variability which potentially affects prognostic power assessment in NAT trials. This procedure is also qualitative and time-consuming. In this paper, we describe a method of automatically assessing cellularity. A pipeline to automatically segment nuclei figures and estimate residual cancer cellularity from within patches and whole slide images (WSIs) of BCa was developed. We have compared the performance of our proposed pipeline in estimating residual cancer cellularity with that of two expert pathologists. We found an intra-class agreement coefficient (ICC) of 0.89 (95{\%} CI of [0.70, 0.95]) between pathologists, 0.74 (95{\%} CI of [0.70, 0.77]) between pathologist {\#}1 and proposed method, and 0.75 (95{\%} CI of [0.71, 0.79]) between pathologist {\#}2 and proposed method. We have also successfully applied our proposed technique on a WSI to locate areas with high concentration of residual cancer. The main advantage of our approach is that it is fully automatic and can be used to find areas with high cellularity in WSIs. This provides a first step in developing an automatic technique for post-NAT tumor response assessment from pathology slides. {\textcopyright} 2017 International Society for Advancement of Cytometry.},
author = {Peikari, Mohammad and Salama, Sherine and Nofech-Mozes, Sharon and Martel, Anne L.},
doi = {10.1002/cyto.a.23244},
issn = {15524930},
journal = {Cytometry Part A},
keywords = {breast cancer,machine learning,neoadjuvant therapy,pathology image analysis},
number = {11},
pages = {1078--1087},
pmid = {28976721},
title = {{Automatic cellularity assessment from post-treated breast surgical specimens}},
volume = {91},
year = {2017}
}
@article{Kaur2015,
abstract = {Cephalometry is an essential clinical and research tool in orthodontics. It has been used for decades to obtain absolute and relative measures of the craniofacial skeleton. Since manual identification of predefined anatomical landmarks is a very tedious approach, there is a strong need for automated methods. This paper explores the use of Zernike moment-based global features for initial landmark estimation and computing small expectation window for each landmark. Using this expectation window and local template matching based on ring and central projection method, a closer approximation of landmark position is obtained. A smaller search window based on this approximation is used to find the exact location of landmark positions based on template matching using a combination of sum of squared distance and normalized cross-correlation. The system was tested on 18 commonly used landmarks using a dataset of 85 randomly selected cephalograms. A total of 89.5 {\%} of the localization of 18 selected landmarks are within a window of {\$}{\$}$\backslash$le $\backslash$!$\backslash$!$\backslash$pm 2$\backslash$text{\{} mm{\}}{\$}{\$}. The average mean error for the 18 landmarks is 1.84 mm and average SD of mean error is 1.24.},
author = {Kaur, Amandeep and Singh, Chandan},
doi = {10.1007/s11760-013-0432-7},
issn = {18631711},
journal = {Signal, Image and Video Processing},
keywords = {Central projections,Cephalometry,Landmarks,Template matching,Zernike moments},
month = {jan},
number = {1},
pages = {117--132},
title = {{Automatic cephalometric landmark detection using Zernike moments and template matching}},
url = {http://link.springer.com/10.1007/s11760-013-0432-7},
volume = {9},
year = {2015}
}
@article{Quellec2020,
abstract = {In the last decades, large datasets of fundus photographs have been collected in diabetic retinopathy (DR) screening networks. Through deep learning, these datasets were used to train automatic detectors for DR and a few other frequent pathologies, with the goal to automate screening. One challenge limits the adoption of such systems so far: automatic detectors ignore rare conditions that ophthalmologists currently detect, such as papilledema or anterior ischemic optic neuropathy. The reason is that standard deep learning requires too many examples of these conditions. However, this limitation can be addressed with few-shot learning, a machine learning paradigm where a classifier has to generalize to a new category not seen in training, given only a few examples of this category. This paper presents a new few-shot learning framework that extends convolutional neural networks (CNNs), trained for frequent conditions, with an unsupervised probabilistic model for rare condition detection. It is based on the observation that CNNs often perceive photographs containing the same anomalies as similar, even though these CNNs were trained to detect unrelated conditions. This observation was based on the t-SNE visualization tool, which we decided to incorporate in our probabilistic model. Experiments on a dataset of 164,660 screening examinations from the OPHDIAT screening network show that 37 conditions, out of 41, can be detected with an area under the ROC curve (AUC) greater than 0.8 (average AUC: 0.938). In particular, this framework significantly outperforms other frameworks for detecting rare conditions, including multitask learning, transfer learning and Siamese networks, another few-shot learning solution. We expect these richer predictions to trigger the adoption of automated eye pathology screening, which will revolutionize clinical practice in ophthalmology.},
archivePrefix = {arXiv},
arxivId = {1907.09449},
author = {Quellec, Gwenol{\'{e}} and Lamard, Mathieu and Conze, Pierre Henri and Massin, Pascale and Cochener, B{\'{e}}atrice},
doi = {10.1016/j.media.2020.101660},
eprint = {1907.09449},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Deep learning,Diabetic retinopathy screening,Few-shot learning,Rare conditions},
pmid = {32028213},
title = {{Automatic detection of rare pathologies in fundus photographs using few-shot learning}},
volume = {61},
year = {2020}
}
@inproceedings{Paszke2017,
abstract = {In this article, we describe an automatic differentiation module of PyTorch-a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
booktitle = {NIPS 2017 Autodiff Workshop: The Future of Gradient-based Machine Learning Software and Techniques},
pages = {8024--8035},
title = {{Automatic differentiation in $\backslash$uppercase{\{}P{\}}y$\backslash$uppercase{\{}T{\}}orch}},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
year = {2017}
}
@article{Deselaers2008,
abstract = {In this paper, the automatic medical annotation task of the 2007 CLEF cross language image retrieval campaign (ImageCLEF) is described. The paper focusses on the images used, the task setup, and the results obtained in the evaluation campaign. Since 2005, the medical automatic image annotation task exists in ImageCLEF with increasing complexity to evaluate the performance of state-of-the-art methods for completely automatic annotation of medical images based on visual properties. The paper also describes the evolution of the task from its origin in 2005-2007. The 2007 task, comprising 11,000 fully annotated training images and 1000 test images to be annotated, is a realistic task with a large number of possible classes at different levels of detail. Detailed analysis of the methods across participating groups is presented with respect to the (i) image representation, (ii) classification method, and (iii) use of the class hierarchy. The results show that methods which build on local image descriptors and discriminative models are able to provide good predictions of the image classes, mostly by using techniques that were originally developed in the machine learning and computer vision domain for object recognition in non-medical images. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Deselaers, Thomas and Deserno, Thomas M. and M{\"{u}}ller, Henning},
doi = {10.1016/j.patrec.2008.03.001},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Automatic image annotation,Benchmark,Evaluation,Medical images},
number = {15},
pages = {1988--1995},
title = {{Automatic medical image annotation in ImageCLEF 2007: Overview, results, and discussion}},
volume = {29},
year = {2008}
}
@article{Peng2017,
author = {Peng, Hanchuan and Zhou, Zhi and Meijering, Erik and Zhao, Ting and Ascoli, Giorgio A. and Hawrylycz, Michael},
doi = {10.1038/nmeth.4233},
issn = {15487105},
journal = {Nature Methods},
month = {apr},
number = {4},
pages = {332--333},
pmid = {28362437},
title = {{Automatic tracing of ultra-volumes of neuronal images}},
url = {http://www.nature.com/articles/nmeth.4233},
volume = {14},
year = {2017}
}
@article{Jaeger2014,
abstract = {Tuberculosis is a major health threat in many regions of the world. Opportunistic infections in immunocompromised HIV/AIDS patients and multi-drug-resistant bacterial strains have exacerbated the problem, while diagnosing tuberculosis still remains a challenge. When left undiagnosed and thus untreated, mortality rates of patients with tuberculosis are high. Standard diagnostics still rely on methods developed in the last century. They are slow and often unreliable. In an effort to reduce the burden of the disease, this paper presents our automated approach for detecting tuberculosis in conventional posteroanterior chest radiographs. We first extract the lung region using a graph cut segmentation method. For this lung region, we compute a set of texture and shape features, which enable the X-rays to be classified as normal or abnormal using a binary classifier. We measure the performance of our system on two datasets: a set collected by the tuberculosis control program of our local county's health department in the United States, and a set collected by Shenzhen Hospital, China. The proposed computer-aided diagnostic system for TB screening, which is ready for field deployment, achieves a performance that approaches the performance of human experts. We achieve an area under the ROC curve (AUC) of 87{\%} (78.3{\%} accuracy) for the first set, and an AUC of 90{\%} (84{\%} accuracy) for the second set. For the first set, we compare our system performance with the performance of radiologists. When trying not to miss any positive cases, radiologists achieve an accuracy of about 82{\%} on this set, and their false positive rate is about half of our system's rate. {\textcopyright} 1982-2012 IEEE.},
author = {Jaeger, Stefan and Karargyris, Alexandros and Candemir, Sema and Folio, Les and Siegelman, Jenifer and Callaghan, Fiona and Xue, Zhiyun and Palaniappan, Kannappan and Singh, Rahul K. and Antani, Sameer and Thoma, George and Wang, Yi Xiang and Lu, Pu Xuan and McDonald, Clement J.},
doi = {10.1109/TMI.2013.2284099},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Computer-aided detection and diagnosis,X-ray imaging,lung,pattern recognition and classification,segmentation,tuberculosis (TB)},
number = {2},
pages = {233--245},
pmid = {24108713},
title = {{Automatic tuberculosis screening using chest radiographs}},
volume = {33},
year = {2014}
}
@article{He2019,
abstract = {Deep learning has penetrated all aspects of our lives and brought us great convenience. However, the process of building a high-quality deep learning system for a specific task is not only time-consuming but also requires lots of resources and relies on human expertise, which hinders the development of deep learning in both industry and academia. To alleviate this problem, a growing number of research projects focus on automated machine learning (AutoML). In this paper, we provide a comprehensive and up-to-date study on the state-of-the-art AutoML. First, we introduce the AutoML techniques in details according to the machine learning pipeline. Then we summarize existing Neural Architecture Search (NAS) research, which is one of the most popular topics in AutoML. We also compare the models generated by NAS algorithms with those human-designed models. Finally, we present several open problems for future research.},
annote = {{\_}eprint: 1908.00709},
archivePrefix = {arXiv},
arxivId = {1908.00709},
author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
doi = {10.1016/j.knosys.2020.106622},
eprint = {1908.00709},
issn = {09507051},
journal = {arXiv},
keywords = {AutoML,Machine learning,NAS,Survey},
title = {{AutoML: A survey of the state-of-the-Art}},
url = {http://arxiv.org/abs/1908.00709},
year = {2019}
}
@article{Ng2006,
abstract = {Helicopters have highly stochastic, nonlinear, dynamics, and autonomous helicopter flight is widely regarded to be a challenging control problem. As helicopters are highly unstable at low speeds, it is particularly difficult to design controllers for low speed aerobatic maneuvers. In this paper, we describe a successful application of reinforcement learning to designing a controller for sustained inverted flight on an autonomous helicopter. Using data collected from the helicopter in flight, we began by learning a stochastic, nonlinear model of the helicopter's dynamics. Then, a reinforcement learning algorithm was applied to automatically learn a controller for autonomous inverted hovering. Finally, the resulting controller was successfully tested on our autonomous helicopter platform. {\textcopyright} Springer-Verlag Berlin/Heidelberg 2006.},
author = {Ng, Andrew Y. and Coates, Adam and Diel, Mark and Ganapathi, Varun and Schulte, Jamie and Tse, Ben and Berger, Eric and Liang, Eric},
doi = {10.1007/11552246_35},
isbn = {3540288163},
issn = {16107438},
journal = {Springer Tracts in Advanced Robotics},
pages = {363--372},
title = {{Autonomous inverted helicopter flight via reinforcement earning}},
volume = {21},
year = {2006}
}
@article{Yang2018,
abstract = {Purpose: This report presents the methods and results of the Thoracic Auto-Segmentation Challenge organized at the 2017 Annual Meeting of American Association of Physicists in Medicine. The purpose of the challenge was to provide a benchmark dataset and platform for evaluating performance of autosegmentation methods of organs at risk (OARs) in thoracic CT images. Methods : Sixty thoracic CT scans provided by three different institutions were separated into 36 training, 12 offline testing, and 12 online testing scans. Eleven participants completed the offline challenge, and seven completed the online challenge. The OARs were left and right lungs, heart, esophagus, and spinal cord. Clinical contours used for treatment planning were quality checked and edited to adhere to the RTOG 1106 contouring guidelines. Algorithms were evaluated using the Dice coefficient, Hausdorff distance, and mean surface distance. A consolidated score was computed by normalizing the metrics against interrater variability and averaging over all patients and structures. Results : The interrater study revealed highest variability in Dice for the esophagus and spinal cord, and in surface distances for lungs and heart. Five out of seven algorithms that participated in the online challenge employed deep-learning methods. Although the top three participants using deep learning produced the best segmentation for all structures, there was no significant difference in the performance among them. The fourth place participant used a multi-atlas-based approach. The highest Dice scores were produced for lungs, with averages ranging from 0.95 to 0.98, while the lowest Dice scores were produced for esophagus, with a range of 0.55–0.72. Conclusion : The results of the challenge showed that the lungs and heart can be segmented fairly accurately by various algorithms, while deep-learning methods performed better on the esophagus. Our dataset together with the manual contours for all training cases continues to be available publicly as an ongoing benchmarking resource.},
author = {Yang, Jinzhong and Veeraraghavan, Harini and Armato, Samuel G. and Farahani, Keyvan and Kirby, Justin S. and Kalpathy-Kramer, Jayashree and van Elmpt, Wouter and Dekker, Andre and Han, Xiao and Feng, Xue and Aljabar, Paul and Oliveira, Bruno and van der Heyden, Brent and Zamdborg, Leonid and Lam, Dao and Gooding, Mark and Sharp, Gregory C.},
doi = {10.1002/mp.13141},
issn = {00942405},
journal = {Medical Physics},
keywords = {automatic segmentation,grand challenge,lung cancer,radiation therapy},
number = {10},
pages = {4568--4581},
pmid = {30144101},
title = {{Autosegmentation for thoracic radiation treatment planning: A grand challenge at AAPM 2017}},
volume = {45},
year = {2018}
}
@article{Aresta2019,
abstract = {Breast cancer is the most common invasive cancer in women, affecting more than 10{\%} of women worldwide. Microscopic analysis of a biopsy remains one of the most important methods to diagnose the type of breast cancer. This requires specialized analysis by pathologists, in a task that i) is highly time- and cost-consuming and ii) often leads to nonconsensual results. The relevance and potential of automatic classification algorithms using hematoxylin-eosin stained histopathological images has already been demonstrated, but the reported results are still sub-optimal for clinical use. With the goal of advancing the state-of-the-art in automatic classification, the Grand Challenge on BreAst Cancer Histology images (BACH) was organized in conjunction with the 15th International Conference on Image Analysis and Recognition (ICIAR 2018). BACH aimed at the classification and localization of clinically relevant histopathological classes in microscopy and whole-slide images from a large annotated dataset, specifically compiled and made publicly available for the challenge. Following a positive response from the scientific community, a total of 64 submissions, out of 677 registrations, effectively entered the competition. The submitted algorithms improved the state-of-the-art in automatic classification of breast cancer with microscopy images to an accuracy of 87{\%}. Convolutional neuronal networks were the most successful methodology in the BACH challenge. Detailed analysis of the collective results allowed the identification of remaining challenges in the field and recommendations for future developments. The BACH dataset remains publicly available as to promote further improvements to the field of automatic classification in digital pathology.},
archivePrefix = {arXiv},
arxivId = {1808.04277},
author = {Aresta, Guilherme and Ara{\'{u}}jo, Teresa and Kwok, Scotty and Chennamsetty, Sai Saketh and Safwan, Mohammed and Alex, Varghese and Marami, Bahram and Prastawa, Marcel and Chan, Monica and Donovan, Michael and Fernandez, Gerardo and Zeineh, Jack and Kohl, Matthias and Walz, Christoph and Ludwig, Florian and Braunewell, Stefan and Baust, Maximilian and Vu, Quoc Dang and To, Minh Nguyen Nhat and Kim, Eal and Kwak, Jin Tae and Galal, Sameh and Sanchez-Freire, Veronica and Brancati, Nadia and Frucci, Maria and Riccio, Daniel and Wang, Yaqi and Sun, Lingling and Ma, Kaiqiang and Fang, Jiannan and Kone, Ismael and Boulmane, Lahsen and Campilho, Aur{\'{e}}lio and Eloy, Catarina and Pol{\'{o}}nia, Ant{\'{o}}nio and Aguiar, Paulo},
doi = {10.1016/j.media.2019.05.010},
eprint = {1808.04277},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Breast cancer,Challenge,Comparative study,Deep learning,Digital pathology,Histology},
month = {aug},
pages = {122--139},
pmid = {31226662},
title = {{BACH: Grand challenge on breast cancer histology images}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841518307941},
volume = {56},
year = {2019}
}
@inproceedings{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82{\%} top-5 test error, exceeding the accuracy of human raters.},
annote = {{\_}eprint: 1502.03167},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1502.03167},
isbn = {9781510810587},
pages = {448--456},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
url = {http://arxiv.org/abs/1502.03167},
volume = {1},
year = {2015}
}
@book{Tsitoara2020,
address = {Berkeley, CA},
author = {Tsitoara, Mariot},
booktitle = {Beginning Git and GitHub},
doi = {10.1007/978-1-4842-5313-7},
isbn = {9781484253120},
publisher = {Apress},
title = {{Beginning Git and GitHub}},
url = {http://link.springer.com/10.1007/978-1-4842-5313-7},
year = {2020}
}
@book{Clark2020,
abstract = {Analysieren Sie Unternehmensdaten schnell und einfach mit den leistungsstarken Datentools von Microsoft. Erfahren Sie, wie Sie skalierbare und robuste Datenmodelle erstellen, verschiedene Datenquellen effektiv bereinigen und kombinieren sowie {\"{u}}berzeugende und professionelle Grafiken erstellen. Beginning Power BI ist eine praktische, aktivit{\"{a}}tsbasierte Anleitung, die Sie durch den Prozess der Analyse Ihrer Daten mit den Tools f{\"{u}}hrt, die den Kern des Self-Service-BI-Angebots von Microsoft bilden. Beginnend mit Power Query erfahren Sie, wie Sie Daten aus verschiedenen Quellen abrufen und wie einfach es ist, die Daten vor dem Import in ein Datenmodell zu bereinigen und zu formen. Mithilfe der Tabelle Power BI und der Datenanalyse-Ausdr{\"{u}}cke (DAX) lernen Sie, robuste skalierbare Datenmodelle zu erstellen, die als Grundlage f{\"{u}}r Ihre Datenanalyse dienen. Von dort aus betreten Sie die Welt {\"{u}}berzeugender interaktiver Visualisierungen, um Ihre Daten zu analysieren und Einblicke in sie zu gewinnen. Sie schlie{\ss}en Ihre Power BI-Reise ab, indem Sie lernen, wie Sie Ihre Berichte und Dashboards verpacken und mit Ihren Kollegen teilen. Der Autor Dan Clark f{\"{u}}hrt Sie anhand von Schritt-f{\"{u}}r-Schritt-Aktivit{\"{a}}ten und zahlreichen Screenshots durch jedes Thema, um Sie mit den Tools vertraut zu machen. Diese dritte Ausgabe behandelt die neuen und sich weiterentwickelnden Funktionen der Power BI-Plattform sowie neue Kapitel zu Datenfl{\"{u}}ssen und zusammengesetzten Modellen. Dieses Buch ist Ihre praktische Anleitung f{\"{u}}r schnelle, zuverl{\"{a}}ssige und wertvolle Dateninformationen. Was du lernen wirst: Vereinfachen Sie das Erkennen, Zuordnen und Bereinigen von Daten Erstellen Sie solide analytische Datenmodelle Erstellen Sie robuste interaktive Datenpr{\"{a}}sentationen Kombinieren Sie analytische und geografische Daten in kartenbasierten Visualisierungen Ver{\"{o}}ffentlichen und teilen Sie Dashboards und Berichte Dan Clark ist ein Senior Business Intelligence (BI) und Programmierberater, der sich auf Microsoft-Technologien spezialisiert hat. Er konzentriert sich darauf, neue BI- und Datentechnologien zu erlernen und andere darin zu schulen, wie die Technologie am besten implementiert werden kann. Dan hat mehrere B{\"{u}}cher und zahlreiche Artikel {\"{u}}ber .NET-Programmierung und BI-Entwicklung ver{\"{o}}ffentlicht. Er h{\"{a}}lt regelm{\"{a}}{\ss}ig Vortr{\"{a}}ge bei verschiedenen Entwickler- und Datenbankkonferenzen sowie Benutzergruppentreffen und interagiert gerne mit den Microsoft-Communities. In einem fr{\"{u}}heren Leben war Dan Physiklehrer. Er ist immer noch inspiriert von dem Wunder und der Ehrfurcht, das Universum zu studieren und herauszufinden, warum sich die Dinge so verhalten, wie sie es tun.},
address = {Berkeley, CA},
author = {Clark, Dan},
booktitle = {Beginning Microsoft Power BI},
doi = {10.1007/978-1-4842-5620-6},
isbn = {978-1-4842-5619-0},
publisher = {Apress},
title = {{Beginning Microsoft Power BI}},
url = {http://link.springer.com/10.1007/978-1-4842-5620-6},
year = {2020}
}
@article{Tobon-Gomez2015,
abstract = {Knowledge of left atrial (LA) anatomy is important for atrial fibrillation ablation guidance, fibrosis quantification and biophysical modelling. Segmentation of the LA from Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) images is a complex problem. This manuscript presents a benchmark to evaluate algorithms that address LA segmentation. The datasets, ground truth and evaluation code have been made publicly available through the http://www.cardiacatlas.org website. This manuscript also reports the results of the Left Atrial Segmentation Challenge (LASC) carried out at the STACOM'13 workshop, in conjunction with MICCAI'13. Thirty CT and 30 MRI datasets were provided to participants for segmentation. Each participant segmented the LA including a short part of the LA appendage trunk and proximal sections of the pulmonary veins (PVs). We present results for nine algorithms for CT and eight algorithms for MRI. Results showed that methodologies combining statistical models with region growing approaches were the most appropriate to handle the proposed task. The ground truth and automatic segmentations were standardised to reduce the influence of inconsistently defined regions (e.g., mitral plane, PVs end points, LA appendage). This standardisation framework, which is a contribution of this work, can be used to label and further analyse anatomical regions of the LA. By performing the standardisation directly on the left atrial surface, we can process multiple input data, including meshes exported from different electroanatomical mapping systems.},
author = {Tobon-Gomez, Catalina and Geers, Arjan J. and Peters, Jochen and Weese, J{\"{u}}rgen and Pinto, Karen and Karim, Rashed and Ammar, Mohammed and Daoudi, Abdelaziz and Margeta, Jan and Sandoval, Zulma and Stender, Birgit and Zheng, Yefeng and Zuluaga, Maria A. and Betancur, Julian and Ayache, Nicholas and Chikh, Mohammed Amine and Dillenseger, Jean Louis and Kelm, B. Michael and Mahmoudi, Sa{\"{i}}d and Ourselin, S{\'{e}}bastien and Schlaefer, Alexander and Schaeffter, Tobias and Razavi, Reza and Rhode, Kawal S.},
doi = {10.1109/TMI.2015.2398818},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Image segmentation,benchmark testing,cardiovascular disease,computed tomography,left atrium,magnetic resonance imaging},
number = {7},
pages = {1460--1473},
title = {{Benchmark for Algorithms Segmenting the Left Atrium From 3D CT and MRI Datasets}},
volume = {34},
year = {2015}
}
@article{Wang2019,
abstract = {Accurate segmentation of infant brain magnetic resonance (MR) images into white matter (WM), gray matter (GM), and cerebrospinal fluid is an indispensable foundation for early studying of brain growth patterns and morphological changes in neurodevelopmental disorders. Nevertheless, in the isointense phase (approximately 6-9months of age), due to inherentmyelination andmaturation process, WM and GM exhibit similar levels of intensity in both T1-weighted and T2-weighted MR images, making tissue segmentation very challenging. Although many efforts were devoted to brain segmentation, only a few studies have focused on the segmentation of six-month infant brain images. With the idea of boosting methodological development in the community, iSeg-2017 challenge (http://iseg2017.web.unc.edu) provides a set of six-month infant subjects with manual labels for training and testing the participating methods. Among the 21 automatic segmentation methods participating in iSeg-2017, we review the eight top-ranked teams, in terms of Dice ratio, modified Hausdorff distance, and average surface distance, and introduce their pipelines, implementations, as well as source codes. We further discuss the limitations and possible future directions. We hope the dataset in iSeg-2017, and this paper could provide insights into methodological development for the community.},
author = {Wang, Li and Nie, Dong and Li, Guannan and Puybareau, {\'{E}}lodie and Dolz, Jose and Zhang, Qian and Wang, Fan and Xia, Jing and Wu, Zhengwang and Chen, Jia Wei and Thung, Kim Han and Bui, Toan Duc and Shin, Jitae and Zeng, Guodong and Zheng, Guoyan and Fonov, Vladimir S. and Doyle, Andrew and Xu, Yongchao and Moeskops, Pim and Pluim, Josien P.W. and Desrosiers, Christian and Ayed, Ismail Ben and Sanroma, Gerard and Benkarim, Oualid M. and Casamitjana, Adri{\`{a}} and Vilaplana, Ver{\'{o}}nica and Lin, Weili and Li, Gang and Shen, Dinggang},
doi = {10.1109/TMI.2019.2901712},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Brain,Challenge,Infant,Isointense phase,Segmentation},
month = {sep},
number = {9},
pages = {2219--2230},
pmid = {30835215},
title = {{Benchmark on automatic six-month-old infant brain segmentation algorithms: The iSeg-2017 challenge}},
url = {https://ieeexplore.ieee.org/document/8654000/},
volume = {38},
year = {2019}
}
@article{Devlin2018,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5{\%} (7.7{\%} point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
annote = {{\_}eprint: 1810.04805},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming Wei and Lee, Kenton and Toutanova, Kristina},
eprint = {1810.04805},
isbn = {9781950737130},
journal = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
pages = {4171--4186},
title = {{BERT: Pre-training of deep bidirectional transformers for language understanding}},
url = {http://arxiv.org/abs/1810.04805},
volume = {1},
year = {2019}
}
@article{Werbos1974,
author = {Thesis, Science and Appl, Ph D and Harvard, Math},
number = {January 1974},
title = {{Beyond Regression : New Tools for Prediction and Analysis in the Behavioral}},
year = {2018}
}
@inproceedings{Wei2016,
abstract = {It is obvious that big data can bring us new opportunities to discover valuable information. Apparently, corresponding big datasets are powerful tools for scholars, which connect theoretical studies to reality. They can help scholars to evaluate their achievements and find new problems. In recent years, there has been a significant growth in research data repositories and registries. However, these infrastructures are fragmented across institutions, countries and research domains. As such, finding research datasets is not a trivial task for many researchers. Thus we investigated 195 papers regarding big data on some notable international conferences in recent 3 years, and also gathered 285 datasets mentioned in them. In this paper, we present and analyze our survey results in terms of the status quo of big data research and datasets from different aspects. In particular, we propose two different taxonomies of big datasets and classify our surveyed datasets into them. In addition, we also give a brief introduction about 7 widely accepted data collections online. Finally, some basic principles for scholars in choosing and using big datasets are given.},
author = {Wei, Yi and Liu, Shijun and Sun, Jiao and Cui, Lizhen and Pan, Li and Wu, Lei},
booktitle = {Proceedings - 2016 IEEE International Congress on Big Data, BigData Congress 2016},
doi = {10.1109/BigDataCongress.2016.62},
isbn = {9781509026227},
keywords = {Big data,Datasets,Survey},
month = {oct},
pages = {394--401},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Big datasets for research: A survey on flagship conferences}},
year = {2016}
}
@book{Pan2020,
address = {Singapore},
doi = {10.1007/978-981-15-3425-6},
editor = {Pan, Linqiang and Liang, Jing and Qu, Boyang},
isbn = {978-981-15-3424-9},
publisher = {Springer Singapore},
series = {Communications in Computer and Information Science},
title = {{Bio-inspired Computing: Theories and Applications}},
url = {http://link.springer.com/10.1007/978-981-15-3425-6},
volume = {1159},
year = {2020}
}
@article{Borovec2019,
abstract = {This report presents a generic image registration benchmark with automatic evaluation using landmark annotations. The key features of the BIRL framework are: easily extendable, performance evaluation, parallel experimentation, simple visualisations, experiment's time-out limit, resuming unfinished experiments. From the research practice, we identified and focused on these two main use-cases: (a) comparison of user's (newly developed) method with some State-of-the-Art (SOTA) methods on a common dataset and (b) experimenting SOTA methods on user's custom dataset (which should contain landmark annotation). Moreover, we present an integration of several standard image registration methods aiming at biomedical imaging into the BIRL framework. This report also contains experimental results of these SOTA methods on the CIMA dataset, which is a dataset of Whole Slice Imaging (WSI) from histology/pathology containing several multi-stain tissue samples from three tissue kinds. Source and results: https://borda.github.io/BIRL},
archivePrefix = {arXiv},
arxivId = {1912.13452},
author = {Borovec, Jiri},
eprint = {1912.13452},
month = {dec},
title = {{BIRL: Benchmark on Image Registration methods with Landmark validation}},
url = {http://arxiv.org/abs/1912.13452},
year = {2019}
}
@article{Zhangb,
abstract = {Multi-organ segmentation is a challenging task due to the label imbalance and structural differences between different organs. In this work, we propose an efficient cascaded V-Net model to improve the performance of multi-organ segmentation by establishing dense Block Level Skip Connections (BLSC) across cascaded V-Net. Our model can take full advantage of features from the first stage network and make the cascaded structure more efficient. We also combine stacked small and large kernels with an inception-like structure to help our model to learn more patterns, which produces superior results for multi-organ segmentation. In addition, some small organs are commonly occluded by large organs and have unclear boundaries with other surrounding tissues, which makes them hard to be segmented. We therefore first locate the small organs through a multi-class network and crop them randomly with the surrounding region, then segment them with a single-class network. We evaluated our model on SegTHOR 2019 challenge unseen testing set and Multi-Atlas Labeling Beyond the Cranial Vault challenge validation set. Our model has achieved an average dice score gain of 1.62 percents and 3.90 percents compared to traditional cascaded networks on these two datasets, respectively. For hard-to-segment small organs, such as the esophagus in SegTHOR 2019 challenge, our technique has achieved a gain of 5.63 percents on dice score, and four organs in Multi-Atlas Labeling Beyond the Cranial Vault challenge have achieved a gain of 5.27 percents on average dice score.},
author = {Zhang, Liang and Zhang, Jiaming and Shen, Peiyi and Zhu, Guangming and Li, Ping and Lu, Xiaoyuan and Zhang, Huan and Shah, Syed Afaq and Bennamoun, Mohammed},
doi = {10.1109/TMI.2020.2975347},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Multi-organ segmentation,cascaded network,hard-to-segment,inception-like structure,skip connections},
number = {9},
pages = {2782--2793},
pmid = {32091995},
title = {{Block Level Skip Connections across Cascaded V-Net for Multi-Organ Segmentation}},
url = {https://ieeexplore.ieee.org/document/9006924},
volume = {39},
year = {2020}
}
@misc{ue-bvs,
abstract = {Brenden Sewell is a lead game designer at E-Line Media, and has spent the last 5 years designing and creating games that are both fun to play and have educational or social impact. He has been building games since 2002, when Neverwinter Nights taught him an invaluable lesson about the expressive power of game design. In 2010, he graduated with a degree in cognitive science from Indiana University. Since then, he has focused on enhancing his own craft of game design while harnessing its power to do good in the world, and exposing more people to the joy the profession holds.},
author = {{Epic Games}},
isbn = {9781785286018},
pages = {273},
title = {{Blueprints Visual Scripting | Unreal Engine Documentation}},
url = {https://docs.unrealengine.com/en-US/Engine/Blueprints/index.html},
year = {2014}
}
@article{Chang2019,
abstract = {Vision science, particularly machine vision, has been revolutionized by introducing large-scale image datasets and statistical learning approaches. Yet, human neuroimaging studies of visual perception still rely on small numbers of images (around 100) due to time-constrained experimental procedures. To apply statistical learning approaches that include neuroscience, the number of images used in neuroimaging must be significantly increased. We present BOLD5000, a human functional MRI (fMRI) study that includes almost 5,000 distinct images depicting real-world scenes. Beyond dramatically increasing image dataset size relative to prior fMRI studies, BOLD5000 also accounts for image diversity, overlapping with standard computer vision datasets by incorporating images from the Scene UNderstanding (SUN), Common Objects in Context (COCO), and ImageNet datasets. The scale and diversity of these image datasets, combined with a slow event-related fMRI design, enables fine-grained exploration into the neural representation of a wide range of visual features, categories, and semantics. Concurrently, BOLD5000 brings us closer to realizing Marr's dream of a singular vision science-the intertwined study of biological and computer vision.},
archivePrefix = {arXiv},
arxivId = {1809.01281},
author = {Chang, Nadine and Pyles, John A. and Marcus, Austin and Gupta, Abhinav and Tarr, Michael J. and Aminoff, Elissa M.},
doi = {10.1038/s41597-019-0052-3},
eprint = {1809.01281},
issn = {20524463},
journal = {Scientific data},
month = {sep},
number = {1},
pages = {49},
title = {{BOLD5000, a public fMRI dataset while viewing 5000 visual images}},
url = {http://dx.doi.org/10.1038/s41597-019-0052-3},
volume = {6},
year = {2019}
}
@inproceedings{Song2015,
abstract = {Bone texture characterization is important for differentiating osteoporotic and healthy subjects. Automated classification is however very challenging due to the high degree of visual similarity between the two types of images. In this paper, we propose to describe the bone textures by extracting dense sets of local descriptors and encoding them with the improved Fisher vector (IFV). Compared to the standard bag-of-visual-words (BoW) model, Fisher encoding is more discriminative by representing the distribution of local descriptors in addition to the occurrence frequencies. Our method is evaluated on the ISBI 2014 challenge dataset of bone texture characterization, and we demonstrate excellent classification performance compared to the challenge entries and large improvement over the BoW model.},
author = {Song, Yang and Cai, Weidong and Zhang, Fan and Huang, Heng and Zhou, Yun and {Dagan Feng}, David},
booktitle = {Proceedings - International Symposium on Biomedical Imaging},
doi = {10.1109/ISBI.2015.7163803},
isbn = {9781479923748},
issn = {19458452},
keywords = {Bone texture,Fisher vector,classification,feature encoding},
month = {apr},
pages = {5--8},
publisher = {IEEE},
title = {{Bone texture characterization with fisher encoding of local descriptors}},
url = {http://ieeexplore.ieee.org/document/7163803/},
volume = {2015-July},
year = {2015}
}
@article{Pereira2016,
abstract = {Among brain tumors, gliomas are the most common and aggressive, leading to a very short life expectancy in their highest grade. Thus, treatment planning is a key stage to improve the quality of life of oncological patients. Magnetic resonance imaging (MRI) is a widely used imaging technique to assess these tumors, but the large amount of data produced by MRI prevents manual segmentation in a reasonable time, limiting the use of precise quantitative measurements in the clinical practice. So, automatic and reliable segmentation methods are required; however, the large spatial and structural variability among brain tumors make automatic segmentation a challenging problem. In this paper, we propose an automatic segmentation method based on Convolutional Neural Networks (CNN), exploring small 3 x 3 kernels. The use of small kernels allows designing a deeper architecture, besides having a positive effect against overfitting, given the fewer number of weights in the network. We also investigated the use of intensity normalization as a pre-processing step, which though not common in CNN-based segmentation methods, proved together with data augmentation to be very effective for brain tumor segmentation in MRI images. Our proposal was validated in the Brain Tumor Segmentation Challenge 2013 database (BRATS 2013), obtaining simultaneously the first position for the complete, core, and enhancing regions in Dice Similarity Coefficient metric (0.88, 0.83, 0.77) for the Challenge data set. Also, it obtained the overall first position by the online evaluation platform. We also participated in the on-site BRATS 2015 Challenge using the same model, obtaining the second place, with Dice Similarity Coefficient metric of 0.78, 0.65, and 0.75 for the complete, core, and enhancing regions, respectively.},
author = {Pereira, Sergio and Pinto, Adriano and Alves, Victor and Silva, Carlos A.},
doi = {10.1109/TMI.2016.2538465},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Brain tumor,brain tumor segmentation,convolutional neural networks,deep learning,glioma,magnetic resonance imaging},
number = {5},
pages = {1240--1251},
pmid = {26960222},
title = {{Brain Tumor Segmentation Using Convolutional Neural Networks in MRI Images}},
volume = {35},
year = {2016}
}
@misc{Schmainda2018,
abstract = {This collection includes datasets from 20 subjects with primary newly diagnosed glioblastoma who were treated with surgery and standard concomitant chemo-radiation therapy (CRT) followed by adjuvant chemotherapy. Two MRI exams are included for each patient: within 90 days following CRT completion and at progression (determined clinically, and based on a combination of clinical performance and/or imaging findings, and punctuated by a change in treatment or intervention). All image sets are in DICOM format and contain T1w (pre and post-contrast agent), FLAIR, T2w, ADC, normalized cerebral blood flow, normalized relative cerebral blood volume, standardized relative cerebral blood volume, and binary tumor masks (generated using T1w images). The perfusion images were generated from dynamic susceptibility contrast (GRE-EPI DSC) imaging following a preload of contrast agent. All of the series are co-registered with the T1+C images. The intent of this dataset is for assessing deep learning algorithm performance to predict tumor progression.},
author = {Schmainda, K.M. and Prah, M.},
booktitle = {Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2018.15quzvnb},
title = {{Brain-Tumor-Progression}},
url = {http://doi.org/10.7937/K9/TCIA.2018.15quzvnb},
year = {2018}
}
@online{brats13,
title = {{BRATS - SICAS Medical Image Repository}},
url = {https://www.virtualskeleton.ch/BRATS/Start2013}
}
@misc{Buda2020a,
author = {Buda, Mateusz and Saha, Ashirbani and Walsh, Ruth and Ghate, Sujata and Li, Nianyi and Swiecicki, Albert and Lo, Joseph Y and Yang, Jichen and Mazurowski, Maciej},
doi = {10.7937/E4WT-CD02},
publisher = {The Cancer Imaging Archive},
title = {{Breast Cancer Screening – Digital Breast Tomosynthesis (BCS-DBT)}},
url = {https://wiki.cancerimagingarchive.net/x/DAbbAw},
year = {2020}
}
@misc{Campanella2019a,
author = {Campanella, Gabriele and Hanna, Matthew G and Brogi, Edi and Fuchs, Thomas J},
doi = {10.7937/TCIA.2019.3XBN2JCC},
publisher = {The Cancer Imaging Archive},
title = {{Breast Metastases to Axillary Lymph Nodes}},
url = {https://wiki.cancerimagingarchive.net/x/yxolAw},
year = {2019}
}
@article{ild,
abstract = {This paper describes the methodology used to create a multimedia collection of cases with interstitial lung diseases (ILDs) at the University Hospitals of Geneva. The dataset contains high-resolution computed tomography (HRCT) image series with three-dimensional annotated regions of pathological lung tissue along with clinical parameters from patients with pathologically proven diagnoses of ILDs. The motivations for this work is to palliate the lack of publicly available collections of ILD cases to serve as a basis for the development and evaluation of image-based computerized diagnostic aid. After 38 months of data collection, the library contains 128 patients affected with one of the 13 histological diagnoses of ILDs, 108 image series with more than 41. l of annotated lung tissue patterns as well as a comprehensive set of 99 clinical parameters related to ILDs. The database is available for research on request and after signature of a license agreement. {\textcopyright} 2011 Elsevier Ltd.},
author = {Depeursinge, Adrien and Vargas, Alejandro and Platon, Alexandra and Geissbuhler, Antoine and Poletti, Pierre Alexandre and M{\"{u}}ller, Henning},
doi = {10.1016/j.compmedimag.2011.07.003},
issn = {08956111},
journal = {Computerized Medical Imaging and Graphics},
keywords = {Case-based retrieval,Computer-aided diagnosis,Content-based image retrieval,High-resolution computed tomography,Interstitial lung diseases,Multimedia database},
number = {3},
pages = {227--238},
pmid = {21803548},
title = {{Building a reference multimedia database for interstitial lung diseases}},
volume = {36},
year = {2012}
}
@book{Rojas2020,
address = {Berkeley, CA},
author = {Rojas, Carlos},
booktitle = {Building Progressive Web Applications with Vue.js},
doi = {10.1007/978-1-4842-5334-2},
isbn = {978-1-4842-5333-5},
publisher = {Apress},
title = {{Building Progressive Web Applications with Vue.js}},
url = {http://link.springer.com/10.1007/978-1-4842-5334-2},
year = {2020}
}
@book{Olsson2020,
address = {Berkeley, CA},
author = {Olsson, Mikael},
booktitle = {C{\#} 8 Quick Syntax Reference},
doi = {10.1007/978-1-4842-5577-3},
isbn = {978-1-4842-5576-6},
publisher = {Apress},
title = {{C{\#} 8 Quick Syntax Reference}},
url = {http://link.springer.com/10.1007/978-1-4842-5577-3},
year = {2020}
}
@online{CADDementia,
author = {Bron, Esther E. and Klein, Stefan and Smits, Marion and van Swieten, John C. and Niessen, Wiro J.},
title = {{CADDementia - Home}},
url = {https://caddementia.grand-challenge.org/},
year = {2014}
}
@article{Grammatikopoulou2019,
abstract = {Video feedback provides a wealth of information about surgical procedures and is the main sensory cue for surgeons. Scene understanding is crucial to computer assisted interventions (CAI) and to post-operative analysis of the surgical procedure. A fundamental building block of such capabilities is the identification and localization of surgical instruments and anatomical structures through semantic segmentation. Deep learning has advanced semantic segmentation techniques in the recent years but is inherently reliant on the availability of labeled datasets for model training. This paper introduces a dataset for semantic segmentation of cataract surgery videos. The annotated images are part of the publicly available CATARACTS challenge dataset. In addition, we benchmark the performance of several state-of-the-art deep learning models for semantic segmentation on the presented dataset. The dataset is publicly available at https://cataracts.grand-challenge.org/CaDIS/ .},
archivePrefix = {arXiv},
arxivId = {1906.11586},
author = {Grammatikopoulou, Maria and Flouty, Evangello and Kadkhodamohammadi, Abdolrahim and Quellec, Gwenol'e and Chow, Andre and Nehme, Jean and Luengo, Imanol and Stoyanov, Danail},
eprint = {1906.11586},
month = {jun},
title = {{CaDIS: Cataract Dataset for Image Segmentation}},
url = {http://arxiv.org/abs/1906.11586},
year = {2019}
}
@inproceedings{Jia2014,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying generalpurpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (≈ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
booktitle = {MM 2014 - Proceedings of the 2014 ACM Conference on Multimedia},
doi = {10.1145/2647868.2654889},
eprint = {1408.5093},
isbn = {9781450330633},
issn = {1450330630},
keywords = {Computer vision,Machine learning,Neural networks,Open source,Parallel computation},
pages = {675--678},
title = {{Caffe: Convolutional architecture for fast feature embedding}},
year = {2014}
}
@article{Chowdhury2020,
abstract = {Coronavirus disease (COVID-19) is a pandemic disease, which has already caused thousands of causalities and infected several millions of people worldwide. Any technological tool enabling rapid screening of the COVID-19 infection with high accuracy can be crucially helpful to the healthcare professionals. The main clinical tool currently in use for the diagnosis of COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which is expensive, less-sensitive and requires specialized medical personnel. X-ray imaging is an easily accessible tool that can be an excellent alternative in the COVID-19 diagnosis. This research was taken to investigate the utility of artificial intelligence (AI) in the rapid and accurate detection of COVID-19 from chest X-ray images. The aim of this paper is to propose a robust technique for automatic detection of COVID-19 pneumonia from digital chest X-ray images applying pre-trained deep-learning algorithms while maximizing the detection accuracy. A public database was created by the authors combining several public databases and also by collecting images from recently published articles. The database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579 normal chest X-ray images. Transfer learning technique was used with the help of image augmentation to train and validate several pre-trained deep Convolutional Neural Networks (CNNs). The networks were trained to classify two different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and COVID-19 pneumonia with and without image augmentation. The classification accuracy, precision, sensitivity, and specificity for both the schemes were 99.7{\%}, 99.7{\%}, 99.7{\%} and 99.55{\%} and 97.9{\%}, 97.95{\%}, 97.9{\%}, and 98.8{\%}, respectively. The high accuracy of this computer-aided diagnostic tool can significantly improve the speed and accuracy of COVID-19 diagnosis. This would be extremely useful in this pandemic where disease burden and need for preventive measures are at odds with available resources.},
archivePrefix = {arXiv},
arxivId = {2003.13145},
author = {Chowdhury, Muhammad E.H. and Rahman, Tawsifur and Khandakar, Amith and Mazhar, Rashid and Kadir, Muhammad Abdul and Mahbub, Zaid Bin and Islam, Khandakar Reajul and Khan, Muhammad Salman and Iqbal, Atif and Emadi, Nasser Al and Reaz, Mamun Bin Ibne and Islam, Mohammad Tariqul},
doi = {10.1109/ACCESS.2020.3010287},
eprint = {2003.13145},
issn = {21693536},
journal = {IEEE Access},
keywords = {Artificial intelligence,COVID-19 pneumonia,computer-aided diagnostic tool,machine learning,transfer learning,viral pneumonia},
pages = {132665--132676},
title = {{Can AI Help in Screening Viral and COVID-19 Pneumonia?}},
url = {https://ieeexplore.ieee.org/document/9144185/},
volume = {8},
year = {2020}
}
@article{LaLonde2018CapsulesFO,
abstract = {Convolutional neural networks (CNNs) have shown remarkable results over the last several years for a wide range of computer vision tasks. A new architecture recently introduced by Sabour et al. [2017], referred to as a capsule networks with dynamic routing, has shown great initial results for digit recognition and small image classification. The success of capsule networks lies in their ability to preserve more information about the input by replacing max-pooling layers with convolutional strides and dynamic routing, allowing for preservation of part-whole relationships in the data. This preservation of the input is demonstrated by reconstructing the input from the output capsule vectors. Our work expands the use of capsule networks to the task of object segmentation for the first time in the literature. We extend the idea of convolutional capsules with locally-connected routing and propose the concept of deconvolutional capsules. Further, we extend the masked reconstruction to reconstruct the positive input class. The proposed convolutionaldeconvolutional capsule network, called SegCaps, shows strong results for the task of object segmentation with substantial decrease in parameter space. As an example application, we applied the proposed SegCaps to segment pathological lungs from low dose CT scans and compared its accuracy and efficiency with other U-Net-based architectures. SegCaps is able to handle large image sizes (512 × 512) as opposed to baseline capsules (typically less than 32 × 32). The proposed SegCaps reduced the number of parameters of U-Net architecture by 95.4{\%} while still providing a better segmentation accuracy.},
archivePrefix = {arXiv},
arxivId = {1804.04241},
author = {LaLonde, Rodney and Bagci, Ulas},
eprint = {1804.04241},
journal = {arXiv},
month = {apr},
title = {{Capsules for object segmentation}},
url = {http://arxiv.org/abs/1804.04241},
volume = {abs/1804.0},
year = {2018}
}
@article{AlHajj2019,
abstract = {Surgical tool detection is attracting increasing attention from the medical image analysis community. The goal generally is not to precisely locate tools in images, but rather to indicate which tools are being used by the surgeon at each instant. The main motivation for annotating tool usage is to design efficient solutions for surgical workflow analysis, with potential applications in report generation, surgical training and even real-time decision support. Most existing tool annotation algorithms focus on laparoscopic surgeries. However, with 19 million interventions per year, the most common surgical procedure in the world is cataract surgery. The CATARACTS challenge was organized in 2017 to evaluate tool annotation algorithms in the specific context of cataract surgery. It relies on more than nine hours of videos, from 50 cataract surgeries, in which the presence of 21 surgical tools was manually annotated by two experts. With 14 participating teams, this challenge can be considered a success. As might be expected, the submitted solutions are based on deep learning. This paper thoroughly evaluates these solutions: in particular, the quality of their annotations are compared to that of human interpretations. Next, lessons learnt from the differential analysis of these solutions are discussed. We expect that they will guide the design of efficient surgery monitoring tools in the near future.},
author = {{Al Hajj}, Hassan and Lamard, Mathieu and Conze, Pierre Henri and Roychowdhury, Soumali and Hu, Xiaowei and Mar{\v{s}}alkaitė, Gabija and Zisimopoulos, Odysseas and Dedmari, Muneer Ahmad and Zhao, Fenqiang and Prellberg, Jonas and Sahu, Manish and Galdran, Adrian and Ara{\'{u}}jo, Teresa and Vo, Duc My and Panda, Chandan and Dahiya, Navdeep and Kondo, Satoshi and Bian, Zhengbing and Vahdat, Arash and Bialopetravi{\v{c}}ius, Jonas and Flouty, Evangello and Qiu, Chenhui and Dill, Sabrina and Mukhopadhyay, Anirban and Costa, Pedro and Aresta, Guilherme and Ramamurthy, Senthil and Lee, Sang Woong and Campilho, Aur{\'{e}}lio and Zachow, Stefan and Xia, Shunren and Conjeti, Sailesh and Stoyanov, Danail and Armaitis, Jogundas and Heng, Pheng Ann and Macready, William G. and Cochener, B{\'{e}}atrice and Quellec, Gwenol{\'{e}}},
doi = {10.1016/j.media.2018.11.008},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Cataract surgery,Challenge,Deep learning,Video analysis},
month = {feb},
pages = {24--41},
pmid = {30468970},
title = {{CATARACTS: Challenge on automatic tool annotation for cataRACT surgery}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S136184151830865X},
volume = {52},
year = {2019}
}
@article{Tessa2019,
abstract = {Task- and stimulus-based neuroimaging studies have begun to unveil the central autonomic network which modulates autonomic nervous system activity. In the present study, we aimed to evaluate the central autonomic network without the bias constituted by the use of a task. Additionally, we assessed whether this circuitry presents signs of dysregulation in the early stages of Parkinson's disease (PD), a condition which may be associated with dysautonomia. We combined heart-rate-variability based methods for time-varying assessments of the autonomic nervous system outflow with resting-state fMRI in 14 healthy controls and 14 de novo PD patients, evaluating the correlations between fMRI time-series and the instantaneous high-frequency component of the heart-rate-variability power spectrum, a marker of parasympathetic outflow. In control subjects, the high-frequency component of the heart-rate-variability power spectrum was significantly anti-correlated with fMRI time-series in several cortical, subcortical and brainstem regions. This complex central network was not detectable in PD patients. In between-group analysis, we found that in healthy controls the brain activation related to the high-frequency component of the heart-rate-variability power spectrum was significantly less than in PD patients in the mid and anterior cingulum, sensorimotor cortex and supplementary motor area, insula and temporal lobe, prefrontal cortex, hippocampus and in a region encompassing posterior cingulum, precuneus and parieto-occipital cortex. Our results indicate that the complex central network which modulates parasympathetic outflow in the resting state is impaired in the early clinical stages of PD.},
author = {Tessa, Carlo and Toschi, Nicola and Orsolini, Stefano and Valenza, Gaetano and Lucetti, Claudio and Barbieri, Riccardo and Diciotti, Stefano},
doi = {10.1371/journal.pone.0210324},
editor = {Koenig, Julian},
issn = {19326203},
journal = {PLoS ONE},
month = {jan},
number = {1},
pages = {e0210324},
pmid = {30653564},
title = {{Central modulation of parasympathetic outflow is impaired in de novo Parkinson's disease patients}},
url = {https://dx.plos.org/10.1371/journal.pone.0210324},
volume = {14},
year = {2019}
}
@article{Pantoni2010,
abstract = {The term cerebral small vessel disease refers to a group of pathological processes with various aetiologies that affect the small arteries, arterioles, venules, and capillaries of the brain. Age-related and hypertension-related small vessel diseases and cerebral amyloid angiopathy are the most common forms. The consequences of small vessel disease on the brain parenchyma are mainly lesions located in the subcortical structures such as lacunar infarcts, white matter lesions, large haemorrhages, and microbleeds. Because lacunar infarcts and white matter lesions are easily detected by neuroimaging, whereas small vessels are not, the term small vessel disease is frequently used to describe the parenchyma lesions rather than the underlying small vessel alterations. This classification, however, restricts the definition of small vessel disease to ischaemic lesions and might be misleading. Small vessel disease has an important role in cerebrovascular disease and is a leading cause of cognitive decline and functional loss in the elderly. Small vessel disease should be a main target for preventive and treatment strategies, but all types of presentation and complications should be taken into account. {\textcopyright} 2010 Elsevier Ltd. All rights reserved.},
author = {Pantoni, Leonardo},
doi = {10.1016/S1474-4422(10)70104-6},
issn = {14744422},
journal = {The Lancet Neurology},
month = {jul},
number = {7},
pages = {689--701},
pmid = {20610345},
title = {{Cerebral small vessel disease: from pathogenesis and clinical characteristics to therapeutic challenges}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1474442210701046},
volume = {9},
year = {2010}
}
@article{Bontempi2019,
abstract = {Many functional and structural neuroimaging studies call for accurate morphometric segmentation of different brain structures starting from image intensity values of MRI scans. Current automatic (multi-) atlas-based segmentation strategies often lack accuracy on difficult-to-segment brain structures and, since these methods rely on atlas-to-scan alignment, they may take long processing times. Alternatively, recent methods deploying solutions based on Convolutional Neural Networks (CNNs) are enabling the direct analysis of out-of-the-scanner data. However, current CNN-based solutions partition the test volume into 2D or 3D patches, which are processed independently. This process entails a loss of global contextual information, thereby negatively impacting the segmentation accuracy. In this work, we design and test an optimised end-to-end CNN architecture that makes the exploitation of global spatial information computationally tractable, allowing to process a whole MRI volume at once. We adopt a weakly supervised learning strategy by exploiting a large dataset composed of 947 out-of-the-scanner (3 Tesla T1-weighted 1mm isotropic MP-RAGE 3D sequences) MR Images. The resulting model is able to produce accurate multi-structure segmentation results in only a few seconds. Different quantitative measures demonstrate an improved accuracy of our solution when compared to state-of-the-art techniques. Moreover, through a randomised survey involving expert neuroscientists, we show that subjective judgements favour our solution with respect to widely adopted atlas-based software.},
archivePrefix = {arXiv},
arxivId = {1909.05085},
author = {Bontempi, Dennis and Benini, Sergio and Signoroni, Alberto and Svanera, Michele and Muckli, Lars},
doi = {10.1016/j.media.2020.101688},
eprint = {1909.05085},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {3D Image analysis,Brain MRI segmentation,Convolutional neural networks,Weakly supervised learning},
month = {sep},
pmid = {32272345},
title = {{CEREBRUM: a fast and fully-volumetric Convolutional Encoder-decodeR for weakly-supervised sEgmentation of BRain strUctures from out-of-the-scanner MRI}},
url = {http://arxiv.org/abs/1909.05085},
volume = {62},
year = {2020}
}
@article{Goodfellow2015,
abstract = {The ICML 2013 Workshop on Challenges in Representation Learning. 11http://deeplearning.net/icml2013-workshop-competition. focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions.},
archivePrefix = {arXiv},
arxivId = {1307.0414},
author = {Goodfellow, Ian J. and Erhan, Dumitru and {Luc Carrier}, Pierre and Courville, Aaron and Mirza, Mehdi and Hamner, Ben and Cukierski, Will and Tang, Yichuan and Thaler, David and Lee, Dong Hyun and Zhou, Yingbo and Ramaiah, Chetan and Feng, Fangxiang and Li, Ruifan and Wang, Xiaojie and Athanasakis, Dimitris and Shawe-Taylor, John and Milakov, Maxim and Park, John and Ionescu, Radu and Popescu, Marius and Grozea, Cristian and Bergstra, James and Xie, Jingjing and Romaszko, Lukasz and Xu, Bing and Chuang, Zhang and Bengio, Yoshua},
doi = {10.1016/j.neunet.2014.09.005},
eprint = {1307.0414},
issn = {18792782},
journal = {Neural Networks},
keywords = {Competition,Dataset,Representation learning},
pages = {59--63},
pmid = {25613956},
title = {{Challenges in representation learning: A report on three machine learning contests}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608014002159},
volume = {64},
year = {2015}
}
@article{Kavur2020,
abstract = {—Segmentation of abdominal organs has been a comprehensive, yet unresolved, research field for many years. In the last decade, intensive developments in deep learning (DL) have introduced new state-of-the-art segmentation systems. Despite outperforming the overall accuracy of existing systems, the effects of DL model properties and parameters on the performance is hard to interpret. This makes comparative analysis a necessary tool to achieve explainable studies and systems. Moreover, the performance of DL for emerging learning approaches such as cross-modality and multi-modal tasks have been rarely discussed. In order to expand the knowledge in these topics, CHAOS – Combined (CT-MR) Healthy Abdominal Organ Segmentation challenge has been organized in the IEEE International Symposium on Biomedical Imaging (ISBI), 2019, in Venice, Italy. Despite a large number of the previous abdomen related challenges, the majority of which are focused on tumor/lesion detection and/or classification with a single modality, CHAOS provides
both abdominal CT and MR data from healthy subjects. Five different and complementary tasks have been designed to analyze the capabilities of the current approaches from multiple perspectives. The results are investigated thoroughly, compared with manual annotations and interactive methods. The outcomes are reported in detail to reflect the latest advancements in the field. CHAOS challenge and data will be available online to provide a continuous benchmark resource for segmentation.},
archivePrefix = {arXiv},
arxivId = {2001.06535},
author = {Kavur, A. Emre and Gezer, N. Sinem and Barış, Mustafa and Conze, Pierre Henri and Groza, Vladimir and Pham, Duc Duy and Chatterjee, Soumick and Ernst, Philipp and {\"{O}}zkan, Savaş and Baydar, Bora and Lachinov, Dmitry and Han, Shuo and Pauli, Josef and Isensee, Fabian and Perkonigg, Matthias and Sathish, Rachana and Rajan, Ronnie and Aslan, Sinem and Sheet, Debdoot and Dovletov, Gurbandurdy and Speck, Oliver and N{\"{u}}rnberger, Andreas and Maier-Hein, Klaus H. and Akar, G{\"{o}}zde Bozdagı and {\"{U}}nal, G{\"{o}}zde and Dicle, Oguz and Selver, M. Alper},
eprint = {2001.06535},
journal = {arXiv},
keywords = {Abdomen,CT,Challenge,MRI,Segmentation},
month = {jan},
title = {{CHAOS Challenge - Combined (CT-MR) healthy abdominal organ segmentation}},
url = {http://arxiv.org/abs/2001.06535},
year = {2020}
}
@dataset{CHAOSdata2019,
abstract = {—Segmentation of abdominal organs has been a comprehensive, yet unresolved, research field for many years. In the last decade, intensive developments in deep learning (DL) have introduced new state-of-the-art segmentation systems. Despite outperforming the overall accuracy of existing systems, the effects of DL model properties and parameters on the performance is hard to interpret. This makes comparative analysis a necessary tool to achieve explainable studies and systems. Moreover, the performance of DL for emerging learning approaches such as cross-modality and multi-modal tasks have been rarely discussed. In order to expand the knowledge in these topics, CHAOS – Combined (CT-MR) Healthy Abdominal Organ Segmentation challenge has been organized in the IEEE International Symposium on Biomedical Imaging (ISBI), 2019, in Venice, Italy. Despite a large number of the previous abdomen related challenges, the majority of which are focused on tumor/lesion detection and/or classification with a single modality, CHAOS provides
both abdominal CT and MR data from healthy subjects. Five different and complementary tasks have been designed to analyze the capabilities of the current approaches from multiple perspectives. The results are investigated thoroughly, compared with manual annotations and interactive methods. The outcomes are reported in detail to reflect the latest advancements in the field. CHAOS challenge and data will be available online to provide a continuous benchmark resource for segmentation.},
archivePrefix = {arXiv},
arxivId = {2001.06535},
author = {Kavur, A. Emre and Gezer, N. Sinem and Barış, Mustafa and Conze, Pierre Henri and Groza, Vladimir and Pham, Duc Duy and Chatterjee, Soumick and Ernst, Philipp and {\"{O}}zkan, Savaş and Baydar, Bora and Lachinov, Dmitry and Han, Shuo and Pauli, Josef and Isensee, Fabian and Perkonigg, Matthias and Sathish, Rachana and Rajan, Ronnie and Aslan, Sinem and Sheet, Debdoot and Dovletov, Gurbandurdy and Speck, Oliver and N{\"{u}}rnberger, Andreas and Maier-Hein, Klaus H. and Akar, G{\"{o}}zde Bozdagı and {\"{U}}nal, G{\"{o}}zde and Dicle, Oguz and Selver, M. Alper},
booktitle = {arXiv},
doi = {10.5281/zenodo.3362844},
eprint = {2001.06535},
keywords = {Abdomen,CT,Challenge,MRI,Segmentation},
pages = {1--10},
publisher = {Zenodo},
title = {{CHAOS Challenge - Combined (CT-MR) healthy abdominal organ segmentation}},
url = {https://chaos.grand-challenge.org/Combined{\_}Healthy{\_}Abdominal{\_}Organ{\_}Segmentation/},
year = {2020}
}
@misc{Desai2020,
abstract = {As the COVID-19 pandemic unfolds, radiology imaging is playing an increasingly vital role in determining therapeutic options, patient management, and research directions. Publicly available data are essential to drive new research into disease etiology, early detection, and response to therapy. In response to the COVID-19 crisis, the National Cancer Institute (NCI) has extended the Cancer Imaging Archive (TCIA) to include COVID-19 related images. Rural populations are one population at risk for underrepresentation in such public repositories. We have published in TCIA a collection of radiographic and CT imaging studies for patients who tested positive for COVID-19 in the state of Arkansas. A set of clinical data describes each patient including demographics, comorbidities, selected lab data and key radiology findings. These data are cross-linked to SARS-COV-2 cDNA sequence data extracted from clinical isolates from the same population, uploaded to the GenBank repository. We believe this collection will help to address population imbalance in COVID-19 data by providing samples from this normally underrepresented population.},
author = {Desai, Shivang and Baghal, Ahmad and Wongsurawat, Thidathip and Jenjaroenpun, Piroon and Powell, Thomas and Al-Shukri, Shaymaa and Gates, Kim and Farmer, Phillip and Rutherford, Michael and Blake, Geri and Nolan, Tracy and Sexton, Kevin and Bennett, William and Smith, Kirk and Syed, Shorabuddin and Prior, Fred},
booktitle = {Scientific Data},
doi = {10.1038/s41597-020-00741-6},
issn = {20524463},
number = {1},
pmid = {33235265},
publisher = {The Cancer Imaging Archive},
title = {{Chest imaging representing a COVID-19 positive rural U.S. population}},
url = {https://wiki.cancerimagingarchive.net/x/C5IvB},
volume = {7},
year = {2020}
}
@article{Desai2020a,
abstract = {As the COVID-19 pandemic unfolds, radiology imaging is playing an increasingly vital role in determining therapeutic options, patient management, and research directions. Publicly available data are essential to drive new research into disease etiology, early detection, and response to therapy. In response to the COVID-19 crisis, the National Cancer Institute (NCI) has extended the Cancer Imaging Archive (TCIA) to include COVID-19 related images. Rural populations are one population at risk for underrepresentation in such public repositories. We have published in TCIA a collection of radiographic and CT imaging studies for patients who tested positive for COVID-19 in the state of Arkansas. A set of clinical data describes each patient including demographics, comorbidities, selected lab data and key radiology findings. These data are cross-linked to SARS-COV-2 cDNA sequence data extracted from clinical isolates from the same population, uploaded to the GenBank repository. We believe this collection will help to address population imbalance in COVID-19 data by providing samples from this normally underrepresented population.},
author = {Desai, Shivang and Baghal, Ahmad and Wongsurawat, Thidathip and Jenjaroenpun, Piroon and Powell, Thomas and Al-Shukri, Shaymaa and Gates, Kim and Farmer, Phillip and Rutherford, Michael and Blake, Geri and Nolan, Tracy and Sexton, Kevin and Bennett, William and Smith, Kirk and Syed, Shorabuddin and Prior, Fred},
doi = {10.1038/s41597-020-00741-6},
issn = {20524463},
journal = {Scientific Data},
month = {dec},
number = {1},
pages = {414},
pmid = {33235265},
title = {{Chest imaging representing a COVID-19 positive rural U.S. population}},
url = {http://www.nature.com/articles/s41597-020-00741-6},
volume = {7},
year = {2020}
}
@inproceedings{Stirenko2018,
abstract = {The results of chest X-ray (CXR) analysis of 2D images to get the statistically reliable predictions (availability of tuberculosis) by computer-aided diagnosis (CADx) on the basis of deep learning are presented. They demonstrate the efficiency of lung segmentation, lossless and lossy data augmentation for CADx of tuberculosis by deep convolutional neural network (CNN) applied to the small and not well-balanced dataset even. CNN demonstrates ability to train (despite overfitting) on the pre-processed dataset obtained after lung segmentation in contrast to the original not-segmented dataset. Lossless data augmentation of the segmented dataset leads to the lowest validation loss (without overfitting) and nearly the same accuracy (within the limits of standard deviation) in comparison to the original and other pre-processed datasets after lossy data augmentation. The additional limited lossy data augmentation results in the lower validation loss, but with a decrease of the validation accuracy. In conclusion, besides the more complex deep CNNs and bigger datasets, the better progress of CADx for the small and not well-balanced datasets even could be obtained by better segmentation, data augmentation, dataset stratification, and exclusion of non-evident outliers.},
author = {Stirenko, Sergii and Kochura, Yuriy and Alienin, Oleg and Rokovyi, Oleksandr and Gordienko, Yuri and Gang, Peng and Zeng, Wei},
booktitle = {2018 IEEE 38th International Conference on Electronics and Nanotechnology, ELNANO 2018 - Proceedings},
doi = {10.1109/ELNANO.2018.8477564},
isbn = {9781538663837},
keywords = {TensorFlow,chest X-ray,computer-aided diagnosis,convolutional neural network,data augmentation,deep learning,lung,mask,open dataset,segmentation,tuberculosis},
pages = {422--428},
title = {{Chest X-Ray Analysis of Tuberculosis by Deep Learning with Segmentation and Augmentation}},
year = {2018}
}
@inproceedings{Madani2018,
abstract = {{\textcopyright} COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only. Medical imaging datasets are limited in size due to privacy issues and the high cost of obtaining annotations. Augmentation is a widely used practice in deep learning to enrich the data in data-limited scenarios and to avoid overfitting. However, standard augmentation methods that produce new examples of data by varying lighting, field of view, and spatial rigid transformations do not capture the biological variance of medical imaging data and could result in unrealistic images. Generative adversarial networks (GANs) provide an avenue to understand the underlying structure of image data which can then be utilized to generate new realistic samples. In this work, we investigate the use of GANs for producing chest X-ray images to augment a dataset. This dataset is then used to train a convolutional neural network to classify images for cardiovascular abnormalities. We compare our augmentation strategy with traditional data augmentation and show higher accuracy for normal vs abnormal classification in chest X-rays.},
annote = {Backup Publisher: International Society for Optics and Photonics},
author = {Moradi, Mehdi and Madani, Ali and Karargyris, Alexandros and Syeda-Mahmood, Tanveer F.},
booktitle = {Medical Imaging 2018: Image Processing},
doi = {10.1117/12.2293971},
editor = {Angelini, Elsa D and Landman, Bennett A},
isbn = {9781510616370},
issn = {1605-7422},
keywords = {Convolutional networks,Generative adversarial networks,data augmentation},
pages = {57},
publisher = {SPIE},
title = {{Chest x-ray generation and data augmentation for cardiovascular abnormality classification}},
url = {https://doi.org/10.1117/12.2293971},
volume = {10574},
year = {2018}
}
@inproceedings{Wang2017a,
abstract = {The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely "ChestX-ray8", which comprises 108,948 frontalview X-ray images of 32,717 unique patients with the textmined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weaklysupervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based "reading chest X-rays" (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems.},
archivePrefix = {arXiv},
arxivId = {1705.02315},
author = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M.},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.369},
eprint = {1705.02315},
isbn = {9781538604571},
pages = {3462--3471},
title = {{ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases}},
volume = {2017-Janua},
year = {2017}
}
@article{Irvin2019,
abstract = {Large, labeled datasets have driven deep learning methods to achieve expert-level performance on a variety of medical imaging tasks. We present CheXpert, a large dataset that contains 224,316 chest radiographs of 65,240 patients. We design a labeler to automatically detect the presence of 14 observations in radiology reports, capturing uncertainties inherent in radiograph interpretation. We investigate different approaches to using the uncertainty labels for training convolutional neural networks that output the probability of these observations given the available frontal and lateral radiographs. On a validation set of 200 chest radiographic studies which were manually annotated by 3 board-certified radiologists, we find that different uncertainty approaches are useful for different pathologies. We then evaluate our best model on a test set composed of 500 chest radiographic studies annotated by a consensus of 5 board-certified radiologists, and compare the performance of our model to that of 3 additional radiologists in the detection of 5 selected pathologies. On Cardiomegaly, Edema, and Pleural Effusion, the model ROC and PR curves lie above all 3 radiologist operating points. We release the dataset to the public as a standard benchmark to evaluate performance of chest radiograph interpretation models.},
archivePrefix = {arXiv},
arxivId = {1901.07031},
author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and Ciurea-Ilcus, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and Seekins, Jayne and Mong, David A. and Halabi, Safwan S. and Sandberg, Jesse K. and Jones, Ricky and Larson, David B. and Langlotz, Curtis P. and Patel, Bhavik N. and Lungren, Matthew P. and Ng, Andrew Y.},
doi = {10.1609/aaai.v33i01.3301590},
eprint = {1901.07031},
isbn = {9781577358091},
issn = {2159-5399},
journal = {33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019},
month = {jan},
pages = {590--597},
title = {{CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison}},
url = {http://arxiv.org/abs/1901.07031},
volume = {33},
year = {2019}
}
@inproceedings{Khvostikov2017,
abstract = {Computer-aided early diagnosis of Alzheimer's Disease (AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the subject of extensive research in recent years. Some recent studies have shown promising results in the AD and MCI determination using structural and functional Magnetic Resonance Imaging (sMRI, fMRI), Positron Emission Tomography (PET) and Diffusion Tensor Imaging (DTI) modalities. This paper reviews the major trends in automatic classification methods such as feature extraction based methods as well as deep learning approaches in medical image analysis applied to the field of Alzheimer's Disease diagnostics. Different fusion methodologies to combine heterogeneous image modalities to improve classification scores are also considered.},
author = {Khvostikov, A. and Benois-Pineau, J. and Krylov, A. and Catheline, G.},
booktitle = {GraphiCon 2017 - 27th International Conference on Computer Graphics and Vision},
isbn = {9785794429633},
keywords = {Alzheimer's Disease,Convolutional neural networks,Deep learning,Fusion,Machine Learning,Medical imaging,Mild cognitive impairment,Review},
pages = {237--242},
title = {{Classification methods on different brain imaging modalities for Alzheimer disease studies}},
year = {2017}
}
@inproceedings{Wegmayr2018,
abstract = {Our ever-aging society faces the growing problem of neurodegenerative diseases, in particular dementia. Magnetic Resonance Imaging provides a unique tool for non-invasive investigation of these brain diseases. However, it is extremely difficult for neurologists to identify complex disease patterns from large amounts of three-dimensional images. In contrast, machine learning excels at automatic pattern recognition from large amounts of data. In particular, deep learning has achieved impressive results in image classification. Unfortunately, its application to medical image classification remains difficult. We consider two reasons for this difficulty: First, volumetric medical image data is considerably scarcer than natural images. Second, the complexity of 3D medical images is much higher compared to common 2D images. To address the problem of small data set size, we assemble the largest dataset ever used for training a deep 3D convolutional neural network to classify brain images as healthy (HC), mild cognitive impairment (MCI) or Alzheimers disease (AD). We use more than 20.000 images from subjects of these three classes, which is almost 9x the size of the previously largest data set. The problem of high dimensionality is addressed by using a deep 3D convolutional neural network, which is state-of-the-art in large-scale image classification. We exploit its ability to process the images directly, only with standard preprocessing, but without the need for elaborate feature engineering. Compared to other work, our workflow is considerably simpler, which increases clinical applicability. Accuracy is measured on the ADNI+AIBL data sets, and the independent CADDementia benchmark.},
author = {Wegmayr, Viktor and Aitharaju, Sai and Buhmann, Joachim},
booktitle = {Medical Imaging 2018: Computer-Aided Diagnosis},
doi = {10.1117/12.2293719},
editor = {Mori, Kensaku and Petrick, Nicholas},
isbn = {9781510616394},
issn = {1605-7422},
month = {feb},
pages = {63},
publisher = {SPIE},
title = {{Classification of brain MRI with big data and deep 3D convolutional neural networks}},
url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10575/2293719/Classification-of-brain-MRI-with-big-data-and-deep-3D/10.1117/12.2293719.full},
year = {2018}
}
@article{Campanella2019,
abstract = {The development of decision support systems for pathology and their deployment in clinical practice have been hindered by the need for large manually annotated datasets. To overcome this problem, we present a multiple instance learning-based deep learning system that uses only the reported diagnoses as labels for training, thereby avoiding expensive and time-consuming pixel-wise manual annotations. We evaluated this framework at scale on a dataset of 44,732 whole slide images from 15,187 patients without any form of data curation. Tests on prostate cancer, basal cell carcinoma and breast cancer metastases to axillary lymph nodes resulted in areas under the curve above 0.98 for all cancer types. Its clinical application would allow pathologists to exclude 65–75{\%} of slides while retaining 100{\%} sensitivity. Our results show that this system has the ability to train accurate classification models at unprecedented scale, laying the foundation for the deployment of computational decision support systems in clinical practice.},
author = {Campanella, Gabriele and Hanna, Matthew G. and Geneslaw, Luke and Miraflor, Allen and {Werneck Krauss Silva}, Vitor and Busam, Klaus J. and Brogi, Edi and Reuter, Victor E. and Klimstra, David S. and Fuchs, Thomas J.},
doi = {10.1038/s41591-019-0508-1},
issn = {1546170X},
journal = {Nature Medicine},
number = {8},
pages = {1301--1309},
pmid = {31308507},
title = {{Clinical-grade computational pathology using weakly supervised deep learning on whole slide images}},
volume = {25},
year = {2019}
}
@article{Jimenez-Del-Toro2016,
abstract = {Variations in the shape and appearance of anatomical structures in medical images are often relevant radiological signs of disease. Automatic tools can help automate parts of this manual process. A cloud-based evaluation framework is presented in this paper including results of benchmarking current state-of-the-art medical imaging algorithms for anatomical structure segmentation and landmark detection: the VISCERAL Anatomy benchmarks. The algorithms are implemented in virtual machines in the cloud where participants can only access the training data and can be run privately by the benchmark administrators to objectively compare their performance in an unseen common test set. Overall, 120 computed tomography and magnetic resonance patient volumes were manually annotated to create a standard Gold Corpus containing a total of 1295 structures and 1760 landmarks. Ten participants contributed with automatic algorithms for the organ segmentation task, and three for the landmark localization task. Different algorithms obtained the best scores in the four available imaging modalities and for subsets of anatomical structures. The annotation framework, resulting data set, evaluation setup, results and performance analysis from the three VISCERAL Anatomy benchmarks are presented in this article. Both the VISCERAL data set and Silver Corpus generated with the fusion of the participant algorithms on a larger set of non-manually-annotated medical images are available to the research community.},
author = {Jimenez-Del-Toro, Oscar and Muller, Henning and Krenn, Markus and Gruenberg, Katharina and Taha, Abdel Aziz and Winterstein, Marianne and Eggel, Ivan and Foncubierta-Rodriguez, Antonio and Goksel, Orcun and Jakab, Andras and Kontokotsios, Georgios and Langs, Georg and Menze, Bjoern H. and {Salas Fernandez}, Tomas and Schaer, Roger and Walleyo, Anna and Weber, Marc Andre and {Dicente Cid}, Yashin and Gass, Tobias and Heinrich, Mattias and Jia, Fucang and Kahl, Fredrik and Kechichian, Razmig and Mai, Dominic and Spanier, Assaf B. and Vincent, Graham and Wang, Chunliang and Wyeth, Daniel and Hanbury, Allan},
doi = {10.1109/TMI.2016.2578680},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Evaluation framework,landmark detection,organ segmentation},
month = {nov},
number = {11},
pages = {2459--2475},
pmid = {27305669},
title = {{Cloud-Based Evaluation of Anatomical Structure Segmentation and Landmark Detection Algorithms: VISCERAL Anatomy Benchmarks}},
url = {http://ieeexplore.ieee.org/document/7488206/},
volume = {35},
year = {2016}
}
@misc{Zhang2020,
author = {Zhang, S and Yoshida, W and Mano, H and Yanagisawa, T and Shibata, K and Kawato, M and Seymour, B},
doi = {10.18112/OPENNEURO.DS002596.V1.0.0},
publisher = {Openneuro},
title = {{Cognitive control of sensory pain encoding in the pregenual anterior cingulate cortex. d1 - decoder construction in day 1, d2 - adaptive control in day 2.}},
url = {https://openneuro.org/datasets/ds002596/versions/1.0.0},
year = {2020}
}
@article{Kumar2018,
abstract = {The analysis of multi-modality positron emission tomography and computed tomography (PET-CT) images for computer-aided diagnosis applications (e.g., detection and segmentation) requires combining the sensitivity of PET to detect abnormal regions with anatomical localization from CT. Current methods for PET-CT image analysis either process the modalities separately or fuse information from each modality based on knowledge about the image analysis task. These methods generally do not consider the spatially varying visual characteristics that encode different information across different modalities, which have different priorities at different locations. For example, a high abnormal PET uptake in the lungs is more meaningful for tumor detection than physiological PET uptake in the heart. Our aim is to improve the fusion of the complementary information in multi-modality PET-CT with a new supervised convolutional neural network (CNN) that learns to fuse complementary information for multi-modality medical image analysis. Our CNN first encodes modality-specific features and then uses them to derive a spatially varying fusion map that quantifies the relative importance of each modality's feature across different spatial locations. These fusion maps are then multiplied with the modality-specific feature maps to obtain a representation of the complementary multi-modality information at different locations, which can then be used for image analysis. We evaluated the ability of our CNN to detect and segment multiple regions (lungs, mediastinum, and tumors) with different fusion requirements using a dataset of PET-CT images of lung cancer. We compared our method to baseline techniques for multi-modality image fusion (fused inputs (FSs), multi-branch (MB) techniques, and multi-channel (MC) techniques) and segmentation. Our findings show that our CNN had a significantly higher foreground detection accuracy (99.29{\%}, p {\textless} {\{}0.05) than the fusion baselines (FS: 99.00{\%}, MB: 99.08{\%}, and TC: 98.92{\%}) and a significantly higher Dice score (63.85{\%}) than the recent PET-CT tumor segmentation methods.}},
annote = {{\_}eprint: 1810.02492},
archivePrefix = {arXiv},
arxivId = {1810.02492},
author = {Kumar, Ashnil and Fulham, Michael and Feng, Dagan and Kim, Jinman},
doi = {10.1109/TMI.2019.2923601},
eprint = {1810.02492},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Multi-modality imaging,PET-CT,deep learning,fusion learning},
number = {1},
pages = {204--217},
pmid = {31217099},
title = {{Co-Learning Feature Fusion Maps from PET-CT Images of Lung Cancer}},
volume = {39},
year = {2020}
}
@incollection{Ebner2020,
address = {Cham},
author = {Ebner, Marc},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_454-1},
pages = {1--9},
publisher = {Springer International Publishing},
title = {{Color Constancy}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}454-1},
year = {2020}
}
@article{Reinhard2001,
author = {Reinhard, Erik and Ashikhmin, Michael and Gooch, Bruce and Shirley, Peter},
doi = {10.1109/38.946629},
issn = {02721716},
journal = {IEEE Computer Graphics and Applications},
number = {5},
pages = {34--41},
title = {{Color transfer between images}},
volume = {21},
year = {2001}
}
@article{Magee2009,
abstract = {Abstract. Colour consistency in light microscopy based histology is an increasingly important problem with the advent of Gigapixel digital slide scanners and automatic image analysis. This paper presents an evaluation of two novel colour normalisation approaches against the previously utilised method of linear normalisation in l$\alpha$$\beta$ colourspace. These approaches map the colour distribution of an over/under stained image to that of a well stained target image. The first novel approach presented is a multi-modal extension to linear normalisation in l$\alpha$$\beta$ colourspace using an automatic image segmentation method and defining separate transforms for each class. The second approach normalises in a representation space obtained using stain specific colour deconvolution. Additionally, we present a method for estimation of the required colour deconvolution vectors directly from the image data. Our evaluation demonstrates the inherent variability in the original data, the known theoretical problems with linear normalisation in l$\alpha$$\beta$ colourspace, and that a multi-modal colour deconvolution based approach overcomes these problems. The segmentation based approach, while producing good results on the majority of images, is less successful than the colour deconvolution method for a significant minority of images as robust segmentation is required to avoid introducing artifacts.},
author = {Magee, Derek and Treanor, Darren and Crellin, Doreen and Shires, Michael and Smith, Katherine and Mohee, Kevin and Quirke, Philip},
journal = {Optical Tissue Image analysis in Microscopy, Histopathology and Endoscopy (MICCAI Workshop)},
pages = {100--111},
title = {{Colour Normalisation in Digital Histopathology Images}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.157.5405},
year = {2009}
}
@article{BrendanMcMahan2017,
abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10–100x as compared to synchronized stochastic gradient descent.},
archivePrefix = {arXiv},
arxivId = {1602.05629},
author = {{Brendan McMahan}, H. and Moore, Eider and Ramage, Daniel and Hampson, Seth and {Ag{\"{u}}era y Arcas}, Blaise},
eprint = {1602.05629},
journal = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017},
month = {feb},
title = {{Communication-efficient learning of deep networks from decentralized data}},
url = {http://arxiv.org/abs/1602.05629},
year = {2017}
}
@article{VanGinneken2010,
abstract = {Numerous publications and commercial systems are available that deal with automatic detection of pulmonary nodules in thoracic computed tomography scans, but a comparative study where many systems are applied to the same data set has not yet been performed. This paper introduces ANODE09 ( http://anode09.isi.uu.nl), a database of 55 scans from a lung cancer screening program and a web-based framework for objective evaluation of nodule detection algorithms. Any team can upload results to facilitate benchmarking. The performance of six algorithms for which results are available are compared; five from academic groups and one commercially available system. A method to combine the output of multiple systems is proposed. Results show a substantial performance difference between algorithms, and demonstrate that combining the output of algorithms leads to marked performance improvements. {\textcopyright} 2010 Elsevier B.V.},
author = {van Ginneken, Bram and Armato, Samuel G. and de Hoop, Bartjan and {van Amelsvoort-van de Vorst}, Saskia and Duindam, Thomas and Niemeijer, Meindert and Murphy, Keelin and Schilham, Arnold and Retico, Alessandra and Fantacci, Maria Evelina and Camarlinghi, Niccol and Bagagli, Francesco and Gori, Ilaria and Hara, Takeshi and Fujita, Hiroshi and Gargano, Gianfranco and Bellotti, Roberto and Tangaro, Sabina and Bolaos, Lourdes and Carlo, Francesco De and Cerello, Piergiorgio and {Cristian Cheran}, Sorin and {Lopez Torres}, Ernesto and Prokop, Mathias},
doi = {10.1016/j.media.2010.05.005},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Computed tomography,Computer-aided detection,Lung cancer,Lung nodules},
number = {6},
pages = {707--722},
pmid = {20573538},
title = {{Comparing and combining algorithms for computer-aided detection of pulmonary nodules in computed tomography scans: The ANODE09 study}},
volume = {14},
year = {2010}
}
@article{Heimann2009,
abstract = {This paper presents a comparison study between 10 automatic and six interactive methods for liver segmentation from contrast-enhanced CT images. It is based on results from the "MICCAI 2007 Grand Challenge" workshop, where 16 teams evaluated their algorithms on a common database. A collection of 20 clinical images with reference segmentations was provided to train and tune algorithms in advance. Participants were also allowed to use additional proprietary training data for that purpose. All teams then had to apply their methods to 10 test datasets and submit the obtained results. Employed algorithms include statistical shape models, atlas registration, level-sets, graph-cuts and rule-based systems. All results were compared to reference segmentations five error measures that highlight different aspects of segmentation accuracy. All measures were combined according to a specific scoring system relating the obtained values to human expert variability. In general, interactive methods reached higher average scores than automatic approaches and featured a better consistency of segmentation quality. However, the best automatic methods (mainly based on statistical shape models with some additional free deformation) could compete well on the majority of test images. The study provides an insight in performance of different segmentation approaches under real-world conditions and highlights achievements and limitations of current image analysis techniques. {\textcopyright} 2009 IEEE.},
author = {Heimann, Tobias and {Van Ginneken}, Brain and Styner, Martin A. and Arzhaeva, Yulia and Aurich, Volker and Bauer, Christian and Beck, Andreas and Becker, Christoph and Beichel, Reinhard and Bekes, Gy{\"{o}}rgy and Bello, Fernando and Binnig, Gerd and Bischof, Horst and Bornik, Alexander and Cashman, Peter M.M. and Chi, Ying and C{\'{o}}rdova, Andr{\'{e}}s and Dawant, Benoit M. and Fidrich, M{\'{a}}rta and Furst, Jacob D. and Furukawa, Daisuke and Grenacher, Lars and Hornegger, Joachim and Kainm{\"{u}}ller, Dagmar and Kitney, Richard I. and Kobatake, Hidefumi and Lamecker, Hans and Lange, Thomas and Lee, Jeongjin and Lennon, Brian and Li, Rui and Li, Senhu and Meinzer, Hans Peter and N{\'{e}}meth, G{\'{a}}bor and Raicu, Daniela S. and Rau, Anne Mareike and {Van Rikxoort}, Eva M. and Rousson, Mika{\"{e}}l and Rusk{\'{o}}, L{\'{a}}szlo and Saddi, Kinda A. and Schmidt, G{\"{u}}nter and Seghers, Dieter and Shimizu, Akinobu and Slagmolen, Pieter and Sorantin, Erich and Soza, Grzegorz and Susomboon, Ruchaneewan and Waite, Jonathan M. and Wimmer, Andreas and Wolf, Ivo},
doi = {10.1109/TMI.2009.2013851},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Evaluation,Liver,Segmentation},
number = {8},
pages = {1251--1265},
pmid = {19211338},
title = {{Comparison and evaluation of methods for liver segmentation from CT datasets}},
volume = {28},
year = {2009}
}
@article{Kavur2020a,
abstract = {PURPOSE We aimed to compare the accuracy and repeatability of emerging machine learning-based (i.e., deep learning) automatic segmentation algorithms with those of well-established interactive semi-automatic methods for determining liver volume in living liver transplant donors at computed tomography (CT) imaging. METHODS A total of 12 methods (6 semi-automatic, 6 full-automatic) were evaluated. The semi-automatic segmentation algorithms were based on both traditional iterative models including watershed, fast marching, region growing, active contours and modern techniques including robust statistics segmenter and super-pixels. These methods entailed some sort of interaction mechanism such as placing initialization seeds on images or determining a parameter range. The automatic methods were based on deep learning and included three framework templates (DeepMedic, NiftyNet and U-Net), the first two of which were applied with default parameter sets and the last two involved adapted novel model designs. For 20 living donors (8 training and 12 test data-sets), a group of imaging scientists and radiologists created ground truths by performing manual segmentations on contrast-enhanced CT images. Each segmentation was evaluated using five metrics (i.e., volume overlap and relative volume errors, average/root-mean-square/maximum symmetrical surface distances). The results were mapped to a scoring system and a final grade was calculated by taking their average. Accuracy and repeatability were evaluated using slice-by-slice comparisons and volumetric analysis. Diversity and complementarity were observed through heatmaps. Majority voting (MV) and simultaneous truth and performance level estimation (STAPLE) algorithms were utilized to obtain the fusion of the individual results. RESULTS The top four methods were automatic deep learning models, with scores of 79.63, 79.46, 77.15, and 74.50. Intra-user score was determined as 95.14. Overall, automatic deep learning segmentation outperformed interactive techniques on all metrics. The mean volume of liver of ground truth was 1409.93±271.28 mL, while it was calculated as 1342.21±231.24 mL using automatic and 1201.26±258.13 mL using interactive methods, showing higher accuracy and less variation with automatic methods. The qualitative analysis of segmentation results showed significant diversity and complementarity, enabling the idea of using ensembles to obtain superior results. The fusion score of automatic methods reached 83.87 with MV and 86.20 with STAPLE, which were only slightly less than fusion of all methods (MV, 86.70) and (STAPLE, 88.74). CONCLUSION Use of the new deep learning-based automatic segmentation algorithms substantially increases the accuracy and repeatability for segmentation and volumetric measurements of liver. Fusion of automatic methods based on ensemble approaches exhibits best results with almost no additional time cost due to potential parallel execution of multiple models.},
author = {Kavur, A. Emre and Gezer, Naciye Sinem and Barış, Mustafa and Şahin, Yusuf and {\"{O}}zkan, Savaş and Baydar, Bora and Y{\"{u}}ksel, Ulaş and Kılık{\c{c}}ıer, {\c{C}}ağlar and Olut, Şahin and Akar, G{\"{o}}zde Bozdağı and {\"{U}}nal, G{\"{o}}zde and Dicle, Oğuz and Selver, M. Alper},
doi = {10.5152/dir.2019.19025},
issn = {13053612},
journal = {Diagnostic and Interventional Radiology},
month = {jan},
number = {1},
pages = {11--21},
pmid = {31904568},
title = {{Comparison of semi-automatic and deep learning-based automatic methods for liver segmentation in living liver transplant donors}},
url = {https://www.dirjournal.org/en/comparison-of-semi-automatic-and-deep-learning-based-automatic-methods-for-liver-segmentation-in-living-liver-transplant-donors-132076},
volume = {26},
year = {2020}
}
@article{Chong2020,
abstract = {We sequenced four severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) genomes from Malaysia during the second wave of infection and found unique mutations which suggest local evolution. Circulating Malaysian strains represent introductions from different countries, particularly during the first wave of infection. Genome sequencing is important for understanding local epidemiology.We sequenced four severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) genomes from Malaysia during the second wave of infection and found unique mutations which suggest local evolution. Circulating Malaysian strains represent introductions from different countries, particularly during the first wave of infection. Genome sequencing is important for understanding local epidemiology.},
author = {Chong, Yoong Min and Sam, I-Ching and Ponnampalavanar, Sasheela and {Syed Omar}, Sharifah Faridah and Kamarulzaman, Adeeba and Munusamy, Vijayan and Wong, Chee Kuan and Jamaluddin, Fadhil Hadi and Gan, Han Ming and Chong, Jennifer and Teh, Cindy Shuan Ju and Chan, Yoke Fun},
doi = {10.1128/mra.00383-20},
editor = {Roux, Simon},
issn = {2576-098X},
journal = {Microbiology Resource Announcements},
month = {may},
number = {20},
title = {{Complete Genome Sequences of SARS-CoV-2 Strains Detected in Malaysia}},
url = {https://mra.asm.org/content/9/20/e00383-20},
volume = {9},
year = {2020}
}
@article{Ger2018,
abstract = {Radiomics has shown promise in improving models for predicting patient outcomes. However, to maximize the information gain of the radiomics features, especially in larger patient cohorts, the variability in radiomics features owing to differences between scanners and scanning protocols must be accounted for. To this aim, the imaging variability of radiomics feature values was evaluated on 100 computed tomography scanners at 35 clinics by imaging a radiomics phantom using a controlled protocol and the commonly used chest and head protocols of the local clinic. We used a linear mixed-effects model to determine the degree to which the manufacturer and individual scanners contribute to the overall variability. Using a controlled protocol reduced the overall variability by 57{\%} and 52{\%} compared to the local chest and head protocols respectively. The controlled protocol also reduced the relative contribution of the manufacturer to the total variability. For almost all variabilities (manufacturer, scanner, and residual with different preprocesssing), the controlled protocol scans had a significantly smaller variability than the local protocol scans did. For most radiomics features, the imaging variability was small relative to the inter-patient feature variability in non–small cell lung cancer and head and neck squamous cell carcinoma patient cohorts. From this study, we conclude that using controlled scans can reduce the variability in radiomics features, and our results demonstrate the importance of using controlled protocols in prospective radiomics studies.},
author = {Ger, Rachel B. and Zhou, Shouhao and Chi, Pai Chun Melinda and Lee, Hannah J. and Layman, Rick R. and Jones, A. Kyle and Goff, David L. and Fuller, Clifton D. and Howell, Rebecca M. and Li, Heng and Stafford, R. Jason and Court, Laurence E. and Mackin, Dennis S.},
doi = {10.1038/s41598-018-31509-z},
issn = {20452322},
journal = {Scientific Reports},
month = {dec},
number = {1},
pages = {13047},
pmid = {30158540},
title = {{Comprehensive Investigation on Controlling for CT Imaging Variabilities in Radiomics Studies}},
url = {http://www.nature.com/articles/s41598-018-31509-z},
volume = {8},
year = {2018}
}
@incollection{Sankaranarayanan2020,
address = {Cham},
author = {Sankaranarayanan, Aswin C. and Baraniuk, Richard G.},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_647-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Compressive Sensing}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}647-1},
year = {2020}
}
@inproceedings{Arunachalam2017,
abstract = {Osteosarcoma is one of the most common types of bone cancer in children. To gauge the extent of cancer treatment response in the patient after surgical resection, the H{\&}E stained image slides are manually evaluated by pathologists to estimate the percentage of necrosis, a time consuming process prone to observer bias and inaccuracy. Digital image analysis is a potential method to automate this process, thus saving time and providing a more accurate evaluation. The slides are scanned in Aperio Scanscope, converted to digital Whole Slide Images (WSIs) and stored in SVS format. These are high resolution images, of the order of 10 9 pixels, allowing up to 40X magnification factor. This paper proposes an image segmentation and analysis technique for segmenting tumor and non-tumor regions in histopathological WSIs of osteosarcoma datasets. Our approach is a combination of pixel-based and object-based methods which utilize tumor properties such as nuclei cluster, density, and circularity to classify tumor regions as viable and non-viable. A K-Means clustering technique is used for tumor isolation using color normalization, followed by multi-threshold Otsu segmentation technique to further classify tumor region as viable and non-viable. Then a Flood-fill algorithm is applied to cluster similar pixels into cellular objects and compute cluster data for further analysis of regions under study. To the best of our knowledge this is the first comprehensive solution that is able to produce such a classification for Osteosarcoma cancer. The results are very conclusive in identifying viable and non-viable tumor regions. In our experiments, the accuracy of the discussed approach is 100{\%} in viable tumor and coagulative necrosis identification while it is around 90{\%} for fibrosis and acellular/hypocellular tumor osteoid, for all the sampled datasets used. We expect the developed software to lead to a significant increase in accuracy and decrease in inter-observer variability in assessment of necrosis by the pathologists and a reduction in the time spent by the pathologists in such assessments.},
author = {Arunachalam, Harish Babu and Mishra, Rashika and Armaselu, Bogdan and Daescu, Ovidiu and Martinez, Maria and Leavey, Patrick and Rakheja, Dinesh and Cederberg, Kevin and Sengupta, Anita and Ni'Suilleabhain, Molly},
booktitle = {Pacific Symposium on Biocomputing},
doi = {10.1142/9789813207813_0020},
issn = {23356936},
keywords = {Image segmentation,Osteosarcoma,Otsu thresholding,SVS image analysis},
number = {212679},
pages = {195--206},
pmid = {27896975},
title = {{Computer aided image segmentation and classification for viable and non-viable tumor identification in osteosarcoma}},
volume = {0},
year = {2017}
}
@inproceedings{Sun2016,
abstract = {Deep learning is considered as a popular and powerful method in pattern recognition and classification. However, there are not many deep structured applications used in medical imaging diagnosis area, because large dataset is not always available for medical images. In this study we tested the feasibility of using deep learning algorithms for lung cancer diagnosis with the cases from Lung Image Database Consortium (LIDC) database. The nodules on each computed tomography (CT) slice were segmented according to marks provided by the radiologists. After down sampling and rotating we acquired 174412 samples with 52 by 52 pixel each and the corresponding truth files. Three deep learning algorithms were designed and implemented, including Convolutional Neural Network (CNN), Deep Belief Networks (DBNs), Stacked Denoising Autoencoder (SDAE). To compare the performance of deep learning algorithms with traditional computer aided diagnosis (CADx) system, we designed a scheme with 28 image features and support vector machine. The accuracies of CNN, DBNs, and SDAE are 0.7976, 0.8119, and 0.7929, respectively; the accuracy of our designed traditional CADx is 0.7940, which is slightly lower than CNN and DBNs. We also noticed that the mislabeled nodules using DBNs are 4{\%} larger than using traditional CADx, this might be resulting from down sampling process lost some size information of the nodules.},
author = {Sun, Wenqing and Zheng, Bin and Qian, Wei},
booktitle = {Medical Imaging 2016: Computer-Aided Diagnosis},
doi = {10.1117/12.2216307},
isbn = {9781510600201},
issn = {0277-786X},
pages = {97850Z},
title = {{Computer aided lung cancer diagnosis with deep learning algorithms}},
volume = {9785},
year = {2016}
}
@article{Litjens2014b,
abstract = {Prostate cancer is one of the major causes of cancer death for men in the western world. Magnetic resonance imaging (MRI) is being increasingly used as a modality to detect prostate cancer. Therefore, computer-aided detection of prostate cancer in MRI images has become an active area of research. In this paper we investigate a fully automated computer-aided detection system which consists of two stages. In the first stage, we detect initial candidates using multi-atlas-based prostate segmentation, voxel feature extraction, classification and local maxima detection. The second stage segments the candidate regions and using classification we obtain cancer likelihoods for each candidate. Features represent pharmacokinetic behavior, symmetry and appearance, among others. The system is evaluated on a large consecutive cohort of 347 patients with MR-guided biopsy as the reference standard. This set contained 165 patients with cancer and 182 patients without prostate cancer. Performance evaluation is based on lesion-based free-response receiver operating characteristic curve and patient-based receiver operating characteristic analysis. The system is also compared to the prospective clinical performance of radiologists. Results show a sensitivity of 0.42, 0.75, and 0.89 at 0.1, 1, and 10 false positives per normal case. In clinical workflow the system could potentially be used to improve the sensitivity of the radiologist. At the high specificity reading setting, which is typical in screening situations, the system does not perform significantly different from the radiologist and could be used as an independent second reader instead of a second radiologist. Furthermore, the system has potential in a first-reader setting. {\textcopyright} 2014 IEEE.},
author = {Litjens, Geert and Debats, Oscar and Barentsz, Jelle and Karssemeijer, Nico and Huisman, Henkjan},
doi = {10.1109/TMI.2014.2303821},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Computer-aided detection,image analysis,machine learning,magnetic resonance imaging,prostate cancer},
month = {may},
number = {5},
pages = {1083--1092},
title = {{Computer-aided detection of prostate cancer in MRI}},
url = {http://ieeexplore.ieee.org/document/6729091/},
volume = {33},
year = {2014}
}
@article{Masood2018ComputerAssistedDS,
abstract = {Pulmonary cancer is considered as one of the major causes of death worldwide. For the detection of lung cancer, computer-assisted diagnosis (CADx) systems have been designed. Internet-of-Things (IoT) has enabled ubiquitous internet access to biomedical datasets and techniques; in result, the progress in CADx is significant. Unlike the conventional CADx, deep learning techniques have the basic advantage of an automatic exploitation feature as they have the ability to learn mid and high level image representations. We proposed a Computer-Assisted Decision Support System in Pulmonary Cancer by using the novel deep learning based model and metastasis information obtained from MBAN (Medical Body Area Network). The proposed model, DFCNet, is based on the deep fully convolutional neural network (FCNN) which is used for classification of each detected pulmonary nodule into four lung cancer stages. The performance of proposed work is evaluated on different datasets with varying scan conditions. Comparison of proposed classifier is done with the existing CNN techniques. Overall accuracy of CNN and DFCNet was 77.6{\%} and 84.58{\%}, respectively. Experimental results illustrate the effectiveness of proposed method for the detection and classification of lung cancer nodules. These results demonstrate the potential for the proposed technique in helping the radiologists in improving nodule detection accuracy with efficiency.},
author = {Masood, Anum and Sheng, Bin and Li, Ping and Hou, Xuhong and Wei, Xiaoer and Qin, Jing and Feng, Dagan},
doi = {10.1016/j.jbi.2018.01.005},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Convolutional neural networks (CNN),Deep learning,Lung cancer stages,MBAN (Medical Body Area Network),Nodule detection,mIoT (medical Internet of Things)},
month = {mar},
pages = {117--128},
pmid = {29366586},
title = {{Computer-Assisted Decision Support System in Pulmonary Cancer detection and stage classification on CT images}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046418300078},
volume = {79},
year = {2018}
}
@inproceedings{scsenet,
abstract = {Fully convolutional neural networks (F-CNNs) have set the state-of-the-art in image segmentation for a plethora of applications. Architectural innovations within F-CNNs have mainly focused on improving spatial encoding or network connectivity to aid gradient flow. In this paper, we explore an alternate direction of recalibrating the feature maps adaptively, to boost meaningful features, while suppressing weak ones. We draw inspiration from the recently proposed squeeze {\&} excitation (SE) module for channel recalibration of feature maps for image classification. Towards this end, we introduce three variants of SE modules for image segmentation, (i) squeezing spatially and exciting channel-wise (cSE), (ii) squeezing channel-wise and exciting spatially (sSE) and (iii) concurrent spatial and channel squeeze {\&} excitation (scSE). We effectively incorporate these SE modules within three different state-of-the-art F-CNNs (DenseNet, SD-Net, U-Net) and observe consistent improvement of performance across all architectures, while minimally effecting model complexity. Evaluations are performed on two challenging applications: whole brain segmentation on MRI scans and organ segmentation on whole body contrast enhanced CT scans.},
archivePrefix = {arXiv},
arxivId = {1803.02579},
author = {Roy, Abhijit Guha and Navab, Nassir and Wachinger, Christian},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-00928-1_48},
eprint = {1803.02579},
isbn = {9783030009274},
issn = {16113349},
pages = {421--429},
title = {{Concurrent Spatial and Channel ‘Squeeze {\&} Excitation' in Fully Convolutional Networks}},
url = {http://link.springer.com/10.1007/978-3-030-00928-1{\_}48},
volume = {11070 LNCS},
year = {2018}
}
@inproceedings{Lillicrap2016,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies “end-to-end”: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
booktitle = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
eprint = {1509.02971},
title = {{Continuous control with deep reinforcement learning}},
year = {2016}
}
@article{Oh2016,
abstract = {In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world). We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures. These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks. While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures. Additionally, we evaluate the generalization performance of the architectures on environments not used during training. The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.},
archivePrefix = {arXiv},
arxivId = {1605.09128},
author = {Oh, Junhyuk and Chockalingam, Valliappa and Singh, Satinder and Lee, Honglak},
eprint = {1605.09128},
isbn = {9781510829008},
journal = {33rd International Conference on Machine Learning, ICML 2016},
month = {may},
pages = {4067--4089},
title = {{Control of memory, active perception, and action in minecraft}},
url = {http://arxiv.org/abs/1605.09128},
volume = {6},
year = {2016}
}
@inproceedings{Mishra2018,
abstract = {Pathologists often deal with high complexity and sometimes disagreement over osteosarcoma tumor classification due to cellular heterogeneity in the dataset. Segmentation and classification of histology tissue in H{\&}E stained tumor image datasets is a challenging task because of intra-class variations, inter-class similarity, crowded context, and noisy data. In recent years, deep learning approaches have led to encouraging results in breast cancer and prostate cancer analysis. In this article, we propose convolutional neural network (CNN) as a tool to improve efficiency and accuracy of osteosarcoma tumor classification into tumor classes (viable tumor, necrosis) versus nontumor. The proposed CNN architecture contains eight learned layers: three sets of stacked two convolutional layers interspersed with max pooling layers for feature extraction and two fully connected layers with data augmentation strategies to boost performance. The use of a neural network results in higher accuracy of average 92{\%} for the classification. We compare the proposed architecture with three existing and proven CNN architectures for image classification: AlexNet, LeNet, and VGGNet. We also provide a pipeline to calculate percentage necrosis in a given whole slide image. We conclude that the use of neural networks can assure both high accuracy and efficiency in osteosarcoma classification.},
author = {Mishra, Rashika and Daescu, Ovidiu and Leavey, Patrick and Rakheja, Dinesh and Sengupta, Anita},
booktitle = {Journal of Computational Biology},
doi = {10.1089/cmb.2017.0153},
issn = {10665277},
keywords = {convolutional neural network,histology image analysis,osteosarcoma},
number = {3},
pages = {313--325},
pmid = {29083930},
title = {{Convolutional neural network for histopathological analysis of osteosarcoma}},
volume = {25},
year = {2018}
}
@inproceedings{Mou2014,
abstract = {Programming language processing (similar to natural language processing) is a hot research topic in the field of software engineering; it has also aroused growing interest in the artificial intelligence community. However, different from a natural language sentence, a program contains rich, explicit, and complicated structural information. Hence, traditional NLP models may be inappropriate for programs. In this paper, we propose a novel tree-based convolutional neural network (TBCNN) for programming language processing, in which a convolution kernel is designed over programs' abstract syntax trees to capture structural information. TBCNN is a generic architecture for programming language processing; our experiments show its effectiveness in two different program analysis tasks: classifying programs according to functionality, and detecting code snippets of certain patterns. TBCNN outperforms baseline methods, including several neural models for NLP.},
annote = {{\_}eprint: 1409.5718},
archivePrefix = {arXiv},
arxivId = {1409.5718},
author = {Mou, Lili and Li, Ge and Zhang, Lu and Wang, Tao and Jin, Zhi},
booktitle = {30th AAAI Conference on Artificial Intelligence, AAAI 2016},
eprint = {1409.5718},
isbn = {9781577357605},
pages = {1287--1293},
title = {{Convolutional neural networks over tree structures for programming language processing}},
url = {http://arxiv.org/abs/1409.5718},
volume = {abs/1409.5},
year = {2016}
}
@article{Tai2016,
abstract = {Large CNNs have delivered impressive performance in various computer vision applications. But the storage and computation requirements make it problematic for deploying these models on mobile devices. Recently, tensor decompositions have been used for speeding up CNNs. In this paper, we further develop the tensor decomposition technique. We propose a new algorithm for computing the low-rank tensor decomposition for removing the redundancy in the convolution kernels. The algorithm finds the exact global optimizer of the decomposition and is more effective than iterative methods. Based on the decomposition, we further propose a new method for training low-rank constrained CNNs from scratch. Interestingly, while achieving a significant speedup, sometimes the low-rank constrained CNNs delivers significantly better performance than their non-constrained counterparts. On the CIFAR-10 dataset, the proposed low-rank NIN model achieves 91.31{\%} accuracy (without data augmentation), which also improves upon state-of-the-art result. We evaluated the proposed method on CIFAR-10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet, NIN, VGG and GoogleNet with success. For example, the forward time of VGG-16 is reduced by half while the performance is still comparable. Empirical success suggests that low-rank tensor decompositions can be a very useful tool for speeding up large CNNs.},
archivePrefix = {arXiv},
arxivId = {1511.06067},
author = {Tai, Cheng and Xiao, Tong and Zhang, Yi and Wang, Xiaogang and Weinan, E.},
eprint = {1511.06067},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
title = {{Convolutional neural networks with low-rank regularization}},
year = {2016}
}
@article{Wang2020b,
abstract = {The COVID-19 Open Research Dataset (CORD-19) is a growing1 resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development of text mining and information retrieval systems over its rich collection of metadata and structured full text papers. Since its release, CORD-19 has been downloaded over 75K times and has served as the basis of many COVID-19 text mining and discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and preview tools and upcoming shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.},
archivePrefix = {arXiv},
arxivId = {2004.10706},
author = {Wang, Lucy Lu and Lo, Kyle and Chandrasekhar, Yoganand and Reas, Russell and Yang, Jiangjiang and Eide, Darrin and Funk, Kathryn and Kinney, Rodney and Liu, Ziyang and Merrill, William and Mooney, Paul and Murdick, Dewey and Rishi, Devvret and Sheehan, Jerry and Shen, Zhihong and Stilson, Brandon and Wade, Alex D. and Wang, Kuansan and Wilhelm, Chris and Xie, Boya and Raymond, Douglas and Weld, Daniel S. and Etzioni, Oren and Kohlmeier, Sebastian},
eprint = {2004.10706},
issn = {2331-8422},
journal = {arXiv},
month = {apr},
pmid = {32510522},
title = {{CORD-19: The COVID-19 open research dataset}},
url = {http://arxiv.org/abs/2004.10706},
year = {2020}
}
@article{Rusu2017,
abstract = {Objective: To develop an approach for radiology-pathology fusion of ex vivo histology of surgically excised pulmonary nodules with pre-operative CT, to radiologically map spatial extent of the invasive adenocarcinomatous component of the nodule. Methods: Six subjects (age: 75 ± 11 years) with pre-operative CT and surgically excised ground-glass nodules (size: 22.5 ± 5.1 mm) with a significant invasive adenocarcinomatous component ({\textgreater}5 mm) were included. The pathologist outlined disease extent on digitized histology specimens; two radiologists and a pulmonary critical care physician delineated the entire nodule on CT (in-plane resolution: {\textless}0.8 mm, inter-slice distance: 1–5 mm). We introduced a novel reconstruction approach to localize histology slices in 3D relative to each other while using CT scan as spatial constraint. This enabled the spatial mapping of the extent of tumour invasion from histology onto CT. Results: Good overlap of the 3D reconstructed histology and the nodule outlined on CT was observed (65.9 ± 5.2{\%}). Reduction in 3D misalignment of corresponding anatomical landmarks on histology and CT was observed (1.97 ± 0.42 mm). Moreover, the CT attenuation (HU) distributions were different when comparing invasive and in situ regions. Conclusion: This proof-of-concept study suggests that our fusion method can enable the spatial mapping of the invasive adenocarcinomatous component from 2D histology slices onto in vivo CT. Key Points: • 3D reconstructions are generated from 2D histology specimens of ground glass nodules. • The reconstruction methodology used pre-operative in vivo CT as 3D spatial constraint. • The methodology maps adenocarcinoma extent from digitized histology onto in vivo CT. • The methodology potentially facilitates the discovery of CT signature of invasive adenocarcinoma.},
author = {Rusu, Mirabela and Rajiah, Prabhakar and Gilkeson, Robert and Yang, Michael and Donatelli, Christopher and Thawani, Rajat and Jacono, Frank J. and Linden, Philip and Madabhushi, Anant},
doi = {10.1007/s00330-017-4813-0},
issn = {14321084},
journal = {European Radiology},
keywords = {Computed tomography,Computer-assisted image processing,Lung adenocarcinoma,Multimodal imaging,Pathology},
number = {10},
pages = {4209--4217},
pmid = {28386717},
title = {{Co-registration of pre-operative CT with ex vivo surgically excised ground glass nodules to define spatial extent of invasive adenocarcinoma on in vivo imaging: a proof-of-concept study}},
url = {https://doi.org/10.1007/s00330-017-4813-0},
volume = {27},
year = {2017}
}
@book{Raposo2020,
address = {Cham},
author = {Raposo, Maria and Ribeiro, Paulo and S{\'{e}}rio, Susana and Staiano, Antonino and Ciaramella, Angelo},
doi = {10.1007/978-3-030-34585-3_31},
editor = {Raposo, Maria and Ribeiro, Paulo and S{\'{e}}rio, Susana and Staiano, Antonino and Ciaramella, Angelo},
isbn = {978-3-030-34584-6},
pages = {C1--C1},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Correction to: Computational Intelligence Methods for Bioinformatics and Biostatistics}},
url = {http://link.springer.com/10.1007/978-3-030-34585-3},
volume = {11925},
year = {2020}
}
@article{Zhao2020,
abstract = {During the outbreak time of COVID-19, computed tomography (CT) is a useful manner for diagnosing COVID-19 patients. To mitigate the lack of publicly available COVID-19 CT images for developing CT-based diagnosis deep learning models of COVID-19, we build an open-sourced dataset COVID-CT, which contains 349 COVID-19 CT images from 216 patients and 463 non-COVID-19 CTs. The utility of this dataset is confirmed by a senior radiologist and via experimental studies. Using this dataset, we develop a joint classification and segmentation method that achieves an F1 of 0.85, an AUC of 0.95, and an accuracy of 0.83. The data and code are available at https://github.com/UCSD-AI4H/COVID-CT.},
archivePrefix = {arXiv},
arxivId = {2003.13865},
author = {Zhao, Jinyu and He, Xuehai and Yang, Xingyi and Zhang, Yichen and Zhang, Shanghang and Xie, Pengtao},
eprint = {2003.13865},
journal = {arXiv},
month = {mar},
title = {{COVID-CT-Dataset: A CT image dataset about COVID-19}},
url = {http://arxiv.org/abs/2003.13865},
year = {2020}
}
@article{Wang2020c,
abstract = {The Coronavirus Disease 2019 (COVID-19) pandemic continues to have a devastating effect on the health and well-being of the global population. A critical step in the fight against COVID-19 is effective screening of infected patients, with one of the key screening approaches being radiology examination using chest radiography. It was found in early studies that patients present abnormalities in chest radiography images that are characteristic of those infected with COVID-19. Motivated by this and inspired by the open source efforts of the research community, in this study we introduce COVID-Net, a deep convolutional neural network design tailored for the detection of COVID-19 cases from chest X-ray (CXR) images that is open source and available to the general public. To the best of the authors' knowledge, COVID-Net is one of the first open source network designs for COVID-19 detection from CXR images at the time of initial release. We also introduce COVIDx, an open access benchmark dataset that we generated comprising of 13,975 CXR images across 13,870 patient patient cases, with the largest number of publicly available COVID-19 positive cases to the best of the authors' knowledge. Furthermore, we investigate how COVID-Net makes predictions using an explainability method in an attempt to not only gain deeper insights into critical factors associated with COVID cases, which can aid clinicians in improved screening, but also audit COVID-Net in a responsible and transparent manner to validate that it is making decisions based on relevant information from the CXR images. By no means a production-ready solution, the hope is that the open access COVID-Net, along with the description on constructing the open source COVIDx dataset, will be leveraged and build upon by both researchers and citizen data scientists alike to accelerate the development of highly accurate yet practical deep learning solutions for detecting COVID-19 cases and accelerate treatment of those who need it the most.},
archivePrefix = {arXiv},
arxivId = {2009.05383},
author = {Wang, Linda and Lin, Zhong Qiu and Wong, Alexander},
doi = {10.1038/s41598-020-76550-z},
eprint = {2009.05383},
issn = {20452322},
journal = {Scientific Reports},
month = {dec},
number = {1},
pages = {19549},
pmid = {33177550},
title = {{COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images}},
url = {http://www.nature.com/articles/s41598-020-76550-z},
volume = {10},
year = {2020}
}
@misc{UlHassan2019,
author = {ul Hassan, Muhammad and Zhang, Geoffrey and Latifi, Kujtim and Ullah, Ghanim and Gillies, Robert and Moros, Eduardo},
doi = {10.7937/TCIA.2019.4L24TZ5G},
publisher = {The Cancer Imaging Archive},
title = {{Credence Cartridge Radiomics Phantom CT Scans with Controlled Scanning Approach}},
url = {https://wiki.cancerimagingarchive.net/x/MoJgAg},
year = {2019}
}
@article{Carreras2015,
abstract = {To stimulate progress in automating the reconstruction of neural circuits, we organized the first international challenge on 2D segmentation of electron microscopic(EM) images of the brain. Participants submitted boundary map spredicted for a test set of images, and were scored based on their agreement with a on sensus of human expert annotations. The winning team had no prior experience with EM images, and employed a convolutional network. This “deeplearning” approach has since become accepted as a standard for segmentation of EM images. The challenge has continued to accept submissions, and the best so far has resulted from co-operation between two teams. The challenge has probably saturated, as algorithms cannot progress beyond limits set by ambiguities inherent in 2D scoring and the size of the test data set. Retrospective evaluation of the challenges coring system reveals that it was not sufficiently robust to variations in the widths of neurite borders. We propose a solution to this problem, which should be useful for a future 3D segmentation challenge.},
author = {Carreras, Ignacio Arganda and Turaga, Srinivas C. and Berger, Daniel R. and San, Dan Cire and Giusti, Alessandro and Gambardella, Luca M. and Schmidhuber, J{\"{u}}rgen and Laptev, Dmitry and Dwivedi, Sarvesh and Buhmann, Joachim M. and Liu, Ting and Seyedhosseini, Mojtaba and Tasdizen, Tolga and Kamentsky, Lee and Burget, Radim and Uher, Vaclav and Tan, Xiao and Sun, Changming and Pham, Tuan D. and Bas, Erhan and Uzunbas, Mustafa G. and Cardona, Albert and Schindelin, Johannes and Seung, H. Sebastian},
doi = {10.3389/fnana.2015.00142},
issn = {16625129},
journal = {Frontiers in Neuroanatomy},
keywords = {Connectomics,Electron microscopy,Image segmentation,Machine learning,Reconstruction},
month = {nov},
number = {November},
pages = {1--13},
title = {{Crowdsourcing the creation of image segmentation algorithms for connectomics}},
url = {http://journal.frontiersin.org/Article/10.3389/fnana.2015.00142/abstract},
volume = {9},
year = {2015}
}
@misc{An2020,
author = {An, Peng and Xu, Sheng and Harmon, Stephanie A and Turkbey, Evrim B and Sanford, Thomas H and Amalou, Amel and Kassin, Michael and Varble, Nicole and Blain, Maxime and Anderson, Victoria and Patella, Francesca and Carrafiello, Gianpaolo and Turkbey, Baris T and Wood, Bradford J},
doi = {10.7937/TCIA.2020.GQRY-NC81},
publisher = {The Cancer Imaging Archive},
title = {{CT Images in COVID-19}},
url = {https://wiki.cancerimagingarchive.net/x/o5QvB},
year = {2020}
}
@misc{An2020a,
author = {An, Peng and Xu, Sheng and Harmon, Stephanie A. and Turkbey, Evrim B. and Sanford, Thomas H. and Amalou, Amel and Kassin, Michael and Varble, Nicole and Blain, Maxime and Anderson, Victoria and Patella, Francesca and Carrafiello, Gianpaolo and Turkbey, Baris T. and Wood, Bradford J.},
booktitle = {The Cancer Imaging Archive},
doi = {https://doi.org/10.7937/tcia.2020.gqry-nc81},
keywords = {communication,crm,e-crm,linear sequential,planning},
title = {{CT Images in COVID-19}},
year = {2020}
}
@article{Rister2018,
abstract = {Fully-convolutional neural networks have achieved superior performance in a variety of image segmentation tasks. However, their training requires laborious manual annotation of large datasets, as well as acceleration by parallel processors with high-bandwidth memory, such as GPUs. We show that simple models can achieve competitive accuracy for organ segmentation on CT images when trained with extensive data augmentation, which leverages existing graphics hardware to quickly apply geometric and photometric transformations to 3D image data. On 3 mm3 CT volumes, our GPU implementation is 2.6-8×faster than a widely-used CPU version, including communication overhead. We also show how to automatically generate training labels using rudimentary morphological operations, which are efficiently computed by 3D Fourier transforms. We combined fully-automatic labels for the lungs and bone with semi-automatic ones for the liver, kidneys and bladder, to create a dataset of 130 labeled CT scans. To achieve the best results from data augmentation, our model uses the intersection-over-union (IOU) loss function, a close relative of the Dice loss. We discuss its mathematical properties and explain why it outperforms the usual weighted cross-entropy loss for unbalanced segmentation tasks. We conclude that there is no unique IOU loss function, as the naive one belongs to a broad family of functions with the same essential properties. When combining data augmentation with the IOU loss, our model achieves a Dice score of 78-92{\%} for each organ. The trained model, code and dataset will be made publicly available, to further medical imaging research.},
annote = {{\_}eprint: 1811.11226},
archivePrefix = {arXiv},
arxivId = {1811.11226},
author = {Rister, Blaine and Yi, Darvin and Shivakumar, Kaushik and Nobashi, Tomomi and Rubin, Daniel L.},
eprint = {1811.11226},
journal = {arXiv},
keywords = {Computed tomography (CT),Computer vision,Data augmentation,Deep learning,Image segmentation},
title = {{CT organ segmentation using GPU data augmentation, unsupervised labels and IOU loss}},
url = {http://arxiv.org/abs/1811.11226},
volume = {abs/1811.1},
year = {2018}
}
@article{Mirsky2019,
abstract = {In 2018, clinics and hospitals were hit with numerous attacks leading to significant data breaches and interruptions in medical services. An attacker with access to medical records can do much more than hold the data for ransom or sell it on the black market. In this paper, we show how an attacker can use deep-learning to add or remove evidence of medical conditions from volumetric (3D) medical scans. An attacker may perform this act in order to stop a political candidate, sabotage research, commit insurance fraud, perform an act of terrorism, or even commit murder. We implement the attack using a 3D conditional GAN and show how the framework (CT-GAN) can be automated. Although the body is complex and 3D medical scans are very large, CT-GAN achieves realistic results which can be executed in milliseconds. To evaluate the attack, we focused on injecting and removing lung cancer from CT scans. We show how three expert radiologists and a state-of-the-art deep learning AI are highly susceptible to the attack. We also explore the attack surface of a modern radiology network and demonstrate one attack vector: we intercepted and manipulated CT scans in an active hospital network with a covert penetration test.},
archivePrefix = {arXiv},
arxivId = {1901.03597},
author = {Mirsky, Yisroel and Mahler, Tom and Shelef, Ilan and Elovici, Yuval},
eprint = {1901.03597},
isbn = {9781939133069},
journal = {Proceedings of the 28th USENIX Security Symposium},
month = {jan},
pages = {461--478},
title = {{CT-GAN: Malicious tampering of 3D medical imagery using deep learning}},
url = {http://arxiv.org/abs/1901.03597},
year = {2019}
}
@misc{TCIA-CT-ORG,
author = {Rister, Blaine and Shivakumar, Kaushik and Nobashi, Tomomi and Rubin, Daniel L},
doi = {10.7937/TCIA.2019.TT7F4V7O},
publisher = {The Cancer Imaging Archive},
title = {{CT-ORG: A Dataset of CT Volumes With Multiple Organ Segmentations}},
url = {https://wiki.cancerimagingarchive.net/x/OgWkAw},
year = {2019}
}
@article{Chetlur2014,
abstract = {We present a library of efficient implementations of deep learning primitives. Deep learning workloads are computationally intensive, and optimizing their kernels is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS). However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, although similarly to the BLAS library, these routines could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36{\%} on a standard model while also reducing memory consumption.},
archivePrefix = {arXiv},
arxivId = {1410.0759},
author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
eprint = {1410.0759},
journal = {arXiv: Neural and Evolutionary Computing},
title = {{cuDNN: Efficient Primitives for Deep Learning}},
url = {http://arxiv.org/abs/1410.0759},
year = {2014}
}
@book{Florentina2019,
address = {Cham},
author = {Florentina, Laura and Eds, Stoica},
doi = {10.1007/978-3-030-39237-6},
editor = {Simian, Dana and Stoica, Laura Florentina},
isbn = {9783030392369},
publisher = {Springer International Publishing},
series = {Communications in Computer and Information Science},
title = {{Dana Simian Modelling and Development of Intelligent Systems}},
url = {http://link.springer.com/10.1007/978-3-030-39237-6},
volume = {1126},
year = {2019}
}
@misc{Liu2018,
abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
archivePrefix = {arXiv},
arxivId = {1806.09055},
author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
booktitle = {7th International Conference on Learning Representations, ICLR 2019arXivarXiv},
eprint = {1806.09055},
title = {{Darts: Differentiable architecture search}},
volume = {abs/1806.0},
year = {2018}
}
@article{Takahashi2019,
abstract = {Deep convolutional neural networks (CNNs) have achieved remarkable results in image processing tasks. However, their high expression ability risks overfitting. Consequently, data augmentation techniques have been proposed to prevent overfitting while enriching datasets. Recent CNN architectures with more parameters are rendering traditional data augmentation techniques insufficient. In this study, we propose a new data augmentation technique called random image cropping and patching (RICAP) which randomly crops four images and patches them to create a new training image. Moreover, RICAP mixes the class labels of the four images, resulting in an advantage of the soft labels. We evaluated RICAP with current state-of-the-art CNNs (e.g., the shake-shake regularization model) by comparison with competitive data augmentation techniques such as cutout and mixup. RICAP achieves a new state-of-the-art test error of 2.19{\%} on CIFAR-10. We also confirmed that deep CNNs with RICAP achieve better results on classification tasks using CIFAR-100 and ImageNet, an image-caption retrieval task using Microsoft COCO, and other computer vision tasks.},
annote = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
archivePrefix = {arXiv},
arxivId = {1811.09030},
author = {Takahashi, Ryo and Matsubara, Takashi and Uehara, Kuniaki},
doi = {10.1109/TCSVT.2019.2935128},
eprint = {1811.09030},
issn = {15582205},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {Data augmentation,convolutional neural network,image classification,image-caption retrieval},
number = {9},
pages = {2917--2931},
title = {{Data Augmentation Using Random Image Cropping and Patching for Deep CNNs}},
url = {http://dx.doi.org/10.1109/TCSVT.2019.2935128},
volume = {30},
year = {2020}
}
@article{Tschandl2018,
abstract = {Training of neural networks for automated diagnosis of pigmented skin lesions is hampered by the small size and lack of diversity of available datasets of dermatoscopic images. We tackle this problem by releasing the HAM10000 (“Human Against Machine with 10000 training images”) dataset. We collected dermatoscopic images from different populations acquired and stored by different modalities. Given this diversity we had to apply different acquisition and cleaning methods and developed semi-automatic workflows utilizing specifically trained neural networks. The final dataset consists of 10015 dermatoscopic images which are released as a training set for academic machine learning purposes and are publicly available through the ISIC archive. This benchmark dataset can be used for machine learning and for comparisons with human experts. Cases include a representative collection of all important diagnostic categories in the realm of pigmented lesions. More than 50{\%} of lesions have been confirmed by pathology, while the ground truth for the rest of the cases was either follow-up, expert consensus, or confirmation by in-vivo confocal microscopy.},
author = {Tschandl, Philipp and Rosendahl, Cliff and Kittler, Harald},
doi = {10.1038/sdata.2018.161},
issn = {20524463},
journal = {Scientific Data},
month = {dec},
number = {1},
pages = {180161},
pmid = {30106392},
title = {{Data descriptor: The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions}},
url = {http://www.nature.com/articles/sdata2018161},
volume = {5},
year = {2018}
}
@article{Lu2020,
abstract = {The rapidly emerging field of computational pathology has the potential to enable objective diagnosis, therapeutic response prediction and identification of new morphological features of clinical relevance. However, deep learning-based computational pathology approaches either require manual annotation of gigapixel whole slide images (WSIs) in fully-supervised settings or thousands of WSIs with slide-level labels in a weakly-supervised setting. Moreover, whole slide level computational pathology methods also suffer from domain adaptation, interpretability and visualization issues. These challenges have prevented the broad adaptation of computational pathology for clinical and research purposes. Here we present CLAM - Clustering-constrained Attention Multiple instance learning (https://github.com/mahmoodlab/CLAM), an easy-to-use, high-throughput and interpretable WSI-level processing and learning method that only requires slide-level labels while being data efficient, adaptable and capable of handling multi-class subtyping problems. CLAM is a deep-learning based weakly-supervised method that uses attention-based learning to automatically identify sub-regions of high diagnostic value in order to accurately classify the whole slide, while also utilizing instance-level clustering over the representative regions identified to constrain and refine the feature space. In three separate analyses, we demonstrate the data efficiency and adaptability of CLAM and its superior performance over standard weakly-supervised classification. We demonstrate that CLAM models are interpretable and can be used to identify well-known and new morphological features without using any spatial labels during training. We further show that models trained using CLAM are adaptable to independent test cohorts, cell phone microscopy images, and varying slide tissue content. CLAM is a flexible, general purpose, and adaptable method that can be used for a variety of different computational pathology tasks in both clinical and research settings.},
archivePrefix = {arXiv},
arxivId = {2004.09666},
author = {Lu, Ming Y. and Williamson, Drew F.K. and Chen, Tiffany Y. and Chen, Richard J. and Barbieri, Matteo and Mahmood, Faisal},
eprint = {2004.09666},
journal = {arXiv},
month = {apr},
title = {{Data efficient and weakly supervised computational pathology on whole slide images}},
url = {http://arxiv.org/abs/2004.09666},
year = {2020}
}
@misc{Cardenas2019,
author = {Cardenas, Carlos E and Mohamed, A and Sharp, G. and Gooding, M. and Veeraraghavan, H. and Yang, J.},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/tcia.2019.bcfjqfqb},
publisher = {The Cancer Imaging Archive},
title = {{Data from AAPM RT-MAC Grand Challenge 2019}},
url = {https://wiki.cancerimagingarchive.net/x/bAP9Ag},
year = {2019}
}
@misc{Kinahan2017,
author = {Kinahan, Paul and Muzi, Mark and Bialecki, Brian and Coombs, Laura},
doi = {10.7937/K9/TCIA.2017.OL20ZMXG},
publisher = {The Cancer Imaging Archive},
title = {{Data from ACRIN-FLT-Breast}},
url = {https://wiki.cancerimagingarchive.net/x/pAHUAQ},
year = {2017}
}
@misc{Bloch2015,
author = {Bloch, B Nicolas and Jain, Ashali and conrade carl Jaffe},
doi = {10.7937/K9/TCIA.2015.SDNRQXXR},
publisher = {The Cancer Imaging Archive},
title = {{Data From BREAST-DIAGNOSIS}},
url = {https://wiki.cancerimagingarchive.net/x/JQAo},
year = {2015}
}
@misc{TCIA-C4KC-KiTS,
author = {Heller, N. and Sathianathen, N. and {Kalapara, A., Walczak}, E. and Moore, K. and Kaluzniak, H. and Rosenberg, J. and Blake, P. and Rengel, Z. and Oestreich, M. and Dean, J. and Tradewell, M. and Shah, A. and Tejpaul, R. and Edgerton, Z. and Peterson, M. and Raza, S. and Regmi, S. and Papanikolopoulos, N. and Weight, C.},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/TCIA.2019.IX49E8NX},
publisher = {The Cancer Imaging Archive},
title = {{Data from C4KC-KiTS [Data set]}},
url = {https://wiki.cancerimagingarchive.net/x/UwakAw},
year = {2019}
}
@misc{Mackin2017,
author = {Mackin, Dennis and Ray, Xenia and Zhang, Lifei and Fried, David and Yang, Jinzhong and Taylor, Brian and Rodriguez-Rivera, Edgardo and Dodge, Cristina and Jones, Aaron and Court, Laurence},
doi = {10.7937/K9/TCIA.2017.ZUZRML5B},
publisher = {The Cancer Imaging Archive},
title = {{Data From Credence Cartridge Radiomics Phantom CT Scans}},
url = {https://wiki.cancerimagingarchive.net/x/jIxyAQ},
year = {2017}
}
@misc{Ger2019,
author = {Ger, Rachel and Zhou, Shouhao and Chi, Pai-Chun and Lee, Hannah and Layman, Rick R and Jones, Kyle and Goff, David and Cardenas, Carlos and Fuller, Clifton and Howell, Rebecca and Li, Heng and Stafford, Jason and Court, Laurence and Mackin, Dennis},
doi = {10.7937/TCIA.2019.J71I4FAH},
publisher = {The Cancer Imaging Archive},
title = {{Data from CT Phantom Scans for Head, Chest, and Controlled Protocols on 100 Scanners (CC-Radiomics-Phantom-3)}},
url = {https://wiki.cancerimagingarchive.net/x/RADDAg},
year = {2019}
}
@misc{Smith2015,
author = {{Smith K}, Clark K and Smith, K and Clark, K and Bennett, W and Nolan, T and Kirby, J and Wolfsberger, M and Moulton, J and Vendt, B and Freymann, J},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2015.NWTESAY1},
publisher = {The Cancer Imaging Archive},
title = {{Data From CT{\_}COLONOGRAPHY}},
url = {https://wiki.cancerimagingarchive.net/x/DQE2},
year = {2015}
}
@misc{Vallieres2017a,
author = {Valli{\`{e}}res, Martin and Kay-Rivest, Emily and Perrin, L{\'{e}}o Jean and Liem, Xavier and Furstoss, Christophe and Khaouam, Nader and Nguyen-Tan, Phuc Felix and Wang, Chang-Shu and Sultanem, Khalil},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2017.8oje5q00},
publisher = {The Cancer Imaging Archive},
title = {{Data from Head-Neck-PET-CT}},
url = {https://wiki.cancerimagingarchive.net/x/24pyAQ},
year = {2017}
}
@misc{TCIAHead-Neck-Radiomics-HN1,
author = {Wee, Leonard and Dekker, Andre},
doi = {10.7937/TCIA.2019.8KAP372N},
publisher = {The Cancer Imaging Archive},
title = {{Data from Head-Neck-Radiomics-HN1}},
url = {https://wiki.cancerimagingarchive.net/x/iBglAw},
year = {2019}
}
@misc{Erickson2017,
author = {Erickson, Bradley and Akkus, Zeynettin and Sedlar, Jiri and Korfiatis, Panagiotis},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2017.dwehtz9v},
publisher = {The Cancer Imaging Archive},
title = {{Data From LGG-1p19qDeletion}},
url = {https://wiki.cancerimagingarchive.net/x/coKJAQ},
year = {2017}
}
@misc{III2015,
abstract = {The Lung Image Database Consortium image collection (LIDC-IDRI) consists of diagnostic and lung cancer screening thoracic computed tomography (CT) scans with marked-up annotated lesions. It is a web-accessible international resource for development, training, and evaluation of computer-assisted diagnostic (CAD) methods for lung cancer detection and diagnosis. Initiated by the National Cancer Institute (NCI), further advanced by the Foundation for the National Institutes of Health (FNIH), and accompanied by the Food and Drug Administration (FDA) through active participation, this public-private partnership demonstrates the success of a consortium founded on a consensus-based process. Seven academic centers and eight medical imaging companies collaborated to create this data set which contains 1018 cases. Each subject includes images from a clinical thoracic CT scan and an associated XML file that records the results of a two-phase image annotation process performed by four experienced thoracic radiologists. In the initial blinded-read phase, each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories ("nodule {\textgreater} or =3 mm," "nodule {\textless}3 mm," and "non-nodule {\textgreater} or =3 mm"). In the subsequent unblinded-read phase, each radiologist independently reviewed their own marks along with the anonymized marks of the three other radiologists to render a final opinion. The goal of this process was to identify as completely as possible all lung nodules in each CT scan without requiring forced consensus.},
author = {III, Armato and G., Samuel and McLennan and Geoffrey and Bidaut and Luc and McNitt-Gray and Michael, F. and R., Meyer and Charles and Reeves},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2015.LO9QL9SX},
publisher = {The Cancer Imaging Archive},
title = {{Data From LIDC-IDRI}},
url = {https://wiki.cancerimagingarchive.net/x/rgAe},
year = {2015}
}
@misc{TCIA-Lung-CT-Segmentation-Challenge,
author = {Yang, Jinzhong and Sharp, Greg and Veeraraghavan, Harini and {Van Elmpt}, Wouter and Dekker, Andre and Lustberg, Tim and Gooding, Mark},
doi = {10.7937/K9/TCIA.2017.3R3FVZ08},
publisher = {The Cancer Imaging Archive},
title = {{Data from Lung CT Segmentation Challenge}},
url = {https://wiki.cancerimagingarchive.net/x/e41yAQ},
year = {2017}
}
@misc{Wee2019,
author = {Wee, Leonard and Aerts, Hugo J W L and Kalendralis, Petros and Dekker, Andre},
doi = {10.7937/TCIA.2019.CWVLPD26},
publisher = {The Cancer Imaging Archive},
title = {{Data from NSCLC-Radiomics-Interobserver1}},
url = {https://wiki.cancerimagingarchive.net/x/bgAlAw},
year = {2019}
}
@misc{TCIA-Pancreas-CT,
author = {Roth, Holger and Farag, Amal and Turkbey, Evrim B and Lu, Le and Liu, Jiamin and Summers, Ronald M},
doi = {10.7937/K9/TCIA.2016.TNB1KQBU},
publisher = {The Cancer Imaging Archive},
title = {{Data From Pancreas-CT}},
url = {https://wiki.cancerimagingarchive.net/x/eIlXAQ},
year = {2016}
}
@misc{Gavrielides2015,
author = {Gavrielides, Marios A and Kinnard, Lisa M and Myers, Kyle J and Peregoy, Jenifer and Pritchard, William F and Zeng, Rongping and Esparza, Juan and Karanian, John and Petrick, Nicholas},
doi = {10.7937/K9/TCIA.2015.ORBJKMUX},
publisher = {The Cancer Imaging Archive},
title = {{Data From Phantom{\_}FDA}},
url = {https://wiki.cancerimagingarchive.net/x/CAA9},
year = {2015}
}
@misc{TCIA-Prostate-3T,
author = {{Litjens Geert; Futterer}, Jurgen; Huisman Henkjan;},
doi = {10.7937/K9/TCIA.2015.QJTV5IL5},
publisher = {The Cancer Imaging Archive},
title = {{Data From Prostate-3T}},
url = {http://dx.doi.org/10.7937/K9/TCIA.2015.QJTV5IL5},
year = {2015}
}
@misc{TCIA-PROSTATE-DIAGNOSIS,
author = {Bloch, B. Nicolas and Jain, Ashali and Jaffe, C. Carl},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2015.FOQEUJVT},
publisher = {The Cancer Imaging Archive},
title = {{Data From PROSTATE-DIAGNOSIS}},
url = {https://wiki.cancerimagingarchive.net/x/xgEy},
year = {2015}
}
@misc{Zhao2015,
author = {Zhao, Binsheng and Schwartz, Lawrence H and Kris, Mark G},
doi = {10.7937/K9/TCIA.2015.U1X8A5NR},
publisher = {The Cancer Imaging Archive},
title = {{Data From RIDER{\_}Lung CT}},
url = {https://wiki.cancerimagingarchive.net/x/XIRXAQ},
year = {2015}
}
@misc{Muzi2015,
author = {Muzi, Peter and Wanner, Michelle and Kinahan, Paul},
doi = {10.7937/K9/TCIA.2015.8WG2KN4W},
publisher = {The Cancer Imaging Archive},
title = {{Data From RIDER{\_}PHANTOM{\_}PET-CT}},
url = {https://wiki.cancerimagingarchive.net/x/bIRXAQ},
year = {2015}
}
@misc{Ger2018b,
author = {Ger, Rachel and Yang, Jinzhong and Ding, Yao and Jacobsen, Megan and Cardenas, Carlos and Fuller, Clifton and Howell, Rebecca and Li, Heng and Stafford, R Jason and Zhou, Shouhao and Court, Laurence},
doi = {10.7937/K9/TCIA.2018.3F08IEJT},
publisher = {The Cancer Imaging Archive},
title = {{Data from Synthetic and Phantom MR Images for Determining Deformable Image Registration Accuracy (MRI-DIR)}},
url = {https://wiki.cancerimagingarchive.net/x/-gA4Ag},
year = {2018}
}
@incollection{Viswanathan2020,
address = {Cham},
author = {Viswanathan, Ramanarayanan},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_298-1},
pages = {1--3},
publisher = {Springer International Publishing},
title = {{Data Fusion}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}298-1},
year = {2020}
}
@book{AnounciaS.M.GohelH.A.&Vairamuthu2020,
abstract = {This book discusses the recent trends and developments in the fields of information processing and information visualization. In view of the increasing amount of data, there is a need to develop visualization techniques to make that data easily understandable. Presenting such approaches from various disciplines, this book serves as a useful resource for graduates.},
address = {Singapore},
author = {{Margret Anouncia}, S. and Gohel, Hardik A. and Vairamuthu, Subbiah},
booktitle = {Data Visualization: Trends and Challenges Toward Multidisciplinary Perception},
doi = {10.1007/978-981-15-2282-6},
editor = {Anouncia, S. Margret and Gohel, Hardik A. and Vairamuthu, Subbiah},
isbn = {9789811522826},
pages = {1--179},
publisher = {Springer Singapore},
title = {{Data visualization: Trends and challenges toward multidisciplinary perception}},
url = {https://link.springer.com/book/10.1007{\%}2F978-981-15-2282-6},
year = {2020}
}
@article{Chapman2019,
abstract = {Generating value from data requires the ability to find, access and make sense of datasets. There are many efforts underway to encourage data sharing and reuse, from scientific publishers asking authors to submit data alongside manuscripts to data marketplaces, open data portals and data communities. Google recently beta-released a search service for datasets, which allows users to discover data stored in various online repositories via keyword queries. These developments foreshadow an emerging research field around dataset search or retrieval that broadly encompasses frameworks, methods and tools that help match a user data need against a collection of datasets. Here, we survey the state of the art of research and commercial systems and discuss what makes dataset search a field in its own right, with unique challenges and open questions. We look at approaches and implementations from related areas dataset search is drawing upon, including information retrieval, databases, entity-centric and tabular search in order to identify possible paths to tackle these questions as well as immediate next steps that will take the field forward.},
archivePrefix = {arXiv},
arxivId = {1901.00735},
author = {Chapman, Adriane and Simperl, Elena and Koesten, Laura and Konstantinidis, George and Ib{\'{a}}{\~{n}}ez, Luis Daniel and Kacprzak, Emilia and Groth, Paul},
doi = {10.1007/s00778-019-00564-x},
eprint = {1901.00735},
issn = {0949877X},
journal = {VLDB Journal},
keywords = {Dataset,Dataset retrieval,Dataset search,Information search and retrieval},
month = {jan},
number = {1},
pages = {251--272},
title = {{Dataset search: a survey}},
url = {http://arxiv.org/abs/1901.00735},
volume = {29},
year = {2020}
}
@article{Wu2018,
abstract = {We present a system for covert automated deception detection using information available in a video. We study the importance of different modalities like vision, audio and text for this task. On the vision side, our system uses classifiers trained on low level video features which predict human micro-expressions. We show that predictions of high-level micro-expressions can be used as features for deception prediction. Surprisingly, IDT (Improved Dense Trajectory) features which have been widely used for action recognition, are also very good at predicting deception in videos. We fuse the score of classifiers trained on IDT features and high-level micro-expressions to improve performance. MFCC (Mel-frequency Cepstral Coefficients) features from the audio domain also provide a significant boost in performance, while information from transcripts is not very beneficial for our system. Using various classifiers, our automated system obtains an AUC of 0.877 (10-fold cross-validation) when evaluated on subjects which were not part of the training set. Even though state-of-the-art methods use human annotations of micro-expressions for deception detection, our fully automated approach outperforms them by 5{\%}. When combined with human annotations of micro-expressions, our AUC improves to 0.922. We also present results of a user-study to analyze how well do average humans perform on this task, what modalities they use for deception detection and how they perform if only one modality is accessible.},
archivePrefix = {arXiv},
arxivId = {1712.04415},
author = {Wu, Zhe and Singh, Bharat and Davis, Larry S. and Subrahmanian, V. S.},
eprint = {1712.04415},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
month = {dec},
pages = {1695--1702},
title = {{Deception detection in videos}},
url = {http://arxiv.org/abs/1712.04415},
year = {2018}
}
@misc{Ikutani2020,
author = {Ikutani, Yoshiharu and {Takatomi Kubo} and Nishida, Satoshi and Hata, Hideaki and Matsumoto, Kenichi and Ikeda, Kazushi and Nishimoto, Shinji},
doi = {10.18112/OPENNEURO.DS002411.V1.1.0},
publisher = {Openneuro},
title = {{Decoding functional category of source code from the brain (fMRI on Java program comprehension)}},
url = {https://openneuro.org/datasets/ds002411/versions/1.1.0},
year = {2020}
}
@article{Aerts2014,
abstract = {Human cancers exhibit strong phenotypic differences that can be visualized noninvasively by medical imaging. Radiomics refers to the comprehensive quantification of tumour phenotypes by applying a large number of quantitative image features. Here we present a radiomic analysis of 440 features quantifying tumour image intensity, shape and texture, which are extracted from computed tomography data of 1,019 patients with lung or head-and-neck cancer. We find that a large number of radiomic features have prognostic power in independent data sets of lung and head-and-neck cancer patients, many of which were not identified as significant before. Radiogenomics analysis reveals that a prognostic radiomic signature, capturing intratumour heterogeneity, is associated with underlying gene-expression patterns. These data suggest that radiomics identifies a general prognostic phenotype existing in both lung and head-and-neck cancer. This may have a clinical impact as imaging is routinely used in clinical practice, providing an unprecedented opportunity to improve decision-support in cancer treatment at low cost. {\textcopyright} 2014 Macmillan Publishers Limited. All rights reserved.},
author = {Aerts, Hugo J.W.L. and Velazquez, Emmanuel Rios and Leijenaar, Ralph T.H. and Parmar, Chintan and Grossmann, Patrick and Cavalho, Sara and Bussink, Johan and Monshouwer, Ren{\'{e}} and Haibe-Kains, Benjamin and Rietveld, Derek and Hoebers, Frank and Rietbergen, Michelle M. and Leemans, C. Ren{\'{e}} and Dekker, Andre and Quackenbush, John and Gillies, Robert J. and Lambin, Philippe},
doi = {10.1038/ncomms5006},
issn = {20411723},
journal = {Nature Communications},
month = {sep},
number = {1},
pages = {4006},
pmid = {24892406},
title = {{Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach}},
url = {http://www.nature.com/articles/ncomms5006},
volume = {5},
year = {2014}
}
@incollection{Bansal2020,
address = {Cham},
author = {Bansal, Ankan and Ranjan, Rajeev and Castillo, Carlos D. and Chellappa, Rama},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_880-1},
pages = {1--9},
publisher = {Springer International Publishing},
title = {{Deep CNN-Based Face Recognition}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}880-1},
year = {2020}
}
@inproceedings{Tran2016,
abstract = {Over the last few years deep learning methods have emerged as one of the most prominent approaches for video analysis. However, so far their most successful applications have been in the area of video classification and detection, i.e., problems involving the prediction of a single class label or a handful of output variables per video. Furthermore, while deep networks are commonly recognized as the best models to use in these domains, there is a widespread perception that in order to yield successful results they often require time-consuming architecture search, manual tweaking of parameters and computationally intensive preprocessing or post-processing methods. In this paper we challenge these views by presenting a deep 3D convolutional architecture trained end to end to perform voxel-level prediction, i.e., to output a variable at every voxel of the video. Most importantly, we show that the same exact architecture can be used to achieve competitive results on three widely different voxel-prediction tasks: video semantic segmentation, optical flow estimation, and video coloring. The three networks learned on these problems are trained from raw video without any form of preprocessing and their outputs do not require post-processing to achieve outstanding performance. Thus, they offer an efficient alternative to traditional and much more computationally expensive methods in these video domains.},
address = {United States},
archivePrefix = {arXiv},
arxivId = {1511.06681},
author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2016.57},
eprint = {1511.06681},
isbn = {9781467388504},
issn = {21607516},
pages = {402--409},
publisher = {IEEE Computer Society},
title = {{Deep End2End Voxel2Voxel Prediction}},
year = {2016}
}
@misc{GuohuaShen2020,
author = {{Guohua Shen} and {Tomoyasu Horikawa} and Majima, Kei and {Yukiyasu Kamitani}},
doi = {10.18112/OPENNEURO.DS001506.V1.3.1},
publisher = {Openneuro},
title = {{Deep Image Reconstruction}},
url = {https://openneuro.org/datasets/ds001506/versions/1.3.1},
year = {2020}
}
@article{Shen2019,
abstract = {The mental contents of perception and imagery are thought to be encoded in hierarchical representations in the brain, but previous attempts to visualize perceptual contents have failed to capitalize on multiple levels of the hierarchy, leaving it challenging to reconstruct internal imagery. Recent work showed that visual cortical activity measured by functional magnetic resonance imaging (fMRI) can be decoded (translated) into the hierarchical features of a pre-trained deep neural network (DNN) for the same input image, providing a way to make use of the information from hierarchical visual features. Here, we present a novel image reconstruction method, in which the pixel values of an image are optimized to make its DNN features similar to those decoded from human brain activity at multiple layers. We found that our method was able to reliably produce reconstructions that resembled the viewed natural images. A natural image prior introduced by a deep generator neural network effectively rendered semantically meaningful details to the reconstructions. Human judgment of the reconstructions supported the effectiveness of combining multiple DNN layers to enhance the visual quality of generated images. While our model was solely trained with natural images, it successfully generalized to artificial shapes, indicating that our model was not simply matching to exemplars. The same analysis applied to mental imagery demonstrated rudimentary reconstructions of the subjective content. Our results suggest that our method can effectively combine hierarchical neural representations to reconstruct perceptual and subjective images, providing a new window into the internal contents of the brain.},
author = {Shen, Guohua and Horikawa, Tomoyasu and Majima, Kei and Kamitani, Yukiyasu},
doi = {10.1371/journal.pcbi.1006633},
editor = {O'Reilly, Jill},
issn = {15537358},
journal = {PLoS Computational Biology},
month = {jan},
number = {1},
pages = {e1006633},
title = {{Deep image reconstruction from human brain activity}},
url = {http://dx.plos.org/10.1371/journal.pcbi.1006633},
volume = {15},
year = {2019}
}
@article{Litjens2016,
author = {Litjens, Geert and S{\'{a}}nchez, Clara I. and Timofeeva, Nadya and Hermsen, Meyke and Nagtegaal, Iris and Kovacs, Iringo and {Hulsbergen - van de Kaa}, Christina and Bult, Peter and van Ginneken, Bram and van der Laak, Jeroen},
doi = {10.1038/srep26286},
issn = {2045-2322},
journal = {Scientific Reports},
month = {sep},
number = {1},
pages = {26286},
title = {{Deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis}},
url = {http://www.nature.com/articles/srep26286},
volume = {6},
year = {2016}
}
@article{Moen2019,
abstract = {Recent advances in computer vision and machine learning underpin a collection of algorithms with an impressive ability to decipher the content of images. These deep learning algorithms are being applied to biological images and are transforming the analysis and interpretation of imaging data. These advances are positioned to render difficult analyses routine and to enable researchers to carry out new, previously impossible experiments. Here we review the intersection between deep learning and cellular image analysis and provide an overview of both the mathematical mechanics and the programming frameworks of deep learning that are pertinent to life scientists. We survey the field's progress in four key applications: image classification, image segmentation, object tracking, and augmented microscopy. Last, we relay our labs' experience with three key aspects of implementing deep learning in the laboratory: annotating training data, selecting and training a range of neural network architectures, and deploying solutions. We also highlight existing datasets and implementations for each surveyed application.},
author = {Moen, Erick and Bannon, Dylan and Kudo, Takamasa and Graf, William and Covert, Markus and {Van Valen}, David},
doi = {10.1038/s41592-019-0403-1},
issn = {15487105},
journal = {Nature Methods},
month = {dec},
number = {12},
pages = {1233--1246},
pmid = {31133758},
title = {{Deep learning for cellular image analysis}},
url = {http://www.nature.com/articles/s41592-019-0403-1},
volume = {16},
year = {2019}
}
@article{Hosny2018,
abstract = {Background: Non-small-cell lung cancer (NSCLC) patients often demonstrate varying clinical courses and outcomes, even within the same tumor stage. This study explores deep learning applications in medical imaging allowing for the automated quantification of radiographic characteristics and potentially improving patient stratification. Methods and findings: We performed an integrative analysis on 7 independent datasets across 5 institutions totaling 1,194 NSCLC patients (age median = 68.3 years [range 32.5–93.3], survival median = 1.7 years [range 0.0–11.7]). Using external validation in computed tomography (CT) data, we identified prognostic signatures using a 3D convolutional neural network (CNN) for patients treated with radiotherapy (n = 771, age median = 68.0 years [range 32.5–93.3], survival median = 1.3 years [range 0.0–11.7]). We then employed a transfer learning approach to achieve the same for surgery patients (n = 391, age median = 69.1 years [range 37.2–88.0], survival median = 3.1 years [range 0.0–8.8]). We found that the CNN predictions were significantly associated with 2-year overall survival from the start of respective treatment for radiotherapy (area under the receiver operating characteristic curve [AUC] = 0.70 [95{\%} CI 0.63–0.78], p {\textless} 0.001) and surgery (AUC = 0.71 [95{\%} CI 0.60–0.82], p {\textless} 0.001) patients. The CNN was also able to significantly stratify patients into low and high mortality risk groups in both the radiotherapy (p {\textless} 0.001) and surgery (p = 0.03) datasets. Additionally, the CNN was found to significantly outperform random forest models built on clinical parameters—including age, sex, and tumor node metastasis stage—as well as demonstrate high robustness against test–retest (intraclass correlation coefficient = 0.91) and inter-reader (Spearman's rank-order correlation = 0.88) variations. To gain a better understanding of the characteristics captured by the CNN, we identified regions with the most contribution towards predictions and highlighted the importance of tumor-surrounding tissue in patient stratification. We also present preliminary findings on the biological basis of the captured phenotypes as being linked to cell cycle and transcriptional processes. Limitations include the retrospective nature of this study as well as the opaque black box nature of deep learning networks. Conclusions: Our results provide evidence that deep learning networks may be used for mortality risk stratification based on standard-of-care CT images from NSCLC patients. This evidence motivates future research into better deciphering the clinical and biological basis of deep learning networks as well as validation in prospective data.},
annote = {From Duplicate 2 (Deep learning for lung cancer prognostication: A retrospective multi-cohort radiomics study - Hosny, Ahmed; Parmar, Chintan; Coroller, Thibaud P; Grossmann, Patrick; Zeleznik, Roman; Kumar, Avnish; Bussink, Johan; Gillies, Robert J; Mak, Raymond H; Aerts, Hugo J.W.L.)

Publisher: Public Library of Science},
author = {Hosny, Ahmed and Parmar, Chintan and Coroller, Thibaud P. and Grossmann, Patrick and Zeleznik, Roman and Kumar, Avnish and Bussink, Johan and Gillies, Robert J. and Mak, Raymond H. and Aerts, Hugo J.W.L.},
doi = {10.1371/journal.pmed.1002711},
issn = {15491676},
journal = {PLoS Medicine},
number = {11},
pmid = {30500819},
title = {{Deep learning for lung cancer prognostication: A retrospective multi-cohort radiomics study}},
url = {https://doi.org/10.1371/journal.pmed.1002711},
volume = {15},
year = {2018}
}
@article{Leclerc2019,
abstract = {Delineation of the cardiac structures from 2D echocardiographic images is a common clinical task to establish a diagnosis. Over the past decades, the automation of this task has been the subject of intense research. In this paper, we evaluate how far the state-of-the-art encoder-decoder deep convolutional neural network methods can go at assessing 2D echocardiographic images, i.e., segmenting cardiac structures and estimating clinical indices, on a dataset, especially, designed to answer this objective. We, therefore, introduce the cardiac acquisitions for multi-structure ultrasound segmentation dataset, the largest publicly-available and fully-annotated dataset for the purpose of echocardiographic assessment. The dataset contains two and four-chamber acquisitions from 500 patients with reference measurements from one cardiologist on the full dataset and from three cardiologists on a fold of 50 patients. Results show that encoder-decoder-based architectures outperform state-of-the-art non-deep learning methods and faithfully reproduce the expert analysis for the end-diastolic and end-systolic left ventricular volumes, with a mean correlation of 0.95 and an absolute mean error of 9.5 ml. Concerning the ejection fraction of the left ventricle, results are more contrasted with a mean correlation coefficient of 0.80 and an absolute mean error of 5.6{\%}. Although these results are below the inter-observer scores, they remain slightly worse than the intra-observer's ones. Based on this observation, areas for improvement are defined, which open the door for accurate and fully-automatic analysis of 2D echocardiographic images.},
archivePrefix = {arXiv},
arxivId = {1908.06948},
author = {Leclerc, Sarah and Smistad, Erik and Pedrosa, Joao and Ostvik, Andreas and Cervenansky, Frederic and Espinosa, Florian and Espeland, Torvald and Berg, Erik Andreas Rye and Jodoin, Pierre Marc and Grenier, Thomas and Lartizien, Carole and Dhooge, Jan and Lovstakken, Lasse and Bernard, Olivier},
doi = {10.1109/TMI.2019.2900516},
eprint = {1908.06948},
issn = {1558254X},
journal = {IEEE transactions on medical imaging},
month = {sep},
number = {9},
pages = {2198--2210},
pmid = {30802851},
title = {{Deep Learning for Segmentation Using an Open Large-Scale Dataset in 2D Echocardiography}},
url = {https://ieeexplore.ieee.org/document/8649738/},
volume = {38},
year = {2019}
}
@inproceedings{Dolph2017,
abstract = {This work proposes multiclass deep learning classification of Alzheimer's disease (AD) using novel texture and other associated features extracted from structural MRI. Two distinct learning models (Model 1 and 2) are presented where both include subcortical area specific feature extraction, feature selection and stacked auto-encoder (SAE) deep neural network (DNN). The models learn highly complex and subtle differences in spatial atrophy patterns using white matter volumes, gray matter volumes, cortical surface area, cortical thickness, and different types of Fractal Brownian Motion co-occurrence matrices for texture as features to classify AD from cognitive normal (CN) and mild cognitive impairment (MCI) in dementia patients. A five layer SAE with state-of-the-art dropout learning is trained on a publicly available ADNI dataset and the model performances are evaluated at two levels: one using in-house tenfold cross validation and another using the publicly available CADDementia competition. The in-house evaluations of our two models achieve 56.6{\%} and 58.0{\%} tenfold cross validation accuracies using 504 ADNI subjects. For the public domain evaluation, we are the first to report DNN to CADDementia and our methods yield competitive classification accuracies of 51.4{\%} and 56.8{\%}. Further, both of our proposed models offer higher True Positive Fraction (TPF) for AD class when compared to the top-overall ranked algorithm while Model 1 also ties for top diseased class sensitivity at 58.2{\%} in the CADDementia challenge. Finally, Model 2 achieves strong disease class sensitivity with improvement in specificity and overall accuracy. Our algorithms have the potential to provide a rapid, objective, and non-invasive assessment of AD.},
author = {Dolph, C. V. and Alam, M. and Shboul, Z. and Samad, M. D. and Iftekharuddin, K. M.},
booktitle = {Proceedings of the International Joint Conference on Neural Networks},
doi = {10.1109/IJCNN.2017.7966129},
isbn = {9781509061815},
keywords = {ADNI,Alzheimer's disease,Biomarkers,Deep learning,Dropout learning,Hippocampus,Neuroimaging classification},
month = {may},
pages = {2259--2266},
publisher = {IEEE},
title = {{Deep learning of texture and structural features for multiclass Alzheimer's disease classification}},
url = {http://ieeexplore.ieee.org/document/7966129/},
volume = {2017-May},
year = {2017}
}
@article{Bernard2018,
abstract = {Delineation of the left ventricular cavity, myocardium, and right ventricle from cardiac magnetic resonance images (multi-slice 2-D cine MRI) is a common clinical task to establish diagnosis. The automation of the corresponding tasks has thus been the subject of intense research over the past decades. In this paper, we introduce the 'Automatic Cardiac Diagnosis Challenge' dataset (ACDC), the largest publicly available and fully annotated dataset for the purpose of cardiac MRI (CMR) assessment. The dataset contains data from 150 multi-equipments CMRI recordings with reference measurements and classification from two medical experts. The overarching objective of this paper is to measure how far state-of-the-art deep learning methods can go at assessing CMRI, i.e., segmenting the myocardium and the two ventricles as well as classifying pathologies. In the wake of the 2017 MICCAI-ACDC challenge, we report results from deep learning methods provided by nine research groups for the segmentation task and four groups for the classification task. Results show that the best methods faithfully reproduce the expert analysis, leading to a mean value of 0.97 correlation score for the automatic extraction of clinical indices and an accuracy of 0.96 for automatic diagnosis. These results clearly open the door to highly accurate and fully automatic analysis of cardiac CMRI. We also identify scenarios for which deep learning methods are still failing. Both the dataset and detailed results are publicly available online, while the platform will remain open for new submissions.},
author = {Bernard, Olivier and Lalande, Alain and Zotti, Clement and Cervenansky, Frederick and Yang, Xin and Heng, Pheng Ann and Cetin, Irem and Lekadir, Karim and Camara, Oscar and {Gonzalez Ballester}, Miguel Angel and Sanroma, Gerard and Napel, Sandy and Petersen, Steffen and Tziritas, Georgios and Grinias, Elias and Khened, Mahendra and Kollerathu, Varghese Alex and Krishnamurthi, Ganapathy and Rohe, Marc Michel and Pennec, Xavier and Sermesant, Maxime and Isensee, Fabian and Jager, Paul and Maier-Hein, Klaus H. and Full, Peter M. and Wolf, Ivo and Engelhardt, Sandy and Baumgartner, Christian F. and Koch, Lisa M. and Wolterink, Jelmer M. and Isgum, Ivana and Jang, Yeonggul and Hong, Yoonmi and Patravali, Jay and Jain, Shubham and Humbert, Olivier and Jodoin, Pierre Marc},
doi = {10.1109/TMI.2018.2837502},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Cardiac segmentation and diagnosis,MRI,deep learning,left and right ventricles,myocardium},
month = {nov},
number = {11},
pages = {2514--2525},
title = {{Deep Learning Techniques for Automatic MRI Cardiac Multi-Structures Segmentation and Diagnosis: Is the Problem Solved?}},
url = {https://ieeexplore.ieee.org/document/8360453/},
volume = {37},
year = {2018}
}
@article{Duan2018DeepNL,
abstract = {In this paper we introduce a novel and accurate optimisation method for segmentation of cardiac MR (CMR) images in patients with pulmonary hypertension (PH). The proposed method explicitly takes into account the image features learned from a deep neural network. To this end, we estimate simultaneous probability maps over region and edge locations in CMR images using a fully convolutional network. Due to the distinct morphology of the heart in patients with PH, these probability maps can then be incorporated in a single nested level set optimisation framework to achieve multi-region segmentation with high efficiency. The proposed method uses an automatic way for level set initialisation and thus the whole optimisation is fully automated. We demonstrate that the proposed deep nested level set (DNLS) method outperforms existing state-of-the-art methods for CMR segmentation in PH patients.},
archivePrefix = {arXiv},
arxivId = {1807.10760},
author = {Duan, Jinming and Schlemper, Jo and Bai, Wenjia and Dawes, Timothy J.W. and Bello, Ghalib and Doumou, Georgia and {De Marvao}, Antonio and O'Regan, Declan P. and Rueckert, Daniel},
doi = {10.1007/978-3-030-00937-3_68},
eprint = {1807.10760},
isbn = {9783030009366},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
month = {jul},
pages = {595--603},
title = {{Deep Nested Level Sets: Fully Automated Segmentation of Cardiac MR Images in Patients with Pulmonary Hypertension}},
url = {http://arxiv.org/abs/1807.10760 http://link.springer.com/10.1007/978-3-030-00937-3{\_}68},
volume = {11073 LNCS},
year = {2018}
}
@book{Singh2020,
address = {Berkeley, CA},
author = {Singh, Himanshu and Lone, Yunis Ahmad},
booktitle = {Deep Neuro-Fuzzy Systems with Python},
doi = {10.1007/978-1-4842-5361-8},
isbn = {978-1-4842-5360-1},
publisher = {Apress},
title = {{Deep Neuro-Fuzzy Systems with Python}},
url = {http://link.springer.com/10.1007/978-1-4842-5361-8},
year = {2020}
}
@article{Hausknecht2015,
abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting Deep Recurrent Q-Network (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
archivePrefix = {arXiv},
arxivId = {1507.06527},
author = {Hausknecht, Matthew and Stone, Peter},
eprint = {1507.06527},
isbn = {9781577357520},
journal = {AAAI Fall Symposium - Technical Report},
month = {jul},
pages = {29--37},
title = {{Deep recurrent q-learning for partially observable MDPs}},
url = {http://arxiv.org/abs/1507.06527},
volume = {FS-15-06},
year = {2015}
}
@article{Li2018a,
abstract = {We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.},
archivePrefix = {arXiv},
arxivId = {1810.06339},
author = {Li, Yuxi},
eprint = {1810.06339},
month = {oct},
title = {{Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1701.07274 http://arxiv.org/abs/1810.06339},
year = {2018}
}
@article{Arulkumaran2017,
abstract = {Deep reinforcement learning (DRL) is poised to revolutionize the field of artificial intelligence (AI) and represents a step toward building autonomous systems with a higherlevel understanding of the visual world. Currently, deep learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to play video games directly from pixels. DRL algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of RL, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep RL, including the deep Q-network (DQN), trust region policy optimization (TRPO), and asynchronous advantage actor critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via RL. To conclude, we describe several current areas of research within the field.},
author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
doi = {10.1109/MSP.2017.2743240},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
month = {nov},
number = {6},
pages = {26--38},
title = {{Deep reinforcement learning: A brief survey}},
url = {http://ieeexplore.ieee.org/document/8103164/},
volume = {34},
year = {2017}
}
@inproceedings{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
isbn = {9781467388504},
issn = {10636919},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
url = {http://arxiv.org/abs/1512.03385},
volume = {2016-Decem},
year = {2016}
}
@article{Kulkarni2016,
abstract = {Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms. There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map. The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards. The value function of a state can be computed as the inner product between the successor map and the reward weights. In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework. DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy. We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine.},
archivePrefix = {arXiv},
arxivId = {1606.02396},
author = {Kulkarni, Tejas D. and Saeedi, Ardavan and Gautam, Simanta and Gershman, Samuel J.},
eprint = {1606.02396},
month = {jun},
title = {{Deep Successor Reinforcement Learning}},
url = {http://arxiv.org/abs/1606.02396},
year = {2016}
}
@inproceedings{Xie2016,
abstract = {As 3D movie viewing becomes mainstream and the Virtual Reality (VR) market emerges, the demand for 3D contents is growing rapidly. Producing 3D videos, however, remains challenging. In this paper we propose to use deep neural networks to automatically convert 2Dvideos and images to a stereoscopic 3D format. In contrast to previous automatic 2D-to-3D conversion algorithms, which have separate stages and need ground truth depth map as supervision, our approach is trained endto-end directly on stereo pairs extracted from existing 3D movies. This novel training scheme makes it possible to exploit orders of magnitude more data and significantly increases performance. Indeed, Deep3D outperforms baselines in both quantitative and human subject evaluations.},
annote = {From Duplicate 2 (Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks - Xie, Junyuan; Girshick, Ross; Farhadi, Ali)

{\_}eprint: 1604.03650},
archivePrefix = {arXiv},
arxivId = {1604.03650},
author = {Xie, Junyuan and Girshick, Ross and Farhadi, Ali},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46493-0_51},
eprint = {1604.03650},
isbn = {9783319464923},
issn = {16113349},
keywords = {Deep convolutional neural networks,Monocular stereo reconstruction},
pages = {842--857},
title = {{Deep3D: Fully automatic 2D-to-3D video conversion with deep convolutional neural networks}},
url = {http://arxiv.org/abs/1604.03650},
volume = {9908 LNCS},
year = {2016}
}
@inproceedings{Zhu2018DeepEMD3,
author = {Zhu, Wentao and Vang, Yeeleng Scott and Huang, Yufang and Xie, Xiaohui},
booktitle = {MICCAI},
title = {{DeepEM: Deep 3D ConvNets with EM for Weakly Supervised Pulmonary Nodule Detection}},
year = {2018}
}
@inproceedings{Zhu2018DeepEMD3,
abstract = {Recently deep learning has been witnessing widespread adoption in various medical image applications. However, training complex deep neural nets requires large-scale datasets labeled with ground truth, which are often unavailable in many medical image domains. For instance, to train a deep neural net to detect pulmonary nodules in lung computed tomography (CT) images, current practice is to manually label nodule locations and sizes in many CT images to construct a sufficiently large training dataset, which is costly and difficult to scale. On the other hand, electronic medical records (EMR) contain plenty of partial information on the content of each medical image. In this work, we explore how to tap this vast, but currently unexplored data source to improve pulmonary nodule detection. We propose DeepEM, a novel deep 3D ConvNet framework augmented with expectation-maximization (EM), to mine weakly supervised labels in EMRs for pulmonary nodule detection. Experimental results show that DeepEM can lead to 1.5{\%} and 3.9{\%} average improvement in free-response receiver operating characteristic (FROC) scores on LUNA16 and Tianchi datasets, respectively, demonstrating the utility of incomplete information in EMRs for improving deep learning algorithms (https://github.com/uci-cbcl/DeepEM-for-Weakly-Supervised-Detection.git ).},
archivePrefix = {arXiv},
arxivId = {1805.05373},
author = {Zhu, Wentao and Vang, Yeeleng S. and Huang, Yufang and Xie, Xiaohui},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-00934-2_90},
eprint = {1805.05373},
isbn = {9783030009335},
issn = {16113349},
keywords = {Deep 3D convolutional nets,DeepEM (deep 3D ConvNets with EM),Pulmonary nodule detection,Weakly supervised detection},
pages = {812--820},
title = {{DeepEM: Deep 3D ConvNets with EM for weakly supervised pulmonary nodule detection}},
url = {http://link.springer.com/10.1007/978-3-030-00934-2{\_}90},
volume = {11071 LNCS},
year = {2018}
}
@article{Jin2020,
abstract = {Background: Diagnosis of rib fractures plays an important role in identifying trauma severity. However, quickly and precisely identifying the rib fractures in a large number of CT images with increasing number of patients is a tough task, which is also subject to the qualification of radiologist. We aim at a clinically applicable automatic system for rib fracture detection and segmentation from CT scans. Methods: A total of 7,473 annotated traumatic rib fractures from 900 patients in a single center were enrolled into our dataset, named RibFrac Dataset, which were annotated with a human-in-the-loop labeling procedure. We developed a deep learning model, named FracNet, to detect and segment rib fractures. 720, 60 and 120 patients were randomly split as training cohort, tuning cohort and test cohort, respectively. Free-Response ROC (FROC) analysis was used to evaluate the sensitivity and false positives of the detection performance, and Intersection-over-Union (IoU) and Dice Coefficient (Dice) were used to evaluate the segmentation performance of predicted rib fractures. Observer studies, including independent human-only study and human-collaboration study, were used to benchmark the FracNet with human performance and evaluate its clinical applicability. A annotated subset of RibFrac Dataset, including 420 for training, 60 for tuning and 120 for test, as well as our code for model training and evaluation, was open to research community to facilitate both clinical and engineering research. Findings: Our method achieved a detection sensitivity of 92.9{\%} with 5.27 false positives per scan and a segmentation Dice of 71.5{\%}on the test cohort. Human experts achieved much lower false positives per scan, while underperforming the deep neural networks in terms of detection sensitivities with longer time in diagnosis. With human-computer collobration, human experts achieved higher detection sensitivities than human-only or computer-only diagnosis. Interpretation: The proposed FracNet provided increasing detection sensitivity of rib fractures with significantly decreased clinical time consumed, which established a clinically applicable method to assist the radiologist in clinical practice. Funding: A full list of funding bodies that contributed to this study can be found in the Acknowledgements section. The funding sources played no role in the study design; collection, analysis, and interpretation of data; writing of the report; or decision to submit the article for publication.},
author = {Jin, Liang and Yang, Jiancheng and Kuang, Kaiming and Ni, Bingbing and Gao, Yiyi and Sun, Yingli and Gao, Pan and Ma, Weiling and Tan, Mingyu and Kang, Hui and Chen, Jiajun and Li, Ming},
doi = {10.1016/j.ebiom.2020.103106},
issn = {23523964},
journal = {EBioMedicine},
keywords = {Deep learning,Detection and segmentation,Rib fracture},
month = {dec},
pages = {103106},
pmid = {33186809},
title = {{Deep-learning-assisted detection and segmentation of rib fractures from CT scans: Development and validation of FracNet}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2352396420304825},
volume = {62},
year = {2020}
}
@article{Bien2018,
abstract = {Background: Magnetic resonance imaging (MRI) of the knee is the preferred method for diagnosing knee injuries. However, interpretation of knee MRI is time-intensive and subject to diagnostic error and variability. An automated system for interpreting knee MRI could prioritize high-risk patients and assist clinicians in making diagnoses. Deep learning methods, in being able to automatically learn layers of features, are well suited for modeling the complex relationships between medical images and their interpretations. In this study we developed a deep learning model for detecting general abnormalities and specific diagnoses (anterior cruciate ligament [ACL] tears and meniscal tears) on knee MRI exams. We then measured the effect of providing the model's predictions to clinical experts during interpretation. Methods and findings: Our dataset consisted of 1,370 knee MRI exams performed at Stanford University Medical Center between January 1, 2001, and December 31, 2012 (mean age 38.0 years; 569 [41.5{\%}] female patients). The majority vote of 3 musculoskeletal radiologists established reference standard labels on an internal validation set of 120 exams. We developed MRNet, a convolutional neural network for classifying MRI series and combined predictions from 3 series per exam using logistic regression. In detecting abnormalities, ACL tears, and meniscal tears, this model achieved area under the receiver operating characteristic curve (AUC) values of 0.937 (95{\%} CI 0.895, 0.980), 0.965 (95{\%} CI 0.938, 0.993), and 0.847 (95{\%} CI 0.780, 0.914), respectively, on the internal validation set. We also obtained a public dataset of 917 exams with sagittal T1-weighted series and labels for ACL injury from Clinical Hospital Centre Rijeka, Croatia. On the external validation set of 183 exams, the MRNet trained on Stanford sagittal T2-weighted series achieved an AUC of 0.824 (95{\%} CI 0.757, 0.892) in the detection of ACL injuries with no additional training, while an MRNet trained on the rest of the external data achieved an AUC of 0.911 (95{\%} CI 0.864, 0.958). We additionally measured the specificity, sensitivity, and accuracy of 9 clinical experts (7 board-certified general radiologists and 2 orthopedic surgeons) on the internal validation set both with and without model assistance. Using a 2-sided Pearson's chi-squared test with adjustment for multiple comparisons, we found no significant differences between the performance of the model and that of unassisted general radiologists in detecting abnormalities. General radiologists achieved significantly higher sensitivity in detecting ACL tears (p-value = 0.002; q-value = 0.019) and significantly higher specificity in detecting meniscal tears (p-value = 0.003; q-value = 0.019). Using a 1-tailed t test on the change in performance metrics, we found that providing model predictions significantly increased clinical experts' specificity in identifying ACL tears (p-value {\textless} 0.001; q-value = 0.006). The primary limitations of our study include lack of surgical ground truth and the small size of the panel of clinical experts. Conclusions: Our deep learning model can rapidly generate accurate clinical pathology classifications of knee MRI exams from both internal and external datasets. Moreover, our results support the assertion that deep learning models can improve the performance of clinical experts during medical imaging interpretation. Further research is needed to validate the model prospectively and to determine its utility in the clinical setting.},
author = {Bien, Nicholas and Rajpurkar, Pranav and Ball, Robyn L. and Irvin, Jeremy and Park, Allison and Jones, Erik and Bereket, Michael and Patel, Bhavik N. and Yeom, Kristen W. and Shpanskaya, Katie and Halabi, Safwan and Zucker, Evan and Fanton, Gary and Amanatullah, Derek F. and Beaulieu, Christopher F. and Riley, Geoffrey M. and Stewart, Russell J. and Blankenberg, Francis G. and Larson, David B. and Jones, Ricky H. and Langlotz, Curtis P. and Ng, Andrew Y. and Lungren, Matthew P.},
doi = {10.1371/journal.pmed.1002699},
editor = {Saria, Suchi},
issn = {15491676},
journal = {PLoS Medicine},
month = {nov},
number = {11},
pages = {e1002699},
pmid = {30481176},
title = {{Deep-learning-assisted diagnosis for knee magnetic resonance imaging: Development and retrospective validation of MRNet}},
url = {https://dx.plos.org/10.1371/journal.pmed.1002699},
volume = {15},
year = {2018}
}
@inproceedings{Zhu2018,
abstract = {In this work, we present a fully automated lung computed tomography (CT) cancer diagnosis system, DeepLung. DeepLung consists of two components, nodule detection (identifying the locations of candidate nodules) and classification (classifying candidate nodules into benign or malignant). Considering the 3D nature of lung CT data and the compactness of dual path networks (DPN), two deep 3D DPN are designed for nodule detection and classification respectively. Specifically, a 3D Faster Regions with Convolutional Neural Net (R-CNN) is designed for nodule detection with 3D dual path blocks and a U-net-like encoder-decoder structure to effectively learn nodule features. For nodule classification, gradient boosting machine (GBM) with 3D dual path network features is proposed. The nodule classification subnetwork was validated on a public dataset from LIDC-IDRI, on which it achieved better performance than state-of-the-art approaches and surpassed the performance of experienced doctors based on image modality. Within the DeepLung system, candidate nodules are detected first by the nodule detection subnetwork, and nodule diagnosis is conducted by the classification subnetwork. Extensive experimental results demonstrate that DeepLung has performance comparable to experienced doctors both for the nodule-level and patient-level diagnosis on the LIDC-IDRI dataset.},
annote = {{\_}eprint: 1801.09555},
archivePrefix = {arXiv},
arxivId = {1801.09555},
author = {Zhu, Wentao and Liu, Chaochun and Fan, Wei and Xie, Xiaohui},
booktitle = {Proceedings - 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018},
doi = {10.1109/WACV.2018.00079},
eprint = {1801.09555},
isbn = {9781538648865},
pages = {673--681},
title = {{DeepLung: Deep 3D dual path nets for automated pulmonary nodule detection and classification}},
volume = {2018-Janua},
year = {2018}
}
@article{Beers2018,
abstract = {Translating neural networks from theory to clinical practice has unique challenges, specifically in the field of neuroimaging. In this paper, we present DeepNeuro, a deep learning framework that is best-suited to putting deep learning algorithms for neuroimaging in practical usage with a minimum of friction. We show how this framework can be used to both design and train neural network architectures, as well as modify state-of-the-art architectures in a flexible and intuitive way. We display the pre- and postprocessing functions common in the medical imaging community that DeepNeuro offers to ensure consistent performance of networks across variable users, institutions, and scanners. And we show how pipelines created in DeepNeuro can be concisely packaged into shareable Docker containers and command-line interfaces using DeepNeuro's pipeline resources.},
annote = {{\_}eprint: 1808.04589},
archivePrefix = {arXiv},
arxivId = {1808.04589},
author = {Beers, Andrew and Brown, James and Chang, Ken and Hoebel, Katharina and Gerstner, Elizabeth and Rosen, Bruce and Kalpathy-Cramer, Jayashree},
eprint = {1808.04589},
title = {{DeepNeuro: an open-source deep learning toolbox for neuroimaging}},
url = {http://arxiv.org/abs/1808.04589},
year = {2018}
}
@incollection{Roth2015,
abstract = {Automatic organ segmentation is an important yet challenging problem for medical image analysis. The pancreas is an abdominal organ with very high anatomical variability. This inhibits previous segmentation methods from achieving high accuracies, especially compared to other organs such as the liver, heart or kidneys. In this paper, we present a probabilistic bottom-up approach for pancreas segmentation in abdominal computed tomography (CT) scans, using multi-level deep convolutional networks (ConvNets). We propose and evaluate several variations of deep ConvNets in the context of hierarchical, coarse-tofine classification on image patches and regions, i.e. superpixels. We first present a dense labeling of local image patches via P-ConvNet and nearest neighbor fusion. Then we describe a regional ConvNet (R1−ConvNet) that samples a set of bounding boxes around each image superpixel at different scales of contexts in a “zoom-out” fashion. Our ConvNets learn to assign class probabilities for each superpixel region of being pancreas. Last, we study a stacked R2−ConvNet leveraging the joint space of CT intensities and the P−ConvNet dense probability maps. Both 3D Gaussian smoothing and 2D conditional random fields are exploited as structured predictions for post-processing. We evaluate on CT images of 82 patients in 4-fold cross-validation. We achieve a Dice Similarity Coefficient of 83.6±6.3{\%} in training and 71.8±10.7{\%} in testing.},
archivePrefix = {arXiv},
arxivId = {1506.06448},
author = {Roth, Holger R. and Lu, Le and Farag, Amal and Shin, Hoo Chang and Liu, Jiamin and Turkbey, Evrim B. and Summers, Ronald M.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24553-9_68},
eprint = {1506.06448},
issn = {16113349},
title = {{Deeporgan: Multi-level deep convolutional networks for automated pancreas segmentation}},
year = {2015}
}
@article{Flynn2015,
annote = {From Duplicate 2 (DeepStereo: Learning to Predict New Views from the World's Imagery - Flynn, John; Neulander, Ivan; Philbin, James; Snavely, Noah)

{\_}eprint: 1506.06825},
archivePrefix = {arXiv},
arxivId = {1506.06825},
author = {Flynn, John and Neulander, Ivan and Philbin, James and Snavely, Noah},
eprint = {1506.06825},
journal = {CoRR},
title = {{DeepStereo: Learning to Predict New Views from the World's Imagery}},
url = {http://arxiv.org/abs/1506.06825},
volume = {abs/1506.0},
year = {2015}
}
@inproceedings{Eppenhof2018DeformableIR,
abstract = {{\textcopyright} COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only. Deformable image registration can be time-consuming and often needs extensive parameterization to perform well on a specific application. We present a step towards a registration framework based on a three-dimensional convolutional neural network. The network directly learns transformations between pairs of three-dimensional images. The outputs of the network are three maps for the x, y, and z components of a thin plate spline transformation grid. The network is trained on synthetic random transformations, which are applied to a small set of representative images for the desired application. Training therefore does not require manually annotated ground truth deformation information. The methodology is demonstrated on public data sets of inspiration-expiration lung CT image pairs, which come with annotated corresponding landmarks for evaluation of the registration accuracy. Advantages of this methodology are its fast registration times and its minimal parameterization.},
author = {Lafarge, Maxime W. and Moeskops, Pim and Veta, Mitko and Pluim, Josien P. W. and Eppenhof, Koen A.. J.},
booktitle = {Medical Imaging 2018: Image Processing},
doi = {10.1117/12.2292443},
editor = {Angelini, Elsa D. and Landman, Bennett A.},
isbn = {9781510616370},
issn = {1605-7422},
month = {mar},
pages = {27},
publisher = {SPIE},
title = {{Deformable image registration using convolutional neural networks}},
url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10574/2292443/Deformable-image-registration-using-convolutional-neural-networks/10.1117/12.2292443.full},
year = {2018}
}
@inproceedings{He2015a,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on the learnable activation and advanced initialization, we achieve 4.94{\%} top-5 test error on the ImageNet 2012 classification dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%} [33]). To our knowledge, our result is the first to surpass the reported human-level performance (5.1{\%}, [26]) on this dataset.},
annote = {{\_}eprint: 1502.01852},
archivePrefix = {arXiv},
arxivId = {1502.01852},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.123},
eprint = {1502.01852},
isbn = {9781467383912},
issn = {15505499},
pages = {1026--1034},
title = {{Delving deep into rectifiers: Surpassing human-level performance on imagenet classification}},
volume = {2015 Inter},
year = {2015}
}
@inproceedings{Huang2016,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L2+1) direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
annote = {{\_}eprint: 1608.06993},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.243},
eprint = {1608.06993},
isbn = {9781538604571},
pages = {2261--2269},
title = {{Densely connected convolutional networks}},
url = {http://arxiv.org/abs/1608.06993},
volume = {2017-Janua},
year = {2017}
}
@article{Lafferty2001,
author = {Lafferty, John and Mccallum, Andrew and Pereira, Fernando C N},
journal = {Machine Learning},
number = {Icml},
pages = {282--289},
title = {{Departmental Papers ( CIS ) Conditional Random Fields : Probabilistic Models for Segmenting and Labeling Sequence Data Conditional Random Fields : Probabilistic Models}},
volume = {2001},
year = {2001}
}
@inproceedings{Baker2016,
abstract = {At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using Qlearning with an -greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.},
archivePrefix = {arXiv},
arxivId = {1611.02167},
author = {Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1611.02167},
title = {{Designing neural network architectures using reinforcement learning}},
volume = {abs/1611.0},
year = {2019}
}
@article{Buda2020,
abstract = {Breast cancer screening is one of the most common radiological tasks with over 39 million exams performed each year. While breast cancer screening has been one of the most studied medical imaging applications of artificial intelligence, the development and evaluation of the algorithms are hindered due to the lack of well-annotated large-scale publicly available datasets. This is particularly an issue for digital breast tomosynthesis (DBT) which is a relatively new breast cancer screening modality. We have curated and made publicly available a large-scale dataset of digital breast tomosynthesis images. It contains 22,032 reconstructed DBT volumes belonging to 5,610 studies from 5,060 patients. This included four groups: (1) 5,129 normal studies, (2) 280 studies where additional imaging was needed but no biopsy was performed, (3) 112 benign biopsied studies, and (4) 89 studies with cancer. Our dataset included masses and architectural distortions which were annotated by two experienced radiologists. Additionally, we developed a single-phase deep learning detection model and tested it using our dataset to serve as a baseline for future research. Our model reached a sensitivity of 65{\%} at 2 false positives per breast. Our large, diverse, and highly-curated dataset will facilitate development and evaluation of AI algorithms for breast cancer screening through providing data for training as well as common set of cases for model validation. The performance of the model developed in our study shows that the task remains challenging and will serve as a baseline for future model development.},
archivePrefix = {arXiv},
arxivId = {2011.07995},
author = {Buda, Mateusz and Saha, Ashirbani and Walsh, Ruth and Ghate, Sujata and Li, Nianyi and {\'{S}}wi{\c{e}}cicki, Albert and Lo, Joseph Y. and Mazurowski, Maciej A.},
eprint = {2011.07995},
title = {{Detection of masses and architectural distortions in digital breast tomosynthesis: a publicly available dataset of 5,060 patients and a deep learning model}},
url = {http://arxiv.org/abs/2011.07995},
year = {2020}
}
@article{Lepping2016a,
abstract = {Music is a strong emotional stimulus; however, it is difficult to differentiate the effects of arousal and valence. While emotional stimuli sets have been created from words and pictures, a normed set of musical stimuli is unavailable. The goal of this project was to identify a set of ecologically valid musical stimuli for use in research studies of emotion and mood that differ on valence but are matched for arousal, and are also matched with emotionally evocative non-musical stimuli. Three rating experiments were conducted. In the first Stimulus Selection experiment, participants rated short music clips for valence and arousal, and the most evocative clips were selected. In two follow-up Stimulus Validation experiments, two additional groups of participants rated the selected musical and non-musical stimuli, with Experiment 3 confirming that these stimuli work effectively in a noisy environment (within an MRI scanner). The result of these three studies is a set of emotionally evocative, positive and negative musical stimuli, matched for valence and arousal with a subset of previously validated non-musical stimuli.},
author = {Lepping, Rebecca J. and Atchley, Ruth Ann and Savage, Cary R.},
doi = {10.1177/0305735615604509},
issn = {17413087},
journal = {Psychology of Music},
keywords = {arousal,auditory perception/cognition,emotion,experimental aesthetics,negative emotions,valence},
month = {sep},
number = {5},
pages = {1012--1028},
title = {{Development of a validated emotionally provocative musical stimulus set for research}},
url = {http://journals.sagepub.com/doi/10.1177/0305735615604509},
volume = {44},
year = {2016}
}
@article{Cassidy2020,
abstract = {Every 20 seconds, a limb is amputated somewhere in the world due to diabetes. This is a global health problem that requires a global solution. The MICCAI challenge discussed in this paper, which concerns the detection of diabetic foot ulcers, will accelerate the development of innovative healthcare technology to address this unmet medical need. In an effort to improve patient care and reduce the strain on healthcare systems, recent research has focused on the creation of cloud-based detection algorithms that can be consumed as a service by a mobile app that patients (or a carer, partner or family member) could use themselves to monitor their condition and to detect the appearance of a diabetic foot ulcer (DFU). Collaborative work between Manchester Metropolitan University, Lancashire Teaching Hospital and the Manchester University NHS Foundation Trust has created a repository of 4000 DFU images for the purpose of supporting research toward more advanced methods of DFU detection. Based on a joint effort involving the lead scientists of the UK, US, India and New Zealand, this challenge will solicit original work, and promote interactions between researchers and interdisciplinary collaborations. This paper presents a dataset description and analysis, assessment methods, benchmark algorithms and initial evaluation results. It facilitates the challenge by providing useful insights into state-of-the-art and ongoing research.},
archivePrefix = {arXiv},
arxivId = {2004.11853},
author = {Cassidy, Bill and Reeves, Neil D. and Joseph, Pappachan and Gillespie, David and O'Shea, Claire and Rajbhandari, Satyan and Maiya, Arun G. and Frank, Eibe and Boulton, Andrew and Armstrong, David and Najafi, Bijan and Wu, Justina and Yap, Moi Hoon},
eprint = {2004.11853},
journal = {arXiv},
month = {apr},
title = {{DFUC 2020: Analysis towards diabetic foot ulcer detection}},
url = {http://arxiv.org/abs/2004.11853},
year = {2020}
}
@online{KaggleDiabeticRetinopathyDetection,
abstract = {Diabetic retinopathy is becoming a more prevalent disease in diabetic patients nowadays. The surprising fact about the disease is it leaves no symptoms at the beginning stage and the patient can realize the disease only when his vision starts to fall. If the disease is not found at the earliest it leads to a stage where the probability of curing the disease is less. But if we find the disease at that stage, the patient might be in a situation of losing the vision completely. Hence, this paper aims at finding the disease at the earliest possible stage by extracting two features from the retinal image namely Microaneurysms which is found to be the starting symptom showing feature and Hemorrhage which shows symptoms of the other stages. Based on these two features we classify the stage of the disease as normal, beginning, mild and severe using convolutional neural network, a deep learning technique which reduces the burden of manual feature extraction and gives higher accuracy. We also locate the position of these features in the disease affected retinal images to help the doctors offer better medical treatment.},
booktitle = {International Journal of Engineering and Advanced Technology},
doi = {10.35940/ijeat.d7786.049420},
number = {4},
pages = {1022--1026},
title = {{Diabetic Retinopathy Detection}},
url = {https://www.kaggle.com/c/diabetic-retinopathy-detection},
volume = {9},
year = {2020}
}
@article{Bejnordi2017,
abstract = {IMPORTANCE: Application of deep learning algorithms to whole-slide pathology imagescan potentially improve diagnostic accuracy and efficiency. OBJECTIVE: Assess the performance of automated deep learning algorithms at detecting metastases in hematoxylin and eosin-stained tissue sections of lymph nodes of women with breast cancer and compare it with pathologists' diagnoses in a diagnostic setting. DESIGN, SETTING, AND PARTICIPANTS: Researcher challenge competition (CAMELYON16) to develop automated solutions for detecting lymph node metastases (November 2015-November 2016). A training data set of whole-slide images from 2 centers in the Netherlands with (n = 110) and without (n = 160) nodal metastases verified by immunohistochemical staining were provided to challenge participants to build algorithms. Algorithm performance was evaluated in an independent test set of 129 whole-slide images (49 with and 80 without metastases). The same test set of corresponding glass slides was also evaluated by a panel of 11 pathologists with time constraint (WTC) from the Netherlands to ascertain likelihood of nodal metastases for each slide in a flexible 2-hour session, simulating routine pathology workflow, and by 1 pathologist without time constraint (WOTC). EXPOSURES: Deep learning algorithms submitted as part of a challenge competition or pathologist interpretation. MAIN OUTCOMES AND MEASURES: The presence of specific metastatic foci and the absence vs presence of lymph node metastasis in a slide or image using receiver operating characteristic curve analysis. The 11 pathologists participating in the simulation exercise rated their diagnostic confidence as definitely normal, probably normal, equivocal, probably tumor, or definitely tumor. RESULTS: The area under the receiver operating characteristic curve (AUC) for the algorithms ranged from 0.556 to 0.994. The top-performing algorithm achieved a lesion-level, true-positive fraction comparable with that of the pathologist WOTC (72.4{\%} [95{\%} CI, 64.3{\%}-80.4{\%}]) at a mean of 0.0125 false-positives per normal whole-slide image. For the whole-slide image classification task, the best algorithm (AUC, 0.994 [95{\%} CI, 0.983-0.999]) performed significantly better than the pathologists WTC in a diagnostic simulation (mean AUC, 0.810 [range, 0.738-0.884]; P {\textless}.001). The top 5 algorithms had a mean AUC that was comparable with the pathologist interpreting the slides in the absence of time constraints (mean AUC, 0.960 [range, 0.923-0.994] for the top 5 algorithms vs 0.966 [95{\%} CI, 0.927-0.998] for the pathologist WOTC). CONCLUSIONS AND RELEVANCE: In the setting of a challenge competition, some deep learning algorithms achieved better diagnostic performance than a panel of 11 pathologists participating in a simulation exercise designed to mimic routine pathology workflow; algorithm performance was comparable with an expert pathologist interpreting whole-slide images without time constraints. Whether this approach has clinical utility will require evaluation in a clinical setting.},
author = {Bejnordi, Babak Ehteshami and Veta, Mitko and {Van Diest}, Paul Johannes and {Van Ginneken}, Bram and Karssemeijer, Nico and Litjens, Geert and {Van Der Laak}, Jeroen A.W.M. and Hermsen, Meyke and Manson, Quirine F. and Balkenhol, Maschenka and Geessink, Oscar and Stathonikos, Nikolaos and {Van Dijk}, Marcory C.R.F. and Bult, Peter and Beca, Francisco and Beck, Andrew H. and Wang, Dayong and Khosla, Aditya and Gargeya, Rishab and Irshad, Humayun and Zhong, Aoxiao and Dou, Qi and Li, Quanzheng and Chen, Hao and Lin, Huang Jing and Heng, Pheng Ann and Ha{\ss}, Christian and Bruni, Elia and Wong, Quincy and Halici, Ugur and {\"{O}}ner, Mustafa {\"{U}}mit and Cetin-Atalay, Rengul and Berseth, Matt and Khvatkov, Vitali and Vylegzhanin, Alexei and Kraus, Oren and Shaban, Muhammad and Rajpoot, Nasir and Awan, Ruqayya and Sirinukunwattana, Korsuk and Qaiser, Talha and Tsang, Yee Wah and Tellez, David and Annuscheit, Jonas and Hufnagl, Peter and Valkonen, Mira and Kartasalo, Kimmo and Latonen, Leena and Ruusuvuori, Pekka and Liimatainen, Kaisa and Albarqouni, Shadi and Mungal, Bharti and George, Ami and Demirci, Stefanie and Navab, Nassir and Watanabe, Seiryo and Seno, Shigeto and Takenaka, Yoichi and Matsuda, Hideo and Phoulady, Hady Ahmady and Kovalev, Vassili and Kalinovsky, Alexander and Liauchuk, Vitali and Bueno, Gloria and Fernandez-Carrobles, M. Milagro and Serrano, Ismael and Deniz, Oscar and Racoceanu, Daniel and Ven{\^{a}}ncio, Rui},
doi = {10.1001/jama.2017.14585},
issn = {15383598},
journal = {JAMA - Journal of the American Medical Association},
month = {dec},
number = {22},
pages = {2199--2210},
pmid = {29234806},
title = {{Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer}},
url = {http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2017.14585},
volume = {318},
year = {2017}
}
@article{Dey2018DiagnosticCO,
abstract = {Lung cancer is the leading cause of cancer-related death worldwide. Early diagnosis of pulmonary nodules in Computed Tomography (CT) chest scans provides an opportunity for designing effective treatment and making financial and care plans. In this paper, we consider the problem of diagnostic classification between benign and malignant lung nodules in CT images, which aims to learn a direct mapping from 3D images to class labels. To achieve this goal, four two-pathway Convolutional Neural Networks (CNN) are proposed, including a basic 3D CNN, a novel multi-output network, a 3D DenseNet, and an augmented 3D DenseNet with multi-outputs. These four networks are evaluated on the public LIDC-IDRI dataset and outperform most existing methods. In particular, the 3D multi-output DenseNet (MoDenseNet) achieves the state-of-the-art classification accuracy on the task of end-to-end lung nodule diagnosis. In addition, the networks pretrained on the LIDC-IDRI dataset can be further extended to handle smaller datasets using transfer learning. This is demonstrated on our dataset with encouraging prediction accuracy in lung nodule classification.},
archivePrefix = {arXiv},
arxivId = {1803.07192},
author = {Dey, Raunak and Lu, Zhongjie and Hong, Yi},
doi = {10.1109/ISBI.2018.8363687},
eprint = {1803.07192},
isbn = {9781538636367},
issn = {19458452},
journal = {Proceedings - International Symposium on Biomedical Imaging},
keywords = {Deep neural networks,LIDC-IDRI,Lung nodule classification,Multi-output networks},
month = {apr},
pages = {774--778},
publisher = {IEEE},
title = {{Diagnostic classification of lung nodules using 3D neural networks}},
url = {http://arxiv.org/abs/1803.07192 http://dx.doi.org/10.1109/ISBI.2018.8363687 https://ieeexplore.ieee.org/document/8363687/},
volume = {2018-April},
year = {2018}
}
@article{Sorensen2017,
abstract = {This paper presents a brain T1-weighted structural magnetic resonance imaging (MRI) biomarker that combines several individual MRI biomarkers (cortical thickness measurements, volumetric measurements, hippocampal shape, and hippocampal texture). The method was developed, trained, and evaluated using two publicly available reference datasets: a standardized dataset from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and the imaging arm of the Australian Imaging Biomarkers and Lifestyle flagship study of ageing (AIBL). In addition, the method was evaluated by participation in the Computer-Aided Diagnosis of Dementia (CADDementia) challenge. Cross-validation using ADNI and AIBL data resulted in a multi-class classification accuracy of 62.7{\%} for the discrimination of healthy normal controls (NC), subjects with mild cognitive impairment (MCI), and patients with Alzheimer's disease (AD). This performance generalized to the CADDementia challenge where the method, trained using the ADNI and AIBL data, achieved a classification accuracy 63.0{\%}. The obtained classification accuracy resulted in a first place in the challenge, and the method was significantly better (McNemar's test) than the bottom 24 methods out of the total of 29 methods contributed by 15 different teams in the challenge. The method was further investigated with learning curve and feature selection experiments using ADNI and AIBL data. The learning curve experiments suggested that neither more training data nor a more complex classifier would have improved the obtained results. The feature selection experiment showed that both common and uncommon individual MRI biomarkers contributed to the performance; hippocampal volume, ventricular volume, hippocampal texture, and parietal lobe thickness were the most important. This study highlights the need for both subtle, localized measurements and global measurements in order to discriminate NC, MCI, and AD simultaneously based on a single structural MRI scan. It is likely that additional non-structural MRI features are needed to further improve the obtained performance, especially to improve the discrimination between NC and MCI.},
author = {S{\o}rensen, Lauge and Igel, Christian and Pai, Akshay and Balas, Ioana and Anker, Cecilie and Lillholm, Martin and Nielsen, Mads},
doi = {10.1016/j.nicl.2016.11.025},
issn = {22131582},
journal = {NeuroImage: Clinical},
keywords = {Alzheimer's disease,Biomarker,Classification,Machine learning,Mild cognitive impairment,Structural MRI},
pages = {470--482},
title = {{Differential diagnosis of mild cognitive impairment and Alzheimer's disease using structural MRI cortical thickness, hippocampal shape, hippocampal texture, and volumetry}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2213158216302340},
volume = {13},
year = {2017}
}
@online{Mascalchi2018a,
abstract = {Diffusion tensor imaging of magnetic resonance imaging, including diffusion tensor tractography, is a unique tool to visualize and segment the white matter pathways in vivo and one can evaluate the segmented trace quantitatively. Three dimensional visualization of the white matter fibers, such as corticospinal (pyramidal) tracts, with relationship to brain lesions (infarcts, vascular malformations and brain tumors) is extremely helpful for stereotactic radiosurgery, preoperative evaluation and intraoperative navigation. Quantitative measurement of the tract is a very sensitive method to detect differences in the tract in neurodegenerative/neurocognitive/psychiatric patients such as amyotrophic lateral sclerosis, schizophrenia and Alzheimer diseases. Importance of this tool will become more significant in clinical and neuroscience fields in the future.},
author = {Mori, Harushi},
booktitle = {Clinical Neurology},
doi = {10.5692/clinicalneurol.48.945},
issn = {0009918X},
keywords = {Anisotropy,Diffusion tensor,Magnetic resonance imaging,Tractography,White matter},
number = {11},
pages = {945--946},
pmid = {19198126},
title = {{Diffusion tensor imaging}},
url = {https://openneuro.org/datasets/ds001378/versions/00003},
volume = {48},
year = {2008}
}
@incollection{Gonzalez-Diaz2020,
address = {Cham},
author = {Gonzalez-Diaz, Rocio and Stelldinger, Peer and Latecki, Longin Jan},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_645-1},
pages = {1--8},
publisher = {Springer International Publishing},
title = {{Digitization}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}645-1},
year = {2020}
}
@incollection{Zaki2018,
address = {Cham},
author = {Zaki, Mohammed J. and {Meira, Jr}, Wagner},
booktitle = {Data Mining and Analysis},
doi = {10.1017/cbo9780511810114.008},
pages = {183--214},
publisher = {Springer International Publishing},
title = {{Dimensionality Reduction}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}652-1},
year = {2018}
}
@incollection{Kafieh2008,
abstract = {This paper introduces a new method for automatic landmark detection in cephalometry. In first step, some feature points of bony structures are extracted to model the size, rotation, and translation of skull, we propose two different methods for bony structure discrimination in cephalograms. The first method is using bit slices of a gray level image to create a layered version of the same image and the second method is to make use of a SUSAN edge detector and discriminate the pixels with enough thickness as bony structures. Then a neural network is used to classify images according to their geometrical specifications. Using NN for every new image, the possible coordinates of landmarks are estimated. Then a modified ASM is applied to locate the exact location of landmarks.On average the first method can discriminate feature points of bony structures in 78{\%} of cephalograms and the second method can do it in 94{\%} of them. {\textcopyright} 2008 Springer-Verlag.},
author = {Kafieh, Rahele and Sadri, Saeed and Mehri, Alireza and Raji, Hamid},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-540-89985-3_75},
isbn = {3540899847},
issn = {18650929},
keywords = {Active Shape Model,Bit slicing,Learning Vector Quantization,Susan edge detector,cephalometry},
pages = {609--620},
title = {{Discrimination of bony structures in cephalograms for automatic landmark detection}},
url = {http://link.springer.com/10.1007/978-3-540-89985-3{\_}75},
volume = {6 CCIS},
year = {2008}
}
@article{Gonzlez2018DiseaseSA,
abstract = {Rationale: Deep learning is a powerful tool that may allow for improved outcome prediction. Objectives: To determine if deep learning, specifically convolutional neural network (CNN) analysis, could detect and stage chronic obstructive pulmonary disease (COPD) and predict acute respiratory disease (ARD) events and mortality in smokers. Methods: A CNN was trained using computed tomography scans from 7, 983 COPDGene participants and evaluated using 1,000 nonoverlapping COPDGene participants and 1, 672 ECLIPSE participants. Logistic regression (C statistic and the Hosmer-Lemeshow test) was used to assess COPD diagnosis and ARD prediction. Cox regression (C index and the Greenwood-Nam-D'Agnostino test) was used to assess mortality. Measurements and Main Results: In COPDGene, the C statistic for the detection of COPD was 0.856. A total of 51.1{\%} of participants in COPDGene were accurately staged and 74.95{\%} were within one stage. In ECLIPSE, 29.4{\%} were accurately staged and 74.6{\%} were within one stage. In COPDGene and ECLIPSE, the C statistics for ARD events were 0.64 and 0.55, respectively, and the Hosmer-Lemeshow P values were 0.502 and 0.380, respectively, suggesting no evidence of poor calibration. In COPDGene and ECLIPSE, CNN predicted mortality with fair discrimination (C indices, 0.72 and 0.60, respectively), and without evidence of poor calibration (Greenwood-Nam-D'Agnostino P values, 0.307 and 0.331, respectively). Conclusions: A deep-learning approach that uses only computed tomography imaging data can identify those smokers who have COPD and predict who are most likely to have ARD events and those with the highest mortality. At a population level CNN analysis may be a powerful tool for risk assessment.},
author = {Gonzalez, German and Ash, Samuel Y. and Vegas-S{\'{a}}nchez-Ferrero, Gonzalo and Onieva, Jorge Onieva and Rahaghi, Farbod N. and Ross, James C. and D{\'{a}}z, Alejandro and Est{\'{e}}par, Raul San Jos{\'{e}} and Washko, George R.},
doi = {10.1164/rccm.201705-0860OC},
issn = {15354970},
journal = {American Journal of Respiratory and Critical Care Medicine},
keywords = {Artificial intelligence (computer vision systems),Chronic obstructive pulmonary disease,Neural networks,X-ray computed tomography},
month = {jan},
number = {2},
pages = {193--203},
pmid = {28892454},
title = {{Disease staging and prognosis in smokers using deep learning in chest computed tomography}},
url = {http://www.atsjournals.org/doi/10.1164/rccm.201705-0860OC},
volume = {197},
year = {2018}
}
@book{Srikant2020,
abstract = {Graph Analytics is important in different domains: social networks, computer networks, and computational biology to name a few. This paper describes the challenges involved in programming the underlying graph algorithms for graph analytics for distributed systems with CPU, GPU, and multi-GPU machines and how to deal with them. It emphasizes how language abstractions and good compilation can ease programming graph analytics on such platforms without sacrificing implementation efficiency.},
address = {Cham},
author = {Srikant, Y. N.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-36987-3_1},
isbn = {9783030369866},
issn = {16113349},
keywords = {GPU computation,Graph analytics,Graph frameworks,Graph processing languages,Multi-core processors,Parallel algorithms,Social networks},
pages = {3--20},
publisher = {Springer International Publishing},
title = {{Distributed Graph Analytics}},
url = {http://link.springer.com/10.1007/978-3-030-41886-1},
volume = {11969 LNCS},
year = {2020}
}
@book{Carter2020,
address = {Singapore},
author = {Carter, Susan and Guerin, Cally and Aitchison, Claire},
doi = {10.1007/978-981-15-1808-9},
isbn = {978-981-15-1807-2},
publisher = {Springer Singapore},
title = {{Doctoral Writing}},
url = {http://link.springer.com/10.1007/978-981-15-1808-9},
year = {2020}
}
@article{Wachinger2016,
abstract = {With the increasing prevalence of Alzheimer's disease, research focuses on the early computer-aided diagnosis of dementia with the goal to understand the disease process, determine risk and preserving factors, and explore preventive therapies. By now, large amounts of data from multi-site studies have been made available for developing, training, and evaluating automated classifiers. Yet, their translation to the clinic remains challenging, in part due to their limited generalizability across different datasets. In this work, we describe a compact classification approach that mitigates overfitting by regularizing the multinomial regression with the mixed ℓ1/ℓ2 norm. We combine volume, thickness, and anatomical shape features from MRI scans to characterize neuroanatomy for the three-class classification of Alzheimer's disease, mild cognitive impairment and healthy controls. We demonstrate high classification accuracy via independent evaluation within the scope of the CADDementia challenge. We, furthermore, demonstrate that variations between source and target datasets can substantially influence classification accuracy. The main contribution of this work addresses this problem by proposing an approach for supervised domain adaptation based on instance weighting. Integration of this method into our classifier allows us to assess different strategies for domain adaptation. Our results demonstrate (i) that training on only the target training set yields better results than the na{\"{i}}ve combination (union) of source and target training sets, and (ii) that domain adaptation with instance weighting yields the best classification results, especially if only a small training component of the target dataset is available. These insights imply that successful deployment of systems for computer-aided diagnostics to the clinic depends not only on accurate classifiers that avoid overfitting, but also on a dedicated domain adaptation strategy.},
author = {Wachinger, Christian and Reuter, Martin},
doi = {10.1016/j.neuroimage.2016.05.053},
issn = {10959572},
journal = {NeuroImage},
keywords = {Alzheimer's disease,Classification,Computer-aided diagnosis,Domain adaptation},
month = {oct},
pages = {470--479},
title = {{Domain adaptation for Alzheimer's disease diagnostics}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811916301732},
volume = {139},
year = {2016}
}
@incollection{Patel2020,
address = {Cham},
author = {Patel, Vishal M. and Nguyen, Hien Van},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_819-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Domain Adaptation Using Dictionaries}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}819-1},
year = {2020}
}
@misc{GrandChallengeDRIVE,
title = {{DRIVE: Digital Retinal Images for Vessel Extraction}}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets. {\textcopyright} 2014 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Deep learning,Model combination,Neural networks,Regularization},
pages = {1929--1958},
title = {{Dropout: A simple way to prevent neural networks from overfitting}},
url = {http://jmlr.org/papers/v15/srivastava14a.html},
volume = {15},
year = {2014}
}
@inproceedings{Chen2017,
abstract = {In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64 × 4d) with 26{\%} smaller model size, 25{\%} less computational cost and 8{\%} lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.},
archivePrefix = {arXiv},
arxivId = {1707.01629},
author = {Chen, Yunpeng and Li, Jianan and Xiao, Huaxin and Jin, Xiaojie and Yan, Shuicheng and Feng, Jiashi},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1707.01629},
issn = {10495258},
pages = {4468--4476},
title = {{Dual path networks}},
volume = {2017-Decem},
year = {2017}
}
@article{Madhyastha2015,
abstract = {Consistent spatial patterns of coherent activity, representing large-scale networks, have been reliably identified in multiple populations. Most often, these studies have examined "stationary" connectivity. However, there is a growing recognition that there is a wealth of information in the time-varying dynamics of networks which has neural underpinnings, which changes with age and disease and that supports behavior. Using factor analysis of overlapping sliding windows across 25 participants with Parkinson disease (PD) and 21 controls (ages 41-86), we identify factors describing the covarying correlations of regions (dynamic connectivity) within attention networks and the default mode network, during two baseline resting-state and task runs. Cortical regions that support attention networks are affected early in PD, motivating the potential utility of dynamic connectivity as a sensitive way to characterize physiological disruption to these networks. We show that measures of dynamic connectivity are more reliable than comparable measures of stationary connectivity. Factors in the dorsal attention network (DAN) and fronto-parietal task control network, obtained at rest, are consistently related to the alerting and orienting reaction time effects in the subsequent Attention Network Task. In addition, the same relationship between the same DAN factor and the alerting effect was present during tasks. Although reliable, dynamic connectivity was not invariant, and changes between factor scores across sessions were related to changes in accuracy. In summary, patterns of time-varying correlations among nodes in an intrinsic network have a stability that has functional relevance.},
author = {Madhyastha, Tara M. and Askren, Mary K. and Boord, Peter and Grabowski, Thomas J.},
doi = {10.1089/brain.2014.0248},
issn = {21580022},
journal = {Brain Connectivity},
keywords = {Parkinson disease,attention network task,dynamic functional connectivity,resting-state connectivity,task connectivity},
month = {feb},
number = {1},
pages = {45--59},
title = {{Dynamic connectivity at rest predicts attention task performance}},
url = {http://www.liebertpub.com/doi/10.1089/brain.2014.0248},
volume = {5},
year = {2015}
}
@inproceedings{Simonovsky2017,
abstract = {A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches.},
annote = {{\_}eprint: 1704.02901},
archivePrefix = {arXiv},
arxivId = {1704.02901},
author = {Simonovsky, Martin and Komodakis, Nikos},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.11},
eprint = {1704.02901},
isbn = {9781538604571},
pages = {29--38},
title = {{Dynamic edge-conditioned filters in convolutional neural networks on graphs}},
url = {http://arxiv.org/abs/1704.02901},
volume = {2017-Janua},
year = {2017}
}
@article{Wang2018,
abstract = {Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information, so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds, including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks, including ModelNet40, ShapeNetPart, and S3DIS.},
annote = {{\_}eprint: 1801.07829},
archivePrefix = {arXiv},
arxivId = {1801.07829},
author = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E and Bronstein, Michael M and Solomon, Justin M},
doi = {10.1145/3326362},
eprint = {1801.07829},
issn = {15577368},
journal = {ACM Transactions on Graphics},
keywords = {Classification,Point cloud,Segmentation},
number = {5},
title = {{Dynamic graph Cnn for learning on point clouds}},
url = {http://arxiv.org/abs/1801.07829},
volume = {38},
year = {2019}
}
@incollection{Blanchini2015,
abstract = {In this section a jump back in the history of control is made and we consider problems which had been theoretically faced in the early 70s, and thereafter almost abandoned. The main reason is that the computational effort necessary to practically implement these techniques was not suitable for the computer technology of the time. Today, the situation is different, and many authors are reconsidering the approach. In this section, the main focus will be put on discrete-time systems, although it will also be shown, given the existing relation between continuous- and discrete-time invariant sets presented in Lemma 4.26, how the proposed algorithms can be used to deal with continuous-time systems as well.},
address = {Cham},
author = {Blanchini, Franco and Miani, Stefano},
booktitle = {Systems and Control: Foundations and Applications},
doi = {10.1007/978-3-319-17933-9_5},
issn = {23249757},
number = {9783319179322},
pages = {193--234},
publisher = {Springer International Publishing},
title = {{Dynamic programming}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}690-1},
year = {2015}
}
@inproceedings{Cai2017,
abstract = {Techniques for automatically designing deep neural network architectures such as reinforcement learning based approaches have recently shown promising results. However, their success is based on vast computational resources (e.g. hundreds of GPUs), making them difficult to be widely used. A noticeable limitation is that they still design and train each network from scratch during the exploration of the architecture space, which is highly inefficient. In this paper, we propose a new framework toward efficient architecture search by exploring the architecture space based on the current network and reusing its weights. We employ a reinforcement learning agent as the meta-controller, whose action is to grow the network depth or layer width with function-preserving transformations. As such, the previously validated networks can be reused for further exploration, thus saves a large amount of computational cost. We apply our method to explore the architecture space of the plain convolutional neural networks (no skip-connections, branching etc.) on image benchmark datasets (CIFAR-10, SVHN) with restricted computational resources (5 GPUs). Our method can design highly competitive networks that outperform existing networks using the same design scheme. On CIFAR-10, our model without skip-connections achieves 4.23{\%} test error rate, exceeding a vast majority of modern architectures and approaching DenseNet. Furthermore, by applying our method to explore the DenseNet architecture space, we are able to achieve more accurate networks with fewer parameters.},
archivePrefix = {arXiv},
arxivId = {1707.04873},
author = {Cai, Han and Chen, Tianyao and Zhang, Weinan and Yu, Yong and Wang, Jun},
booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
eprint = {1707.04873},
isbn = {9781577358008},
pages = {2787--2794},
title = {{Efficient architecture search by network transformation}},
year = {2018}
}
@article{Kamnitsas2016,
abstract = {We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network's soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumours, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.},
annote = {{\_}eprint: 1603.05959},
archivePrefix = {arXiv},
arxivId = {1603.05959},
author = {Kamnitsas, Konstantinos and Ledig, Christian and Newcombe, Virginia F.J. and Simpson, Joanna P. and Kane, Andrew D. and Menon, David K. and Rueckert, Daniel and Glocker, Ben},
doi = {10.1016/j.media.2016.10.004},
eprint = {1603.05959},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {3D convolutional neural network,Brain lesions,Deep learning,Fully connected CRF,Segmentation},
pages = {61--78},
pmid = {27865153},
title = {{Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation}},
url = {http://arxiv.org/abs/1603.05959},
volume = {36},
year = {2017}
}
@inproceedings{Pham2018,
abstract = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. ENAS constructs a large computational graph, where each subgraph represents a neural network architecture, hence forcing all architectures to share their parameters. A controller is trained with policy gradient to search for a subgraph that maximizes the expected reward on a validation set. Meanwhile a model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Sharing parameters among child models allows ENAS to deliver strong empirical performances, whilst using much fewer GPU-hours than existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On Penn Treebank, ENAS discovers a novel architecture that achieves a test perplexity of 56.3, on par with the existing state-of-the-art among all methods without post-training processing. On CIFAR-10, ENAS finds a novel architecture that achieves 2.89{\%} test error, which is on par with the 2.65{\%} test error of NASNet (Zoph et al., 2018).},
archivePrefix = {arXiv},
arxivId = {1802.03268},
author = {Pham, Hieu and Guan, Melody Y and Zoph, Barret and Le, Quoc V and Dean, Jeff},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1802.03268},
isbn = {9781510867963},
pages = {6522--6531},
title = {{Efficient Neural Architecture Search via parameter Sharing}},
volume = {9},
year = {2018}
}
@incollection{Yamamoto2020,
address = {Cham},
author = {Yamamoto, Masanobu},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_873-1},
pages = {1--7},
publisher = {Springer International Publishing},
title = {{Ego-Motion and EPI Analysis}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}873-1},
year = {2020}
}
@incollection{Takahashi2020,
address = {Cham},
author = {Takahashi, Tomokazu and Murase, Hiroshi},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_711-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Eigenspace Methods}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}711-1},
year = {2020}
}
@misc{docker,
author = {Johnston, Scott},
keywords = {docker},
title = {{Empowering App Development for Developers | Docker}},
url = {https://www.docker.com/}
}
@article{Ali2019,
abstract = {Endoscopic artifacts are a core challenge in facilitating the diagnosis and treatment of diseases in hollow organs. Precise detection of specific artifacts like pixel saturations, motion blur, specular reflections, bubbles and debris is essential for high-quality frame restoration and is crucial for realizing reliable computer-assisted tools for improved patient care. At present most videos in endoscopy are currently not analyzed due to the abundant presence of multi-class artifacts in video frames. Through the endoscopic artifact detection (EAD 2019) challenge, we address this key bottleneck problem by solving the accurate identification and localization of endoscopic frame artifacts to enable further key quantitative analysis of unusable video frames such as mosaicking and 3D reconstruction which is crucial for delivering improved patient care. This paper summarizes the challenge tasks and describes the dataset and evaluation criteria established in the EAD 2019 challenge.},
archivePrefix = {arXiv},
arxivId = {1905.03209},
author = {Ali, Sharib and Zhou, Felix and Daul, Christian and Braden, Barbara and Bailey, Adam and Realdon, Stefano and East, James and Wagni{\`{e}}res, Georges and Loschenov, Victor and Grisan, Enrico and Blondel, Walter and Rittscher, Jens},
doi = {10.17632/C7FJBXCGJ9.1},
eprint = {1905.03209},
month = {may},
title = {{Endoscopy artifact detection (EAD 2019) challenge dataset}},
url = {http://arxiv.org/abs/1905.03209{\%}0Ahttp://dx.doi.org/10.17632/C7FJBXCGJ9.1},
year = {2019}
}
@misc{Levine2016,
abstract = {Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control. In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately? To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors. The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method. We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.},
author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
booktitle = {Journal of Machine Learning Research},
issn = {15337928},
keywords = {Neural networks,Optimal control,Reinforcement learning,Vision},
title = {{End-to-end training of deep visuomotor policies}},
year = {2016}
}
@book{Wallwork2016c,
address = {Cham},
author = {Wallwork, Adrian},
doi = {10.1007/978-3-319-26435-6},
isbn = {978-3-319-26433-2},
publisher = {Springer International Publishing},
title = {{English for Academic Correspondence}},
url = {http://link.springer.com/10.1007/978-3-319-26435-6},
year = {2016}
}
@book{Wallwork2019,
address = {Cham},
author = {Wallwork, Adrian},
doi = {10.1007/978-3-030-11090-1},
isbn = {978-3-030-11089-5},
publisher = {Springer International Publishing},
series = {English for Academic Research},
title = {{English for Academic CVs, Resumes, and Online Profiles}},
url = {http://link.springer.com/10.1007/978-3-030-11090-1},
year = {2019}
}
@book{Wallwork2016a,
address = {Cham},
author = {Wallwork, Adrian},
doi = {10.1007/978-3-319-32687-0},
isbn = {978-3-319-32685-6},
publisher = {Springer International Publishing},
title = {{English for Academic Research: A Guide for Teachers}},
url = {http://link.springer.com/10.1007/978-3-319-32687-0},
year = {2016}
}
@book{Wallwork2013a,
address = {Boston, MA},
author = {Wallwork, Adrian},
doi = {10.1007/978-1-4614-4289-9},
isbn = {978-1-4614-4288-2},
publisher = {Springer US},
title = {{English for Academic Research: Grammar Exercises}},
url = {http://link.springer.com/10.1007/978-1-4614-4289-9},
year = {2013}
}
@book{Wallwork2013b,
address = {Boston, MA},
author = {Wallwork, Adrian},
doi = {10.1007/978-1-4614-4268-4},
isbn = {978-1-4614-4267-7},
publisher = {Springer US},
title = {{English for Academic Research: Vocabulary Exercises}},
url = {http://link.springer.com/10.1007/978-1-4614-4268-4},
year = {2013}
}
@book{Wallwork2016d,
address = {Cham},
author = {Wallwork, Adrian},
doi = {10.1007/978-3-319-28734-8},
isbn = {978-3-319-28732-4},
publisher = {Springer International Publishing},
title = {{English for Interacting on Campus}},
url = {http://link.springer.com/10.1007/978-3-319-28734-8},
year = {2016}
}
@book{Wallwork2016,
address = {Cham},
author = {Wallwork, Adrian},
doi = {10.1007/978-3-319-26330-4},
isbn = {978-3-319-26328-1},
publisher = {Springer International Publishing},
title = {{English for Presentations at International Conferences}},
url = {http://link.springer.com/10.1007/978-3-319-26330-4},
year = {2016}
}
@book{Wallwork2013,
address = {Boston, MA},
author = {Wallwork, Adrian},
doi = {10.1007/978-1-4614-1593-0},
isbn = {978-1-4614-1592-3},
publisher = {Springer US},
title = {{English for Research: Grammar, Usage and Style}},
url = {http://link.springer.com/10.1007/978-1-4614-1593-0},
year = {2013}
}
@book{Wallwork2016b,
address = {Cham},
author = {Wallwork, Adrian},
doi = {10.1007/978-3-319-26094-5},
isbn = {978-3-319-26092-1},
publisher = {Springer International Publishing},
title = {{English for Writing Research Papers}},
url = {http://link.springer.com/10.1007/978-3-319-26094-5},
year = {2016}
}
@online{ENIGMACerebellum2017,
title = {{ENIGMA Cerebellum | MICCAI 2017 Workshop {\&} Challenge}},
url = {https://my.vanderbilt.edu/enigmacerebellum/},
urldate = {2020-05-12}
}
@article{Bawa2020,
abstract = {In this work, we take aim towards increasing the effectiveness of surgical assistant robots. We intended to make assistant robots safer by making them aware about the actions of surgeon, so it can take appropriate assisting actions. In other words, we aim to solve the problem of surgeon action detection in endoscopic videos. To this, we introduce a challenging dataset for surgeon action detection in real world endoscopic videos. Action classes are picked based on the feedback of surgeons and annotated by medical professional. Given a video frame, we draw bounding box around surgical tool which is performing action and label it with action label. Finally, we present a frame-level action detection baseline model based on recent advances in object detection. Results on our new dataset show that our presented dataset provides enough interesting challenges for future method and it can serve as strong benchmark corresponding research in surgeon action detection in endoscopic videos.},
archivePrefix = {arXiv},
arxivId = {2006.07164},
author = {Bawa, Vivek Singh and Singh, Gurkirt and Kaping'a, Francis and Skarga-Bandurova, Inna and Leporini, Alice and Landolfo, Carmela and Stabile, Armando and Setti, Francesco and Muradore, Riccardo and Oleari, Elettra and Cuzzolin, Fabio},
eprint = {2006.07164},
journal = {arXiv},
keywords = {Action detection,Endoscopic video,Prostatectomy,Surgeon action detection,Surgical robotics},
month = {jun},
title = {{ESAD: Endoscopic surgeon action detection dataset}},
url = {http://arxiv.org/abs/2006.07164},
year = {2020}
}
@inproceedings{Klokov2017,
abstract = {We present a new deep learning architecture (called Kdnetwork) that is designed for 3D model recognition tasks and works with unstructured point clouds. The new architecture performs multiplicative transformations and shares parameters of these transformations according to the subdivisions of the point clouds imposed onto them by kdtrees. Unlike the currently dominant convolutional architectures that usually require rasterization on uniform twodimensional or three-dimensional grids, Kd-networks do not rely on such grids in any way and therefore avoid poor scaling behavior. In a series of experiments with popular shape recognition benchmarks, Kd-networks demonstrate competitive performance in a number of shape recognition tasks such as shape classification, shape retrieval and shape part segmentation.},
annote = {{\_}eprint: 1704.01222},
archivePrefix = {arXiv},
arxivId = {1704.01222},
author = {Klokov, Roman and Lempitsky, Victor},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.99},
eprint = {1704.01222},
isbn = {9781538610329},
issn = {15505499},
pages = {863--872},
title = {{Escape from Cells: Deep Kd-Networks for the Recognition of 3D Point Cloud Models}},
url = {http://arxiv.org/abs/1704.01222},
volume = {2017-Octob},
year = {2017}
}
@article{Kalpathy-Cramer2015,
abstract = {Medical image retrieval and classification have been extremely active research topics over the past 15 years. Within the ImageCLEF benchmark in medical image retrieval and classification, a standard test bed was created that allows researchers to compare their approaches and ideas on increasingly large and varied data sets including generated ground truth. This article describes the lessons learned in ten evaluation campaigns. A detailed analysis of the data also highlights the value of the resources created.},
author = {Kalpathy-Cramer, Jayashree and de Herrera, Alba Garc{\'{i}}a Seco and Demner-Fushman, Dina and Antani, Sameer and Bedrick, Steven and M{\"{u}}ller, Henning},
doi = {10.1016/j.compmedimag.2014.03.004},
issn = {18790771},
journal = {Computerized Medical Imaging and Graphics},
keywords = {Biomedical literature,Content-based retrieval,Image retrieval,Multimodal medical retrieval,Text-based image retrieval},
pmid = {24746250},
title = {{Evaluating performance of biomedical image retrieval systems-An overview of the medical image retrieval task at ImageCLEF 2004-2013}},
year = {2015}
}
@article{Zhao2009,
abstract = {Purpose: To evaluate the variability of tumor unidimensional, bidimensional, and volumetric measurements on same-day repeat computed tomographic (CT) scans in patients with non-small cell lung cancer. Materials and Methods: This HIPAA-compliant study was approved by the institutional review board, with informed patient consent. Thirty-two patients with non-small cell lung cancer, each of whom underwent two CT scans of the chest within 15 minutes by using the same imaging protocol, were included in this study. Three radiologists independently measured the two greatest diameters of each lesion on both scans and, during another session, measured the same tumors on the first scan. In a separate analysis, computer software was applied to assist in the calculation of the two greatest diameters and the volume of each lesion on both scans. Concordance correlation coefficients (CCCs) and Bland-Altman plots were used to assess the agreements between the measurements of the two repeat scans (reproducibility) and between the two repeat readings of the same scan (repeatability). Results: The reproducibility and repeatability of the three radiologists' measurements were high (all CCCs, ≥0.96). The reproducibility of the computer-aided measurements was even higher (all CCCs, 1.00). The 95{\%} limits of agreements for the computer-aided unidimensional, bidimensional, and volumetric measurements on two repeat scans were (-7.3{\%}, 6.2{\%}), (-17.6{\%}, 19.8{\%}), and (-12.1{\%}, 13.4{\%}), respectively. Conclusion: Chest CT scans are well reproducible. Changes in unidimensional lesion size of 8{\%} or greater exceed the measurement variability of the computer method and can be considered significant when estimating the outcome of therapy in a patient. {\textcopyright} RSNA, 2009.},
author = {Zhao, Binsheng and James, Leonard P. and Moskowitz, Chaya S. and Guo, Pingzhen and Ginsberg, Michelle S. and Lefkowitz, Robert A. and Qin, Yilin and Riely, Gregory J. and Kris, Mark G. and Schwartz, Lawrence H.},
doi = {10.1148/radiol.2522081593},
issn = {00338419},
journal = {Radiology},
month = {jul},
number = {1},
pages = {263--272},
title = {{Evaluating variability in tumor measurements from same-day repeat CT scans of patients with non-small cell lung cancer}},
url = {http://pubs.rsna.org/doi/10.1148/radiol.2522081593},
volume = {252},
year = {2009}
}
@article{Rueda2014,
abstract = {This paper presents the evaluation results of the methods submitted to Challenge US: Biometric Measurements from Fetal Ultrasound Images, a segmentation challenge held at the IEEE International Symposium on Biomedical Imaging 2012. The challenge was set to compare and evaluate current fetal ultrasound image segmentation methods. It consisted of automatically segmenting fetal anatomical structures to measure standard obstetric biometric parameters, from 2D fetal ultrasound images taken on fetuses at different gestational ages (21 weeks, 28 weeks, and 33 weeks) and with varying image quality to reflect data encountered in real clinical environments. Four independent sub-challenges were proposed, according to the objects of interest measured in clinical practice: abdomen, head, femur, and whole fetus. Five teams participated in the head sub-challenge and two teams in the femur sub-challenge, including one team who tackled both. Nobody attempted the abdomen and whole fetus sub-challenges. The challenge goals were two-fold and the participants were asked to submit the segmentation results as well as the measurements derived from the segmented objects. Extensive quantitative (region-based, distance-based, and Bland-Altman measurements) and qualitative evaluation was performed to compare the results from a representative selection of current methods submitted to the challenge. Several experts (three for the head sub-challenge and two for the femur sub-challenge), with different degrees of expertise, manually delineated the objects of interest to define the ground truth used within the evaluation framework. For the head sub-challenge, several groups produced results that could be potentially used in clinical settings, with comparable performance to manual delineations. The femur sub-challenge had inferior performance to the head sub-challenge due to the fact that it is a harder segmentation problem and that the techniques presented relied more on the femur's appearance. {\textcopyright} 1982-2012 IEEE.},
author = {Rueda, Sylvia and Fathima, Sana and Knight, Caroline L. and Yaqub, Mohammad and Papageorghiou, Aris T. and Rahmatullah, Bahbibi and Foi, Alessandro and Maggioni, Matteo and Pepe, Antonietta and Tohka, Jussi and Stebbing, Richard V. and McManigle, John E. and Ciurte, Anca and Bresson, Xavier and Cuadra, Meritxell Bach and Sun, Changming and Ponomarev, Gennady V. and Gelfand, Mikhail S. and Kazanov, Marat D. and Wang, Ching Wei and Chen, Hsiang Chou and Peng, Chun Wei and Hung, Chu Mei and Noble, J. Alison},
doi = {10.1109/TMI.2013.2276943},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Challenge,Ultrasound (US),evaluation,fetal biometry,image quality,segmentation},
title = {{Evaluation and comparison of current fetal ultrasound image segmentation methods for biometric measurements: A grand challenge}},
year = {2014}
}
@article{Hameeteman2011,
abstract = {This paper describes an evaluation framework that allows a standardized and objective quantitative comparison of carotid artery lumen segmentation and stenosis grading algorithms. We describe the data repository comprising 56 multi-center, multi-vendor CTA datasets, their acquisition, the creation of the reference standard and the evaluation measures. This framework has been introduced at the MICCAI 2009 workshop 3D Segmentation in the Clinic: A Grand Challenge III, and we compare the results of eight teams that participated. These results show that automated segmentation of the vessel lumen is possible with a precision that is comparable to manual annotation. The framework is open for new submissions through the website http://cls2009.bigr.nl. {\textcopyright} 2011 Elsevier B.V.},
author = {Hameeteman, K. and Zuluaga, M. A. and Freiman, M. and Joskowicz, L. and Cuisenaire, O. and Valencia, L. Fl{\'{o}}rez and G{\"{u}}ls{\"{u}}n, M. A. and Krissian, K. and Mille, J. and Wong, W. C.K. and Orkisz, M. and Tek, H. and Hoyos, M. Hern{\'{a}}ndez and Benmansour, F. and Chung, A. C.S. and Rozie, S. and van Gils, M. and van den Borne, L. and Sosna, J. and Berman, P. and Cohen, N. and Douek, P. C. and S{\'{a}}nchez, I. and Aissat, M. and Schaap, M. and Metz, C. T. and Krestin, G. P. and van der Lugt, A. and Niessen, W. J. and {Van Walsum}, T.},
doi = {10.1016/j.media.2011.02.004},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {CTA,Carotid,Evaluation framework,Lumen segmentation,Stenosis grading},
month = {aug},
number = {4},
pages = {477--488},
title = {{Evaluation framework for carotid bifurcation lumen segmentation and stenosis grading}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841511000260},
volume = {15},
year = {2011}
}
@article{Isgum2015,
abstract = {A number of algorithms for brain segmentation in preterm born infants have been published, but a reliable comparison of their performance is lacking. The NeoBrainS12 study (http://neobrains12.isi.uu.nl), providing three different image sets of preterm born infants, was set up to provide such a comparison. These sets are (i) axial scans acquired at 40. weeks corrected age, (ii) coronal scans acquired at 30. weeks corrected age and (iii) coronal scans acquired at 40. weeks corrected age. Each of these three sets consists of three T1- and T2-weighted MR images of the brain acquired with a 3T MRI scanner. The task was to segment cortical grey matter, non-myelinated and myelinated white matter, brainstem, basal ganglia and thalami, cerebellum, and cerebrospinal fluid in the ventricles and in the extracerebral space separately. Any team could upload the results and all segmentations were evaluated in the same way. This paper presents the results of eight participating teams. The results demonstrate that the participating methods were able to segment all tissue classes well, except myelinated white matter.},
author = {I{\v{s}}gum, Ivana and Benders, Manon J.N.L. and Avants, Brian and Cardoso, M. Jorge and Counsell, Serena J. and Gomez, Elda Fischi and Gui, Laura and Huppi, Petra S. and Kersbergen, Karina J. and Makropoulos, Antonios and Melbourne, Andrew and Moeskops, Pim and Mol, Christian P. and Kuklisova-Murgasova, Maria and Rueckert, Daniel and Schnabel, Julia A. and Srhoj-Egekher, Vedran and Wu, Jue and Wang, Siying and de Vries, Linda S. and Viergever, Max A.},
doi = {10.1016/j.media.2014.11.001},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Brain segmentation,MRI,Neonatal brain,Segmentation comparison,Segmentation evaluation},
month = {feb},
number = {1},
pages = {135--151},
title = {{Evaluation of automatic neonatal brain segmentation algorithms: The NeoBrainS12 challenge}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841514001583},
volume = {20},
year = {2015}
}
@article{Litjens2014,
abstract = {Prostate MRI image segmentation has been an area of intense research due to the increased use of MRI as a modality for the clinical workup of prostate cancer. Segmentation is useful for various tasks, e.g. to accurately localize prostate boundaries for radiotherapy or to initialize multi-modal registration algorithms. In the past, it has been difficult for research groups to evaluate prostate segmentation algorithms on multi-center, multi-vendor and multi-protocol data. Especially because we are dealing with MR images, image appearance, resolution and the presence of artifacts are affected by differences in scanners and/or protocols, which in turn can have a large influence on algorithm accuracy. The Prostate MR Image Segmentation (PROMISE12) challenge was setup to allow a fair and meaningful comparison of segmentation methods on the basis of performance and robustness. In this work we will discuss the initial results of the online PROMISE12 challenge, and the results obtained in the live challenge workshop hosted by the MICCAI2012 conference. In the challenge, 100 prostate MR cases from 4 different centers were included, with differences in scanner manufacturer, field strength and protocol. A total of 11 teams from academic research groups and industry participated. Algorithms showed a wide variety in methods and implementation, including active appearance models, atlas registration and level sets. Evaluation was performed using boundary and volume based metrics which were combined into a single score relating the metrics to human expert performance. The winners of the challenge where the algorithms by teams Imorphics and ScrAutoProstate, with scores of 85.72 and 84.29 overall. Both algorithms where significantly better than all other algorithms in the challenge (p {\textless} 0.05) and had an efficient implementation with a run time of 8. min and 3. s per case respectively. Overall, active appearance model based approaches seemed to outperform other approaches like multi-atlas registration, both on accuracy and computation time. Although average algorithm performance was good to excellent and the Imorphics algorithm outperformed the second observer on average, we showed that algorithm combination might lead to further improvement, indicating that optimal performance for prostate segmentation is not yet obtained. All results are available online at http://promise12.grand-challenge.org/. {\textcopyright} 2013 Elsevier B.V.},
author = {Litjens, Geert and Toth, Robert and van de Ven, Wendy and Hoeks, Caroline and Kerkstra, Sjoerd and van Ginneken, Bram and Vincent, Graham and Guillard, Gwenael and Birbeck, Neil and Zhang, Jindang and Strand, Robin and Malmberg, Filip and Ou, Yangming and Davatzikos, Christos and Kirschner, Matthias and Jung, Florian and Yuan, Jing and Qiu, Wu and Gao, Qinquan and Edwards, Philip Eddie and Maan, Bianca and van der Heijden, Ferdinand and Ghose, Soumya and Mitra, Jhimli and Dowling, Jason and Barratt, Dean and Huisman, Henkjan and Madabhushi, Anant},
doi = {10.1016/j.media.2013.12.002},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Challenge,MRI,Prostate,Segmentation},
title = {{Evaluation of prostate segmentation algorithms for MRI: The PROMISE12 challenge}},
year = {2014}
}
@article{Raudaschl2017,
abstract = {Purpose: Automated delineation of structures and organs is a key step in medical imaging. However, due to the large number and diversity of structures and the large variety of segmentation algorithms, a consensus is lacking as to which automated segmentation method works best for certain applications. Segmentation challenges are a good approach for unbiased evaluation and comparison of segmentation algorithms. Methods: In this work, we describe and present the results of the Head and Neck Auto-Segmentation Challenge 2015, a satellite event at the Medical Image Computing and Computer Assisted Interventions (MICCAI) 2015 conference. Six teams participated in a challenge to segment nine structures in the head and neck region of CT images: brainstem, mandible, chiasm, bilateral optic nerves, bilateral parotid glands, and bilateral submandibular glands. Results: This paper presents the quantitative results of this challenge using multiple established error metrics and a well-defined ranking system. The strengths and weaknesses of the different auto-segmentation approaches are analyzed and discussed. Conclusions: The Head and Neck Auto-Segmentation Challenge 2015 was a good opportunity to assess the current state-of-the-art in segmentation of organs at risk for radiotherapy treatment. Participating teams had the possibility to compare their approaches to other methods under unbiased and standardized circumstances. The results demonstrate a clear tendency toward more general purpose and fewer structure-specific segmentation algorithms.},
author = {Raudaschl, Patrik F. and Zaffino, Paolo and Sharp, Gregory C. and Spadea, Maria Francesca and Chen, Antong and Dawant, Benoit M. and Albrecht, Thomas and Gass, Tobias and Langguth, Christoph and Luthi, Marcel and Jung, Florian and Knapp, Oliver and Wesarg, Stefan and Mannion-Haworth, Richard and Bowes, Mike and Ashman, Annaliese and Guillard, Gwenael and Brett, Alan and Vincent, Graham and Orbes-Arteaga, Mauricio and Cardenas-Pena, David and Castellanos-Dominguez, German and Aghdasi, Nava and Li, Yangming and Berens, Angelique and Moe, Kris and Hannaford, Blake and Schubert, Rainer and Fritscher, Karl D.},
doi = {10.1002/mp.12197},
issn = {00942405},
journal = {Medical Physics},
keywords = {Atlas-based segmentation,Automated segmentation,Model-based segmentation,Segmentation challenge},
month = {may},
number = {5},
pages = {2020--2036},
title = {{Evaluation of segmentation methods on head and neck CT: Auto-segmentation challenge 2015}},
url = {http://doi.wiley.com/10.1002/mp.12197},
volume = {44},
year = {2017}
}
@book{Johannsen2011,
abstract = {This paper presents a novel hybrid approach for solving the Container Loading (CL) problem based on the combination of Integer$\backslash$n  Linear Programming (ILP) and Genetic Algorithms (GAs). More precisely, a GA engine works as a generator of reduced instances$\backslash$n  for the original CL problem, which are formulated as ILP models. These instances, in turn, are solved by an exact optimization$\backslash$n  technique (solver), and the performance measures accomplished by the respective models are interpreted as fitness values by$\backslash$n  the genetic algorithm, thus guiding its evolutionary process. Computational experiments performed on standard benchmark problems,$\backslash$n  as well as a practical case study developed in a metallurgic factory, are also reported and discussed here in a manner as$\backslash$n  to testify the potentialities behind the novel approach.},
address = {Cham},
author = {Johannsen, Daniel},
doi = {10.1142/9789814282673_0003},
editor = {Paquete, Lu{\'{i}}s and Zarges, Christine},
isbn = {978-3-030-43679-7},
pages = {53--99},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Evolutionary Computation in Combinatorial Optimization}},
url = {http://link.springer.com/10.1007/978-3-030-43680-3},
volume = {12102},
year = {2011}
}
@inproceedings{Koutnik2013,
abstract = {The idea of using evolutionary computation to train artificial neural networks, or neuroevolution (NE), for reinforcement learning (RL) tasks has now been around for over 20 years. However, as RL tasks become more challenging, the networks required become larger, so do their genomes. But, scaling NE to large nets (i.e. tens of thousands of weights) is infeasible using direct encodings that map genes one-to-one to network components. In this paper, we scale-up our "compressed" network encoding where network weight matrices are represented indirectly as a set of Fourier-type coefficients, to tasks that require very-large networks due to the high-dimensionality of their input space. The approach is demonstrated successfully on two reinforcement learning tasks in which the control networks receive visual input: (1) a vision-based version of the octopus control task requiring networks with over 3 thousand weights, and (2) a version of the TORCS driving game where networks with over 1 million weights are evolved to drive a car around a track using video images from the driver's perspective. Copyright {\textcopyright} 2013 ACM.},
author = {Koutn{\'{i}}k, Jan and Cuccu, Giuseppe and Schmidhuber, J{\"{u}}rgen and Gomez, Faustino},
booktitle = {GECCO 2013 - Proceedings of the 2013 Genetic and Evolutionary Computation Conference},
doi = {10.1145/2463372.2463509},
isbn = {9781450319638},
keywords = {Games,Indirect encodings,Neuroevolution,Reinforcement learning,Vision-based TORCS},
title = {{Evolving large-scale neural networks for vision-based reinforcement learning}},
year = {2013}
}
@inproceedings{Gomez2005,
abstract = {In practice, almost all control systems in use today implement some form of linear control. However, there are many tasks for which conventional control engineering methods are not directly applicable because there is not enough information about how the system should be controlled (i.e. reinforcement learning problems). In this paper, we explore an approach to such problems that evolves fast-weight neural networks. These networks, although capable of implementing arbitrary non-linear mappings, can more easily exploit the piecewise linearity inherent in most systems, in order to produce simpler and more comprehensible controllers. The method is tested on 2D mobile robot version of the pole balancing task where the controller must learn to switch between two operating modes, one using a single pole and the other using a jointed pole version that has not before been solved. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
author = {Gomez, Faustino and Schmidhuber, J{\"{u}}rgen},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
isbn = {3540287558},
issn = {03029743},
title = {{Evolving modular fast-weight networks for control}},
year = {2005}
}
@article{Stanley2001,
abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
author = {Stanley, Kenneth O and Miikkulainen, Risto},
doi = {10.1162/106365602320169811},
issn = {10636560},
journal = {Evolutionary Computation},
keywords = {Competing conventions,Genetic algorithms,Network topologies,Neural networks,Neuroevolution,Speciation},
number = {2},
pages = {99--127},
pmid = {12180173},
title = {{Evolving neural networks through augmenting topologies}},
volume = {10},
year = {2002}
}
@article{Boord2017,
abstract = {Attention dysfunction is a common but often undiagnosed cognitive impairment in Parkinson's disease that significantly reduces quality of life. We sought to increase understanding of the mechanisms underlying attention dysfunction using functional neuroimaging. Functional MRI was acquired at two repeated sessions in the resting state and during the Attention Network Test, for 25 non-demented subjects with Parkinson's disease and 21 healthy controls. Behavioral and MRI contrasts were calculated for alerting, orienting, and executive control components of attention. Brain regions showing group differences in attention processing were used as seeds in a functional connectivity analysis of a separate resting state run. Parkinson's disease subjects showed more activation during increased executive challenge in four regions of the dorsal attention and frontoparietal networks, namely right frontal eye field, left and right intraparietal sulcus, and precuneus. In three regions we saw reduced resting state connectivity to the default mode network. Further, whereas higher task activation in the right intraparietal sulcus correlated with reduced resting state connectivity between right intraparietal sulcus and the precuneus in healthy controls, this relationship was absent in Parkinson's disease subjects. Our results suggest that a weakened interaction between the default mode and task positive networks might alter the way in which the executive response is processed in PD.},
author = {Boord, Peter and Madhyastha, Tara M. and Askren, Mary K. and Grabowski, Thomas J.},
doi = {10.1016/j.nicl.2016.11.004},
issn = {22131582},
journal = {NeuroImage: Clinical},
keywords = {Attention network test,Default mode network,Executive attention,Functional connectivity,Parkinson's disease},
pages = {1--8},
title = {{Executive attention networks show altered relationship with default mode network in PD}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2213158216302108},
volume = {13},
year = {2017}
}
@incollection{Flach2020,
abstract = {tep involves the computation of the expectation of the likelihood of allmodel parameters by including the hid- den variables as if they were observed. Eachmaximiza- tion step involves the computation of the maximum likelihood estimates of the parameters by maximizing the expected likelihood found during the expectation step. The parameters produced by the maximization step are then used to begin another expectation step, and the process is repeated. It can be shown that an EM iteration will not decrease the observed data likelihood function. How- ever, there is no guarantee that the iteration converges to amaximum likelihood estimator. “Expectation-maximization” has developed to be a general recipe and umbrella term for a class of algo- rithms that iterates between a type of expectation and maximization step. The Baum–Welch algorithm is an example of an EM algorithm specifically suited to HMMs.},
address = {Cham},
author = {Flach, Boris and Hlavac, Vaclav},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_692-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Expectation-Maximization Algorithm}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}692-1},
year = {2020}
}
@article{Ikutani2020a,
abstract = {Expertise enables humans to achieve outstanding performance on domain-specific tasks, and programming is no exception. Many have shown that expert programmers exhibit remarkable differences from novices in behavioral performance, knowledge structure, and selective attention. However, the underlying differences in the brain are still unclear. We here address this issue by associating the cortical representation of source code with individual programming expertise using a data-driven decoding approach. This approach enabled us to identify seven brain regions, widely distributed in the frontal, parietal, and temporal cortices, that have a tight relationship with programming expertise. In these brain regions, functional categories of source code could be decoded from brain activity and the decoding accuracies were significantly correlated with individual behavioral performances on source-code categorization. Our results suggest that programming expertise is built up on fine-tuned cortical representations specialized for the domain of programming.},
author = {Ikutani, Yoshiharu and Kubo, Takatomi and Nishida, Satoshi and Hata, Hideaki and Matsumoto, Kenichi and Ikeda, Kazushi and Nishimoto, Shinji},
doi = {10.1101/2020.01.28.923953},
journal = {bioRxiv},
pages = {2020.01.28.923953},
publisher = {Cold Spring Harbor Laboratory},
title = {{Expert programmers have fine-tuned cortical representations of source code}},
url = {https://www.biorxiv.org/content/10.1101/2020.01.28.923953v1},
year = {2020}
}
@article{Denton2014,
abstract = {We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2 ×, while keeping the accuracy within 1{\%} of the original model.},
archivePrefix = {arXiv},
arxivId = {1404.0736},
author = {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
eprint = {1404.0736},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {1269--1277},
title = {{Exploiting linear structure within convolutional networks for efficient evaluation}},
volume = {2},
year = {2014}
}
@article{Lo2012,
abstract = {This paper describes a framework for establishing a reference airway tree segmentation, which was used to quantitatively evaluate 15 different airway tree extraction algorithms in a standardized manner. Because of the sheer difficulty involved in manually constructing a complete reference standard from scratch, we propose to construct the reference using results from all algorithms that are to be evaluated. We start by subdividing each segmented airway tree into its individual branch segments. Each branch segment is then visually scored by trained observers to determine whether or not it is a correctly segmented part of the airway tree. Finally, the reference airway trees are constructed by taking the union of all correctly extracted branch segments. Fifteen airway tree extraction algorithms from different research groups are evaluated on a diverse set of 20 chest computed tomography (CT) scans of subjects ranging from healthy volunteers to patients with severe pathologies, scanned at different sites, with different CT scanner brands, models, and scanning protocols. Three performance measures covering different aspects of segmentation quality were computed for all participating algorithms. Results from the evaluation showed that no single algorithm could extract more than an average of 74{\%} of the total length of all branches in the reference standard, indicating substantial differences between the algorithms. A fusion scheme that obtained superior results is presented, demonstrating that there is complementary information provided by the different algorithms and there is still room for further improvements in airway segmentation algorithms. {\textcopyright} 1982-2012 IEEE.},
author = {Lo, Pechin and {Van Ginneken}, Bram and Reinhardt, Joseph M. and Yavarna, Tarunashree and {De Jong}, Pim A. and Irving, Benjamin and Fetita, Catalin and Ortner, Margarete and Pinho, R{\^{o}}mulo and Sijbers, Jan and Feuerstein, Marco and Fabijanska, Anna and Bauer, Christian and Beichel, Reinhard and Mendoza, Carlos S. and Wiemker, Rafael and Lee, Jaesung and Reeves, Anthony P. and Born, Silvia and Weinheimer, Oliver and {Van Rikxoort}, Eva M. and Tschirren, Juerg and Mori, Ken and Odry, Benjamin and Naidich, David P. and Hartmann, Ieneke and Hoffman, Eric A. and Prokop, Mathias and Pedersen, Jesper H. and {De Bruijne}, Marleen},
doi = {10.1109/TMI.2012.2209674},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Computed tomography,evaluation,pulmonary airways,segmentation},
pmid = {22855226},
title = {{Extraction of airways from CT (EXACT'09)}},
year = {2012}
}
@incollection{Kumar2020,
abstract = {https://www.pyimagesearch.com/2017/05/22/face-alignment-with-opencv-and-python/},
address = {Cham},
author = {Kumar, Amit and Chellappa, Rama},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_879-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Face Alignment}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}879-1},
year = {2020}
}
@incollection{Loy2020,
abstract = {In recent years, face recognition has attracted much attention and its research has rapidly expanded by not only engineers but also neuroscientists, since it has many potential applications in computer vision},
address = {Cham},
author = {Loy, Chen Change},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_798-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Face Detection}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}798-1},
year = {2020}
}
@incollection{Thom2020,
address = {Cham},
author = {Thom, Nathan and Hand, Emily M.},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_815-1},
pages = {1--13},
publisher = {Springer International Publishing},
title = {{Facial Attribute Recognition: A Survey}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}815-1},
year = {2020}
}
@article{Qaiser2019,
abstract = {Tumor segmentation in whole-slide images of histology slides is an important step towards computer-assisted diagnosis. In this work, we propose a tumor segmentation framework based on the novel concept of persistent homology profiles (PHPs). For a given image patch, the homology profiles are derived by efficient computation of persistent homology, which is an algebraic tool from homology theory. We propose an efficient way of computing topological persistence of an image, alternative to simplicial homology. The PHPs are devised to distinguish tumor regions from their normal counterparts by modeling the atypical characteristics of tumor nuclei. We propose two variants of our method for tumor segmentation: one that targets speed without compromising accuracy and the other that targets higher accuracy. The fast version is based on a selection of exemplar image patches from a convolution neural network (CNN) and patch classification by quantifying the divergence between the PHPs of exemplars and the input image patch. Detailed comparative evaluation shows that the proposed algorithm is significantly faster than competing algorithms while achieving comparable results. The accurate version combines the PHPs and high-level CNN features and employs a multi-stage ensemble strategy for image patch labeling. Experimental results demonstrate that the combination of PHPs and CNN features outperform competing algorithms. This study is performed on two independently collected colorectal datasets containing adenoma, adenocarcinoma, signet, and healthy cases. Collectively, the accurate tumor segmentation produces the highest average patch-level F1-score, as compared with competing algorithms, on malignant and healthy cases from both the datasets. Overall the proposed framework highlights the utility of persistent homology for histopathology image analysis.},
archivePrefix = {arXiv},
arxivId = {1805.03699},
author = {Qaiser, Talha and Tsang, Yee Wah and Taniyama, Daiki and Sakamoto, Naoya and Nakane, Kazuaki and Epstein, David and Rajpoot, Nasir},
doi = {10.1016/j.media.2019.03.014},
eprint = {1805.03699},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Colorectal (colon) cancer,Computational pathology,Deep learning,Histology image analysis,Persistent homology,Tumor segmentation},
pages = {1--14},
pmid = {30991188},
title = {{Fast and accurate tumor segmentation of histology images using persistent homology and deep convolutional features}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841518302688},
volume = {55},
year = {2019}
}
@inproceedings{Klein2016,
abstract = {Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for large datasets, training and validating a single configuration often takes hours, days, or even weeks, which limits the achievable performance. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed FABOLAS, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that FABOLAS often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods or the recently proposed bandit strategy Hyperband.},
archivePrefix = {arXiv},
arxivId = {1605.07079},
author = {Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017},
eprint = {1605.07079},
title = {{Fast Bayesian optimization of machine learning hyperparameters on large datasets}},
volume = {abs/1605.0},
year = {2017}
}
@article{Lebedev2016,
abstract = {We revisit the idea of brain damage, i.e. the pruning of the coefficients of a neural network, and suggest how brain damage can be modified and used to speedup convolutional layers in ConvNets. The approach uses the fact that many efficient implementations reduce generalized convolutions to matrix multiplications. The suggested brain damage process prunes the convolutional kernel tensor in a group-wise fashion. After such pruning, convolutions can be reduced to multiplications of thinned dense matrices, which leads to speedup. We investigate different ways to add group-wise prunning to the learning process, and show that severalfold speedups of convolutional layers can be attained using group-sparsity regularizers. Our approach can adjust the shapes of the receptive fields in the convolutional layers, and even prune excessive feature maps from ConvNets, all in data-driven way.},
archivePrefix = {arXiv},
arxivId = {1506.02515},
author = {Lebedev, Vadim and Lempitsky, Victor},
doi = {10.1109/CVPR.2016.280},
eprint = {1506.02515},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2554--2564},
title = {{Fast ConvNets Using Group-Wise Brain Damage}},
volume = {2016-Decem},
year = {2016}
}
@article{Ren2015,
annote = {From Duplicate 1 (Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks - Ren, Shaoqing; He, Kaiming; Girshick, Ross B; Sun, Jian)

{\_}eprint: 1506.01497},
archivePrefix = {arXiv},
arxivId = {1506.01497},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross B and Sun, Jian},
eprint = {1506.01497},
journal = {CoRR},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
url = {http://arxiv.org/abs/1506.01497},
volume = {abs/1506.0},
year = {2015}
}
@article{Zbontar2018,
abstract = {Accelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements has the potential to reduce medical costs, minimize stress to patients and make MRI possible in applications where it is currently prohibitively slow or expensive. We introduce the fastMRI dataset, a large-scale collection of both raw MR measurements and clinical MR images, that can be used for training and evaluation of machine-learning approaches to MR image reconstruction. By introducing standardized evaluation criteria and a freely-accessible dataset, our goal is to help the community make rapid advances in the state of the art for MR image reconstruction. We also provide a self-contained introduction to MRI for machine learning researchers with no medical imaging background.},
archivePrefix = {arXiv},
arxivId = {1811.08839},
author = {Zbontar, Jure and Knoll, Florian and Sriram, Anuroop and Murrell, Tullie and Huang, Zhengnan and Muckley, Matthew J. and Defazio, Aaron and Stern, Ruben and Johnson, Patricia and Bruno, Mary and Parente, Marc and Geras, Krzysztof J. and Katsnelson, Joe and Chandarana, Hersh and Zhang, Zizhao and Drozdzal, Michal and Romero, Adriana and Rabbat, Michael and Vincent, Pascal and Yakubova, Nafissa and Pinkerton, James and Wang, Duo and Owens, Erich and Zitnick, C. Lawrence and Recht, Michael P. and Sodickson, Daniel K. and Lui, Yvonne W.},
eprint = {1811.08839},
month = {nov},
title = {{fastMRI: An Open Dataset and Benchmarks for Accelerated MRI}},
url = {http://arxiv.org/abs/1811.08839},
year = {2018}
}
@article{Henschel2019,
abstract = {Traditional neuroimage analysis pipelines involve computationally intensive, time-consuming optimization steps, and thus, do not scale well to large cohort studies with thousands or tens of thousands of individuals. In this work we propose a fast and accurate deep learning based neuroimaging pipeline for the automated processing of structural human brain MRI scans, including surface reconstruction and cortical parcellation. To this end, we introduce an advanced deep learning architecture capable of whole brain segmentation into 95 classes in under 1 minute, mimicking FreeSurfer's anatomical segmentation and cortical parcellation. The network architecture incorporates local and global competition via competitive dense blocks and competitive skip pathways, as well as multi-slice information aggregation that specifically tailor network performance towards accurate segmentation of both cortical and sub-cortical structures. Further, we perform fast cortical surface reconstruction and thickness analysis by introducing a spectral spherical embedding and by directly mapping the cortical labels from the image to the surface. This approach provides a full FreeSurfer alternative for volumetric analysis (within 1 minute) and surface-based thickness analysis (within only around 1h run time). For sustainability of this approach we perform extensive validation: we assert high segmentation accuracy on several unseen datasets, measure generalizability and demonstrate increased test-retest reliability, and increased sensitivity to disease effects relative to traditional FreeSurfer.},
annote = {{\_}eprint: 1910.03866},
archivePrefix = {arXiv},
arxivId = {1910.03866},
author = {Henschel, Leonie and Conjeti, Sailesh and Estrada, Santiago and Diers, Kersten and Fischl, Bruce and Reuter, Martin},
eprint = {1910.03866},
title = {{FastSurfer -- A fast and accurate deep learning based neuroimaging pipeline}},
url = {http://arxiv.org/abs/1910.03866},
year = {2019}
}
@incollection{Gitchat2018,
abstract = {The advent of Big Data, and specially the advent of datasets with high dimensionality, has brought an important necessity to identify the relevant features of the data. In this scenario, the importance of feature selection is beyond doubt and different methods have been developed, although researchers do not agree on which one is the best method for any given setting. This chapter provides the reader with the foundations about feature selection (see Sect. 2.1) as well as a description of the state-of-the-art feature selection methods (Sect. 2.2). Then, these methods will be analyzed on several synthetic datasets (Sect. 2.3) trying to draw conclusions about their performance when dealing with a crescent number of irrelevant features, noise in the data, redundancy and interaction between attributes, as well as a small ratio between number of samples and number of features. Finally, in Sect. 2.4, some state-of-the-art methods will be analyzed to study their scalability, i.e. the impact of an increase in the training set on the computational performance of an algorithm in terms of accuracy, training time and stability.},
address = {Cham},
author = {Bol{\'{o}}n-Canedo, Ver{\'{o}}nica and Alonso-Betanzos, Amparo},
booktitle = {Intelligent Systems Reference Library},
doi = {10.1007/978-3-319-90080-3_2},
issn = {18684408},
pages = {13--37},
publisher = {Springer International Publishing},
title = {{Feature selection}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}299-1},
volume = {147},
year = {2018}
}
@article{Song2013FeatureBasedIP,
abstract = {In this paper, we propose a new classification method for five categories of lung tissues in high-resolution computed tomography (HRCT) images, with feature-based image patch approximation. We design two new feature descriptors for higher feature descriptiveness, namely the rotation-invariant Gabor-local binary patterns (RGLBP) texture descriptor and multi-coordinate histogram of oriented gradients (MCHOG) gradient descriptor. Together with intensity features, each image patch is then labeled based on its feature approximation from reference image patches. And a new patch-adaptive sparse approximation (PASA) method is designed with the following main components: minimum discrepancy criteria for sparse-based classification, patch-specific adaptation for discriminative approximation, and feature-space weighting for distance computation. The patch-wise labelings are then accumulated as probabilistic estimations for region-level classification. The proposed method is evaluated on a publicly available ILD database, showing encouraging performance improvements over the state-of-the-arts. {\textcopyright} 1982-2012 IEEE.},
author = {{Yang Song} and {Weidong Cai} and {Yun Zhou} and Feng, David Dagan},
doi = {10.1109/TMI.2013.2241448},
issn = {0278-0062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Adaptive,gradient,reference,texture},
month = {apr},
number = {4},
pages = {797--808},
pmid = {23340591},
title = {{Feature-Based Image Patch Approximation for Lung Tissue Classification}},
url = {http://ieeexplore.ieee.org/document/6415277/},
volume = {32},
year = {2013}
}
@article{Ramaswamy2019,
abstract = {We show that a word-level recurrent neural network can predict emoji from text typed on a mobile keyboard. We demonstrate the usefulness of transfer learning for predicting emoji by pretraining the model using a language modeling task. We also propose mechanisms to trigger emoji and tune the diversity of candidates. The model is trained using a distributed on-device learning framework called federated learning. The federated model is shown to achieve better performance than a server-trained model. This work demonstrates the feasibility of using federated learning to train production-quality models for natural language understanding tasks while keeping users' data on their devices.},
archivePrefix = {arXiv},
arxivId = {1906.04329},
author = {Ramaswamy, Swaroop and Mathews, Rajiv and Rao, Kanishka and Beaufays, Fran{\c{c}}oise},
eprint = {1906.04329},
month = {jun},
title = {{Federated Learning for Emoji Prediction in a Mobile Keyboard}},
url = {http://arxiv.org/abs/1906.04329},
year = {2019}
}
@article{Hard2018,
abstract = {We train a recurrent neural network language model using a distributed, on-device learning framework called federated learning for the purpose of next-word prediction in a virtual keyboard for smartphones. Server-based training using stochastic gradient descent is compared with training on client devices using the Federated Averaging algorithm. The federated algorithm, which enables training on a higher-quality dataset for this use case, is shown to achieve better prediction recall. This work demonstrates the feasibility and benefit of training language models on client devices without exporting sensitive user data to servers. The federated learning environment gives users greater control over the use of their data and simplifies the task of incorporating privacy by default with distributed training and aggregation across a population of client devices.},
archivePrefix = {arXiv},
arxivId = {1811.03604},
author = {Hard, Andrew and Rao, Kanishka and Mathews, Rajiv and Ramaswamy, Swaroop and Beaufays, Fran{\c{c}}oise and Augenstein, Sean and Eichner, Hubert and Kiddon, Chlo{\'{e}} and Ramage, Daniel},
eprint = {1811.03604},
month = {nov},
title = {{Federated Learning for Mobile Keyboard Prediction}},
url = {http://arxiv.org/abs/1811.03604},
year = {2018}
}
@article{Konecny2016,
abstract = {Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.},
archivePrefix = {arXiv},
arxivId = {1610.05492},
author = {Kone{\v{c}}n{\'{y}}, Jakub and McMahan, H. Brendan and Yu, Felix X. and Richt{\'{a}}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
eprint = {1610.05492},
month = {oct},
title = {{Federated Learning: Strategies for Improving Communication Efficiency}},
url = {http://arxiv.org/abs/1610.05492},
year = {2016}
}
@article{Yang2019a,
abstract = {Today's artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security.We propose a possible solution to these challenges: Secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning.We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.},
archivePrefix = {arXiv},
arxivId = {1902.04885},
author = {Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
doi = {10.1145/3298981},
eprint = {1902.04885},
issn = {21576912},
journal = {ACM Transactions on Intelligent Systems and Technology},
keywords = {Federated learning,GDPR,transfer learning},
month = {feb},
number = {2},
title = {{Federated machine learning: Concept and applications}},
url = {http://arxiv.org/abs/1902.04885},
volume = {10},
year = {2019}
}
@article{Mondal2018,
abstract = {We address the problem of segmenting 3D multi-modal medical images in scenarios where very few labeled examples are available for training. Leveraging the recent success of adversarial learning for semi-supervised segmentation, we propose a novel method based on Generative Adversarial Networks (GANs) to train a segmentation model with both labeled and unlabeled images. The proposed method prevents over-fitting by learning to discriminate between true and fake patches obtained by a generator network. Our work extends current adversarial learning approaches, which focus on 2D single-modality images, to the more challenging context of 3D volumes of multiple modalities. The proposed method is evaluated on the problem of segmenting brain MRI from the iSEG-2017 and MRBrainS 2013 datasets. Significant performance improvement is reported, compared to state-of-art segmentation networks trained in a fully-supervised manner. In addition, our work presents a comprehensive analysis of different GAN architectures for semi-supervised segmentation, showing recent techniques like feature matching to yield a higher performance than conventional adversarial training approaches. Our code is publicly available at https://github.com/arnab39/FewShot{\_}GAN-Unet3D},
archivePrefix = {arXiv},
arxivId = {1810.12241},
author = {Mondal, Arnab Kumar and Dolz, Jose and Desrosiers, Christian},
eprint = {1810.12241},
month = {oct},
title = {{Few-shot 3D Multi-modal Medical Image Segmentation using Generative Adversarial Learning}},
url = {http://arxiv.org/abs/1810.12241},
year = {2018}
}
@incollection{Larochelle2020,
address = {Cham},
author = {Larochelle, Hugo},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_861-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Few-Shot Learning}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}861-1},
year = {2020}
}
@article{Neher2014,
abstract = {Purpose: Phantom-based validation of diffusion-weighted image processing techniques is an important key to innovation in the field and is widely used. Openly available and user friendly tools for the flexible generation of tailor-made datasets for the specific tasks at hand can greatly facilitate the work of researchers around the world.
Methods: We present an open-source framework, Fiberfox, that enables (1) the intuitive definition of arbitrary artificial white matter fiber tracts, (2) signal generation from those fibers by means of the most recent multi-compartment modeling techniques, and (3) simulation of the actual MR acquisition that allows for the introduction of realistic MRI-related effects into the final image.
Results: We show that real acquisitions can be closely approximated by simulating the acquisition of the well-known FiberCup phantom. We further demonstrate the advantages of our framework by evaluating the effects of imaging artifacts and acquisition settings on the outcome of 12 tractography algorithms.
Conclusion: Our findings suggest that experiments on a realistic software phantom might change the conclusions drawn from earlier hardware phantom experiments. Fiberfox may find application in validating and further developing methods such as tractography, super-resolution, diffusion modeling or artifact correction.},
author = {Neher, Peter F. and Laun, Frederik B. and Stieltjes, Bram and Maier-Hein, Klaus H.},
doi = {10.1002/mrm.25045},
issn = {15222594},
journal = {Magnetic Resonance in Medicine},
keywords = {Artifact simulation,Diffusion-weighted imaging,Open-source software,S synthetic white matter fibers,Software phantoms},
month = {nov},
number = {5},
pages = {1460--1470},
pmid = {24323973},
title = {{Fiberfox: Facilitating the creation of realistic white matter software phantoms}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24323973},
volume = {72},
year = {2014}
}
@incollection{Maybank2020,
address = {Cham},
author = {Maybank, Stephen J.},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_657-1},
pages = {1--3},
publisher = {Springer International Publishing},
title = {{Fisher-Rao Metric}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}657-1},
year = {2020}
}
@article{Kachelrieß2006,
abstract = {In the beginning of 2004 medical spiral-CT scanners that acquire up to 64 slices simultaneously became available. Most manufacturers use a straightforward acquisition principle, namely an x-ray focus rotating on a circular path and an opposing cylindrical detector whose rotational center coincides with the x-ray focus. The 64-slice scanner available to us, a Somatom Sensation 64 spiral cone-beam CT scanner (Siemens, Medical Solutions, Forchheim, Germany), makes use of a flying focal spot (FFS) that allows for view-by-view deflections of the focal spot in the rotation direction (aFFS) and in the z-direction (zFFS) with the goal of reducing aliasing artifacts. The FFS feature doubles the sampling density in the radial direction (channel direction, aFFS) and in the longitudinal direction (detector row direction or z-direction, zFFS). The cost of increased radial and azimuthal sampling is a two- or four-fold reduction of azimuthal sampling (angular sampling). To compensate for the potential reduction of azimuthal sampling the scanner simply increases the number of detector read-outs (readings) per rotation by a factor two or four. Then, up to four detector readings contribute to what we define as one view or one projection. A significant reduction of in-plane aliasing and of aliasing in the z -direction can be expected. Especially the latter is of importance to spiral CT scans where aliasing is known to produce so-called windmill artifacts. We have derived and analyzed the optimal focal spot deflection values da and dz as they would ideally occur in our scanner. Based upon these we show how image reconstruction can be performed in general. A simulation study showing reconstructions of mathematical phantoms further provides evidence that image quality can be significantly improved with the FFS. Aliasing artifacts, that manifest as streaks emerging from high-contrast objects, and windmill artifacts are reduced by almost an order of magnitude with the FFS compared to a simulation without FFS. Patient images acquired with our 64-slice cone-beam CT scanner support these results. {\textcopyright} 2006 IEEE.},
author = {Kachelrie{\ss}, Marc and Knaup, Michael and Pen{\ss}el, Christian and Kalender, Willi A.},
doi = {10.1109/TNS.2006.874076},
issn = {00189499},
journal = {IEEE Transactions on Nuclear Science},
keywords = {Computed tomography,Cone-beam CT,Image quality,Image reconstruction,Spiral-CT},
title = {{Flying focal spot (FFS) in cone-beam CT}},
year = {2006}
}
@book{Fiadeiro2015,
address = {Cham},
author = {Fiadeiro, Jos{\'{e}} Luiz and Liu, Zhiming},
booktitle = {Science of Computer Programming},
doi = {10.1016/j.scico.2015.11.001},
editor = {Arbab, Farhad and Jongmans, Sung-Shik},
isbn = {978-3-030-40913-5},
issn = {01676423},
pages = {221--222},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Formal Aspects of Component Software (FACS 2013)}},
url = {http://link.springer.com/10.1007/978-3-030-40914-2},
volume = {113},
year = {2015}
}
@article{Edward2010FourDD,
abstract = {A four-dimensional deformable image registration (4D DIR) algorithm, referred to as 4D local trajectory modeling (4DLTM), is presented and applied to thoracic 4D computed tomography (4DCT) image sets. The theoretical framework on which this algorithm is built exploits the incremental continuity present in 4DCT component images to calculate a dense set of parameterized voxel trajectories through space as functions of time. The spatial accuracy of the 4DLTM algorithm is compared with an alternative registration approach in which component phase to phase (CPP) DIR is utilized to determine the full displacement between maximum inhale and exhale images. A publically available DIR reference database (http://www.dir-lab.com) is utilized for the spatial accuracy assessment. The database consists of ten 4DCT image sets and corresponding manually identified landmark points between the maximum phases. A subset of points are propagated through the expiratory 4DCT component images. Cubic polynomials were found to provide sufficient flexibility and spatial accuracy for describing the point trajectories through the expiratory phases. The resulting average spatial error between the maximum phases was 1.25 mm for the 4DLTM and 1.44 mm for the CPP. The 4DLTM method captures the long-range motion between 4DCT extremes with high spatial accuracy. {\textcopyright} 2010 Institute of Physics and Engineering in Medicine.},
author = {Castillo, Edward and Castillo, Richard and Martinez, Josue and Shenoy, Maithili and Guerrero, Thomas},
doi = {10.1088/0031-9155/55/1/018},
issn = {00319155},
journal = {Physics in Medicine and Biology},
month = {jan},
number = {1},
pages = {305--327},
pmid = {20009196},
title = {{Four-dimensional deformable image registration using trajectory modeling}},
url = {https://iopscience.iop.org/article/10.1088/0031-9155/55/1/018},
volume = {55},
year = {2010}
}
@misc{Fischl2012,
abstract = {FreeSurfer is a suite of tools for the analysis of neuroimaging data that provides an array of algorithms to quantify the functional, connectional and structural properties of the human brain. It has evolved from a package primarily aimed at generating surface representations of the cerebral cortex into one that automatically creates models of most macroscopically visible structures in the human brain given any reasonable T1-weighted input image. It is freely available, runs on a wide variety of hardware and software platforms, and is open source. {\textcopyright} 2012 Elsevier Inc.},
author = {Fischl, Bruce},
booktitle = {NeuroImage},
doi = {10.1016/j.neuroimage.2012.01.021},
issn = {10538119},
keywords = {MRI,Morphometry,Registration,Segmentation},
number = {2},
pages = {774--781},
pmid = {22248573},
title = {{FreeSurfer}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3685476/},
volume = {62},
year = {2012}
}
@article{Bandi2019,
author = {Bandi, Peter and Geessink, Oscar and Manson, Quirine and {Van Dijk}, Marcory and Balkenhol, Maschenka and Hermsen, Meyke and {Ehteshami Bejnordi}, Babak and Lee, Byungjae and Paeng, Kyunghyun and Zhong, Aoxiao and Li, Quanzheng and Zanjani, Farhad Ghazvinian and Zinger, Svitlana and Fukuta, Keisuke and Komura, Daisuke and Ovtcharov, Vlado and Cheng, Shenghua and Zeng, Shaoqun and Thagaard, Jeppe and Dahl, Anders B. and Lin, Huangjing and Chen, Hao and Jacobsson, Ludwig and Hedlund, Martin and Cetin, Melih and Halici, Eren and Jackson, Hunter and Chen, Richard and Both, Fabian and Franke, Jorg and Kusters-Vandevelde, Heidi and Vreuls, Willem and Bult, Peter and van Ginneken, Bram and van der Laak, Jeroen and Litjens, Geert},
doi = {10.1109/TMI.2018.2867350},
issn = {0278-0062},
journal = {IEEE Transactions on Medical Imaging},
month = {feb},
number = {2},
pages = {550--560},
title = {{From Detection of Individual Metastases to Classification of Lymph Node Status at the Patient Level: The CAMELYON17 Challenge}},
url = {https://ieeexplore.ieee.org/document/8447230/},
volume = {38},
year = {2019}
}
@article{Peng2015,
author = {Peng, Hanchuan and Meijering, Erik and Ascoli, Giorgio A.},
doi = {10.1007/s12021-015-9270-9},
issn = {15392791},
journal = {Neuroinformatics},
month = {jul},
number = {3},
pages = {259--260},
title = {{From DIADEM to BigNeuron}},
url = {http://link.springer.com/10.1007/s12021-015-9270-9},
volume = {13},
year = {2015}
}
@article{Jenkinson2012,
abstract = {FSL (the FMRIB Software Library) is a comprehensive library of analysis tools for functional, structural and diffusion MRI brain imaging data, written mainly by members of the Analysis Group, FMRIB, Oxford. For this NeuroImage special issue on "20 years of fMRI" we have been asked to write about the history, developments and current status of FSL. We also include some descriptions of parts of FSL that are not well covered in the existing literature. We hope that some of this content might be of interest to users of FSL, and also maybe to new research groups considering creating, releasing and supporting new software packages for brain image analysis.},
author = {Jenkinson, Mark and Beckmann, Christian F and Behrens, Timothy E J and Woolrich, Mark W and Smith, Stephen M},
doi = {10.1016/j.neuroimage.2011.09.015},
issn = {1095-9572},
journal = {NeuroImage},
keywords = {FSL,Software},
number = {2},
pages = {782--90},
pmid = {21979382},
title = {{Fsl.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21979382},
volume = {62},
year = {2012}
}
@article{Radul2001,
abstract = {We introduce a class of Lawson monads and show that these monads have a functional representation. A characterisation of such a representation for the inclusion hyperspace monad is given.},
author = {Radul, Taras},
doi = {10.1023/A:1012052928198},
issn = {09272852},
journal = {Applied Categorical Structures},
keywords = {Functional representations,Lawson monad},
number = {5},
pages = {457--463},
title = {{Functional representations of Lawson monads}},
volume = {9},
year = {2001}
}
@book{Chowdhary2020,
address = {New Delhi},
author = {Chowdhary, K.R.},
booktitle = {Fundamentals of Artificial Intelligence},
doi = {10.1007/978-81-322-3972-7},
isbn = {978-81-322-3970-3},
publisher = {Springer India},
title = {{Fundamentals of Artificial Intelligence}},
url = {http://link.springer.com/10.1007/978-81-322-3972-7},
year = {2020}
}
@misc{Madabhushi2018,
author = {Madabhushi, Anant and Rusu, Mirabela},
doi = {10.7937/K9/TCIA.2018.SMT36LPN},
keywords = {Computed Tomography,Invasive Adenocarcinoma,Pathology,Pulmonary Nodules},
publisher = {The Cancer Imaging Archive},
title = {{Fused Radiology-Pathology Lung Dataset}},
url = {https://wiki.cancerimagingarchive.net/x/LoBgAg},
year = {2018}
}
@article{Xia2021,
abstract = {GAN inversion aims to invert a given image back into the latent space of a pretrained GAN model, for the image to be faithfully reconstructed from the inverted code by the generator. As an emerging technique to bridge the real and fake image domains, GAN inversion plays an essential role in enabling the pretrained GAN models such as StyleGAN and BigGAN to be used for real image editing applications. Meanwhile, GAN inversion also provides insights on the interpretation of GAN's latent space and how the realistic images can be generated. In this paper, we provide an overview of GAN inversion with a focus on its recent algorithms and applications. We cover important techniques of GAN inversion and their applications to image restoration and image manipulation. We further elaborate on some trends and challenges for future directions.},
archivePrefix = {arXiv},
arxivId = {2101.05278},
author = {Xia, Weihao and Zhang, Yulun and Yang, Yujiu and Xue, Jing-Hao and Zhou, Bolei and Yang, Ming-Hsuan},
eprint = {2101.05278},
month = {jan},
title = {{GAN Inversion: A Survey}},
url = {http://arxiv.org/abs/2101.05278},
year = {2021}
}
@incollection{Lecun1989,
author = {Lecun, Yann},
booktitle = {Connectionism in perspective},
editor = {Pfeifer, R and Schreter, Z and Fogelman, F and Steels, L},
publisher = {Elsevier},
title = {{Generalization and network design strategies}},
year = {1989}
}
@article{Wang2020a,
abstract = {Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research.1},
archivePrefix = {arXiv},
arxivId = {1904.05046},
author = {Wang, Yaqing and Yao, Quanming and Kwok, James T. and Ni, Lionel M.},
doi = {10.1145/3386252},
eprint = {1904.05046},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Few-shot learning,low-shot learning,meta-learning,one-shot learning,prior knowledge,small sample learning},
month = {apr},
number = {3},
title = {{Generalizing from a Few Examples: A Survey on Few-shot Learning}},
url = {http://arxiv.org/abs/1904.05046},
volume = {53},
year = {2020}
}
@article{Yi2019,
abstract = {Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.},
author = {Yi, Xin and Walia, Ekta and Babyn, Paul},
doi = {https://doi.org/10.1016/j.media.2019.101552},
issn = {1361-8415},
journal = {Medical Image Analysis},
keywords = {Deep learning,Generative adversarial network,Generative model,Medical imaging,Review},
pages = {101552},
title = {{Generative adversarial network in medical imaging: A review}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841518308430},
volume = {58},
year = {2019}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
eprint = {1406.2661},
month = {jun},
title = {{Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1406.2661},
year = {2014}
}
@article{Wang2019b,
abstract = {Generative adversarial networks (GANs) have been extensively studied in the past few years. Arguably their most significant impact has been in the area of computer vision where great advances have been made in challenges such as plausible image generation, image-to-image translation, facial attribute manipulation and similar domains. Despite the significant successes achieved to date, applying GANs to real-world problems still poses significant challenges, three of which we focus on here. These are: (1) the generation of high quality images, (2) diversity of image generation, and (3) stable training. Focusing on the degree to which popular GAN technologies have made progress against these challenges, we provide a detailed review of the state of the art in GAN-related research in the published scientific literature. We further structure this review through a convenient taxonomy we have adopted based on variations in GAN architectures and loss functions. While several reviews for GANs have been presented to date, none have considered the status of this field based on their progress towards addressing practical challenges relevant to computer vision. Accordingly, we review and critically discuss the most popular architecture-variant, and loss-variant GANs, for tackling these challenges. Our objective is to provide an overview as well as a critical analysis of the status of GAN research in terms of relevant progress towards important computer vision application requirements. As we do this we also discuss the most compelling applications in computer vision in which GANs have demonstrated considerable success along with some suggestions for future research directions. Code related to GAN-variants studied in this work is summarized on https://github.com/sheqi/GAN{\_}Review.},
archivePrefix = {arXiv},
arxivId = {1906.01529},
author = {Wang, Zhengwei and She, Qi and Ward, Tomas E.},
eprint = {1906.01529},
month = {jun},
title = {{Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy}},
url = {http://arxiv.org/abs/1906.01529},
year = {2019}
}
@article{Brock2016,
abstract = {When working with three-dimensional data, choice of representation is key. We explore voxel-based models, and present evidence for the viability of voxellated representations in applications including shape modeling and object classification. Our key contributions are methods for training voxel-based variational autoencoders, a user interface for exploring the latent space learned by the autoencoder, and a deep convolutional neural network architecture for object classification. We address challenges unique to voxel-based representations, and empirically evaluate our models on the ModelNet benchmark, where we demonstrate a 51.5{\%} relative improvement in the state of the art for object classification.},
annote = {{\_}eprint: 1608.04236},
archivePrefix = {arXiv},
arxivId = {1608.04236},
author = {Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
eprint = {1608.04236},
journal = {CoRR},
title = {{Generative and Discriminative Voxel Modeling with Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1608.04236},
volume = {abs/1608.0},
year = {2016}
}
@article{Zamir2017,
annote = {From Duplicate 1 (Generic 3D Representation via Pose Estimation and Matching - Zamir, Amir Roshan; Wekel, Tilman; Agrawal, Pulkit; Wei, Colin; Malik, Jitendra; Savarese, Silvio)

{\_}eprint: 1710.08247},
archivePrefix = {arXiv},
arxivId = {1710.08247},
author = {Zamir, Amir Roshan and Wekel, Tilman and Agrawal, Pulkit and Wei, Colin and Malik, Jitendra and Savarese, Silvio},
eprint = {1710.08247},
journal = {CoRR},
title = {{Generic 3D Representation via Pose Estimation and Matching}},
url = {http://arxiv.org/abs/1710.08247},
volume = {abs/1710.0},
year = {2017}
}
@article{Horikawa2017,
abstract = {Object recognition is a key function in both human and machine vision. While brain decoding of seen and imagined objects has been achieved, the prediction is limited to training examples. We present a decoding approach for arbitrary objects using the machine vision principle that an object category is represented by a set of features rendered invariant through hierarchical processing. We show that visual features, including those derived from a deep convolutional neural network, can be predicted from fMRI patterns, and that greater accuracy is achieved for low-/high-level features with lower-/higher-level visual areas, respectively. Predicted features are used to identify seen/imagined object categories (extending beyond decoder training) from a set of computed features for numerous object images. Furthermore, decoding of imagined objects reveals progressive recruitment of higher-to-lower visual representations. Our results demonstrate a homology between human and machine vision and its utility for brain-based information retrieval.},
archivePrefix = {arXiv},
arxivId = {1510.06479},
author = {Horikawa, Tomoyasu and Kamitani, Yukiyasu},
doi = {10.1038/ncomms15037},
eprint = {1510.06479},
issn = {20411723},
journal = {Nature Communications},
month = {aug},
number = {1},
pages = {15037},
pmid = {28530228},
title = {{Generic decoding of seen and imagined objects using hierarchical visual features}},
url = {http://www.nature.com/articles/ncomms15037},
volume = {8},
year = {2017}
}
@misc{Bronstein2016,
abstract = {Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.},
annote = {{\_}eprint: 1611.08097},
archivePrefix = {arXiv},
arxivId = {1611.08097},
author = {Bronstein, Michael M and Bruna, Joan and Lecun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
booktitle = {IEEE Signal Processing Magazine},
doi = {10.1109/MSP.2017.2693418},
eprint = {1611.08097},
issn = {10535888},
number = {4},
pages = {18--42},
title = {{Geometric Deep Learning: Going beyond Euclidean data}},
url = {http://arxiv.org/abs/1611.08097},
volume = {34},
year = {2017}
}
@article{Meagher1982,
abstract = {A geometric modeling technique called Octree Encoding is presented. Arbitrary 3-D objects can be represented to any specified resolution in a hierarchical 8-ary tree structure or "octree" Objects may be concave or convex, have holes (including interior holes), consist of disjoint parts, and possess sculptured (i.e., "free-form") surfaces. The memory required for representation and manipulation is on the order of the surface area of the object. A complexity metric is proposed based on the number of nodes in an object's tree representation. Efficient (linear time) algorithms have been developed for the Boolean operations (union, intersection and difference), geometric operations (translation, scaling and rotation), N-dimensional interference detection, and display from any point in space with hidden surfaces removed. The algorithms require neither floating-point operations, integer multiplications, nor integer divisions. In addition, many independent sets of very simple calculations are typically generated, allowing implementation over many inexpensive high-bandwidth processors operating in parallel. Real time analysis and manipulation of highly complex situations thus becomes possible. {\textcopyright} 1982.},
author = {Meagher, Donald},
doi = {10.1016/0146-664X(82)90104-6},
issn = {0146664X},
journal = {Computer Graphics and Image Processing},
number = {2},
pages = {129--147},
title = {{Geometric modeling using octree encoding}},
url = {https://www.sciencedirect.com/science/article/pii/0146664X82901046{\%}0Ahttp://fab.cba.mit.edu/classes/S62.12/docs/Meagher{\_}octree.pdf},
volume = {19},
year = {1982}
}
@book{Strauss2020,
address = {Berkeley, CA},
author = {Strauss, Dirk},
booktitle = {Getting Started with Visual Studio 2019},
doi = {10.1007/978-1-4842-5449-3},
isbn = {978-1-4842-5448-6},
publisher = {Apress},
title = {{Getting Started with Visual Studio 2019}},
url = {http://link.springer.com/10.1007/978-1-4842-5449-3},
year = {2020}
}
@inproceedings{Han2011,
abstract = {Tumor segmentation in PET and CT images is notoriously challenging due to the low spatial resolution in PET and low contrast in CT images. In this paper, we have proposed a general framework to use both PET and CT images simultaneously for tumor segmentation. Our method utilizes the strength of each imaging modality: the superior contrast of PET and the superior spatial resolution of CT. We formulate this problem as a Markov Random Field (MRF) based segmentation of the image pair with a regularized term that penalizes the segmentation difference between PET and CT. Our method simulates the clinical practice of delineating tumor simultaneously using both PET and CT, and is able to concurrently segment tumor from both modalities, achieving globally optimal solutions in low-order polynomial time by a single maximum flow computation. The method was evaluated on clinically relevant tumor segmentation problems. The results showed that our method can effectively make use of both PET and CT image information, yielding segmentation accuracy of 0.85 in Dice similarity coefficient and the average median hausdorff distance (HD) of 6.4 mm, which is 10 {\%} (resp., 16 {\%}) improvement compared to the graph cuts method solely using the PET (resp., CT) images. {\textcopyright} 2011 Springer-Verlag.},
author = {Han, Dongfeng and Bayouth, John and Song, Qi and Taurani, Aakant and Sonka, Milan and Buatti, John and Wu, Xiaodong},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-22092-0_21},
isbn = {9783642220913},
issn = {03029743},
pages = {245--256},
title = {{Globally optimal tumor segmentation in PET-CT images: A graph-based co-segmentation method}},
volume = {6801 LNCS},
year = {2011}
}
@article{Altaf2019GoingDI,
abstract = {Medical image analysis is currently experiencing a paradigm shift due to deep learning. This technology has recently attracted so much interest of the Medical Imaging Community that it led to a specialized conference in ''Medical Imaging with Deep Learning'' in the year 2018. This paper surveys the recent developments in this direction and provides a critical review of the related major aspects. We organize the reviewed literature according to the underlying pattern recognition tasks and further sub-categorize it following a taxonomy based on human anatomy. This paper does not assume prior knowledge of deep learning and makes a significant contribution in explaining the core deep learning concepts to the non-experts in the Medical Community. This paper provides a unique computer vision/machine learning perspective taken on the advances of deep learning in medical imaging. This enables us to single out ''lack of appropriately annotated large-scale data sets" as the core challenge (among other challenges) in this research direction. We draw on the insights from the sister research fields of computer vision, pattern recognition, and machine learning, where the techniques of dealing with such challenges have already matured, to provide promising directions for the Medical Imaging Community to fully harness deep learning in the future.},
archivePrefix = {arXiv},
arxivId = {1902.05655},
author = {Altaf, Fouzia and Islam, Syed M.S. and Akhtar, Naveed and Janjua, Naeem Khalid},
doi = {10.1109/ACCESS.2019.2929365},
eprint = {1902.05655},
issn = {21693536},
journal = {IEEE Access},
keywords = {Artificial neural networks,Data sets,Deep learning,Medical imaging,Survey,Tutorial},
month = {feb},
pages = {99540--99572},
title = {{Going deep in medical image analysis: Concepts, methods, challenges, and future directions}},
url = {http://arxiv.org/abs/1902.05655 https://ieeexplore.ieee.org/document/8764525/},
volume = {7},
year = {2019}
}
@inproceedings{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
isbn = {9781467369640},
issn = {10636919},
pages = {1--9},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@incollection{Xu2020,
address = {Cham},
author = {Xu, Chenyang and Prince, Jerry L.},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_712-1},
pages = {1--8},
publisher = {Springer International Publishing},
title = {{Gradient Vector Flow}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}712-1},
year = {2020}
}
@article{LeCun1998a,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Vinyals2019,
abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1–3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8{\%} of officially ranked human players.},
author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"{e}}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'{e}}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"{u}}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
doi = {10.1038/s41586-019-1724-z},
issn = {14764687},
journal = {Nature},
month = {nov},
number = {7782},
pages = {350--354},
pmid = {31666705},
title = {{Grandmaster level in StarCraft II using multi-agent reinforcement learning}},
url = {http://www.nature.com/articles/s41586-019-1724-z},
volume = {575},
year = {2019}
}
@inproceedings{Zoph2016,
abstract = {Graph Neural Networks (GNNs) have been popularly used for analyzing non-Euclidean data such as social network data and biological data. Despite their success, the design of graph neural networks requires a lot of manual work and domain knowledge. In this paper, we propose a Graph Neural Architecture Search method (GraphNAS for short) that enables automatic search of the best graph neural architecture based on reinforcement learning. Specifically, GraphNAS first uses a recurrent network to generate variable-length strings that describe the architectures of graph neural networks, and then trains the recurrent network with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation data set. Extensive experimental results on node classification tasks in both transductive and inductive learning settings demonstrate that GraphNAS can achieve consistently better performance on the Cora, Citeseer, Pubmed citation network, and protein-protein interaction network. On node classification tasks, GraphNAS can design a novel network architecture that rivals the best humaninvented architecture in terms of test set accuracy.},
archivePrefix = {arXiv},
arxivId = {1904.09981},
author = {Gao, Yang and Yang, Hong and Zhang, Peng and Zhou, Chuan and Hu, Yue},
booktitle = {arXiv},
eprint = {1904.09981},
isbn = {9780999241165},
issn = {10450823},
title = {{GraphNAS: Graph neural architecture search with reinforcement learning}},
volume = {abs/1611.0},
year = {2019}
}
@article{Koenders2016,
abstract = {Cannabis is the most frequently used illicit drug worldwide. Cross-sectional neuroimaging studies suggest that chronic cannabis exposure and the development of cannabis use disorders may affect brain morphology. However, cross-sectional studies cannot make a conclusive distinction between cause and consequence and longitudinal neuroimaging studies are lacking. In this prospective study we investigate whether continued cannabis use and higher levels of cannabis exposure in young adults are associated with grey matter reductions. Heavy cannabis users (N = 20, age baseline M = 20.5, SD = 2.1) and non-cannabis using healthy controls (N = 22, age baseline M = 21.6, SD = 2.45) underwent a comprehensive psychological assessment and a T1- structural MRI scan at baseline and 3 years follow-up. Grey matter volumes (orbitofrontal cortex, anterior cingulate cortex, insula, striatum, thalamus, amygdala, hippocampus and cerebellum) were estimated using the software package SPM (VBM-8 module). Continued cannabis use did not have an effect on GM volume change at follow-up. Cross-sectional analyses at baseline and follow-up revealed consistent negative correlations between cannabis related problems and cannabis use (in grams) and regional GM volume of the left hippocampus, amygdala and superior temporal gyrus. These results suggests that small GM volumes in the medial temporal lobe are a risk factor for heavy cannabis use or that the effect of cannabis on GM reductions is limited to adolescence with no further damage of continued use after early adulthood. Long-term prospective studies starting in early adolescence are needed to reach final conclusions.},
author = {Koenders, Laura and Cousijn, Janna and Vingerhoets, Wilhelmina A.M. and {Van Den Brink}, Wim and Wiers, Reinout W. and Meijer, Carin J. and Machielsen, Marise W.J. and Veltman, Dick J. and Goudriaan, Anneke E. and {De Haan}, Lieuwe},
doi = {10.1371/journal.pone.0152482},
editor = {Chen, Kewei},
issn = {19326203},
journal = {PLoS ONE},
month = {may},
number = {5},
pages = {e0152482},
pmid = {27224247},
title = {{Grey matter changes associated with heavy cannabis use: A longitudinal sMRI study}},
url = {https://dx.plos.org/10.1371/journal.pone.0152482},
volume = {11},
year = {2016}
}
@article{Armato2015,
abstract = {Challenges, in the context of medical imaging, are valuable in that they allow for a direct comparison of different algorithms designed for a specific radiologic task, with all algorithms abiding by the same set of rules, operating on a common set of images, and being evaluated with a uniform performance assessment paradigm. The variability of system performance based on database composition and subtlety, definition of “truth,” and scoring metric is well-known;1–3 challenges serve to level the differences across these various dimensions. The medical imaging community has hosted a number of successful thoracic imaging challenges that have spanned a wide range of tasks,4,5 including lung nodule detection,6 lung nodule change, vessel segmentation,7 and vessel tree extraction.8 Each challenge presents its own unique set of circumstances and considerations; however, important common themes exist. Future challenge organizers (and participants) could benefit from an open discussion of successes achieved, pitfalls encountered, and lessons learned from each completed challenge.},
author = {Armato, Samuel G. and Hadjiiski, Lubomir and Tourassi, Georgia D. and Drukker, Karen and Giger, Maryellen L. and Li, Feng and Redmond, George and Farahani, Keyvan and Kirby, Justin S. and Clarke, Laurence P.},
doi = {10.1117/1.jmi.2.2.020103},
issn = {2329-4302},
journal = {Journal of Medical Imaging},
month = {jun},
number = {2},
pages = {020103},
title = {{Guest Editorial: LUNGx Challenge for computerized lung nodule classification: reflections and lessons learned}},
url = {http://medicalimaging.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JMI.2.2.020103},
volume = {2},
year = {2015}
}
@book{Streib2011,
address = {Cham},
author = {Streib, James T.},
booktitle = {Guide to Assembly Language},
doi = {10.1007/978-0-85729-271-1},
isbn = {978-3-030-35638-5},
publisher = {Springer International Publishing},
series = {Undergraduate Topics in Computer Science},
title = {{Guide to Assembly Language}},
url = {http://link.springer.com/10.1007/978-3-030-35639-2},
year = {2011}
}
@book{Skansi2020,
address = {Cham},
booktitle = {Guide to Deep Learning Basics},
doi = {10.1007/978-3-030-37591-1},
editor = {Skansi, Sandro},
isbn = {978-3-030-37590-4},
publisher = {Springer International Publishing},
title = {{Guide to Deep Learning Basics}},
url = {http://link.springer.com/10.1007/978-3-030-37591-1},
year = {2020}
}
@incollection{Ge2020,
abstract = {Real-time and accurate hand pose estimation can open new doors for making the entire world more interactive. The existing systems for hand pose estimation fail to produce an accurate and a physically valid pose in real-time. The approach in this project aims to tackle these problems by applying a discriminative model for real-time prediction of joint locations and, incorporates kinematic constraints for producing a geometrically valid pose, thus leading to accurate pose estimation. The discriminative model is a deep network consisting of convolutional, fully connected and dropout layers. The kinematic constraints are incorporated as a kinematic layer towards the end of the network, which acts as a prior for the hand pose. The results are evaluated on the NYU hand pose data-set and compared with state-of-the-art methods.},
address = {Cham},
author = {Ge, Liuhao and Yuan, Junsong},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_875-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Hand Pose Estimation}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}875-1},
year = {2020}
}
@book{Sabharwal2020,
address = {Berkeley, CA},
author = {Sabharwal, Navin and Edward, Shakuntala Gupta},
booktitle = {Hands On Google Cloud SQL and Cloud Spanner},
doi = {10.1007/978-1-4842-5537-7},
isbn = {978-1-4842-5536-0},
publisher = {Apress},
title = {{Hands On Google Cloud SQL and Cloud Spanner}},
url = {http://link.springer.com/10.1007/978-1-4842-5537-7},
year = {2020}
}
@incollection{Karaman2020,
address = {Cham},
author = {Karaman, Svebor and Chang, Shih-Fu},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_817-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Hashing for Face Search}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}817-1},
year = {2020}
}
@article{Marlow2010,
abstract = {Haskell is a general purpose, purely functional programming language incorporating many recent innovations in programming language design. Haskell provides higher-order functions, non-strict semantics, static poly- morphic typing, user-defined algebraic datatypes, pattern-matching, list comprehensions, a module system, a monadic I/O system, and a rich set of primitive datatypes, including lists, arrays, arbitrary and fixed precision integers, and floating-point numbers. Haskell is both the culmination and solidification of many years of research on non-strict functional languages.},
author = {Marlow, Simon},
journal = {Language},
pages = {329},
title = {{Haskell 2010 Language Report}},
url = {http://haskell.org/definition/haskell2010.pdf},
year = {2010}
}
@article{Shao2018,
abstract = {We present a novel spatial hashing based data structure to facilitate 3D shape analysis using convolutional neural networks (CNNs). Our method builds hierarchical hash tables for an input model under different resolutions that leverage the sparse occupancy of 3D shape boundary. Based on this data structure, we design two efficient GPU algorithms namely hash2col and col2hash so that the CNN operations like convolution and pooling can be efficiently parallelized. The perfect spatial hashing is employed as our spatial hashing scheme, which is not only free of hash collision but also nearly minimal so that our data structure is almost of the same size as the raw input. Compared with existing 3D CNN methods, our data structure significantly reduces the memory footprint during the CNN training. As the input geometry features are more compactly packed, CNN operations also run faster with our data structure. The experiment shows that, under the same network structure, our method yields comparable or better benchmark results compared with the state-of-the-art while it has only one-third memory consumption when under high resolutions (i.e. 256 3).},
annote = {{\_}eprint: 1803.11385},
archivePrefix = {arXiv},
arxivId = {1803.11385},
author = {Shao, Tianjia and Yang, Yin and Weng, Yanlin and Hou, Qiming and Zhou, Kun},
doi = {10.1109/TVCG.2018.2887262},
eprint = {1803.11385},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Computational modeling,Convolution,Data structures,Shape,Solid modeling,Three-dimensional displays,Two dimensional displays,convolutional neural network,perfect hashing,shape classification,shape retrieval,shape segmentation},
title = {{H-CNN: Spatial Hashing Based CNN for 3D Shape Analysis}},
year = {2018}
}
@article{Goswami2020,
abstract = {Developing nations lack adequate number of hospitals with modern equipment and skilled doctors. Hence, a significant proportion of these nations' population, particularly in rural areas, is not able to avail specialized and timely healthcare facilities. In recent years, deep learning (DL) models, a class of artificial intelligence (AI) methods, have shown impressive results in medical domain. These AI methods can provide immense support to developing nations as affordable healthcare solutions. This work is focused on one such application of blood cancer diagnosis. However, there are some challenges to DL models in cancer research because of the unavailability of a large data for adequate training and the difficulty of capturing heterogeneity in data at different levels ranging from acquisition characteristics, session, to subject-level (within subjects and across subjects). These challenges render DL models prone to overfitting and hence, models lack generalization on prospective subjects' data. In this work, we address these problems in the application of B-cell Acute Lymphoblastic Leukemia (B-ALL) diagnosis using deep learning. We propose heterogeneity loss that captures subject-level heterogeneity, thereby, forcing the neural network to learn subject-independent features. We also propose an unorthodox ensemble strategy that helps us in providing improved classification over models trained on 7-folds giving a weighted-{\$}F{\_}1{\$} score of 95.26{\%} on unseen (test) subjects' data that are, so far, the best results on the C-NMC 2019 dataset for B-ALL classification.},
archivePrefix = {arXiv},
arxivId = {2003.03295},
author = {Goswami, Shubham and Mehta, Suril and Sahrawat, Dhruva and Gupta, Anubha and Gupta, Ritu},
eprint = {2003.03295},
month = {mar},
title = {{Heterogeneity Loss to Handle Intersubject and Intrasubject Variability in Cancer}},
url = {http://arxiv.org/abs/2003.03295},
year = {2020}
}
@inproceedings{Liu2017,
abstract = {We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6{\%} on CIFAR-10 and 20.3{\%} when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3{\%} less top-1 accuracy on CIFAR-10 and 0.1{\%} less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.},
archivePrefix = {arXiv},
arxivId = {1711.00436},
author = {Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
booktitle = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
eprint = {1711.00436},
title = {{Hierarchical representations for efficient architecture search}},
volume = {abs/1711.0},
year = {2018}
}
@book{Performance2013,
address = {Cham},
author = {Performance, High},
doi = {10.1007/978-3-030-41050-6},
editor = {Bianchini, Calebe and Osthoff, Carla and Souza, Paulo and Ferreira, Renato},
isbn = {9783319102139},
publisher = {Springer International Publishing},
series = {Communications in Computer and Information Science},
title = {{High Performance Computing Systems}},
url = {http://link.springer.com/10.1007/978-3-030-41050-6},
volume = {1171},
year = {2013}
}
@inproceedings{Schulman2016,
abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD($\lambda$). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
archivePrefix = {arXiv},
arxivId = {1506.02438},
author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael I. and Abbeel, Pieter},
booktitle = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
eprint = {1506.02438},
title = {{High-dimensional continuous control using generalized advantage estimation}},
year = {2016}
}
@article{Mascalchi2018,
abstract = {Purpose To assess the potential of histogram metrics of diffusion-tensor imaging (DTI)-derived indices in revealing neurodegeneration and its progression in spinocerebellar ataxia type 2 (SCA2). Materials and methods Nine SCA2 patients and 16 age-matched healthy controls, were examined twice (SCA2 patients 3.6±0.7 years and controls 3.3±1.0 years apart) on the same 1.5T scanner by acquiring T1-weighted and diffusion-weighted (b-value = 1000 s/mm2) images. Cerebrum and brainstem-cerebellum regions were segmented using FreeSurfer suite. Histogram analysis of DTI-derived indices, including mean diffusivity (MD), fractional anisotropy (FA), axial (AD) / radial (RD) diffusivity and mode of anisotropy (MO), was performed. Results At baseline, significant differences between SCA2 patients and controls were confined to brainstem-cerebellum. Median values of MD/AD/RD and FA/MO were significantly (p{\textless}0.001) higher and lower, respectively, in SCA2 patients (1.11/1.30/1.03×10−3 mm2/s and 0.14/0.19) than in controls (0.80/1.00/0.70×10−3 mm2/s and 0.20/0.41). Also, peak location values of MD/AD/RD and FA were significantly (p{\textless}0.001) higher and lower, respectively, in SCA2 patients (0.91/1.11/0.81×10−3 mm2/s and 0.12) than in controls (0.71/0.91/0.63×10−3 mm2/s and 0.18). Peak height values of FA and MD/AD/RD/MO were significantly (p{\textless}0.001) higher and lower, respectively, in SCA2 patients (0.20 and 0.07/0.06/0.07×10−3 mm2/s/year /0.07) than in controls (0.15 and 0.14/0.11/0.12/×10−3 mm2/s/year /0.09). The rate of change of MD median values was significantly (p{\textless}0.001) higher (i.e., increased) in SCA2 patients (0.010×10−3 mm2/s/year) than in controls (-0.003×10−3 mm2/s/year) in the brainstem-cerebellum, whereas no significant difference was found for other indices and in the cerebrum. Conclusion Histogram analysis of DTI-derived indices is a relatively straightforward approach which reveals microstructural changes associated with pontocerebellar degeneration in SCA2 and the median value of MD is capable to track its progression.},
author = {Mascalchi, Mario and Marzi, Chiara and Giannelli, Marco and Ciulli, Stefano and Bianchi, Andrea and Ginestroni, Andrea and Tessa, Carlo and Nicolai, Emanuele and Aiello, Marco and Salvatore, Elena and Soricelli, Andrea and Diciotti, Stefano},
doi = {10.1371/journal.pone.0200258},
editor = {Lenglet, Christophe},
issn = {19326203},
journal = {PLoS ONE},
month = {jul},
number = {7},
pages = {e0200258},
title = {{Histogram analysis of dti-derived indices reveals pontocerebellar degeneration and its progression in SCA2}},
url = {https://dx.plos.org/10.1371/journal.pone.0200258},
volume = {13},
year = {2018}
}
@inproceedings{Mishra2017,
abstract = {Pathologists often deal with high complexity and sometimes disagreement over Osteosarcoma tumor classification due to cellular heterogeneity in the dataset. Segmentation and classification of histology tissue in H{\&}E stained tumor image datasets is challenging due to intra-class variations and inter-class similarity, crowded context, and noisy data. In recent years, deep learning approaches have led to encouraging results in breast cancer and prostate cancer analysis. In this paper, we propose a Convolutional neural network (CNN) as a tool to improve efficiency and accuracy of Osteosarcoma tumor classification into tumor classes (viable tumor, necrosis) vs non-tumor. The proposed CNN architecture contains five learned layers: three convolutional layers interspersed with max pooling layers for feature extraction and two fully-connected layers with data augmentation strategies to boost performance. We conclude that the use of neural network can assure high accuracy and efficiency in Osteosarcoma classification.},
author = {Mishra, Rashika and Daescu, Ovidiu and Leavey, Patrick and Rakheja, Dinesh and Sengupta, Anita},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-59575-7_2},
isbn = {9783319595740},
issn = {16113349},
keywords = {Convolutional neural network,Histology image analysis,Osteosarcoma},
title = {{Histopathological diagnosis for viable and non-viable tumor prediction for osteosarcoma using convolutional neural network}},
year = {2017}
}
@article{Gao2018HolisticCO,
abstract = {Interstitial lung diseases (ILD) involve several abnormal imaging patterns observed in computed tomography (CT) images. Accurate classification of these patterns plays a significant role in precise clinical decision making of the extent and nature of the diseases. Therefore, it is important for developing automated pulmonary computer-aided detection systems. Conventionally, this task relies on experts' manual identification of regions of interest (ROIs) as a prerequisite to diagnose potential diseases. This protocol is time consuming and inhibits fully automatic assessment. In this paper, we present a new method to classify ILD imaging patterns on CT images. The main difference is that the proposed algorithm uses the entire image as a holistic input. By circumventing the prerequisite of manual input ROIs, our problem set-up is significantly more difficult than previous work but can better address the clinical workflow. Qualitative and quantitative results using a publicly available ILD database demonstrate state-of-the-art classification accuracy under the patch-based classification and shows the potential of predicting the ILD type using holistic image.},
author = {Gao, Mingchen and Bagci, Ulas and Lu, Le and Wu, Aaron and Buty, Mario and Shin, Hoo Chang and Roth, Holger and Papadakis, Georgios Z. and Depeursinge, Adrien and Summers, Ronald M. and Xu, Ziyue and Mollura, Daniel J.},
doi = {10.1080/21681163.2015.1124249},
issn = {21681171},
journal = {Computer Methods in Biomechanics and Biomedical Engineering: Imaging and Visualization},
keywords = {Interstitial lung disease,convolutional neural network,holistic medical image classification},
month = {jan},
number = {1},
pages = {1--6},
title = {{Holistic classification of CT attenuation patterns for interstitial lung diseases via deep convolutional neural networks}},
url = {https://www.tandfonline.com/doi/full/10.1080/21681163.2015.1124249},
volume = {6},
year = {2018}
}
@online{GrandChallengLOLA11,
title = {{Home - LOLA11 - Grand Challenge}},
url = {https://lola11.grand-challenge.org/Home/}
}
@article{Graham2019,
abstract = {Nuclear segmentation and classification within Haematoxylin {\&} Eosin stained histology images is a fundamental prerequisite in the digital pathology work-flow. The development of automated methods for nuclear segmentation and classification enables the quantitative analysis of tens of thousands of nuclei within a whole-slide pathology image, opening up possibilities of further analysis of large-scale nuclear morphometry. However, automated nuclear segmentation and classification is faced with a major challenge in that there are several different types of nuclei, some of them exhibiting large intra-class variability such as the nuclei of tumour cells. Additionally, some of the nuclei are often clustered together. To address these challenges, we present a novel convolutional neural network for simultaneous nuclear segmentation and classification that leverages the instance-rich information encoded within the vertical and horizontal distances of nuclear pixels to their centres of mass. These distances are then utilised to separate clustered nuclei, resulting in an accurate segmentation, particularly in areas with overlapping instances. Then, for each segmented instance the network predicts the type of nucleus via a devoted up-sampling branch. We demonstrate state-of-the-art performance compared to other methods on multiple independent multi-tissue histology image datasets. As part of this work, we introduce a new dataset of Haematoxylin {\&} Eosin stained colorectal adenocarcinoma image tiles, containing 24,319 exhaustively annotated nuclei with associated class labels.},
archivePrefix = {arXiv},
arxivId = {1812.06499},
author = {Graham, Simon and Vu, Quoc Dang and Raza, Shan E.Ahmed and Azam, Ayesha and Tsang, Yee Wah and Kwak, Jin Tae and Rajpoot, Nasir},
doi = {10.1016/j.media.2019.101563},
eprint = {1812.06499},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Computational pathology,Deep learning,Nuclear classification,Nuclear segmentation},
title = {{Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images}},
volume = {58},
year = {2019}
}
@article{Graham2018,
abstract = {Nuclear segmentation and classification within Haematoxylin {\&} Eosin stained histology images is a fundamental prerequisite in the digital pathology work-flow. The development of automated methods for nuclear segmentation and classification enables the quantitative analysis of tens of thousands of nuclei within a whole-slide pathology image, opening up possibilities of further analysis of large-scale nuclear morphometry. However, automated nuclear segmentation and classification is faced with a major challenge in that there are several different types of nuclei, some of them exhibiting large intra-class variability such as the nuclei of tumour cells. Additionally, some of the nuclei are often clustered together. To address these challenges, we present a novel convolutional neural network for simultaneous nuclear segmentation and classification that leverages the instance-rich information encoded within the vertical and horizontal distances of nuclear pixels to their centres of mass. These distances are then utilised to separate clustered nuclei, resulting in an accurate segmentation, particularly in areas with overlapping instances. Then, for each segmented instance the network predicts the type of nucleus via a devoted up-sampling branch. We demonstrate state-of-the-art performance compared to other methods on multiple independent multi-tissue histology image datasets. As part of this work, we introduce a new dataset of Haematoxylin {\&} Eosin stained colorectal adenocarcinoma image tiles, containing 24,319 exhaustively annotated nuclei with associated class labels.},
annote = {{\_}eprint: 1812.06499},
archivePrefix = {arXiv},
arxivId = {1812.06499},
author = {Graham, Simon and Vu, Quoc Dang and {Ahmed Raza}, Shan E. and Azam, Ayesha and Tsang, Yee Wah and Kwak, Jin Tae and Rajpoot, Nasir},
eprint = {1812.06499},
journal = {arXiv},
keywords = {Computational pathology,,Deep learning,Nuclear classification,,Nuclear segmentation,},
pages = {1--11},
title = {{HoVer-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images}},
url = {http://arxiv.org/abs/1812.06499},
volume = {abs/1812.0},
year = {2018}
}
@book{Pequegnat2011,
address = {Boston, MA},
doi = {10.1007/978-1-4419-1454-5},
editor = {Pequegnat, Willo and Stover, Ellen and Boyce, Cheryl Anne},
isbn = {978-1-4419-1453-8},
publisher = {Springer US},
title = {{How to Write a Successful Research Grant Application}},
url = {http://link.springer.com/10.1007/978-1-4419-1454-5},
year = {2011}
}
@book{Hering2010,
address = {Berlin, Heidelberg},
author = {Hering, Lutz and Hering, Heike},
doi = {10.1007/978-3-540-69929-3},
isbn = {978-3-540-69928-6},
publisher = {Springer Berlin Heidelberg},
title = {{How to Write Technical Reports}},
url = {http://link.springer.com/10.1007/978-3-540-69929-3},
year = {2010}
}
@book{Hering2019,
address = {Berlin, Heidelberg},
author = {Hering, Heike},
doi = {10.1007/978-3-662-58107-0},
isbn = {978-3-662-58105-6},
publisher = {Springer Berlin Heidelberg},
title = {{How to Write Technical Reports}},
url = {http://link.springer.com/10.1007/978-3-662-58107-0},
year = {2019}
}
@article{Matek2019,
abstract = {Reliable recognition of malignant white blood cells is a key step in the diagnosis of hematologic malignancies such as Acute Myeloid Leukemia. Microscopic morphological examination of blood cells is usually performed by trained human examiners, making the process tedious, time-consuming and hard to standardise. We compile an annotated image dataset of over 18,000 white blood cells, use it to train a convolutional neural network for leukocyte classification, and evaluate the network's performance. The network classifies the most important cell types with high accuracy. It also allows us to decide two clinically relevant questions with human-level performance, namely (i) if a given cell has blast character, and (ii) if it belongs to the cell types normally present in non-pathological blood smears. Our approach holds the potential to be used as a classification aid for examining much larger numbers of cells in a smear than can usually be done by a human expert. This will allow clinicians to recognize malignant cell populations with lower prevalence at an earlier stage of the disease.},
author = {Matek, Christian and Schwarz, Simone and Spiekermann, Karsten and Marr, Carsten},
doi = {10.1038/s42256-019-0101-9},
issn = {2522-5839},
journal = {Nature Machine Intelligence},
month = {nov},
number = {11},
pages = {538--544},
title = {{Human-level recognition of blast cells in acute myeloid leukaemia with convolutional neural networks}},
url = {http://www.nature.com/articles/s42256-019-0101-9},
volume = {1},
year = {2019}
}
@article{Dolz2018,
abstract = {Recently, dense connections have attracted substantial attention in computer vision because they facilitate gradient flow and implicit deep supervision during training. Particularly, DenseNet that connects each layer to every other layer in a feed-forward fashion and has shown impressive performances in natural image classification tasks. We propose HyperDenseNet, a 3-D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems. Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path but also between those across different paths. This contrasts with the existing multi-modal CNN approaches, in which modeling several modalities relies entirely on a single joint layer (or level of abstraction) for fusion, typically either at the input or at the output of the network. Therefore, the proposed network has total freedom to learn more complex combinations between the modalities, within and in-between all the levels of abstraction, which increases significantly the learning representation. We report extensive evaluations over two different and highly competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing on six month infant data and the latter on adult images. HyperDenseNet yielded significant improvements over many state-of-the-art segmentation networks, ranking at the top on both benchmarks. We further provide a comprehensive experimental analysis of features re-use, which confirms the importance of hyper-dense connections in multi-modal representation learning. Our code is publicly available.},
annote = {{\_}eprint: 1804.02967},
archivePrefix = {arXiv},
arxivId = {1804.02967},
author = {Dolz, Jose and Gopinath, Karthik and Yuan, Jing and Lombaert, Herve and Desrosiers, Christian and {Ben Ayed}, Ismail},
doi = {10.1109/TMI.2018.2878669},
eprint = {1804.02967},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {3-D CNN,Deep learning,brain MRI,multi-modal imaging,segmentation},
number = {5},
pages = {1116--1126},
title = {{HyperDense-Net: A Hyper-Densely Connected CNN for Multi-Modal Image Segmentation}},
url = {http://arxiv.org/abs/1804.02967},
volume = {38},
year = {2019}
}
@article{Singanamalli2016,
abstract = {Background To identify computer extracted in vivo dynamic contrast enhanced (DCE) MRI markers associated with quantitative histomorphometric (QH) characteristics of microvessels and Gleason scores (GS) in prostate cancer. Methods This study considered retrospective data from 23 biopsy confirmed prostate cancer patients who underwent 3 Tesla multiparametric MRI before radical prostatectomy (RP). Representative slices from RP specimens were stained with vascular marker CD31. Tumor extent was mapped from RP sections onto DCE MRI using nonlinear registration methods. Seventy-seven microvessel QH features and 18 DCE MRI kinetic features were extracted and evaluated for their ability to distinguish low from intermediate and high GS. The effect of temporal sampling on kinetic features was assessed and correlations between those robust to temporal resolution and microvessel features discriminative of GS were examined. Results A total of 12 microvessel architectural features were discriminative of low and intermediate/high grade tumors with area under the receiver operating characteristic curve (AUC)  0.7. These features were most highly correlated with mean washout gradient (WG) (max rho = −0.62). Independent analysis revealed WG to be moderately robust to temporal resolution (intraclass correlation coefficient [ICC] = 0.63) and WG variance, which was poorly correlated with microvessel features, to be predictive of low grade tumors (AUC = 0.77). Enhancement ratio was the most robust (ICC = 0.96) and discriminative (AUC = 0.78) kinetic feature but was moderately correlated with microvessel features (max rho = −0.52). Conclusion Computer extracted features of prostate DCE MRI appear to be correlated with microvessel architecture and may be discriminative of low versus intermediate and high GS. J. MAGN. RESON. IMAGING 2016;43:149–158.},
annote = {{\_}eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jmri.24975},
author = {Singanamalli, Asha and Rusu, Mirabela and Sparks, Rachel E and Shih, Natalie N C and Ziober, Amy and Wang, Li-Ping and Tomaszewski, John and Rosen, Mark and Feldman, Michael and Madabhushi, Anant},
doi = {10.1002/jmri.24975},
journal = {Journal of Magnetic Resonance Imaging},
keywords = {DCE MRI,Gleason grades,imaging biomarkers,microvessel architecture,prostate cancer,quantitative histomorphometry},
number = {1},
pages = {149--158},
title = {{Identifying in vivo DCE MRI markers associated with microvessel architecture and gleason grades of prostate cancer}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jmri.24975},
volume = {43},
year = {2016}
}
@article{Kermany2018,
abstract = {The implementation of clinical-decision support algorithms for medical imaging faces challenges with reliability and interpretability. Here, we establish a diagnostic tool based on a deep-learning framework for the screening of patients with common treatable blinding retinal diseases. Our framework utilizes transfer learning, which trains a neural network with a fraction of the data of conventional approaches. Applying this approach to a dataset of optical coherence tomography images, we demonstrate performance comparable to that of human experts in classifying age-related macular degeneration and diabetic macular edema. We also provide a more transparent and interpretable diagnosis by highlighting the regions recognized by the neural network. We further demonstrate the general applicability of our AI system for diagnosis of pediatric pneumonia using chest X-ray images. This tool may ultimately aid in expediting the diagnosis and referral of these treatable conditions, thereby facilitating earlier treatment, resulting in improved clinical outcomes. Video Abstract: [Figure presented] Image-based deep learning classifies macular degeneration and diabetic retinopathy using retinal optical coherence tomography images and has potential for generalized applications in biomedical image interpretation and medical decision making.},
author = {Kermany, Daniel S. and Goldbaum, Michael and Cai, Wenjia and Valentim, Carolina C.S. and Liang, Huiying and Baxter, Sally L. and McKeown, Alex and Yang, Ge and Wu, Xiaokang and Yan, Fangbing and Dong, Justin and Prasadha, Made K. and Pei, Jacqueline and Ting, Magdalena and Zhu, Jie and Li, Christina and Hewett, Sierra and Dong, Jason and Ziyar, Ian and Shi, Alexander and Zhang, Runze and Zheng, Lianghong and Hou, Rui and Shi, William and Fu, Xin and Duan, Yaou and Huu, Viet A.N. and Wen, Cindy and Zhang, Edward D. and Zhang, Charlotte L. and Li, Oulan and Wang, Xiaobo and Singer, Michael A. and Sun, Xiaodong and Xu, Jie and Tafreshi, Ali and Lewis, M. Anthony and Xia, Huimin and Zhang, Kang},
doi = {10.1016/j.cell.2018.02.010},
issn = {10974172},
journal = {Cell},
keywords = {age-related macular degeneration,artificial intelligence,choroidal neovascularization,deep learning,diabetic macular edema,diabetic retinopathy,optical coherence tomography,pneumonia,screening,transfer learning},
month = {feb},
number = {5},
pages = {1122--1131.e9},
pmid = {29474911},
title = {{Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0092867418301545},
volume = {172},
year = {2018}
}
@article{Bakas2018,
abstract = {Gliomas are the most common primary brain malignancies, with different degrees of aggressiveness, variable prognosis and various heterogeneous histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic core, active and non-enhancing core. This intrinsic heterogeneity is also portrayed in their radio-phenotype, as their sub-regions are depicted by varying intensity profiles disseminated across multi-parametric magnetic resonance imaging (mpMRI) scans, reflecting varying biological properties. Their heterogeneous shape, extent, and location are some of the factors that make these tumors difficult to resect, and in some cases inoperable. The amount of resected tumor is a factor also considered in longitudinal scans, when evaluating the apparent tumor for potential diagnosis of progression. Furthermore, there is mounting evidence that accurate segmentation of the various tumor sub-regions can offer the basis for quantitative image analysis towards prediction of patient overall survival. This study assesses the state-of-the-art machine learning (ML) methods used for brain tumor image analysis in mpMRI scans, during the last seven instances of the International Brain Tumor Segmentation (BraTS) challenge, i.e., 2012-2018. Specifically, we focus on i) evaluating segmentations of the various glioma sub-regions in pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue of longitudinal growth of tumor sub-regions, beyond use of the RECIST/RANO criteria, and iii) predicting the overall survival from pre-operative mpMRI scans of patients that underwent gross total resection. Finally, we investigate the challenge of identifying the best ML algorithms for each of these tasks, considering that apart from being diverse on each instance of the challenge, the multi-institutional mpMRI BraTS dataset has also been a continuously evolving/growing dataset.},
archivePrefix = {arXiv},
arxivId = {1811.02629},
author = {Bakas, Spyridon and Reyes, Mauricio and Jakab, Andras and Bauer, Stefan and Rempfler, Markus and Crimi, Alessandro and Shinohara, Russell Takeshi and Berger, Christoph and Ha, Sung Min and Rozycki, Martin and Prastawa, Marcel and Alberts, Esther and Lipkova, Jana and Freymann, John and Kirby, Justin and Bilello, Michel and Fathallah-Shaykh, Hassan and Wiest, Roland and Kirschke, Jan and Wiestler, Benedikt and Colen, Rivka and Kotrotsou, Aikaterini and Lamontagne, Pamela and Marcus, Daniel and Milchenko, Mikhail and Nazeri, Arash and Weber, Marc-Andre and Mahajan, Abhishek and Baid, Ujjwal and Gerstner, Elizabeth and Kwon, Dongjin and Acharya, Gagan and Agarwal, Manu and Alam, Mahbubul and Albiol, Alberto and Albiol, Antonio and Albiol, Francisco J. and Alex, Varghese and Allinson, Nigel and Amorim, Pedro H. A. and Amrutkar, Abhijit and Anand, Ganesh and Andermatt, Simon and Arbel, Tal and Arbelaez, Pablo and Avery, Aaron and Azmat, Muneeza and B., Pranjal and Bai, W and Banerjee, Subhashis and Barth, Bill and Batchelder, Thomas and Batmanghelich, Kayhan and Battistella, Enzo and Beers, Andrew and Belyaev, Mikhail and Bendszus, Martin and Benson, Eze and Bernal, Jose and Bharath, Halandur Nagaraja and Biros, George and Bisdas, Sotirios and Brown, James and Cabezas, Mariano and Cao, Shilei and Cardoso, Jorge M. and Carver, Eric N and Casamitjana, Adri{\`{a}} and Castillo, Laura Silvana and Cat{\`{a}}, Marcel and Cattin, Philippe and Cerigues, Albert and Chagas, Vinicius S. and Chandra, Siddhartha and Chang, Yi-Ju and Chang, Shiyu and Chang, Ken and Chazalon, Joseph and Chen, Shengcong and Chen, Wei and Chen, Jefferson W and Chen, Zhaolin and Cheng, Kun and Choudhury, Ahana Roy and Chylla, Roger and Cl{\'{e}}rigues, Albert and Colleman, Steven and Colmeiro, Ramiro German Rodriguez and Combalia, Marc and Costa, Anthony and Cui, Xiaomeng and Dai, Zhenzhen and Dai, Lutao and Daza, Laura Alexandra and Deutsch, Eric and Ding, Changxing and Dong, Chao and Dong, Shidu and Dudzik, Wojciech and Eaton-Rosen, Zach and Egan, Gary and Escudero, Guilherme and Estienne, Th{\'{e}}o and Everson, Richard and Fabrizio, Jonathan and Fan, Yong and Fang, Longwei and Feng, Xue and Ferrante, Enzo and Fidon, Lucas and Fischer, Martin and French, Andrew P. and Fridman, Naomi and Fu, Huan and Fuentes, David and Gao, Yaozong and Gates, Evan and Gering, David and Gholami, Amir and Gierke, Willi and Glocker, Ben and Gong, Mingming and Gonz{\'{a}}lez-Vill{\'{a}}, Sandra and Grosges, T. and Guan, Yuanfang and Guo, Sheng and Gupta, Sudeep and Han, Woo-Sup and Han, Il Song and Harmuth, Konstantin and He, Huiguang and Hern{\'{a}}ndez-Sabat{\'{e}}, Aura and Herrmann, Evelyn and Himthani, Naveen and Hsu, Winston and Hsu, Cheyu and Hu, Xiaojun and Hu, Xiaobin and Hu, Yan and Hu, Yifan and Hua, Rui and Huang, Teng-Yi and Huang, Weilin and {Van Huffel}, Sabine and Huo, Quan and HV, Vivek and Iftekharuddin, Khan M. and Isensee, Fabian and Islam, Mobarakol and Jackson, Aaron S. and Jambawalikar, Sachin R. and Jesson, Andrew and Jian, Weijian and Jin, Peter and Jose, V Jeya Maria and Jungo, Alain and Kainz, B and Kamnitsas, Konstantinos and Kao, Po-Yu and Karnawat, Ayush and Kellermeier, Thomas and Kermi, Adel and Keutzer, Kurt and Khadir, Mohamed Tarek and Khened, Mahendra and Kickingereder, Philipp and Kim, Geena and King, Nik and Knapp, Haley and Knecht, Urspeter and Kohli, Lisa and Kong, Deren and Kong, Xiangmao and Koppers, Simon and Kori, Avinash and Krishnamurthi, Ganapathy and Krivov, Egor and Kumar, Piyush and Kushibar, Kaisar and Lachinov, Dmitrii and Lambrou, Tryphon and Lee, Joon and Lee, Chengen and Lee, Yuehchou and Lee, M and Lefkovits, Szidonia and Lefkovits, Laszlo and Levitt, James and Li, Tengfei and Li, Hongwei and Li, Wenqi and Li, Hongyang and Li, Xiaochuan and Li, Yuexiang and Li, Heng and Li, Zhenye and Li, Xiaoyu and Li, Zeju and Li, XiaoGang and Li, Wenqi and Lin, Zheng-Shen and Lin, Fengming and Lio, Pietro and Liu, Chang and Liu, Boqiang and Liu, Xiang and Liu, Mingyuan and Liu, Ju and Liu, Luyan and Llado, Xavier and Lopez, Marc Moreno and Lorenzo, Pablo Ribalta and Lu, Zhentai and Luo, Lin and Luo, Zhigang and Ma, Jun and Ma, Kai and Mackie, Thomas and Madabushi, Anant and Mahmoudi, Issam and Maier-Hein, Klaus H. and Maji, Pradipta and Mammen, CP and Mang, Andreas and Manjunath, B. S. and Marcinkiewicz, Michal and McDonagh, S and McKenna, Stephen and McKinley, Richard and Mehl, Miriam and Mehta, Sachin and Mehta, Raghav and Meier, Raphael and Meinel, Christoph and Merhof, Dorit and Meyer, Craig and Miller, Robert and Mitra, Sushmita and Moiyadi, Aliasgar and Molina-Garcia, David and Monteiro, Miguel A. B. and Mrukwa, Grzegorz and Myronenko, Andriy and Nalepa, Jakub and Ngo, Thuyen and Nie, Dong and Ning, Holly and Niu, Chen and Nuechterlein, Nicholas K and Oermann, Eric and Oliveira, Arlindo and Oliveira, Diego D. C. and Oliver, Arnau and Osman, Alexander F. I. and Ou, Yu-Nian and Ourselin, Sebastien and Paragios, Nikos and Park, Moo Sung and Paschke, Brad and Pauloski, J. Gregory and Pawar, Kamlesh and Pawlowski, Nick and Pei, Linmin and Peng, Suting and Pereira, Silvio M. and Perez-Beteta, Julian and Perez-Garcia, Victor M. and Pezold, Simon and Pham, Bao and Phophalia, Ashish and Piella, Gemma and Pillai, G. N. and Piraud, Marie and Pisov, Maxim and Popli, Anmol and Pound, Michael P. and Pourreza, Reza and Prasanna, Prateek and Prkovska, Vesna and Pridmore, Tony P. and Puch, Santi and Puybareau, {\'{E}}lodie and Qian, Buyue and Qiao, Xu and Rajchl, Martin and Rane, Swapnil and Rebsamen, Michael and Ren, Hongliang and Ren, Xuhua and Revanuru, Karthik and Rezaei, Mina and Rippel, Oliver and Rivera, Luis Carlos and Robert, Charlotte and Rosen, Bruce and Rueckert, Daniel and Safwan, Mohammed and Salem, Mostafa and Salvi, Joaquim and Sanchez, Irina and S{\'{a}}nchez, Irina and Santos, Heitor M. and Sartor, Emmett and Schellingerhout, Dawid and Scheufele, Klaudius and Scott, Matthew R. and Scussel, Artur A. and Sedlar, Sara and Serrano-Rubio, Juan Pablo and Shah, N. Jon and Shah, Nameetha and Shaikh, Mazhar and Shankar, B. Uma and Shboul, Zeina and Shen, Haipeng and Shen, Dinggang and Shen, Linlin and Shen, Haocheng and Shenoy, Varun and Shi, Feng and Shin, Hyung Eun and Shu, Hai and Sima, Diana and Sinclair, M and Smedby, Orjan and Snyder, James M. and Soltaninejad, Mohammadreza and Song, Guidong and Soni, Mehul and Stawiaski, Jean and Subramanian, Shashank and Sun, Li and Sun, Roger and Sun, Jiawei and Sun, Kay and Sun, Yu and Sun, Guoxia and Sun, Shuang and Suter, Yannick R and Szilagyi, Laszlo and Talbar, Sanjay and Tao, Dacheng and Tao, Dacheng and Teng, Zhongzhao and Thakur, Siddhesh and Thakur, Meenakshi H and Tharakan, Sameer and Tiwari, Pallavi and Tochon, Guillaume and Tran, Tuan and Tsai, Yuhsiang M. and Tseng, Kuan-Lun and Tuan, Tran Anh and Turlapov, Vadim and Tustison, Nicholas and Vakalopoulou, Maria and Valverde, Sergi and Vanguri, Rami and Vasiliev, Evgeny and Ventura, Jonathan and Vera, Luis and Vercauteren, Tom and Verrastro, C. A. and Vidyaratne, Lasitha and Vilaplana, Veronica and Vivekanandan, Ajeet and Wang, Guotai and Wang, Qian and Wang, Chiatse J. and Wang, Weichung and Wang, Duo and Wang, Ruixuan and Wang, Yuanyuan and Wang, Chunliang and Wang, Guotai and Wen, Ning and Wen, Xin and Weninger, Leon and Wick, Wolfgang and Wu, Shaocheng and Wu, Qiang and Wu, Yihong and Xia, Yong and Xu, Yanwu and Xu, Xiaowen and Xu, Peiyuan and Yang, Tsai-Ling and Yang, Xiaoping and Yang, Hao-Yu and Yang, Junlin and Yang, Haojin and Yang, Guang and Yao, Hongdou and Ye, Xujiong and Yin, Changchang and Young-Moxon, Brett and Yu, Jinhua and Yue, Xiangyu and Zhang, Songtao and Zhang, Angela and Zhang, Kun and Zhang, Xuejie and Zhang, Lichi and Zhang, Xiaoyue and Zhang, Yazhuo and Zhang, Lei and Zhang, Jianguo and Zhang, Xiang and Zhang, Tianhao and Zhao, Sicheng and Zhao, Yu and Zhao, Xiaomei and Zhao, Liang and Zheng, Yefeng and Zhong, Liming and Zhou, Chenhong and Zhou, Xiaobing and Zhou, Fan and Zhu, Hongtu and Zhu, Jin and Zhuge, Ying and Zong, Weiwei and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Davatzikos, Christos and van Leemput, Koen and Menze, Bjoern},
eprint = {1811.02629},
month = {nov},
title = {{Identifying the Best Machine Learning Algorithms for Brain Tumor Segmentation, Progression Assessment, and Overall Survival Prediction in the BRATS Challenge}},
url = {http://arxiv.org/abs/1811.02629},
year = {2018}
}
@article{Porwal2020,
abstract = {Diabetic Retinopathy (DR) is the most common cause of avoidable vision loss, predominantly affecting the working-age population across the globe. Screening for DR, coupled with timely consultation and treatment, is a globally trusted policy to avoid vision loss. However, implementation of DR screening programs is challenging due to the scarcity of medical professionals able to screen a growing global diabetic population at risk for DR. Computer-aided disease diagnosis in retinal image analysis could provide a sustainable approach for such large-scale screening effort. The recent scientific advances in computing capacity and machine learning approaches provide an avenue for biomedical scientists to reach this goal. Aiming to advance the state-of-the-art in automatic DR diagnosis, a grand challenge on “Diabetic Retinopathy – Segmentation and Grading” was organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI - 2018). In this paper, we report the set-up and results of this challenge that is primarily based on Indian Diabetic Retinopathy Image Dataset (IDRiD). There were three principal sub-challenges: lesion segmentation, disease severity grading, and localization of retinal landmarks and segmentation. These multiple tasks in this challenge allow to test the generalizability of algorithms, and this is what makes it different from existing ones. It received a positive response from the scientific community with 148 submissions from 495 registrations effectively entered in this challenge. This paper outlines the challenge, its organization, the dataset used, evaluation methods and results of top-performing participating solutions. The top-performing approaches utilized a blend of clinical information, data augmentation, and an ensemble of models. These findings have the potential to enable new developments in retinal image analysis and image-based DR screening in particular.},
author = {Porwal, Prasanna and Pachade, Samiksha and Kokare, Manesh and Deshmukh, Girish and Son, Jaemin and Bae, Woong and Liu, Lihong and Wang, Jianzong and Liu, Xinhui and Gao, Liangxin and Wu, Tian Bo and Xiao, Jing and Wang, Fengyan and Yin, Baocai and Wang, Yunzhi and Danala, Gopichandh and He, Linsheng and Choi, Yoon Ho and Lee, Yeong Chan and Jung, Sang Hyuk and Li, Zhongyu and Sui, Xiaodan and Wu, Junyan and Li, Xiaolong and Zhou, Ting and Toth, Janos and Baran, Agnes and Kori, Avinash and Chennamsetty, Sai Saketh and Safwan, Mohammed and Alex, Varghese and Lyu, Xingzheng and Cheng, Li and Chu, Qinhao and Li, Pengcheng and Ji, Xin and Zhang, Sanyuan and Shen, Yaxin and Dai, Ling and Saha, Oindrila and Sathish, Rachana and Melo, T{\^{a}}nia and Ara{\'{u}}jo, Teresa and Harangi, Balazs and Sheng, Bin and Fang, Ruogu and Sheet, Debdoot and Hajdu, Andras and Zheng, Yuanjie and Mendon{\c{c}}a, Ana Maria and Zhang, Shaoting and Campilho, Aur{\'{e}}lio and Zheng, Bin and Shen, Dinggang and Giancardo, Luca and Quellec, Gwenol{\'{e}} and M{\'{e}}riaudeau, Fabrice},
doi = {10.1016/j.media.2019.101561},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Challenge,Deep learning,Diabetic Retinopathy,Retinal image analysis},
month = {jan},
pages = {101561},
pmid = {31671320},
title = {{IDRiD: Diabetic Retinopathy – Segmentation and Grading Challenge}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841519301033},
volume = {59},
year = {2020}
}
@incollection{Lepetit2020,
address = {Cham},
author = {Lepetit, Vincent},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_797-1},
pages = {1--8},
publisher = {Springer International Publishing},
title = {{Image Descriptors and Similarity Measures}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}797-1},
year = {2020}
}
@incollection{Yu2020,
address = {Cham},
author = {Yu, Guoshen and Sapiro, Guillermo},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_233-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Image Enhancement and Restoration: Traditional Approaches}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}233-1},
year = {2020}
}
@incollection{Farid2020,
address = {Cham},
author = {Farid, Hany},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_877-1},
pages = {1--10},
publisher = {Springer International Publishing},
title = {{Image Forensics}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}877-1},
year = {2020}
}
@article{Wang2004,
abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.},
author = {Wang, Zhou and Bovik, Alan Conrad and Sheikh, Hamid Rahim and Simoncelli, Eero P.},
doi = {10.1109/TIP.2003.819861},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Error sensitivity,Human visual system (HVS),Image coding,Image quality assessment,JPEG,JPEG2000,Perceptual quality,Structural information,Structural similarity (SSIM)},
month = {apr},
number = {4},
pages = {600--612},
pmid = {15376593},
title = {{Image quality assessment: From error visibility to structural similarity}},
url = {http://ieeexplore.ieee.org/document/1284395/},
volume = {13},
year = {2004}
}
@article{Flohr2005,
abstract = {We present a theoretical overview and a performance evaluation of a novel z-sampling technique for multidetector row CT (MDCT), relying on a periodic motion of the focal spot in the longitudinal direction (z-flying focal spot) to double the number of simultaneously acquired slices. The z-flying focal spot technique has been implemented in a recently introduced MDCT scanner. Using 32 × 0.6 mm collimation, this scanner acquires 64 overlapping 0.6 mm slices per rotation in its spiral (helical) mode of operation, with the goal of improved longitudinal resolution and reduction of spiral artifacts. The longitudinal sampling distance at isocenter is 0.3 mm. We discuss in detail the impact of the z-flying focal spot technique on image reconstruction. We present measurements of spiral slice sensitivity profiles (SSPs) and of longitudinal resolution, both in the isocenter and off-center. We evaluate the pitch dependence of the image noise measured in a centered 20 cm water phantom. To investigate spiral image quality we present images of an anthropomorphic thorax phantom and patient scans. The full width at half maximum (FWHM) of the spiral SSPs shows only minor variations as a function of the pitch, measured values differ by less than 0.15 mm from the nominal values 0.6, 0.75, 1, 1.5, and 2 mm. The measured FWHM of the smallest slice ranges between 0.66 and 0.68 mm at isocenter, except for pitch 0.55 (0.72 mm). In a centered z-resolution phantom, bar patterns up to 15 lp/cm can be visualized independent of the pitch, corresponding to 0.33 mm longitudinal resolution. 100 mm off-center, bar patterns up to 14 lp/cm are visible, corresponding to an object size of 0.36 mm that can be resolved in the z direction. Image noise for constant effective mAs is almost independent of the pitch. Measured values show a variation of less than 7{\%} as a function of the pitch, which demonstrates correct utilization of the applied radiation dose at any pitch. The product of image noise and square root of the slice width (FWHM of the respective SSP) is the same constant for all slices except 0.6 mm. For the thinnest slice, relative image noise is increased by 17{\%}. Spiral windmill-type artifacts are effectively suppressed with the z-flying focal spot technique, which has the potential to maintain a low artifact level up to pitch 1.5, in this way increasing the maximum volume coverage speed that can be clinically used. {\textcopyright} 2005 American Association of Physicists in Medicine.},
author = {Flohr, T. G. and Stierstorfer, K. and Ulzheimer, S. and Bruder, H. and Primak, A. N. and McCollough, C. H.},
doi = {10.1118/1.1949787},
issn = {00942405},
journal = {Medical Physics},
keywords = {CT data sampling,CT image quality evaluation,Cone-beam CT,Multi-detector row CT},
title = {{Image reconstruction and image quality evaluation for a 64-slice CT scanner with z-flying focal spot}},
year = {2005}
}
@incollection{Brown2020,
abstract = {Image stitching or photo stitching is the process of combining multiple photographic images with overlapping fields of view to produce a segmented panorama or high-resolution image. Commonly performed through the use of computer software, most approaches to image stitching require nearly exact overlaps between images and identical exposures to produce seamless results, although some stitching algorithms actually benefit from differently exposed images by doing HDR (High Dynamic Range) imaging in regions of overlap. Some digital cameras can stitch their photos internally. Image stitching is widely used in today's world in applications such as$\backslash$n“Image Stabilization” feature in camcorders which use frame-rate image alignment.$\backslash$nHigh resolution photo mosaics in digital maps and satellite photos.$\backslash$nMedical Imaging.$\backslash$nMultiple image super-resolution.$\backslash$nVideo Stitching.$\backslash$nObject Insertion.},
address = {Cham},
author = {Brown, Matthew},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_13-1},
pages = {1--3},
publisher = {Springer International Publishing},
title = {{Image Stitching}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}13-1},
year = {2020}
}
@inproceedings{Krizhevsky2017,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
booktitle = {Communications of the ACM},
doi = {10.1145/3065386},
isbn = {9781627480031},
issn = {15577317},
number = {6},
pages = {84--90},
title = {{ImageNet classification with deep convolutional neural networks}},
volume = {60},
year = {2017}
}
@book{Turner2020,
address = {Cham},
author = {Turner, Phil},
doi = {10.1007/978-3-030-37348-1},
isbn = {978-3-030-37347-4},
publisher = {Springer International Publishing},
series = {Human–Computer Interaction Series},
title = {{Imagination + Technology}},
url = {http://link.springer.com/10.1007/978-3-030-37348-1},
year = {2020}
}
@article{P.2017,
abstract = {Background: In light of failed efforts to intensify treatment based on tumor necrosis after 10- weeks of chemotherapy, a novel biomarker to predict or measure response is critically needed to develop new therapies for patients with osteosarcoma. Objectives:We hypothesize that such a biomarker can be discovered by combining the emerging virtual microscopy technology of whole-slide digital imaging with computer-based image pattern recognition algorithms. Initial development of the biomarker would be based on histological features of tumor response. Design/Method: We retrieved data from resection specimens of 50 osteosarcoma patients treated at Children'sMedical Center Dallas between 1995-2015.We implemented a multi-step process to develop an automated image analysis protocol for histological features that involved digitalization of all slides, development of an image visualization and navigation tool, and an image segmentation and processing module for extracting major features within an image. Results: Tumor preparation: all resected tumors were prepared for histological evaluation with standard procedures that are briefly summarized. After surgical resection, each patient tumor was bisected in the plane predicted to provide the largest surface area for histology slide preparation. Using a pre-determined grid, each area within the grid was harvested to produce one slide (20-50 slides/patient). Using an Aperio scanner, we digitalized all histology slides for each patient, within which digitalized slides retain histological details from 2x to 20x magnification (each 20x image = 1 tile). Since each digitalized whole slide has approximately 4000 tiles, each patient whole tumor is represented by approximately 200,000 tiles.Main image processing algorithm development: Two-hundred image tiles were specifically identified from 3 patients' cases, to represent the main tumor features. An image segmentation pipeline was then designed and implemented to differentiate tumor from non-tumor and viable tumor from nonviable tumor based on color, shape, and cellular density characteristics. Implementing Color segmentation for chromatically distinct regions, and threshold segmentation using Multi-Otsu with clustering and contouring of segmented images for shape and density analysis, we completed the initial image analysis protocol. Using a flood-fill algorithm applied to pixels, cell counts, and cell density, we identified viable tumor and coagulative necrosis with 100{\%} accuracy, fibrosis with 93{\%} accuracy, and tumor osteoid with 89{\%} accuracy. We then identified 2,500 tiles based on a stratified random sample approach to train and validate a set of machine learning algorithms. These efforts are ongoing. Conclusion: We have completed the first phase necessary to automate the interpretation of histological features of tumor response in osteosarcoma.},
author = {P., Leavey and H.B., Arunachalam and B., Armaselu and A., Sengupta and D., Rakheja and S., Skapek and K., Cederberg and J.-P., Bach and S., Glick and M., Ni'Suilleabhain and R., Mishra and M., Martinez and R., Ziraldo and D., Leonard},
doi = {http://dx.doi.org/10.1002/pbc.26591},
issn = {1545-5017},
journal = {Pediatric Blood and Cancer},
keywords = {*biological marker,*osteosarcoma,*pattern recognition,*tumor necrosis,cell count,cell density,clinical article,digital imaging,female,fibrosis,histology,human,human tissue,image analysis,image processing,image segmentation,machine learning,male,microscopy,pipeline,random sample,surgery},
title = {{Implementation of computer-based image pattern recognition algorithms to interpret tumor necrosis; A first step in development of a novel biomarker in osteosarcoma}},
year = {2017}
}
@article{Paper2015,
author = {Paper, Scientific and Lillholm, M and Pai, A and Balas, I and Anker, C and Igel, C and Nielsen, M},
title = {{Improved Alzheimer ' s disease diagnostic performance using structural MRI : validation of the MRI combination biomarker that won the CADDementia challenge}},
year = {2015}
}
@book{Summers2018,
annote = {{\_}eprint: 1805.11272},
author = {Summers, Cecilia and Dinneen, Michael J},
title = {{Improved Mixed-Example Data Augmentation}},
year = {2018}
}
@article{Vanhoucke2011,
abstract = {Recent advances in deep learning have made the use of large, deep neural net- works with tens of millions of parameters suitable for a number of applications that require real-time processing. The sheer size of these networks can represent a challenging computational burden, even for modern CPUs. For this reason, GPUs are routinely used instead to train and run such networks. This paper is a tutorial for students and researchers on some of the techniques that can be used to reduce this computational cost considerably on modern x86 CPUs. We emphasize data layout, batching of the computation, the use of SSE2 instructions, and particularly leverage SSSE3 and SSE4 fixed-point instructions which provide a 3× improve- ment over an optimized floating-point baseline. We use speech recognition as an example task, and show that a real-time hybrid hidden Markov model / neural network (HMM/NN) large vocabulary system can be built with a 10× speedup over an unoptimized baseline and a 4× speedup over an aggressively optimized floating-point baseline at no cost in accuracy. The techniques described extend readily to neural network training and provide an effective alternative to the use of specialized hardware.},
author = {Vanhoucke, Vincent and Senior, Andrew and Mao, Mz},
issn = {9781450329569},
journal = {Proc. Deep Learning and {\ldots}},
pages = {1--8},
title = {{Improving the speed of neural networks on CPUs}},
url = {http://research.google.com/pubs/archive/37631.pdf},
year = {2011}
}
@inproceedings{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and nonresidual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08{\%} top-5 error on the test set of the ImageNet classification (CLS) challenge.},
annote = {{\_}eprint: 1602.07261},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
booktitle = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
eprint = {1602.07261},
pages = {4278--4284},
title = {{Inception-v4, inception-ResNet and the impact of residual connections on learning}},
year = {2017}
}
@data{GrandChallengeIDRID,
abstract = {Diabetic Retinopathy is the most prevalent cause of avoidable vision impairment, mainly affecting working age population in the world. Recent research has given a better understanding of requirement in clinical eye care practice to identify better and cheaper ways of identification, management, diagnosis and treatment of retinal disease. The importance of diabetic retinopathy screening programs and difficulty in achieving reliable early diagnosis of diabetic retinopathy at a reasonable cost needs attention to develop computer-aided diagnosis tool. Computer-aided disease diagnosis in retinal image analysis could ease mass screening of population with diabetes mellitus and help clinicians in utilizing their time more efficiently. The recent technological advances in computing power, communication systems, and machine learning techniques provide opportunities to the biomedical engineers and computer scientists to meet the requirements of clinical practice. Diverse and representative retinal image sets are essential for developing and testing digital screening programs and the automated algorithms at their core. To the best of our knowledge, the database for this challenge, IDRiD (Indian Diabetic Retinopathy Image Dataset), is the first database representative of an Indian population. Moreover, it is the only dataset constituting typical diabetic retinopathy lesions and also normal retinal structures annotated at a pixel level. This dataset provides information on the disease severity of diabetic retinopathy, and diabetic macular edema for each image. This makes it perfect for development and evaluation of image analysis algorithms for early detection of diabetic retinopathy.},
author = {Porwal, Prasanna and Pachade, Samiksha and Kamble, Ravi and Kokare, Manesh and Deshmukh, Girish and Sahasrabuddhe, Vivek and Meriaudeau, Fabrice},
booktitle = {IEEE Dataport},
doi = {10.21227/H25W98},
publisher = {IEEE Dataport},
title = {{Indian Diabetic Retinopathy Image Dataset (IDRiD)}},
url = {http://dx.doi.org/10.21227/H25W98},
year = {2018}
}
@article{Porwal2018,
abstract = {Diabetic Retinopathy is the most prevalent cause of avoidable vision impairment, mainly affecting the working-age population in the world. Recent research has given a better understanding of the requirement in clinical eye care practice to identify better and cheaper ways of identification, management, diagnosis and treatment of retinal disease. The importance of diabetic retinopathy screening programs and difficulty in achieving reliable early diagnosis of diabetic retinopathy at a reasonable cost needs attention to develop computer-aided diagnosis tool. Computer-aided disease diagnosis in retinal image analysis could ease mass screening of populations with diabetes mellitus and help clinicians in utilizing their time more efficiently. The recent technological advances in computing power, communication systems, and machine learning techniques provide opportunities to the biomedical engineers and computer scientists to meet the requirements of clinical practice. Diverse and representative retinal image sets are essential for developing and testing digital screening programs and the automated algorithms at their core. To the best of our knowledge, IDRiD (Indian Diabetic Retinopathy Image Dataset), is the first database representative of an Indian population. It constitutes typical diabetic retinopathy lesions and normal retinal structures annotated at a pixel level. The dataset provides information on the disease severity of diabetic retinopathy, and diabetic macular edema for each image. This makes it perfect for development and evaluation of image analysis algorithms for early detection of diabetic retinopathy.},
author = {Porwal, Prasanna and Pachade, Samiksha and Kamble, Ravi and Kokare, Manesh and Deshmukh, Girish and Sahasrabuddhe, Vivek and Meriaudeau, Fabrice},
doi = {10.3390/data3030025},
issn = {23065729},
journal = {Data},
keywords = {Diabetic macular edema,Diabetic retinopathy,Retinal fundus images},
month = {jul},
number = {3},
pages = {25},
title = {{Indian diabetic retinopathy image dataset (IDRiD): A database for diabetic retinopathy screening research}},
url = {http://www.mdpi.com/2306-5729/3/3/25},
volume = {3},
year = {2018}
}
@inproceedings{Silberman2012,
abstract = {We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation. {\textcopyright} 2012 Springer-Verlag.},
address = {Berlin, Heidelberg},
annote = {event-place: Florence, Italy},
author = {Silberman, Nathan and Hoiem, Derek and Kohli, Pushmeet and Fergus, Rob},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33715-4_54},
isbn = {9783642337147},
issn = {03029743},
number = {PART 5},
pages = {746--760},
publisher = {Springer-Verlag},
series = {ECCV'12},
title = {{Indoor segmentation and support inference from RGBD images}},
url = {http://dx.doi.org/10.1007/978-3-642-33715-4{\_}54},
volume = {7576 LNCS},
year = {2012}
}
@article{Crankshaw2018,
abstract = {The dominant cost in production machine learning workloads is not training individual models but serving predictions from increasingly complex prediction pipelines spanning multiple models, machine learning frameworks, and parallel hardware accelerators. Due to the complex interaction between model configurations and parallel hardware, prediction pipelines are challenging to provision and costly to execute when serving interactive latency-sensitive applications. This challenge is exacerbated by the unpredictable dynamics of bursty workloads. In this paper we introduce InferLine, a system which efficiently provisions and executes ML inference pipelines subject to end-to-end latency constraints by proactively optimizing and reactively controlling per-model configuration in a fine-grained fashion. Unpredictable changes in the serving workload are dynamically and cost-optimally accommodated with minimal service level degradation. InferLine introduces (1) automated model profiling and pipeline lineage extraction, (2) a fine-grain, cost-minimizing pipeline configuration planner, and (3) a fine-grain reactive controller. InferLine is able to configure and deploy prediction pipelines across a wide range of workload patterns and latency goals. It outperforms coarse-grained configuration alternatives by up 7.6x in cost while achieving up to 32x lower SLO miss rate on real workloads and generalizes across state-of-the-art model serving frameworks.},
annote = {{\_}eprint: 1812.01776},
archivePrefix = {arXiv},
arxivId = {1812.01776},
author = {Crankshaw, Daniel and Sela, Gur-Eyal and Zumar, Corey and Mo, Xiangxi and Gonzalez, Joseph E and Stoica, Ion and Tumanov, Alexey},
eprint = {1812.01776},
title = {{InferLine: ML Inference Pipeline Composition Framework}},
url = {http://arxiv.org/abs/1812.01776},
year = {2018}
}
@article{Faust2019,
abstract = {Deep learning is an emerging transformative tool in diagnostic medicine, yet limited access and the interpretability of learned parameters hinders widespread adoption. Here we have generated a diverse repository of 838,644 histopathologic images and used them to optimize and discretize learned representations into 512-dimensional feature vectors. Importantly, we show that individual machine-engineered features correlate with salient human-derived morphologic constructs and ontological relationships. Deciphering the overlap between human and machine reasoning may aid in eliminating biases and improving automation and accountability for artificial intelligence-assisted medicine.},
author = {Faust, Kevin and Bala, Sudarshan and van Ommeren, Randy and Portante, Alessia and {Al Qawahmed}, Raniah and Djuric, Ugljesa and Diamandis, Phedias},
doi = {10.1038/s42256-019-0068-6},
journal = {Nature Machine Intelligence},
number = {7},
pages = {316--321},
title = {{Intelligent feature engineering and ontological mapping of brain tumour histomorphologies by deep learning}},
volume = {1},
year = {2019}
}
@inproceedings{Pace2015,
abstract = {We present an interactive algorithm to segment the heart chambers and epicardial surfaces, including the great vessel walls, in pediatric cardiac MRI of congenital heart disease. Accurate whole-heart segmentation is necessary to create patient-specific 3D heart models for surgical planning in the presence of complex heart defects. Anatomical variability due to congenital defects precludes fully automatic atlas-based segmentation. Our interactive segmentation method exploits expert segmentations of a small set of short-axis slice regions to automatically delineate the remaining volume using patch-based segmentation. We also investigate the potential of active learning to automatically solicit user input in areas where segmentation error is likely to be high. Validation is performed on four subjects with double outlet right ventricle, a severe congenital heart defect. We show that strategies asking the user to manually segment regions of interest within short-axis slices yield higher accuracy with less user input than those querying entire short-axis slices.},
author = {Pace, Danielle F. and Dalca, Adrian V. and Geva, Tal and Powell, Andrew J. and Moghari, Mehdi H. and Golland, Polina},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24574-4_10},
isbn = {9783319245737},
issn = {16113349},
title = {{Interactive whole-heart segmentation in congenital heart disease}},
year = {2015}
}
@article{Hssayeni2020a,
abstract = {Traumatic brain injuries may cause intracranial hemorrhages (ICH). ICH could lead to disability or death if it is not accurately diagnosed and treated in a time-sensitive procedure. The current clinical protocol to diagnose ICH is examining Computerized Tomography (CT) scans by radiologists to detect ICH and localize its regions. However, this process relies heavily on the availability of an experienced radiologist. In this paper, we designed a study protocol to collect a dataset of 82 CT scans of subjects with a traumatic brain injury. Next, the ICH regions were manually delineated in each slice by a consensus decision of two radiologists. The dataset is publicly available online at the PhysioNet repository for future analysis and comparisons. In addition to publishing the dataset, which is the main purpose of this manuscript, we implemented a deep Fully Convolutional Networks (FCNs), known as U-Net, to segment the ICH regions from the CT scans in a fully-automated manner. The method as a proof of concept achieved a Dice coefficient of 0.31 for the ICH segmentation based on 5-fold cross-validation.},
author = {Hssayeni, Murtadha D. and Croock, Muayad S. and Salman, Aymen D. and Al-Khafaji, Hassan Falah and Yahya, Zakaria A. and Ghoraani, Behnaz},
doi = {10.3390/data5010014},
issn = {23065729},
journal = {Data},
keywords = {CT scans dataset,Fully convolutional network,ICH detection,Intracranial hemorrhage segmentation,U-Net},
month = {feb},
number = {1},
pages = {14},
title = {{Intracranial hemorrhage segmentation using a deep convolutional model}},
url = {https://www.mdpi.com/2306-5729/5/1/14},
volume = {5},
year = {2020}
}
@article{Hssayeni2020,
abstract = {Traumatic brain injuries may cause intracranial hemorrhages (ICH). ICH could lead to disability or death if it is not accurately diagnosed and treated in a time-sensitive procedure. The current clinical protocol to diagnose ICH is examining Computerized Tomography (CT) scans by radiologists to detect ICH and localize its regions. However, this process relies heavily on the availability of an experienced radiologist. In this paper, we designed a study protocol to collect a dataset of 82 CT scans of subjects with a traumatic brain injury. Next, the ICH regions were manually delineated in each slice by a consensus decision of two radiologists. The dataset is publicly available online at the PhysioNet repository for future analysis and comparisons. In addition to publishing the dataset, which is the main purpose of this manuscript, we implemented a deep Fully Convolutional Networks (FCNs), known as U-Net, to segment the ICH regions from the CT scans in a fully-automated manner. The method as a proof of concept achieved a Dice coefficient of 0.31 for the ICH segmentation based on 5-fold cross-validation.},
archivePrefix = {arXiv},
arxivId = {1910.08643},
author = {Hssayeni, Murtadha D. and Croock, Muayad S. and Salman, Aymen D. and Al-Khafaji, Hassan Falah and Yahya, Zakaria A. and Ghoraani, Behnaz},
doi = {10.3390/data5010014},
eprint = {1910.08643},
issn = {23065729},
journal = {Data},
keywords = {CT scans dataset,Fully convolutional network,ICH detection,Intracranial hemorrhage segmentation,U-Net},
month = {feb},
number = {1},
pages = {14},
title = {{Intracranial hemorrhage segmentation using a deep convolutional model}},
url = {https://www.mdpi.com/2306-5729/5/1/14},
volume = {5},
year = {2020}
}
@book{Manelli2020,
address = {Berkeley, CA},
author = {Manelli, Luciano},
booktitle = {Introducing Algorithms in C},
doi = {10.1007/978-1-4842-5623-7},
isbn = {978-1-4842-5622-0},
publisher = {Apress},
title = {{Introducing Algorithms in C}},
url = {http://link.springer.com/10.1007/978-1-4842-5623-7},
year = {2020}
}
@book{DeRosa2014,
address = {Berkeley, CA},
author = {{De Rosa}, Aurelio},
booktitle = {SidePoint},
doi = {10.1007/978-1-4842-5735-7},
isbn = {978-1-4842-5734-0},
keywords = {Speech Recognition,Web Speech API},
publisher = {Apress},
title = {{Introducing the Web Speech API}},
url = {http://www.sitepoint.com/introducing-web-speech-api/},
year = {2014}
}
@article{Maier2017,
abstract = {Ischemic stroke is the most common cerebrovascular disease, and its diagnosis, treatment, and study relies on non-invasive imaging. Algorithms for stroke lesion segmentation from magnetic resonance imaging (MRI) volumes are intensely researched, but the reported results are largely incomparable due to different datasets and evaluation schemes. We approached this urgent problem of comparability with the Ischemic Stroke Lesion Segmentation (ISLES) challenge organized in conjunction with the MICCAI 2015 conference. In this paper we propose a common evaluation framework, describe the publicly available datasets, and present the results of the two sub-challenges: Sub-Acute Stroke Lesion Segmentation (SISS) and Stroke Perfusion Estimation (SPES). A total of 16 research groups participated with a wide range of state-of-the-art automatic segmentation algorithms. A thorough analysis of the obtained data enables a critical evaluation of the current state-of-the-art, recommendations for further developments, and the identification of remaining challenges. The segmentation of acute perfusion lesions addressed in SPES was found to be feasible. However, algorithms applied to sub-acute lesion segmentation in SISS still lack accuracy. Overall, no algorithmic characteristic of any method was found to perform superior to the others. Instead, the characteristics of stroke lesion appearances, their evolution, and the observed challenges should be studied in detail. The annotated ISLES image datasets continue to be publicly available through an online evaluation system to serve as an ongoing benchmarking resource (www.isles-challenge.org).},
author = {Maier, Oskar and Menze, Bjoern H. and von der Gablentz, Janina and H{\"{a}}ni, Levin and Heinrich, Mattias P. and Liebrand, Matthias and Winzeck, Stefan and Basit, Abdul and Bentley, Paul and Chen, Liang and Christiaens, Daan and Dutil, Francis and Egger, Karl and Feng, Chaolu and Glocker, Ben and G{\"{o}}tz, Michael and Haeck, Tom and Halme, Hanna Leena and Havaei, Mohammad and Iftekharuddin, Khan M. and Jodoin, Pierre Marc and Kamnitsas, Konstantinos and Kellner, Elias and Korvenoja, Antti and Larochelle, Hugo and Ledig, Christian and Lee, Jia Hong and Maes, Frederik and Mahmood, Qaiser and Maier-Hein, Klaus H. and McKinley, Richard and Muschelli, John and Pal, Chris and Pei, Linmin and Rangarajan, Janaki Raman and Reza, Syed M.S. and Robben, David and Rueckert, Daniel and Salli, Eero and Suetens, Paul and Wang, Ching Wei and Wilms, Matthias and Kirschke, Jan S. and Kr{\"{a}}mer, Ulrike M. and M{\"{u}}nte, Thomas F. and Schramm, Peter and Wiest, Roland and Handels, Heinz and Reyes, Mauricio},
doi = {10.1016/j.media.2016.07.009},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Benchmark,Challenge,Comparison,Ischemic stroke,MRI,Segmentation},
month = {jan},
pages = {250--269},
pmid = {27475911},
title = {{ISLES 2015 - A public evaluation benchmark for ischemic stroke lesion segmentation from multispectral MRI}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841516301268},
volume = {35},
year = {2017}
}
@online{ISLES2015,
author = {Oskar, Maier and Bj{\"{o}}rn, Menze and Mauricio, Reyes},
title = {{ISLES: Ischemic Stroke Lesion Segmentation Challenge 2015}},
url = {http://www.isles-challenge.org/ISLES2015/},
urldate = {2020-05-20},
year = {2015}
}
@online{ISLES2016,
author = {Egger, Karl and Maier, Oskar and Reyes, Mauricio and Wiest, Roland},
title = {{ISLES: Ischemic Stroke Lesion Segmentation Challenge 2016}},
url = {http://www.isles-challenge.org/ISLES2017/{\%}0Ahttp://www.isles-challenge.org/ISLES2016/},
year = {2016}
}
@online{ISLES2017,
author = {Hakim, Arsany and Reyes, Mauricio and Wiest, Roland and Winzeck, Stefan},
title = {{ISLES: Ischemic Stroke Lesion Segmentation Challenge 2017}},
url = {http://www.isles-challenge.org/ISLES2017/},
urldate = {2020-05-20},
year = {2017}
}
@misc{ISLES2018,
author = {Hakim, Arsany and Reyes, Mauricio and Wiest, Roland and Lansberg, Maarten G and Christensen, S{\o}ren and Zaharchuk, Greg},
title = {{ISLES: Ischemic Stroke Lesion Segmentation Challenge 2018}},
year = {2018}
}
@article{Armeni2017,
abstract = {We present a dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. The dataset covers over 6,000m2 and contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360{\{}$\backslash$deg{\}} equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces. The dataset is available here: http://3Dsemantics.stanford.edu/},
annote = {{\_}eprint: 1702.01105},
archivePrefix = {arXiv},
arxivId = {1702.01105},
author = {Armeni, Iro and Sax, Sasha and Zamir, Amir Roshan and Savarese, Silvio},
eprint = {1702.01105},
journal = {CoRR},
keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Robotics},
title = {{Joint 2D-3D-Semantic Data for Indoor Scene Understanding}},
url = {http://arxiv.org/abs/1702.01105},
volume = {abs/1702.0},
year = {2017}
}
@article{Chiu2015,
abstract = {{\textcopyright} 2015 Optical Society of America. We present a fully automatic algorithm to identify fluid-filled regions and seven retinal layers on spectral domain optical coherence tomography images of eyes with diabetic macular edema (DME). To achieve this, we developed a kernel regression (KR)-based classification method to estimate fluid and retinal layer positions. We then used these classification estimates as a guide to more accurately segment the retinal layer boundaries using our previously described graph theory and dynamic programming (GTDP) framework. We validated our algorithm on 110 Bscans from ten patients with severe DME pathology, showing an overall mean Dice coefficient of 0.78 when comparing our KR + GTDP algorithm to an expert grader. This is comparable to the inter-observer Dice coefficient of 0.79. The entire data set is available online, including our automatic and manual segmentation results. To the best of our knowledge, this is the first validated, fully-automated, seven-layer and fluid segmentation method which has been applied to real-world images containing severe DME.},
author = {Chiu, Stephanie J. and Allingham, Michael J. and Mettu, Priyatham S. and Cousins, Scott W. and Izatt, Joseph A. and Farsiu, Sina},
doi = {10.1364/boe.6.001172},
issn = {2156-7085},
journal = {Biomedical Optics Express},
month = {apr},
number = {4},
pages = {1172},
title = {{Kernel regression based segmentation of optical coherence tomography images with diabetic macular edema}},
url = {https://www.osapublishing.org/abstract.cfm?URI=boe-6-4-1172},
volume = {6},
year = {2015}
}
@article{Sekuboyina2020a,
author = {Sekuboyina, Anjany and Rempfler, Markus and Valentinitsch, Alexander and Menze, Bjoern H. and Kirschke, Jan S.},
doi = {10.1148/ryai.2020190074},
issn = {2638-6100},
journal = {Radiology: Artificial Intelligence},
title = {{Labeling Vertebrae with Two-dimensional Reformations of Multidetector CT Images: An Adversarial Approach for Incorporating Prior Knowledge of Spine Anatomy}},
year = {2020}
}
@article{Ghiasi2016,
annote = {From Duplicate 1 (Laplacian Reconstruction and Refinement for Semantic Segmentation - Ghiasi, Golnaz; Fowlkes, Charless C)

{\_}eprint: 1605.02264},
archivePrefix = {arXiv},
arxivId = {1605.02264},
author = {Ghiasi, Golnaz and Fowlkes, Charless C},
eprint = {1605.02264},
journal = {CoRR},
title = {{Laplacian Reconstruction and Refinement for Semantic Segmentation}},
url = {http://arxiv.org/abs/1605.02264},
volume = {abs/1605.0},
year = {2016}
}
@article{DanielKermanyKangZhang2018,
abstract = {Be sure to download the most recent version of this dataset to maintain accuracy. This dataset contains thousands of validated OCT and Chest X-Ray images described and analyzed in "Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning". The images are split into a training set and a testing set of independent patients. Images are labeled as (disease)-(randomized patient ID)-(image number by this patient) and split into 4 directories: CNV, DME, DRUSEN, and NORMAL. This repository of images is made available for use in research only. How to cite this data:},
author = {{Daniel Kermany, Kang Zhang}, Michael Goldbaum},
doi = {10.17632/RSCBJBR9SJ.3},
title = {{Large Dataset of Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images}},
url = {https://data.mendeley.com/datasets/rscbjbr9sj/3},
volume = {3},
year = {2018}
}
@article{Song2015LargeML,
abstract = {Medical images usually exhibit large intra-class variation and inter-class ambiguity in the feature space, which could affect classification accuracy. To tackle this issue, we propose a new Large Margin Local Estimate (LMLE) classification model with sub-categorization based sparse representation. We first sub-categorize the reference sets of different classes into multiple clusters, to reduce feature variation within each subcategory compared to the entire reference set. Local estimates are generated for the test image using sparse representation with reference subcategories as the dictionaries. The similarity between the test image and each class is then computed by fusing the distances with the local estimates in a learning-based large margin aggregation construct to alleviate the problem of inter-class ambiguity. The derived similarities are finally used to determine the class label. We demonstrate that our LMLE model is generally applicable to different imaging modalities, and applied it to three tasks: interstitial lung disease (ILD) classification on high-resolution computed tomography (HRCT) images, phenotype binary classification and continuous regression on brain magnetic resonance (MR) imaging. Our experimental results show statistically significant performance improvements over existing popular classifiers.},
author = {Song, Yang and Cai, Weidong and Huang, Heng and Zhou, Yun and Feng, David Dagan and Wang, Yue and Fulham, Michael J. and Chen, Mei},
doi = {10.1109/TMI.2015.2393954},
issn = {0278-0062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Large margin fusion,medical image classification,sparse representation,sub-categorization},
month = {jun},
number = {6},
pages = {1362--1377},
pmid = {25616009},
title = {{Large Margin Local Estimate With Applications to Medical Image Classification}},
url = {http://ieeexplore.ieee.org/document/7014242/},
volume = {34},
year = {2015}
}
@inproceedings{Real2017,
abstract = {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6{\%} (95.6{\%} for ensemble) and 77.0{\%}, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.},
archivePrefix = {arXiv},
arxivId = {1703.01041},
author = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V and Kurakin, Alexey},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
eprint = {1703.01041},
isbn = {9781510855144},
pages = {4429--4446},
title = {{Large-scale evolution of image classifiers}},
volume = {6},
year = {2017}
}
@article{Paper2015a,
author = {Paper, Scientific and Bron, E and Smits, M and Barkhof, F and Niessen, W and Klein, S},
keywords = {applications-detection,cad,comparative studies,computer,computer applications,diagnosis,mr,neuroradiology brain},
pages = {1--18},
title = {{Large-scale objective comparison of 29 novel algorithms for computer-aided diagnosis of dementia based on structural}},
year = {2015}
}
@book{Datta2017,
address = {Cham},
author = {Datta, Dilip},
doi = {10.1007/978-3-319-47831-9},
isbn = {978-3-319-47830-2},
publisher = {Springer International Publishing},
title = {{LaTeX in 24 Hours}},
url = {http://link.springer.com/10.1007/978-3-319-47831-9},
year = {2017}
}
@misc{Dalca2020,
author = {Dalca, Adrian and {Yipeng Hu} and Vercauteren, Tom and Heinrich, Mattias and Hansen, Lasse and Modat, Marc and Vos, Bob De and {Yiming Xiao} and Rivaz, Hassan and Chabanas, Matthieu and Reinertsen, Ingerid and Landman, Bennett and Cardoso, Jorge and Ginneken, Bram Van and {Alessa Hering} and Murphy, Keelin},
doi = {10.5281/ZENODO.3715651},
keywords = {Abdomen,Biomedical Challenges,Brain,Deformable,MICCAI,MICCAI Challenges,Multimodal,Realtime,Registration,Thorax},
publisher = {Zenodo},
title = {{Learn2Reg - The Challenge}},
url = {https://zenodo.org/record/3715651},
year = {2020}
}
@inproceedings{Vahadane2016a,
abstract = {For better perception and analysis of images, good quality and high resolution (HR) are always preferred over degraded and low resolution (LR) images. Getting HR images can be cost and time prohibitive. Super resolution (SR) techniques can be an affordable alternative for small zoom factors. In medical imaging, specifically in the case of histological images, estimating an HR image from an LR one requires preservation of complex textures and edges defining various biological features (nuclei, cytoplasm etc.). This challenge is further aggravated by the scale variance of histological images that are taken of a flat biopsy slide instead of a 3D world. We propose an algorithm for SR of histological images that learns a mapping from zero-phase component analysis (ZCA)-whitened LR patches to ZCA-whitened HR patches at the desired scale. ZCA-whitening exploits the redundancy in data and enhances the texture and edges energies to better learn the desired LR to HR mapping, which we learn using a neural network. The qualitative and quantitative validation shows that improvements in HR estimation by proposed algorithm are statistically significant over benchmark learning-based SR algorithms.},
author = {Vahadane, Abhishek and Kumar, Neeraj and Sethi, Amit},
booktitle = {Proceedings - International Symposium on Biomedical Imaging},
doi = {10.1109/ISBI.2016.7493391},
isbn = {9781479923502},
issn = {19458452},
keywords = {Image super-resolution,histological image,neural network},
month = {apr},
pages = {816--819},
title = {{Learning based super-resolution of histological images}},
volume = {2016-June},
year = {2016}
}
@incollection{Concepts2020,
address = {Cham},
author = {Concepts, Related},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_823-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Learning from a Neuroscience Perspective}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}823-1},
year = {2020}
}
@inproceedings{lung_bm_1,
abstract = {Due to recent progress in Convolutional Neural Networks (CNNs),developing image-based CNN models for predictive diagnosis is gaining enormous interest. However,to date,insufficient imaging samples with truly pathological-proven labels impede the evaluation of CNN models at scale. In this paper,we formulate a domain-adaptation framework that learns transferable deep features for patient-level lung cancer malignancy prediction. The presented work learns CNN-based features from a large discovery set (2272 lung nodules) with malignancy likelihood labels involving multiple radiologists' assessments,and then tests the transferable predictability of these CNN-based features on a diagnosis-definite set (115 cases) with true pathologically-proven lung cancer labels. We evaluate our approach on the Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI) dataset,where both human expert labeling information on cancer malignancy likelihood and a set of pathologically-proven malignancy labels were provided. Experimental results demonstrate the superior predictive performance of the transferable deep features on predicting true patient-level lung cancer malignancy (Acc=70.69 {\%},AUC=0.66),which outperforms a nodule level CNN model (Acc=65.38 {\%},AUC=0.63) and is even comparable to that of using the radiologists' knowledge (Acc=72.41 {\%},AUC=0.76). The proposed model can largely reduce the demand for pathologically proven data,holding promise to empower cancer diagnosis by leveraging multi-source CT imaging datasets.},
address = {Cham},
author = {Shen, Wei and Zhou, Mu and Yang, Feng and Dong, Di and Yang, Caiyun and Zang, Yali and Tian, Jie},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46723-8_15},
editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R and Unal, Gozde and Wells, William},
isbn = {9783319467221},
issn = {16113349},
pages = {124--131},
publisher = {Springer International Publishing},
title = {{Learning from Experts: Developing Transferable Deep Features for Patient-Level Lung Cancer Prediction}},
url = {http://link.springer.com/10.1007/978-3-319-46723-8{\_}15},
volume = {9901 LNCS},
year = {2016}
}
@inproceedings{Biffi2018LearningIA,
abstract = {Alterations in the geometry and function of the heart define well-established causes of cardiovascular disease. However, current approaches to the diagnosis of cardiovascular diseases often rely on subjective human assessment as well as manual analysis of medical images. Both factors limit the sensitivity in quantifying complex structural and functional phenotypes. Deep learning approaches have recently achieved success for tasks such as classification or segmentation of medical images, but lack interpretability in the feature extraction and decision processes, limiting their value in clinical diagnosis. In this work, we propose a 3D convolutional generative model for automatic classification of images from patients with cardiac diseases associated with structural remodeling. The model leverages interpretable task-specific anatomic patterns learned from 3D segmentations. It further allows to visualise and quantify the learned pathology-specific remodeling patterns in the original input space of the images. This approach yields high accuracy in the categorization of healthy and hypertrophic cardiomyopathy subjects when tested on unseen MR images from our own multi-centre dataset (100{\%}) as well on the ACDC MICCAI 2017 dataset (90{\%}). We believe that the proposed deep learning approach is a promising step towards the development of interpretable classifiers for the medical imaging domain, which may help clinicians to improve diagnostic accuracy and enhance patient risk-stratification.},
archivePrefix = {arXiv},
arxivId = {1807.06843},
author = {Biffi, Carlo and Oktay, Ozan and Tarroni, Giacomo and Bai, Wenjia and {De Marvao}, Antonio and Doumou, Georgia and Rajchl, Martin and Bedair, Reem and Prasad, Sanjay and Cook, Stuart and O'Regan, Declan and Rueckert, Daniel},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-00934-2_52},
eprint = {1807.06843},
isbn = {9783030009335},
issn = {16113349},
pages = {464--471},
title = {{Learning interpretable anatomical features through deep generative models: Application to cardiac remodeling}},
url = {http://link.springer.com/10.1007/978-3-030-00934-2{\_}52},
volume = {11071 LNCS},
year = {2018}
}
@inproceedings{Esteves2017,
abstract = {We address the problem of 3D rotation equivariance in convolutional neural networks. 3D rotations have been a challenging nuisance in 3D classification tasks requiring higher capacity and extended data augmentation in order to tackle it. We model 3D data with multi-valued spherical functions and we propose a novel spherical convolutional network that implements exact convolutions on the sphere by realizing them in the spherical harmonic domain. Resulting filters have local symmetry and are localized by enforcing smooth spectra. We apply a novel pooling on the spectral domain and our operations are independent of the underlying spherical resolution throughout the network. We show that networks with much lower capacity and without requiring data augmentation can exhibit performance comparable to the state of the art in standard retrieval and classification benchmarks.},
annote = {{\_}eprint: 1711.06721},
archivePrefix = {arXiv},
arxivId = {1711.06721},
author = {Esteves, Carlos and Allen-Blanchette, Christine and Makadia, Ameesh and Daniilidis, Kostas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01261-8_4},
eprint = {1711.06721},
isbn = {9783030012601},
issn = {16113349},
number = {November},
pages = {54--70},
title = {{Learning SO(3) Equivariant Representations with Spherical CNNs}},
url = {http://arxiv.org/abs/1711.06721},
volume = {11217 LNCS},
year = {2018}
}
@inproceedings{Tran2014,
abstract = {We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets, 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets, and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8{\%} accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.},
annote = {{\_}eprint: 1412.0767},
archivePrefix = {arXiv},
arxivId = {1412.0767},
author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.510},
eprint = {1412.0767},
isbn = {9781467383912},
issn = {15505499},
keywords = {Computer Science - Computer Vision and Pattern Rec},
pages = {4489--4497},
title = {{Learning spatiotemporal features with 3D convolutional networks}},
volume = {2015 Inter},
year = {2015}
}
@article{Swiderska-Chadaj2019,
abstract = {The immune system is of critical importance in the development of cancer. The evasion of destruction by the immune system is one of the emerging hallmarks of cancer. We have built a dataset of 171,166 manually annotated CD3+ and CD8+ cells, which we used to train deep learning algorithms for automatic detection of lymphocytes in histopathology images to better quantify immune response. Moreover, we investigate the effectiveness of four deep learning based methods when different subcompartments of the whole-slide image are considered: normal tissue areas, areas with immune cell clusters, and areas containing artifacts. We have compared the proposed methods in breast, colon and prostate cancer tissue slides collected from nine different medical centers. Finally, we report the results of an observer study on lymphocyte quantification, which involved four pathologists from different medical centers, and compare their performance with the automatic detection. The results give insights on the applicability of the proposed methods for clinical use. U-Net obtained the highest performance with an F1-score of 0.78 and the highest agreement with manual evaluation ($\kappa$=0.72), whereas the average pathologists agreement with reference standard was $\kappa$=0.64. The test set and the automatic evaluation procedure are publicly available at lyon19.grand-challenge.org.},
author = {Swiderska-Chadaj, Zaneta and Pinckaers, Hans and van Rijthoven, Mart and Balkenhol, Maschenka and Melnikova, Margarita and Geessink, Oscar and Manson, Quirine and Sherman, Mark and Polonia, Antonio and Parry, Jeremy and Abubakar, Mustapha and Litjens, Geert and van der Laak, Jeroen and Ciompi, Francesco},
doi = {10.1016/j.media.2019.101547},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Computational pathology,Deep learning,Immune cell detection,Immunohistochemistry},
pmid = {31476576},
title = {{Learning to detect lymphocytes in immunohistochemistry with deep learning}},
year = {2019}
}
@inproceedings{Zoph2017,
abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the 'NASNet search space') which enables transferability. In our experiments, we search for the best convolutional layer (or 'cell') on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a 'NASNet architecture'. We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4{\%} error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7{\%} top-1 and 96.2{\%} top-5 on ImageNet. Our model is 1.2{\%} better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28{\%} in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74{\%} top-1 accuracy, which is 3.1{\%} better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0{\%} achieving 43.1{\%} mAP on the COCO dataset.},
archivePrefix = {arXiv},
arxivId = {1707.07012},
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00907},
eprint = {1707.07012},
isbn = {9781538664209},
issn = {10636919},
pages = {8697--8710},
title = {{Learning Transferable Architectures for Scalable Image Recognition}},
year = {2018}
}
@article{Li2018c,
abstract = {When building a unified vision system or gradually adding new apabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance.},
archivePrefix = {arXiv},
arxivId = {1606.09282},
author = {Li, Zhizhong and Hoiem, Derek},
doi = {10.1109/TPAMI.2017.2773081},
eprint = {1606.09282},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional neural networks,deep learning,multi-task learning,transfer learning,visual recognition},
pmid = {29990101},
title = {{Learning without Forgetting}},
year = {2018}
}
@inproceedings{Seff2015,
abstract = {Histograms of oriented gradients (HOG) are widely employed image descriptors in modern computer-aided diagnosis systems. Built upon a set of local, robust statistics of low-level image gradients, HOG features are usually computed on raw intensity images. In this paper, we explore a learned image transformation scheme for producing higher-level inputs to HOG. Leveraging semantic object boundary cues, our methods compute data-driven image feature maps via a supervised boundary detector. Compared with the raw image map, boundary cues offer mid-level, more object-specific visual responses that can be suited for subsequent HOG encoding. We validate integrations of several image transformation maps with an application of computer-aided detection of lymph nodes on thoracoabdominal CT images. Our experiments demonstrate that semantic boundary cues based HOG descriptors complement and enrich the raw intensity alone. We observe an overall system with substantially improved results (∼ 78{\%} versus 60{\%} recall at 3 FP/volume for two target regions). The proposed system also moderately outperforms the state-of-the-art deep convolutional neural network (CNN) system in the mediastinum region, without relying on data augmentation and requiring significantly fewer training samples.},
author = {Seff, Ari and Lu, Le and Barbu, Adrian and Roth, Holger and Shin, Hoo Chang and Summers, Ronald M.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24571-3_7},
isbn = {9783319245706},
issn = {16113349},
title = {{Leveraging mid-level semantic boundary cues for automated lymph node detection}},
year = {2015}
}
@article{Zhang2019,
abstract = {Reconstructing 3D geometry from satellite imagery is an important topic of research. However, disparities exist between how this 3D reconstruction problem is handled in the remote sensing context and how multi-view reconstruction pipelines have been developed in the computer vision community. In this paper, we explore whether state-of-the-art reconstruction pipelines from the vision community can be applied to the satellite imagery. Along the way, we address several challenges adapting vision-based structure from motion and multi-view stereo methods. We show that vision pipelines can offer competitive speed and accuracy in the satellite context.},
annote = {{\_}eprint: 1910.02989},
archivePrefix = {arXiv},
arxivId = {1910.02989},
author = {Zhang, Kai and Snavely, Noah and Sun, Jin},
eprint = {1910.02989},
title = {{Leveraging Vision Reconstruction Pipelines for Satellite Imagery}},
url = {http://arxiv.org/abs/1910.02989},
year = {2019}
}
@incollection{Govindu2020,
address = {Cham},
author = {Govindu, Venu Madhav},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_871-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Lie Algebra}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}871-1},
year = {2020}
}
@article{Dugre2019,
abstract = {Objective: It has long been assumed that paranoid ideation may stem from an aberrant limbic response to threatening stimuli. However, results from functional neuroimaging studies using negative emotional stimuli have failed to confirm this assumption. One of the potential reasons for the lack of effect is that study participants with psychosis may display aberrant brain responses to neutral material rather than to threatening stimuli. The authors conducted a functional neuroimaging meta-analysis to test this hypothesis. Methods: A literature search was performed with PubMed, Google Scholar, and Embase to identify functional neuroimaging studies examining brain responses to neutral material in patients with psychosis. A total of 23 studies involving schizophrenia patients were retrieved. Using t-maps of peak coordinates to calculate effect sizes, a random-effects model meta-analysis was performed with the anisotropic effect-size version of Seed-based d Mapping software. Results: In schizophrenia patients relative to healthy control subjects, increased activations were observed in the left and right amygdala and parahippocampus and the left putamen, hippocampus, and insula in response to neutral stimuli. Conclusions: Given that several limbic regions were found to be more activated in schizophrenia patients than in control subjects, the results of this meta-analysis strongly suggest that these patients confer aberrant emotional significance to nonthreatening stimuli. In theory, this abnormal brain reactivity may fuel delusional thoughts. Studies are needed in individuals at risk of psychosis to determine whether aberrant limbic reactivity to neutral stimuli is an early neurofunctional marker of psychosis vulnerability.},
author = {Dugr{\'{e}}, Jules R. and Bitar, Nathalie and Dumais, Alexandre and Potvin, St{\'{e}}phane},
doi = {10.1176/appi.ajp.2019.19030247},
issn = {15357228},
journal = {American Journal of Psychiatry},
month = {dec},
number = {12},
pages = {1021--1029},
pmid = {31509006},
title = {{Limbic hyperactivity in response to emotionally neutral stimuli in schizophrenia: A neuroimaging meta-analysis of the hypervigilant mind}},
url = {http://ajp.psychiatryonline.org/doi/10.1176/appi.ajp.2019.19030247},
volume = {176},
year = {2019}
}
@incollection{Sugihara2020,
address = {Cham},
author = {Sugihara, Kokichi},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_390-1},
pages = {1--3},
publisher = {Springer International Publishing},
title = {{Line Drawing Labeling}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}390-1},
year = {2020}
}
@incollection{Murota1955,
address = {Cham},
author = {Murota, Kazuo},
booktitle = {The Economic Studies Quarterly (Tokyo. 1950)},
doi = {10.11398/economics1950.5.3-4_156},
issn = {2185-4408},
number = {3-4},
pages = {156--163},
publisher = {Springer International Publishing},
title = {{Linear Programmingモデルの簡単な応用について}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}648-1},
volume = {5},
year = {1955}
}
@article{Carass2017a,
abstract = {The data presented in this article is related to the research article entitled “Longitudinal multiple sclerosis lesion segmentation: Resource and challenge” (Carass et al., 2017) [1]. In conjunction with the 2015 International Symposium on Biomedical Imaging, we organized a longitudinal multiple sclerosis (MS) lesion segmentation challenge providing training and test data to registered participants. The training data consists of five subjects with a mean of 4.4 (±0.55) time-points, and test data of fourteen subjects with a mean of 4.4 (±0.67) time-points. All 82 data sets had the white matter lesions associated with multiple sclerosis delineated by two human expert raters. The training data including multi-modal scans and manually delineated lesion masks is available for download.1 In addition, the testing data is also being made available in conjunction with a website for evaluating the automated analysis of the testing data.},
author = {Carass, Aaron and Roy, Snehashis and Jog, Amod and Cuzzocreo, Jennifer L. and Magrath, Elizabeth and Gherman, Adrian and Button, Julia and Nguyen, James and Bazin, Pierre Louis and Calabresi, Peter A. and Crainiceanu, Ciprian M. and Ellingsen, Lotta M. and Reich, Daniel S. and Prince, Jerry L. and Pham, Dzung L.},
doi = {10.1016/j.dib.2017.04.004},
issn = {23523409},
journal = {Data in Brief},
month = {jun},
pages = {346--350},
title = {{Longitudinal multiple sclerosis lesion segmentation data resource}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2352340917301361},
volume = {12},
year = {2017}
}
@article{Carass2017,
abstract = {In conjunction with the ISBI 2015 conference, we organized a longitudinal lesion segmentation challenge providing training and test data to registered participants. The training data consisted of five subjects with a mean of 4.4 time-points, and test data of fourteen subjects with a mean of 4.4 time-points. All 82 data sets had the white matter lesions associated with multiple sclerosis delineated by two human expert raters. Eleven teams submitted results using state-of-the-art lesion segmentation algorithms to the challenge, with ten teams presenting their results at the conference. We present a quantitative evaluation comparing the consistency of the two raters as well as exploring the performance of the eleven submitted results in addition to three other lesion segmentation algorithms. The challenge presented three unique opportunities: (1) the sharing of a rich data set; (2) collaboration and comparison of the various avenues of research being pursued in the community; and (3) a review and refinement of the evaluation metrics currently in use. We report on the performance of the challenge participants, as well as the construction and evaluation of a consensus delineation. The image data and manual delineations will continue to be available for download, through an evaluation website2 The Challenge Evaluation Website is: http://smart-stats-tools.org/lesion-challenge-2015 as a resource for future researchers in the area. This data resource provides a platform to compare existing methods in a fair and consistent manner to each other and multiple manual raters.},
author = {Carass, Aaron and Roy, Snehashis and Jog, Amod and Cuzzocreo, Jennifer L. and Magrath, Elizabeth and Gherman, Adrian and Button, Julia and Nguyen, James and Prados, Ferran and Sudre, Carole H. and {Jorge Cardoso}, Manuel and Cawley, Niamh and Ciccarelli, Olga and Wheeler-Kingshott, Claudia A.M. and Ourselin, S{\'{e}}bastien and Catanese, Laurence and Deshpande, Hrishikesh and Maurel, Pierre and Commowick, Olivier and Barillot, Christian and Tomas-Fernandez, Xavier and Warfield, Simon K. and Vaidya, Suthirth and Chunduru, Abhijith and Muthuganapathy, Ramanathan and Krishnamurthi, Ganapathy and Jesson, Andrew and Arbel, Tal and Maier, Oskar and Handels, Heinz and Iheme, Leonardo O. and Unay, Devrim and Jain, Saurabh and Sima, Diana M. and Smeets, Dirk and Ghafoorian, Mohsen and Platel, Bram and Birenbaum, Ariel and Greenspan, Hayit and Bazin, Pierre Louis and Calabresi, Peter A. and Crainiceanu, Ciprian M. and Ellingsen, Lotta M. and Reich, Daniel S. and Prince, Jerry L. and Pham, Dzung L.},
doi = {10.1016/j.neuroimage.2016.12.064},
issn = {10959572},
journal = {NeuroImage},
month = {mar},
pages = {77--102},
title = {{Longitudinal multiple sclerosis lesion segmentation: Resource and challenge}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811916307819},
volume = {148},
year = {2017}
}
@incollection{Gortler1996,
abstract = {This paper discusses a new method for capturing the complete appearance of both synthetic and real world objects and scenes, representing this information, and then using this representation to render images of the object from new camera positions. Unlike the shape capture process traditionally used in computer vision and the rendering process traditionally used in computer graphics, our approach does not rely on geometric representations. Instead we sample and reconstruct a 4D function, which we call a Lumigraph. The Lumigraph is a subset of the complete plenoptic function that describes the flow of light at all positions in all directions. With the Lumigraph, new images of the object can be generated very quickly, independent of the geometric or illumination complexity of the scene or object. The paper discusses a complete working system including the capture of samples, the construction of the Lumigraph, and the subsequent rendering of images from this new representation.},
address = {Cham},
author = {Gortler, Steven J. and Grzeszczuk, Radek and Szeliski, Richard and Cohen, Michael F.},
booktitle = {Proceedings of the ACM SIGGRAPH Conference on Computer Graphics},
doi = {10.1007/978-3-030-03243-2_8-1},
pages = {43--54},
publisher = {Springer International Publishing},
title = {{Lumigraph}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}8-1},
year = {1996}
}
@article{han2019,
abstract = {The annotated medical images are usually expensive to be collected. This paper proposes a deep learning method on small data to classify Common Imaging Signs of Lung diseases (CISL) in computed tomography (CT) images. We explore both the real data and the data generated by Generative Adversarial Network (GAN) to improve the reliability and the generalization of learning. First, we use GAN to generate a large number of CISLs from small annotated data, which are difficult to be distinguished from real counterparts. These generated samples are used to pre-train a Convolutional Neural Network (CNN) for classifying CISLs. Second, we fine-tune the CNN classification model with real data. Experiments were conducted on the LISS database of CISLs. We successfully convinced radiologists that our generated CISLs samples were real for 56.7{\%} of our experiments. The pre-trained CNN model achieves 88.4{\%} of mean accuracy of binary classification, and after fine-tuning, the mean accuracy is significantly increased to 95.0{\%}. For multi-classification of all types of CISLs and normal tissues, through the two stages of training, the mean accuracy, sensitivity and specificity are up to about 91.83{\%}, 92.73{\%} and 99.0{\%}, respectively. To our knowledge, this is the best result achieved on the LISS database, which demonstrates that the proposed method is effective and promising for fulfilling deep learning on small data.},
archivePrefix = {arXiv},
arxivId = {1903.00183},
author = {He, Guocai},
eprint = {1903.00183},
issn = {23318422},
journal = {arXiv},
keywords = {Convolutional Neural Networks (CNNs),Deep learning,Generative Adversarial Network (GAN),Lung CT imaging signs classification,Small data},
month = {mar},
title = {{Lung CT Imaging Sign Classification through Deep Learning on Small Data}},
url = {http://arxiv.org/abs/1903.00183},
volume = {abs/1903.0},
year = {2019}
}
@inproceedings{Nam2018LungNS,
abstract = {Lung nodule segmentation can help radiologists' analysis of nodule risk. Recent deep learning based approaches have shown promising results in the segmentation task. However, a 3D segmentation map necessary for training the algorithms requires an expensive effort from expert radiologists. We propose a new method to train the deep neural network, only utilizing diameter information for each nodule. We validate our model with the LUNA16 dataset, showing competitive results compared to the previous state-of-the-art methods in various evaluation metrics. Our experiments also provide plausible qualitative results comparable to the ground truth segmentation.},
author = {Kim, Jihang},
booktitle = {Medical Imaging with Deep Learning (MIDL 2018)},
number = {Midl},
pages = {3--5},
title = {{Lung nodule segmentation with convolutional neural network trained by simple diameter information}},
year = {2018}
}
@article{Candemir2014,
abstract = {The National Library of Medicine (NLM) is developing a digital chest X-ray (CXR) screening system for deployment in resource constrained communities and developing countries worldwide with a focus on early detection of tuberculosis. A critical component in the computer-aided diagnosis of digital CXRs is the automatic detection of the lung regions. In this paper, we present a nonrigid registration-driven robust lung segmentation method using image retrieval-based patient specific adaptive lung models that detects lung boundaries, surpassing state-of-the-art performance. The method consists of three main stages: 1) a content-based image retrieval approach for identifying training images (with masks) most similar to the patient CXR using a partial Radon transform and Bhattacharyya shape similarity measure, 2) creating the initial patient-specific anatomical model of lung shape using SIFT-flow for deformable registration of training masks to the patient CXR, and 3) extracting refined lung boundaries using a graph cuts optimization approach with a customized energy function. Our average accuracy of 95.4{\%} on the public JSRT database is the highest among published results. A similar degree of accuracy of 94.1{\%} and 91.7{\%} on two new CXR datasets from Montgomery County, MD, USA, and India, respectively, demonstrates the robustness of our lung segmentation approach. {\textcopyright} 2013 IEEE.},
author = {Candemir, Sema and Jaeger, Stefan and Palaniappan, Kannappan and Musco, Jonathan P. and Singh, Rahul K. and Xue, Zhiyun and Karargyris, Alexandros and Antani, Sameer and Thoma, George and McDonald, Clement J.},
doi = {10.1109/TMI.2013.2290491},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Chest X-ray imaging,Computer-aided detection,Image registration,Image segmentation,Tuberculosis (TB)},
title = {{Lung segmentation in chest radiographs using anatomical atlases with nonrigid registration}},
year = {2014}
}
@article{Armato2016,
abstract = {{\textcopyright} 2016 Society of Photo-Optical Instrumentation Engineers (SPIE). The purpose of this work is to describe the LUNGx Challenge for the computerized classification of lung nodules on diagnostic computed tomography (CT) scans as benign or malignant and report the performance of participants' computerized methods along with that of six radiologists who participated in an observer study performing the same Challenge task on the same dataset. The Challenge provided sets of calibration and testing scans, established a performance assessment process, and created an infrastructure for case dissemination and result submission. Ten groups applied their own methods to 73 lung nodules (37 benign and 36 malignant) that were selected to achieve approximate size matching between the two cohorts. Area under the receiver operating characteristic curve (AUC) values for these methods ranged from 0.50 to 0.68; only three methods performed statistically better than random guessing. The radiologists' AUC values ranged from 0.70 to 0.85; three radiologists performed statistically better than the best-performing computer method. The LUNGx Challenge compared the performance of computerized methods in the task of differentiating benign from malignant lung nodules on CT scans, placed in the context of the performance of radiologists on the same task. The continued public availability of the Challenge cases will provide a valuable resource for the medical imaging research community.},
author = {Armato, Samuel G. and Drukker, Karen and Li, Feng and Hadjiiski, Lubomir and Tourassi, Georgia D. and Engelmann, Roger M. and Giger, Maryellen L. and Redmond, George and Farahani, Keyvan and Kirby, Justin S. and Clarke, Laurence P.},
doi = {10.1117/1.jmi.3.4.044506},
issn = {2329-4302},
journal = {Journal of Medical Imaging},
month = {dec},
number = {4},
pages = {044506},
title = {{LUNGx Challenge for computerized lung nodule classification}},
url = {http://medicalimaging.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JMI.3.4.044506},
volume = {3},
year = {2016}
}
@book{Mitchell1997MachineLearning,
author = {Mitchell, Tom M},
isbn = {0070428077},
publisher = {McGraw-Hill Education},
title = {{Machine Learning}},
url = {https://www.xarg.org/ref/a/0070428077/},
year = {1997}
}
@article{Komura2018,
abstract = {Abundant accumulation of digital histopathological images has led to the increased demand for their analysis, such as computer-aided diagnosis using machine learning techniques. However, digital pathological images and related tasks have some issues to be considered. In this mini-review, we introduce the application of digital pathological image analysis using machine learning algorithms, address some problems specific to such analysis, and propose possible solutions.},
author = {Komura, Daisuke and Ishikawa, Shumpei},
doi = {10.1016/j.csbj.2018.01.001},
issn = {20010370},
journal = {Computational and Structural Biotechnology Journal},
keywords = {Computer assisted diagnosis,Deep learning,Digital image analysis,Histopathology,Machine learning,Whole slide images},
pages = {34--42},
title = {{Machine Learning Methods for Histopathological Image Analysis}},
volume = {16},
year = {2018}
}
@book{Norris2020,
address = {Berkeley, CA},
author = {Norris, Donald J.},
booktitle = {Machine Learning with the Raspberry Pi},
doi = {10.1007/978-1-4842-5174-4},
isbn = {978-1-4842-5173-7},
publisher = {Apress},
title = {{Machine Learning with the Raspberry Pi}},
url = {http://link.springer.com/10.1007/978-1-4842-5174-4},
year = {2020}
}
@book{Flach2012,
abstract = {The emerging field of Ecosystem Informatics applies methods from computer science and mathematics to address fundamental and applied problems in the ecosystem sciences. The ecosystem sciences are in the midst of a revolution driven by a combination of emerging technologies for improved sensing and the critical need for better science to help manage global climate change. This paper describes several initiatives at Oregon State University in ecosystem informatics. At the level of sensor technologies, this paper describes two projects: (a) wireless, battery-free sensor networks for forests and (b) rapid throughput automated arthropod population counting. At the level of data preparation and data cleaning, this paper describes the application of linear gaussian dynamic Bayesian networks to automated anomaly detection in temperature data streams. Finally, the paper describes two educational activities: (a) a summer institute in ecosystem informatics and (b) an interdisciplinary Ph.D. program in Ecosystem Informatics for mathematics, computer science, and the ecosystem sciences.},
author = {Flach, Peter},
isbn = {1107422221, 9781107422223},
publisher = {Posts Telecom Press and Cambridge University Press},
title = {{Machine Learning: The Art and Science of Algorithms That Make Sense of Data}},
translator = {Duan, Fei},
year = {2012}
}
@incollection{Zheng2009,
address = {Cham},
author = {Zheng, Nanning and Xue, Jianru},
booktitle = {Computer Vision},
doi = {10.1007/978-1-84882-312-9_4},
pages = {87--119},
publisher = {Springer International Publishing},
title = {{Manifold Learning}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}824-1},
year = {2009}
}
@article{Skibbe2019,
abstract = {Understanding the connectivity in the brain is an important prerequisite for understanding how the brain processes information. In the Brain/MINDS project, a connectivity study on marmoset brains uses two-photon microscopy fluorescence images of axonal projections to collect the neuron connectivity from defined brain regions at the mesoscopic scale. The processing of the images requires the detection and segmentation of the axonal tracer signal. The objective is to detect as much tracer signal as possible while not misclassifying other background structures as the signal. This can be challenging because of imaging noise, a cluttered image background, distortions or varying image contrast cause problems. We are developing MarmoNet, a pipeline that processes and analyzes tracer image data of the common marmoset brain. The pipeline incorporates state-of-the-art machine learning techniques based on artificial convolutional neural networks (CNN) and image registration techniques to extract and map all relevant information in a robust manner. The pipeline processes new images in a fully automated way. This report introduces the current state of the tracer signal analysis part of the pipeline.},
annote = {{\_}eprint: 1908.00876},
archivePrefix = {arXiv},
arxivId = {1908.00876},
author = {Skibbe, Henrik and Watakabe, Akiya and Nakae, Ken and Gutierrez, Carlos Enrique and Tsukada, Hiromichi and Hata, Junichi and Kawase, Takashi and Gong, Rui and Woodward, Alexander and Doya, Kenji and Okano, Hideyuki and Yamamori, Tetsuo and Ishii, Shin},
eprint = {1908.00876},
title = {{MarmoNet: a pipeline for automated projection mapping of the common marmoset brain from whole-brain serial two-photon tomography}},
url = {http://arxiv.org/abs/1908.00876},
year = {2019}
}
@article{He2017,
abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron.},
archivePrefix = {arXiv},
arxivId = {1703.06870},
author = {He, Kaiming and Gkioxari, Georgia and Doll{\'{a}}r, Piotr and Girshick, Ross},
doi = {10.1109/TPAMI.2018.2844175},
eprint = {1703.06870},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Instance segmentation,convolutional neural network,object detection,pose estimation},
number = {2},
pages = {386--397},
pmid = {29994331},
title = {{Mask R-CNN}},
volume = {42},
year = {2020}
}
@book{Rogers2014,
address = {Berlin, Heidelberg},
author = {Rogers, Silvia M.},
doi = {10.1007/978-3-642-39446-1},
isbn = {978-3-642-39445-4},
publisher = {Springer Berlin Heidelberg},
title = {{Mastering Scientific and Medical Writing}},
url = {http://link.springer.com/10.1007/978-3-642-39446-1},
year = {2014}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
issn = {14764687},
journal = {Nature},
month = {jan},
number = {7587},
pages = {484--489},
pmid = {26819042},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://www.nature.com/articles/nature16961},
volume = {529},
year = {2016}
}
@book{Osterhage2020,
address = {Berlin, Heidelberg},
author = {Osterhage, Wolfgang W.},
booktitle = {Mathematical Theory of Advanced Computing},
doi = {10.1007/978-3-662-60359-8},
isbn = {978-3-662-60358-1},
publisher = {Springer Berlin Heidelberg},
title = {{Mathematical Theory of Advanced Computing}},
url = {http://link.springer.com/10.1007/978-3-662-60359-8},
year = {2020}
}
@article{Dice1945,
abstract = {The coefficient of association of Forbes indicates the amount of association be- tween two given species compared to the amount of association between them expected by chance. In order to provide a simple direct measure of the amount of association of one species with another the association index is proposed. If a is the number of random samples of a given series in which species A occurs and h is the number of samples in which another species B occurs together with A, then the association index B/A = h/a. Similarly, if b is the number of samples in which species B occurs, then the associa- tion index A/B = h/b. There is also proposed a coincidence index, 2h/(a + b), whose value is intermediate between the two reciprocal association indices. As a measure of the statistical reliability of the deviation shown by the samples of a given series from the amount of associa- tion expected by chance, the chi-square test may be used.},
author = {Dice, Lee R.},
doi = {10.2307/1932409},
issn = {0012-9658},
journal = {Ecology},
number = {3},
pages = {297--302},
title = {{Measures of the Amount of Ecologic Association Between Species}},
volume = {26},
year = {1945}
}
@article{Zhang2020a,
abstract = {In the past decade, deep learning (DL) has achieved unprecedented success in numerous fields including computer vision, natural language processing, and healthcare. In particular, DL is experiencing an increasing development in applications for advanced medical image analysis in terms of analysis, segmentation, classification, and furthermore. On the one hand, tremendous needs that leverage the power of DL for medical image analysis are arising from the research community of a medical, clinical, and informatics background to jointly share their expertise, knowledge, skills, and experience. On the other hand, barriers between disciplines are on the road for them often hampering a full and efficient collaboration. To this end, we propose our novel open-source platform, i.e., MeDaS -- the MeDical open-source platform as Service. To the best of our knowledge, MeDaS is the first open-source platform proving a collaborative and interactive service for researchers from a medical background easily using DL related toolkits, and at the same time for scientists or engineers from information sciences to understand the medical knowledge side. Based on a series of toolkits and utilities from the idea of RINV (Rapid Implementation aNd Verification), our proposed MeDaS platform can implement pre-processing, post-processing, augmentation, visualization, and other phases needed in medical image analysis. Five tasks including the subjects of lung, liver, brain, chest, and pathology, are validated and demonstrated to be efficiently realisable by using MeDaS.},
archivePrefix = {arXiv},
arxivId = {2007.06013},
author = {Zhang, Liang and Li, Johann and Li, Ping and Lu, Xiaoyuan and Shen, Peiyi and Zhu, Guangming and Shah, Syed Afaq and Bennarmoun, Mohammed and Qian, Kun and Schuller, Bj{\"{o}}rn W.},
eprint = {2007.06013},
month = {jul},
title = {{MeDaS: An open-source platform as service to help break the walls between medicine and informatics}},
url = {http://arxiv.org/abs/2007.06013},
year = {2020}
}
@article{Li,
author = {Li, Johann},
title = {{Medical Image Datasets and Challenges for Machine Learning: A Review}}
}
@dataset{Commowick2018a,
author = {Commowick, Olivier and Istace, Audrey and Kain, Michael and Laurent, Baptiste and Leray, Florent and Simon, Mathieu and Camarasu-Pop, Sorina and Girard, Pascal and Ameli, Roxana and Ferr{\'{e}}, Jean-Christophe and Kerbrat, Anne and Tourdias, Thomas and Cervenansky, Fr{\'{e}}d{\'{e}}ric and Glatard, Tristan and Beaumont, J{\'{e}}r{\'{e}}my and Doyle, Senan and Forbes, Florence and Knight, Jesse and Khademi, April and Mahbod, Amirreza and Wang, Chunliang and {Mc Kinley}, Richard and Wagner, Franca and Muschelli, John and Sweeney, Elizabeth and Roura, Eloy and Llad{\`{o}}, Xavier and Santos, Michel M and Santos, Wellington P and Silva-Filho, Abel G and Tomas-Fernandez, Xavier and Urien, H{\'{e}}l{\`{e}}ne and Bloch, Isabelle and Valverde, Sergi and Cabezas, Mariano and Vera-Olmos, Francisco Javier and Malpica, Norberto and Guttmann, Charles and Vukusic, Sandra and Edan, Gilles and Dojat, Michel and Styner, Martin and Warfield, Simon K and Cotton, Fran{\c{c}}ois and Barillot, Christian},
doi = {10.5281/zenodo.1307653},
publisher = {Zenodo},
title = {{MICCAI 2016 MS lesion segmentation challenge: supplementary results}},
url = {https://doi.org/10.5281/zenodo.1307653},
year = {2018}
}
@online{Bakas2020,
author = {Bakas, Spyridon and Menze, Bjoern and Davatzikos, Christos and Tiwari, Pallavi and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Freymann, John B. and Kirby, Justin S. and Davatzikos, Christos and Huang, Raymond and Colen, Rivka R. and Marcus, Daniel and Fathallah-Shaykh, Hassan and Wiest, Roland and Jakab, Andras and Weber, Marc-Andre and Mahajan, Abhishek},
title = {{MICCAI BRATS - The Multimodal Brain Tumor Segmentation Challenge 2020}},
url = {http://braintumorsegmentation.org/},
year = {2020}
}
@online{brats12,
title = {{MICCAI BRATS 2012}},
url = {http://www2.imm.dtu.dk/projects/BRATS2012/}
}
@online{Bakas2017a,
author = {Bakas, Spyridon and Menze, Bjoern and Davatzikos, Christos and Reyes, Mauricio and Farahani, Keyvan and Freymann, John B. and Kirby, Justin S. and Davatzikos, Christos and Shaykh, Hassan Fathallah and Wiest, Roland and Jakab, Andras and Weber, Marc-Andre},
keywords = {analysis,biomedical,brain,computational,image,imaging,medical,research},
title = {{MICCAI BraTS 2017: Scope | Section for Biomedical Image Analysis (SBIA) | Perelman School of Medicine at the University of Pennsylvania}},
url = {https://www.med.upenn.edu/sbia/brats2017.html},
year = {2017}
}
@online{brats18,
keywords = {analysis,biomedical,brain,computational,image,imaging,medical,research},
title = {{MICCAI BraTS 2018: Scope | Section for Biomedical Image Analysis (SBIA) | Perelman School of Medicine at the University of Pennsylvania}},
url = {https://www.med.upenn.edu/sbia/brats2018.html}
}
@misc{Wang2019a,
author = {Wang, Li and Bui, Toan Duc and Li, Gang and Lin, Weili and Shen, Dinggang},
title = {{MICCAI Grand Challenge on 6-month Infant Brain MRI Segmentation}},
url = {http://iseg2019.web.unc.edu/},
year = {2019}
}
@misc{,
title = {{MICCAI Grand Challenge on 6-month Infant Brain MRI Segmentation from Multiple Sites}}
}
@book{Bucchiarone2020,
address = {Cham},
booktitle = {Microservices},
doi = {10.1007/978-3-030-31646-4},
editor = {Bucchiarone, Antonio and Dragoni, Nicola and Dustdar, Schahram and Lago, Patricia and Mazzara, Manuel and Rivera, Victor and Sadovykh, Andrey},
isbn = {978-3-030-31645-7},
publisher = {Springer International Publishing},
title = {{Microservices}},
url = {http://link.springer.com/10.1007/978-3-030-31646-4},
year = {2020}
}
@article{Johnson2019,
abstract = {Chest radiography is an extremely powerful imaging modality, allowing for a detailed inspection of a patient's chest, but requires specialized training for proper interpretation. With the advent of high performance general purpose computer vision algorithms, the accurate automated analysis of chest radiographs is becoming increasingly of interest to researchers. Here we describe MIMIC-CXR, a large dataset of 227,835 imaging studies for 65,379 patients presenting to the Beth Israel Deaconess Medical Center Emergency Department between 2011-2016. Each imaging study can contain one or more images, usually a frontal view and a lateral view. A total of 377,110 images are available in the dataset. Studies are made available with a semi-structured free-text radiology report that describes the radiological findings of the images, written by a practicing radiologist contemporaneously during routine clinical care. All images and reports have been de-identified to protect patient privacy. The dataset is made freely available to facilitate and encourage a wide range of research in computer vision, natural language processing, and clinical data mining.},
author = {Johnson, Alistair E.W. and Pollard, Tom J. and Berkowitz, Seth J. and Greenbaum, Nathaniel R. and Lungren, Matthew P. and Deng, Chih Ying and Mark, Roger G. and Horng, Steven},
doi = {10.1038/s41597-019-0322-0},
issn = {20524463},
journal = {Scientific data},
pmid = {31831740},
title = {{MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports}},
year = {2019}
}
@misc{Gupta2019,
author = {Gupta, Ritu and Gupta, Anubha},
doi = {10.7937/TCIA.2019.PNN6AYPL},
publisher = {The Cancer Imaging Archive},
title = {{MiMM{\_}SBILab Dataset: Microscopic Images of Multiple Myeloma}},
url = {https://wiki.cancerimagingarchive.net/x/-AElAw},
year = {2019}
}
@article{Muller2019,
annote = {{\_}eprint: 1910.09308},
archivePrefix = {arXiv},
arxivId = {1910.09308},
author = {M{\"{u}}ller, Dominik and Kramer, Frank},
eprint = {1910.09308},
title = {{MIScnn: A Framework for Medical Image Segmentation with Convolutional Neural Networks and Deep Learning}},
year = {2019}
}
@inproceedings{Wei2020,
abstract = {Electron microscopy (EM) allows the identification of intracellular organelles such as mitochondria, providing insights for clinical and scientific studies. However, public mitochondria segmentation datasets only contain hundreds of instances with simple shapes. It is unclear if existing methods achieving human-level accuracy on these small datasets are robust in practice. To this end, we introduce the MitoEM dataset, a 3D mitochondria instance segmentation dataset with two (30 $\mu$ m)3 volumes from human and rat cortices respectively, 3,600× larger than previous benchmarks. With around 40K instances, we find a great diversity of mitochondria in terms of shape and density. For evaluation, we tailor the implementation of the average precision (AP) metric for 3D data with a 45× speedup. On MitoEM, we find existing instance segmentation methods often fail to correctly segment mitochondria with complex shapes or close contacts with other instances. Thus, our MitoEM dataset poses new challenges to the field. We release our code and data: https://donglaiw.github.io/page/mitoEM/index.html.},
author = {Wei, Donglai and Lin, Zudi and Franco-Barranco, Daniel and Wendt, Nils and Liu, Xingyu and Yin, Wenjie and Huang, Xin and Gupta, Aarush and Jang, Won Dong and Wang, Xueying and Arganda-Carreras, Ignacio and Lichtman, Jeff W. and Pfister, Hanspeter},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-59722-1_7},
isbn = {9783030597214},
issn = {16113349},
keywords = {3D instance segmentation,EM dataset,Mitochondria},
title = {{MitoEM Dataset: Large-Scale 3D Mitochondria Instance Segmentation from EM Images}},
year = {2020}
}
@article{Chen2016,
abstract = {The number of mitoses per tissue area gives an important aggressiveness indication of the invasive breast carcinoma. However, automatic mitosis detection in histology images remains a challenging problem. Traditional methods either employ hand-crafted features to discriminate mitoses from other cells or construct a pixel-wise classifier to label every pixel in a sliding window way. While the former suffers from the large shape variation of mitoses and the existence of many mimics with similar appearance, the slow speed of the later prohibits its use in clinical practice. In order to overcome these shortcomings, we propose a fast and accurate method to detect mitosis by designing a novel deep cascaded convolutional neural network, which is composed of two components. First, by leveraging the fully convolutional neural network, we propose a coarse retrieval model to identify and locate the candidates of mitosis while preserving a high sensitivity. Based on these candidates, a fine discrimination model utilizing knowledge transferred from cross-domain is developed to further single out mitoses from hard mimics. Our approach outperformed other methods by a large margin in 2014 ICPR MITOS-ATYPIA challenge in terms of detection accuracy. When compared with the state-of-the-art methods on the 2012 ICPR MITOSIS data (a smaller and less challenging dataset), our method achieved comparable or better results with a roughly 60 times faster speed.},
author = {Chen, Hao and Dou, Qi and Wang, Xi and Qin, Jing and Heng, Pheng Ann},
isbn = {9781577357605},
journal = {30th AAAI Conference on Artificial Intelligence, AAAI 2016},
pages = {1160--1166},
title = {{Mitosis detection in breast cancer histology images via deep cascaded networks}},
year = {2016}
}
@article{Blundell2016,
abstract = {State of the art deep reinforcement learning algorithms take many millions of interactions to attain human-level performance. Humans, on the other hand, can very quickly exploit highly rewarding nuances of an environment upon first discovery. In the brain, such rapid learning is thought to depend on the hippocampus and its capacity for episodic memory. Here we investigate whether a simple model of hippocampal episodic control can learn to solve difficult sequential decision-making tasks. We demonstrate that it not only attains a highly rewarding strategy significantly faster than state-of-the-art deep reinforcement learning algorithms, but also achieves a higher overall reward on some of the more challenging domains.},
archivePrefix = {arXiv},
arxivId = {1606.04460},
author = {Blundell, Charles and Uria, Benigno and Pritzel, Alexander and Li, Yazhe and Ruderman, Avraham and Leibo, Joel Z and Rae, Jack and Wierstra, Daan and Hassabis, Demis},
eprint = {1606.04460},
month = {jun},
title = {{Model-Free Episodic Control}},
url = {http://arxiv.org/abs/1606.04460},
year = {2016}
}
@book{Guevarra2020,
address = {Berkeley, CA},
author = {Guevarra, Ezra Thess Mendoza},
booktitle = {Modeling and Animation Using Blender},
doi = {10.1007/978-1-4842-5340-3},
isbn = {978-1-4842-5339-7},
publisher = {Apress},
title = {{Modeling and Animation Using Blender}},
url = {http://link.springer.com/10.1007/978-1-4842-5340-3},
year = {2020}
}
@book{Davis2020,
address = {Berkeley, CA},
author = {Davis, Adam L.},
booktitle = {Modern Programming Made Easy},
doi = {10.1007/978-1-4842-5569-8},
isbn = {978-1-4842-5568-1},
publisher = {Apress},
title = {{Modern Programming Made Easy}},
url = {http://link.springer.com/10.1007/978-1-4842-5569-8},
year = {2020}
}
@article{Alkhasli2019,
abstract = {Background: The fronto-striatal network is involved in various motor, cognitive, and emotional processes, such as spatial attention, working memory, decision-making, and emotion regulation. Intermittent theta burst transcranial magnetic stimulation (iTBS) has been shown to modulate functional connectivity of brain networks. Long stimulation intervals, as well as high stimulation intensities are typically applied in transcranial magnetic stimulation (TMS) therapy for mood disorders. The role of stimulation intensity on network function and homeostasis has not been explored systematically yet. Objective: In this pilot study, we aimed to modulate fronto-striatal connectivity by applying iTBS at different intensities to the left dorso-lateral prefrontal cortex (DLPFC). We measured individual and group changes by comparing resting state functional magnetic resonance imaging (rsfMRI) both pre-iTBS and post-iTBS. Differential effects of individual sub- vs. supra-resting motor-threshold stimulation intensities were assessed. Methods: Sixteen healthy subjects underwent excitatory iTBS at two intensities [90{\%} and 120{\%} of individual resting motor threshold (rMT)] on separate days. Six-hundred pulses (2 s trains, 8 s pauses, duration of 3 min, 20 s) were applied over the left DLPFC. Directly before and 7 min after stimulation, task-free rsfMRI sessions, lasting 10 min each, were conducted. Individual seed-to-seed functional connectivity changes were calculated for 10 fronto-striatal and amygdala regions of interest with the SPM toolbox DPABI. Results: Sub-threshold-iTBS increased functional connectivity directly between the left DLPFC and the left and right caudate, respectively. Supra-threshold stimulation did not change fronto-striatal functional connectivity but increased functional connectivity between the right amygdala and the right caudate. Conclusion: A short iTBS protocol applied at sub-threshold intensities was not only sufficient, but favorable, in order to increase bilateral fronto-striatal functional connectivity, while minimizing side effects. The absence of an increase in functional connectivity after supra-threshold stimulation was possibly caused by network homeostatic effects.},
author = {Alkhasli, Isabel and Sakreida, Katrin and Mottaghy, Felix M. and Binkofski, Ferdinand},
doi = {10.3389/fnhum.2019.00190},
issn = {16625161},
journal = {Frontiers in Human Neuroscience},
keywords = {DLPFC,Fronto-striatal network,Functional connectivity,Intermittent theta burst stimulation (iTBS),Prefrontal cortex,Resting state,Striatum},
month = {jun},
title = {{Modulation of fronto-striatal functional connectivity using transcranial magnetic stimulation}},
url = {https://www.frontiersin.org/article/10.3389/fnhum.2019.00190/full},
volume = {13},
year = {2019}
}
@incollection{Luo2020,
address = {Cham},
author = {Luo, Chong and Zeng, Wenjun},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_872-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Monocular and Binocular People Tracking}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}872-1},
year = {2020}
}
@incollection{Kemp2019,
address = {Cham},
author = {Kemp, Jonathan and Kemp, Jonathan},
booktitle = {Film on Video},
doi = {10.4324/9780429468872-5},
pages = {45--54},
publisher = {Springer International Publishing},
title = {{Motion blur}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}512-1},
year = {2019}
}
@misc{Takata2020,
author = {Takata, Norio and Sato, Nobuhiko and Komaki, Yuji and Okano, Hideyuki and Tanaka, Kenji F},
doi = {10.18112/OPENNEURO.DS002551.V1.0.0},
publisher = {Openneuro},
title = {{Mouse{\_}awake{\_}rest}},
url = {https://openneuro.org/datasets/ds002551/versions/1.0.0},
year = {2020}
}
@misc{Nadkarni2019,
author = {Nadkarni, N A and Bougacha, S and Garin, C and Picq, J L and Dhenain, M},
doi = {10.18112/OPENNEURO.DS001945.V1.0.0},
publisher = {Openneuro},
title = {{MouseLemurAtlas{\_}MRIraw}},
url = {https://openneuro.org/datasets/ds001945/versions/1.0.0},
year = {2019}
}
@article{Mendrik2015,
abstract = {Many methods have been proposed for tissue segmentation in brain MRI scans. The multitude of methods proposed complicates the choice of one method above others. We have therefore established the MRBrainS online evaluation framework for evaluating (semi)automatic algorithms that segment gray matter (GM), white matter (WM), and cerebrospinal fluid (CSF) on 3T brain MRI scans of elderly subjects (65-80 y). Participants apply their algorithms to the provided data, after which their results are evaluated and ranked. Full manual segmentations of GM, WM, and CSF are available for all scans and used as the reference standard. Five datasets are provided for training and fifteen for testing. The evaluated methods are ranked based on their overall performance to segment GM, WM, and CSF and evaluated using three evaluation metrics (Dice, H95, and AVD) and the results are published on the MRBrainS13 website. We present the results of eleven segmentation algorithms that participated in the MRBrainS13 challenge workshop at MICCAI, where the framework was launched, and three commonly used freeware packages: FreeSurfer, FSL, and SPM. The MRBrainS evaluation framework provides an objective and direct comparison of all evaluated algorithms and can aid in selecting the best performing method for the segmentation goal at hand.},
author = {Mendrik, Adri{\"{e}}nne M. and Vincken, Koen L. and Kuijf, Hugo J. and Breeuwer, Marcel and Bouvy, Willem H. and {De Bresser}, Jeroen and Alansary, Amir and {De Bruijne}, Marleen and Carass, Aaron and El-Baz, Ayman and Jog, Amod and Katyal, Ranveer and Khan, Ali R. and {Van Der Lijn}, Fedde and Mahmood, Qaiser and Mukherjee, Ryan and {Van Opbroek}, Annegreet and Paneri, Sahil and Pereira, S{\'{e}}rgio and Persson, Mikael and Rajchl, Martin and Sarikaya, Duygu and Smedby, {\"{O}}rjan and Silva, Carlos A. and Vrooman, Henri A. and Vyas, Saurabh and Wang, Chunliang and Zhao, Liang and Biessels, Geert Jan and Viergever, Max A.},
doi = {10.1155/2015/813696},
issn = {16875273},
journal = {Computational Intelligence and Neuroscience},
pages = {1--16},
title = {{MRBrainS Challenge: Online Evaluation Framework for Brain Image Segmentation in 3T MRI Scans}},
url = {http://www.hindawi.com/journals/cin/2015/813696/},
volume = {2015},
year = {2015}
}
@misc{Kuijf2018,
author = {Kuijf, Hugo J. and Bennink, H. Edwin and Biessels, Geert Jan and Viergever, Max A. and Weaver, Nick A. and Vincken, Koen L.},
title = {{MRBrainS18 - Grand Challenge on MR Brain Segmentation 2018}},
url = {https://mrbrains18.isi.uu.nl/},
year = {2018}
}
@online{MRIandAlzheimersKaggle,
author = {Boysen, Jacob},
keywords = {health,health sciences,healthcare,image data,medical facilities and services,neurological conditions,neurology,neuroscience,old age},
title = {{MRI and Alzheimers}},
url = {https://www.kaggle.com/jboysen/mri-and-alzheimers},
urldate = {2020-05-25},
year = {2017}
}
@online{Malekzadeh2019,
author = {Malekzadeh, S.},
keywords = {biology,deep learning,health foundations and medical research,medical facilities and services,neurological conditions,object segmentation,old age},
title = {{MRI Hippocampus Segmentation | Kaggle}},
url = {https://www.kaggle.com/sabermalek/mrihs},
urldate = {2020-05-25},
year = {2019}
}
@article{Varmazyar2020,
author = {Varmazyar, Hadi and Ghareaghaji, Zahra and Malekzadeh, Saber},
title = {{MRI Hippocampus Segmentation using Deep Learning autoencoders}},
year = {2020}
}
@online{MSlesion2008,
author = {Styner, Martin and Warfield, Simon and Lee, Joohwi},
title = {{MS lesion segmentation challenage 2008}},
url = {http://www.ia.unc.edu/MSseg/},
urldate = {2020-05-21},
year = {2008}
}
@online{MSSEG,
author = {FLI-IAM},
title = {{MS segmentation challenge using a data management and processing infrastructure}},
url = {https://portal.fli-iam.irisa.fr/msseg-challenge/overview},
urldate = {2020-05-21},
year = {2016}
}
@online{MSChallengeIACL2015,
author = {Pham, Dzung and Bazin, Pierre-Louis and Carass, Aaron and Calabresi, Peter and Crainiceanu, Ciprian and Ellingsen, Lotta and He, Qing and Prince, Jerry and Reich, Daniel and Roy, Snehashis},
title = {{MSChallenge - IACL}},
url = {http://iacl.ece.jhu.edu/index.php/MSChallenge},
urldate = {2020-05-21},
year = {2015}
}
@article{Campello2020,
author = {Campello and V{\'{i}}ctor, M.},
journal = {IEEE Transactions on Medical Imaging},
title = {{Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Segmentation: The M{\&}Ms Challenge}},
year = {2020}
}
@incollection{Boyarski2020,
address = {Cham},
author = {Boyarski, Amit and Bronstein, Alex},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_827-1},
pages = {1--14},
publisher = {Springer International Publishing},
title = {{Multidimensional Scaling}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}827-1},
year = {2020}
}
@incollection{Sheller2019,
abstract = {Deep learning models for semantic segmentation of images require large amounts of data. In the medical imaging domain, acquiring sufficient data is a significant challenge. Labeling medical image data requires expert knowledge. Collaboration between institutions could address this challenge, but sharing medical data to a centralized location faces various legal, privacy, technical, and data-ownership challenges, especially among international institutions. In this study, we introduce the first use of federated learning for multi-institutional collaboration, enabling deep learning modeling without sharing patient data. Our quantitative results demonstrate that the performance of federated semantic segmentation models (Dice = 0.852) on multimodal brain scans is similar to that of models trained by sharing data (Dice = 0.862). We compare federated learning with two alternative collaborative learning methods and find that they fail to match the performance of federated learning.},
archivePrefix = {arXiv},
arxivId = {1810.04304},
author = {Sheller, Micah J. and Reina, G. Anthony and Edwards, Brandon and Martin, Jason and Bakas, Spyridon},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-11723-8_9},
eprint = {1810.04304},
isbn = {9783030117221},
issn = {16113349},
keywords = {BraTS,Deep learning,Federated,Glioma,Incremental,Machine learning,Segmentation},
pages = {92--104},
title = {{Multi-institutional deep learning modeling without sharing patient data: A feasibility study on brain tumor segmentation}},
url = {http://link.springer.com/10.1007/978-3-030-11723-8{\_}9},
volume = {11383 LNCS},
year = {2019}
}
@article{Song2019,
abstract = {It is a challenging problem to achieve generalized nuclear segmentation in digital histopathology images. Existing techniques, using either handcrafted features in learning-based models or traditional image analysis-based approaches, do not effectively tackle the challenging cases, such as crowded nuclei, chromatin-sparse, and heavy background clutter. In contrast, deep networks have achieved state-of-the-art performance in modeling various nuclear appearances. However, their success is limited due to the size of the considered networks. We solve these problems by reformulating nuclear segmentation in terms of a cascade 2-class classification problem and propose a multi-layer boosting sparse convolutional (ML-BSC)model. In the proposed ML-BSC model, discriminative probabilistic binary decision trees (PBDTs)are designed as weak learners in each layer to cope with challenging cases. A sparsity-constrained cascade structure enables the ML-BSC model to improve representation learning. Comparing to the existing techniques, our method can accurately separate individual nuclei in complex histopathology images, and it is more robust against chromatin-sparse and heavy background clutter. An evaluation carried out using three disparate datasets demonstrates the superiority of our method over the state-of-the-art supervised approaches in terms of segmentation accuracy.},
author = {Song, Jie and Xiao, Liang and Molaei, Mohsen and Lian, Zhichao},
doi = {10.1016/j.knosys.2019.03.031},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Cascade classification,Multi-layer boosting sparse convolutional model,Nucleus segmentation,Probabilistic binary decision tree,Representation learning},
pages = {40--53},
title = {{Multi-layer boosting sparse convolutional model for generalized nuclear segmentation from histopathology images}},
url = {http://www.sciencedirect.com/science/article/pii/S095070511930156X},
volume = {176},
year = {2019}
}
@online{Bakas2019,
author = {Bakas, Spyridon and Menze, Bjoern and Davatzikos, Christos and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Freymann, John B. and Kirby, Justin S. and Davatzikos, Christos and Fathallah-Shaykh, Hassan and Wiest, Roland and Jakab, Andras and Colen, Rivka R. and Marcus, Daniel and Weber, Marc-Andre and Mahajan, Abhishek},
title = {{Multimodal Brain Tumor Segmentation Challenge 2019 | CBICA | Perelman School of Medicine at the University of Pennsylvania}},
url = {https://www.med.upenn.edu/cbica/brats-2019/},
year = {2019}
}
@article{Mahmood2018,
abstract = {Humans make accurate decisions by interpreting complex data from multiple sources. Medical diagnostics, in particular, often hinge on human interpretation of multimodal information. In order for artificial intelligence to make progress in automated, objective, and accurate diagnosis and prognosis, methods to fuse information from multiple medical imaging modalities are required. However, combining information from multiple data sources has several challenges, as current deep learning architectures lack the ability to extract useful representations from multimodal information, and often simple concatenation is used to fuse such information. In this work, we propose Multimodal DenseNet, a novel architecture for fusing multimodal data. Instead of focusing on concatenation or early and late fusion, our proposed architectures fuses information over several layers and gives the model flexibility in how it combines information from multiple sources. We apply this architecture to the challenge of polyp characterization and landmark identification in endoscopy. Features from white light images are fused with features from narrow band imaging or depth maps. This study demonstrates that Multimodal DenseNet outperforms monomodal classification as well as other multimodal fusion techniques by a significant margin on two different datasets.},
annote = {{\_}eprint: 1811.07407},
archivePrefix = {arXiv},
arxivId = {1811.07407},
author = {Mahmood, Faisal and Yang, Ziyun and Ashley, Thomas and Durr, Nicholas J.},
eprint = {1811.07407},
journal = {arXiv},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning},
title = {{Multimodal densenet}},
url = {http://arxiv.org/abs/1811.07407},
year = {2018}
}
@article{Verma2020,
author = {Verma, Ruchika and Kumar, Neeraj and Patil, Abhijeet and Kurian, Nikhil Cherian and Rane, Swapnil and Sethi, Amit},
doi = {10.13140/RG.2.2.12290.02244},
number = {February},
pages = {1--3},
title = {{Multi-organ Nuclei Segmentation and Classification Challenge 2020}},
year = {2020}
}
@article{Trullo2019,
abstract = {{\textcopyright} 2019 Society of Photo-Optical Instrumentation Engineers (SPIE). Segmentation of organs at risk (OAR) in computed tomography (CT) is of vital importance in radiotherapy treatment. This task is time consuming and for some organs, it is very challenging due to low-intensity contrast in CT. We propose a framework to perform the automatic segmentation of multiple OAR: esophagus, heart, trachea, and aorta. Different from previous works using deep learning techniques, we make use of global localization information, based on an original distance map that yields not only the localization of each organ, but also the spatial relationship between them. Instead of segmenting directly the organs, we first generate the localization map by minimizing a reconstruction error within an adversarial framework. This map that includes localization information of all organs is then used to guide the segmentation task in a fully convolutional setting. Experimental results show encouraging performance on CT scans of 60 patients totaling 11,084 slices in comparison with other state-of-the-art methods.},
author = {Trullo, Roger and Petitjean, Caroline and Dubray, Bernard and Ruan, Su},
doi = {10.1117/1.jmi.6.1.014001},
issn = {2329-4302},
journal = {Journal of Medical Imaging},
number = {01},
pages = {1},
title = {{Multiorgan segmentation using distance-aware adversarial networks}},
volume = {6},
year = {2019}
}
@inproceedings{Zhuang2016,
abstract = {Cardiac segmentation is commonly a prerequisite for functional analysis of the heart,such as to identify and quantify the infarcts and edema from the normal myocardium using the late-enhanced (LE) and T2-weighted MRI. The automatic delineation of myocardium is however challenging due to the heterogeneous intensity distributions and indistinct boundaries in the images. In this work,we present a multivariate mixture model (MvMM) for text classification,which combines the complementary information from multi-sequence (MS) cardiac MRI and perform the segmentation of them simultaneously. The expectation maximization (EM) method is adopted to estimate the segmentation and model parameters from the log-likelihood (LL) of the mixture model,where a probabilistic atlas is used for initialization. Furthermore,to correct the intra- and inter-image misalignments,we formulate the MvMM with transformations,which are embedded into the LL framework and thus can be optimized by the iterative conditional mode approach. We applied MvMM for segmentation of eighteen subjects with three sequences and obtained promising results. We compared with two conventional methods,and the improvements of segmentation performance on LE and T2 MRI were evident and statistically significant by MvMM.},
author = {Zhuang, Xiahai},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46723-8_67},
isbn = {9783319467221},
issn = {16113349},
pages = {581--588},
title = {{Multivariate mixture model for cardiac segmentation from multi-sequence MRI}},
volume = {9901 LNCS},
year = {2016}
}
@article{Zhuang2019,
abstract = {The author proposes a method for simultaneous registration and segmentation of multi-source images, using the multivariate mixture model (MvMM) and maximum of log-likelihood (LL) framework. Specifically, the method is applied to the problem of myocardial segmentation combining the complementary information from multi-sequence (MS) cardiac magnetic resonance (CMR) images. For the image misalignment and incongruent data, the MvMM is formulated with transformations and is further generalized for dealing with the hetero-coverage multi-modality images (HC-MMIs). The segmentation of MvMM is performed in a virtual common space, to which all the images and misaligned slices are simultaneously registered. Furthermore, this common space can be divided into a number of sub-regions, each of which contains congruent data, thus the HC-MMIs can be modeled using a set of conventional MvMMs. Results show that MvMM obtained significantly better performance compared to the conventional approaches and demonstrated good potential for scar quantification as well as myocardial segmentation. The generalized MvMM has also demonstrated better robustness in the incongruent data, where some images may not fully cover the region of interest, and the full coverage can only be reconstructed combining the images from multiple sources.},
author = {Zhuang, Xiahai},
doi = {10.1109/TPAMI.2018.2869576},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Multivariate image,cardiac MRI,medical image analysis,multi-modality,registration,segmentation},
number = {12},
pages = {2933--2946},
title = {{Multivariate Mixture Model for Myocardial Segmentation Combining Multi-Source Images}},
volume = {41},
year = {2019}
}
@article{Tatarchenko2015,
abstract = {We present a convolutional network capable of inferring a 3D representation of a previously unseen object given a single image of this object. Concretely, the network can predict an RGB image and a depth map of the object as seen from an arbitrary view. Several of these depth maps fused together give a full point cloud of the object. The point cloud can in turn be transformed into a surface mesh. The network is trained on renderings of synthetic 3D models of cars and chairs. It successfully deals with objects on cluttered background and generates reasonable predictions for real images of cars.},
annote = {{\_}eprint: 1511.06702},
archivePrefix = {arXiv},
arxivId = {1511.06702},
author = {Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},
doi = {10.1007/978-3-319-46478-7_20},
eprint = {1511.06702},
isbn = {9783319464770},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {3D from single image,Convolutional networks,Deep learning},
pages = {322--337},
title = {{Multi-view 3D models from single images with a convolutional network}},
url = {http://arxiv.org/abs/1511.06702},
volume = {9911 LNCS},
year = {2016}
}
@inproceedings{Tatarchenko2015a,
abstract = {We present a convolutional network capable of inferring a 3D representation of a previously unseen object given a single image of this object. Concretely, the network can predict an RGB image and a depth map of the object as seen from an arbitrary view. Several of these depth maps fused together give a full point cloud of the object. The point cloud can in turn be transformed into a surface mesh. The network is trained on renderings of synthetic 3D models of cars and chairs. It successfully deals with objects on cluttered background and generates reasonable predictions for real images of cars.},
archivePrefix = {arXiv},
arxivId = {1511.06702},
author = {Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46478-7_20},
eprint = {1511.06702},
isbn = {9783319464770},
issn = {16113349},
keywords = {3D from single image,Convolutional networks,Deep learning},
pages = {322--337},
title = {{Multi-view 3D models from single images with a convolutional network}},
url = {http://arxiv.org/abs/1511.06702},
volume = {9911 LNCS},
year = {2016}
}
@article{Su2015,
abstract = {A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.},
annote = {From Duplicate 1 (Multi-view Convolutional Neural Networks for 3D Shape Recognition - Su, Hang; Maji, Subhransu; Kalogerakis, Evangelos; Learned-Miller, Erik G)

{\_}eprint: 1505.00880},
archivePrefix = {arXiv},
arxivId = {1505.00880},
author = {Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik},
doi = {10.1109/ICCV.2015.114},
eprint = {1505.00880},
isbn = {9781467383912},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {945--953},
title = {{Multi-view convolutional neural networks for 3D shape recognition}},
url = {http://arxiv.org/abs/1505.00880},
volume = {2015 Inter},
year = {2015}
}
@article{Rajpurkar2017,
abstract = {We introduce MURA, a large dataset of musculoskeletal radiographs containing 40,561 images from 14,863 studies, where each study is manually labeled by radiologists as either normal or abnormal. To evaluate models robustly and to get an estimate of radiologist performance, we collect additional labels from six boardcertified Stanford radiologists on the test set, consisting of 207 musculoskeletal studies. On this test set, the majority vote of a group of three radiologists serves as gold standard. We train a 169-layer DenseNet baseline model to detect and localize abnormalities. Our model achieves an AUROC of 0.929, with an operating point of 0.815 sensitivity and 0.887 specificity. We compare our model and radiologists on the Cohen's kappa statistic, which expresses the agreement of our model and of each radiologist with the gold standard. Model performance is comparable to the best radiologist performance in detecting abnormalities on finger and wrist studies. However, model performance is lower than best radiologist performance in detecting abnormalities on elbow, forearm, hand, humerus, and shoulder studies. We believe that the task is a good challenge for future research. To encourage advances, we have made our dataset freely available at http://stanfordmlgroup.github.io/competitions/mura.},
archivePrefix = {arXiv},
arxivId = {1712.06957},
author = {Rajpurkar, Pranav and Irvin, Jeremy and Bagul, Aarti and Ding, Daisy and Duan, Tony and Mehta, Hershel and Yang, Brandon and Zhu, Kaylie and Laird, Dillon and Ball, Robyn L. and Langlotz, Curtis and Shpanskaya, Katie and Lungren, Matthew P. and Ng, Andrew Y.},
eprint = {1712.06957},
journal = {arXiv},
month = {dec},
title = {{MURA: Large dataset for abnormality detection in musculoskeletal radiographs}},
url = {http://arxiv.org/abs/1712.06957},
year = {2017}
}
@article{Tustison2010,
abstract = {A variant of the popular nonparametric nonuniform intensity normalization (N3) algorithm is proposed for bias field correction. Given the superb performance of N3 and its public availability, it has been the subject of several evaluation studies. These studies have demonstrated the importance of certain parameters associated with the B-spline least-squares fitting. We propose the substitution of a recently developed fast and robust B-spline approximation routine and a modified hierarchical optimization scheme for improved bias field correction over the original N3 algorithm. Similar to the N3 algorithm, we also make the source code, testing, and technical documentation of our contribution, which we denote as N4ITK, available to the public through the Insight Toolkit of the National Institutes of Health. Performance assessment is demonstrated using simulated data from the publicly available Brainweb database, hyperpolarized 3He lung image data, and 9.4T postmortem hippocampus data. {\textcopyright} 2006 IEEE.},
author = {Tustison, Nicholas J. and Avants, Brian B. and Cook, Philip A. and Zheng, Yuanjie and Egan, Alexander and Yushkevich, Paul A. and Gee, James C.},
doi = {10.1109/TMI.2010.2046908},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {B-spline approximation,Bias field,Inhomogeneity,N3},
month = {jun},
number = {6},
pages = {1310--1320},
pmid = {20378467},
title = {{N4ITK: Improved N3 bias correction}},
volume = {29},
year = {2010}
}
@misc{TCIA-NCI-ISBI-2013,
author = {Bloch, Nicholas and Madabhushi, Anant and Huisman, Henkjan and Freymann, John and Kirby, Justin and Grauer, Michael and Enquobahrie, Andinet and Jaffe, Carl and Clarke, Larry and Farahani, Keyvan},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2015.ZF0VLOPV},
publisher = {The Cancer Imaging Archive},
title = {{NCI-ISBI 2013 Challenge: Automated Segmentation of Prostate Structures}},
url = {https://wiki.cancerimagingarchive.net/x/B4NEAQ},
year = {2015}
}
@book{Pan2019,
address = {Singapore},
author = {Pan, Yongsheng and Liu, Mingxia and Xia, Yong and Shen, Dinggang},
doi = {10.1007/978-981-15-0798-4},
editor = {Gupta, Anubha and Gupta, Ritu},
isbn = {9789811507984},
keywords = {Microscopic image classification,Fisher vector,Res,b,b-lymphoblast cells,development institute of northwestern,fisher vector,leukemia,microscopic image classification,pan,polytechnical university,research,residual network,xia,y},
publisher = {Springer Singapore},
series = {Lecture Notes in Bioengineering},
title = {{Neighborhood-Correction Algorithm for Cells}},
url = {http://dx.doi.org/10.1007/978-981-15-0798-4{\_}8},
year = {2019}
}
@inproceedings{Chen2015,
abstract = {We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.},
archivePrefix = {arXiv},
arxivId = {1511.05641},
author = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
booktitle = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
eprint = {1511.05641},
title = {{Net2Net: Accelerating learning via knowledge transfer}},
volume = {abs/1511.0},
year = {2016}
}
@inproceedings{Wei2016,
abstract = {We present a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as network morphism in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous nonlinear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.},
archivePrefix = {arXiv},
arxivId = {1603.01670},
author = {Wei, Tao and Wang, Changhu and Rui, Yong and Chen, Chang Wen},
booktitle = {33rd International Conference on Machine Learning, ICML 2016},
eprint = {1603.01670},
isbn = {9781510829008},
pages = {842--850},
title = {{Network morphism}},
volume = {2},
year = {2016}
}
@book{Haykin2011,
author = {Haykin, Simon},
isbn = {9780133002553},
publisher = {China Machine Press and Pearson Education},
title = {{Neural Networks and Learning Machines | 3rd edition | Pearson}},
translator = {Shen, Furao and Xu, Ye and Zheng, Jun and Chao, Jin},
url = {https://www.pearson.com/store/p/neural-networks-and-learning-machines/P100001415658},
year = {2008}
}
@article{Lepping2016,
abstract = {Background Anterior cingulate cortex (ACC) and striatum are part of the emotional neural circuitry implicated in major depressive disorder (MDD). Music is often used for emotion regulation, and pleasurable music listening activates the dopaminergic system in the brain, including the ACC. The present study uses functional MRI (fMRI) and an emotional nonmusical and musical stimuli paradigm to examine how neural processing of emotionally provocative auditory stimuli is altered within the ACC and striatum in depression. Method Nineteen MDD and 20 never-depressed (ND) control participants listened to standardized positive and negative emotional musical and nonmusical stimuli during fMRI scanning and gave subjective ratings of valence and arousal following scanning. Results ND participants exhibited greater activation to positive versus negative stimuli in ventral ACC. When compared with ND participants, MDD participants showed a different pattern of activation in ACC. In the rostral part of the ACC, ND participants showed greater activation for positive information, while MDD participants showed greater activation to negative information. In dorsal ACC, the pattern of activation distinguished between the types of stimuli, with ND participants showing greater activation to music compared to nonmusical stimuli, while MDD participants showed greater activation to nonmusical stimuli, with the greatest response to negative nonmusical stimuli. No group differences were found in striatum. Conclusions These results suggest that people with depression may process emotional auditory stimuli differently based on both the type of stimulation and the emotional content of that stimulation. This raises the possibility that music may be useful in retraining ACC function, potentially leading to more effective and targeted treatments.},
author = {Lepping, Rebecca J. and Atchley, Ruth Ann and Chrysikou, Evangelia and Martin, Laura E. and Clair, Alicia A. and Ingram, Rick E. and Simmons, W. Kyle and Savage, Cary R.},
doi = {10.1371/journal.pone.0156859},
editor = {Kotz, Sonja},
issn = {19326203},
journal = {PLoS ONE},
month = {jun},
number = {6},
pages = {e0156859},
pmid = {27284693},
title = {{Neural processing of emotional musical and nonmusical stimuli in depression}},
url = {http://dx.plos.org/10.1371/journal.pone.0156859},
volume = {11},
year = {2016}
}
@article{Rumelhart1988,
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
address = {Cambridge, MA, USA},
annote = {Section: Learning Representations by Back-propagating Errors},
author = {Mingolla, Ennio and Bullock, Daniel},
doi = {10.1016/0893-6080(89)90025-7},
editor = {Anderson, James A and Rosenfeld, Edward},
isbn = {0-262-01097-6},
issn = {08936080},
journal = {Neural Networks},
number = {5},
pages = {405--409},
publisher = {MIT Press},
title = {{Neurocomputing: Foundations of Research}},
url = {http://dl.acm.org/citation.cfm?id=65669.104451},
volume = {2},
year = {1989}
}
@article{Wardlaw2013,
abstract = {Cerebral small vessel disease (SVD) is a common accompaniment of ageing. Features seen on neuroimaging include recent small subcortical infarcts, lacunes, white matter hyperintensities, perivascular spaces, microbleeds, and brain atrophy. SVD can present as a stroke or cognitive decline, or can have few or no symptoms. SVD frequently coexists with neurodegenerative disease, and can exacerbate cognitive deficits, physical disabilities, and other symptoms of neurodegeneration. Terminology and definitions for imaging the features of SVD vary widely, which is also true for protocols for image acquisition and image analysis. This lack of consistency hampers progress in identifying the contribution of SVD to the pathophysiology and clinical features of common neurodegenerative diseases. We are an international working group from the Centres of Excellence in Neurodegeneration. We completed a structured process to develop definitions and imaging standards for markers and consequences of SVD. We aimed to achieve the following: first, to provide a common advisory about terms and definitions for features visible on MRI; second, to suggest minimum standards for image acquisition and analysis; third, to agree on standards for scientific reporting of changes related to SVD on neuroimaging; and fourth, to review emerging imaging methods for detection and quantification of preclinical manifestations of SVD. Our findings and recommendations apply to research studies, and can be used in the clinical setting to standardise image interpretation, acquisition, and reporting. This Position Paper summarises the main outcomes of this international effort to provide the STandards for ReportIng Vascular changes on nEuroimaging (STRIVE). {\textcopyright} 2013 Elsevier Ltd.},
author = {Wardlaw, Joanna M. and Smith, Eric E. and Biessels, Geert J. and Cordonnier, Charlotte and Fazekas, Franz and Frayne, Richard and Lindley, Richard I. and O'Brien, John T. and Barkhof, Frederik and Benavente, Oscar R. and Black, Sandra E. and Brayne, Carol and Breteler, Monique and Chabriat, Hugues and DeCarli, Charles and de Leeuw, Frank Erik and Doubal, Fergus and Duering, Marco and Fox, Nick C. and Greenberg, Steven and Hachinski, Vladimir and Kilimann, Ingo and Mok, Vincent and van Oostenbrugge, Robert and Pantoni, Leonardo and Speck, Oliver and Stephan, Blossom C.M. and Teipel, Stefan and Viswanathan, Anand and Werring, David and Chen, Christopher and Smith, Colin and van Buchem, Mark and Norrving, Bo and Gorelick, Philip B. and Dichgans, Martin},
doi = {10.1016/S1474-4422(13)70124-8},
issn = {14744422},
journal = {The Lancet Neurology},
month = {aug},
number = {8},
pages = {822--838},
pmid = {23867200},
title = {{Neuroimaging standards for research into small vessel disease and its contribution to ageing and neurodegeneration}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1474442213701248},
volume = {12},
year = {2013}
}
@article{Rajchl2018,
abstract = {NeuroNet is a deep convolutional neural network mimicking multiple popular and state-of-the-art brain segmentation tools including FSL, SPM, and MALPEM. The network is trained on 5,000 T1-weighted brain MRI scans from the UK Biobank Imaging Study that have been automatically segmented into brain tissue and cortical and sub-cortical structures using the standard neuroimaging pipelines. Training a single model from these complementary and partially overlapping label maps yields a new powerful "all-in-one", multi-output segmentation tool. The processing time for a single subject is reduced by an order of magnitude compared to running each individual software package. We demonstrate very good reproducibility of the original outputs while increasing robustness to variations in the input data. We believe NeuroNet could be an important tool in large-scale population imaging studies and serve as a new standard in neuroscience by reducing the risk of introducing bias when choosing a specific software package.},
annote = {{\_}eprint: 1806.04224},
archivePrefix = {arXiv},
arxivId = {1806.04224},
author = {Rajchl, Martin and Pawlowski, Nick and Rueckert, Daniel and Matthews, Paul M. and Glocker, Ben},
eprint = {1806.04224},
journal = {arXiv},
title = {{NeuroNet: Fast and robust reproduction of multiple brain image segmentation pipelines}},
url = {http://arxiv.org/abs/1806.04224},
year = {2018}
}
@article{Gibson2018,
abstract = {Background and objectives: Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. Established deep-learning platforms are flexible but do not provide specific functionality for medical image analysis and adapting them for this domain of application requires substantial implementation effort. Consequently, there has been substantial duplication of effort and incompatible infrastructure developed across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon. Methods: The NiftyNet infrastructure provides a modular deep-learning pipeline for a range of medical imaging applications including segmentation, regression, image generation and representation learning applications. Components of the NiftyNet pipeline including data loading, data augmentation, network architectures, loss functions and evaluation metrics are tailored to, and take advantage of, the idiosyncracies of medical image analysis and computer-assisted intervention. NiftyNet is built on the TensorFlow framework and supports features such as TensorBoard visualization of 2D and 3D images and computational graphs by default. Results: We present three illustrative medical image analysis applications built using NiftyNet infrastructure: (1) segmentation of multiple abdominal organs from computed tomography; (2) image regression to predict computed tomography attenuation maps from brain magnetic resonance images; and (3) generation of simulated ultrasound images for specified anatomical poses. Conclusions: The NiftyNet infrastructure enables researchers to rapidly develop and distribute deep learning solutions for segmentation, regression, image generation and representation learning applications, or extend the platform to new applications.},
archivePrefix = {arXiv},
arxivId = {1709.03485},
author = {Gibson, Eli and Li, Wenqi and Sudre, Carole and Fidon, Lucas and Shakir, Dzhoshkun I. and Wang, Guotai and Eaton-Rosen, Zach and Gray, Robert and Doel, Tom and Hu, Yipeng and Whyntie, Tom and Nachev, Parashkev and Modat, Marc and Barratt, Dean C. and Ourselin, S{\'{e}}bastien and Cardoso, M. Jorge and Vercauteren, Tom},
doi = {10.1016/j.cmpb.2018.01.025},
eprint = {1709.03485},
issn = {18727565},
journal = {Computer Methods and Programs in Biomedicine},
keywords = {Convolutional neural network,Deep learning,Generative adversarial network,Image regression,Medical image analysis,Segmentation},
pages = {113--122},
pmid = {29544777},
title = {{NiftyNet: a deep-learning platform for medical imaging}},
url = {http://www.sciencedirect.com/science/article/pii/S0169260717311823},
volume = {158},
year = {2018}
}
@article{S.Senthilraja2014,
abstract = {Image processing concept play a important role in the field of Medical to diagnosis of diseases. Noise is introduced in the medical images due to various reasons. In Medical Imaging, Noise degrades the quality of images. This degradation includes suppression of edges, blurring boundaries etc. Edge and preservation details are very important to discover a disease. Noise removal is a very challenging issue in the Medical Image Processing. Denoising can help the physicians to diagnose the diseases. Medical Images include CT, MRI scan, X-ray and ultrasound images etc. This paper we implemented a new filter called WB-Filter for Medical Image denoising. WB-Filter mainly focuses on speckle noise {\&} Gaussian Noise removal especially in the CT scan images. Experimental results are compared with other three filtering concepts. The result images quality is measured by the PSNR, RMSE and MSE. The results demonstrate that the proposed WB-Filter concept obtaining the optimum result quality of the Medical Image.},
author = {Senthilraja, S and Suresh, P and Suganthi, M},
issn = {2229-5518},
journal = {International Journal of Scientific and Engineering Research},
month = {mar},
number = {3},
pages = {243},
title = {{Noise Reduction in Computed Tomography Image Using WB–Filter}},
volume = {5},
year = {2014}
}
@article{Gevaert2012,
abstract = {Purpose: To identify prognostic imaging biomarkers in non-small cell lung cancer (NSCLC) by means of a radiogenomics strategy that integrates gene expression and medical images in patients for whom survival outcomes are not available by leveraging survival data in public gene expression data sets. Materials and Methods: A radiogenomics strategy for associating image features with clusters of coexpressed genes (metagenes) was defined. First, a radiogenomics correlation map is created for a pairwise association between image features and metagenes. Next, predictive models of metagenes are built in terms of image features by using sparse linear regression. Similarly, predictive models of image features are built in terms of metagenes. Finally, the prognostic significance of the predicted image features are evaluated in a public gene expression data set with survival outcomes. This radiogenomics strategy was applied to a cohort of 26 patients with NSCLC for whom gene expression and 180 image features from computed tomography (CT) and positron emission tomography (PET)/CT were available. Results: There were 243 statistically significant pairwise correlations between image features and metagenes of NSCLC. Metagenes were predicted in terms of image features with an accuracy of 59{\%}-83{\%}. One hundred fourteen of 180 CT image features and the PET standardized uptake value were predicted in terms of metagenes with an accuracy of 65{\%}-86{\%}. When the predicted image features were mapped to a public gene expression data set with survival outcomes, tumor size, edge shape, and sharpness ranked highest for prognostic significance. Conclusion: This radiogenomics strategy for identifying imaging biomarkers may enable a more rapid evaluation of novel imaging modalities, thereby accelerating their translation to personalized medicine. {\textcopyright} RSNA, 2012.},
author = {Gevaert, Olivier and Xu, Jiajing and Hoang, Chuong D. and Leung, Ann N. and Xu, Yue and Quon, Andrew and Rubin, Daniel L. and Napel, Sandy and Plevritis, Sylvia K.},
doi = {10.1148/radiol.12111607},
issn = {00338419},
journal = {Radiology},
month = {aug},
number = {2},
pages = {387--396},
pmid = {22723499},
title = {{Non-small cell lung cancer: Identifying prognostic imaging biomarkers by leveraging public gene expression microarray data - Methods and preliminary results}},
url = {http://pubs.rsna.org/doi/10.1148/radiol.12111607},
volume = {264},
year = {2012}
}
@misc{Napel2014,
author = {Napel, S and Plevritis and Sylvia, K},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9/TCIA.2014.X7ONY6B1},
publisher = {The Cancer Imaging Archive},
title = {{NSCLC Radiogenomics: Initial Stanford Study of 26 Cases}},
url = {https://wiki.cancerimagingarchive.net/x/Gglp},
year = {2014}
}
@misc{nvidia-docker,
abstract = {Over the last few years there has been a dramatic rise in the use of containers for deploying data center applications at scale. The reason for this is simple: containers encapsulate an application's dependencies to provide reproducible and reliable execution of applications and services without the overhead of a full virtual machine. If you have ever spent a day provisioning a server with a multitude of packages for a scientific or deep learning application, or have put in weeks of effort to ensure your application can be built and deployed in multiple linux environments.},
author = {{Ryan Olson, Jonathan Calmels}, Felix Abecassis and Phil Rogers |},
title = {{NVIDIA Docker: GPU Server Application Deployment Made Easy}},
url = {https://devblogs.nvidia.com/nvidia-docker-gpu-server-application-deployment-made-easy/},
year = {2016}
}
@online{OASIS,
keywords = {MRI,data,longitudinal,neuroimaging,open source},
title = {{OASIS Brains - Open Access Series of Imaging Studies}},
url = {https://www.oasis-brains.org/{\#}data},
year = {2019}
}
@article{LaMontagne2019,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
author = {LaMontagne, Pamela and Benzinger, Tammie LS and Morris, John and Keefe, Sarah and Hornbeck, Russ and Xiong, Chengjie and Grant, Elizabeth and Hassenstab, Jason and Moulder, Krista and Vlassenko, Andrei and Raichle, Marcus and Cruchaga, Carlos and Marcus, Daniel},
doi = {10.1101/2019.12.13.19014902},
journal = {medRxiv},
month = {jan},
pages = {2019.12.13.19014902},
title = {{OASIS-3: Longitudinal Neuroimaging, Clinical, and Cognitive Dataset for Normal Aging and Alzheimer Disease}},
url = {http://medrxiv.org/content/early/2019/12/15/2019.12.13.19014902.abstract},
year = {2019}
}
@incollection{Wu2012,
abstract = {Over the past twenty years, data-driven methods have become a dominant paradigm for computer vision, with numerous practical successes. In difficult computer vision tasks, such as the detection of object categories (for example, the detection of faces of various gender, age, race, and pose, under various illumination and background conditions), researchers generally learn a classifier that can distinguish an image patch that contains the object of interest from all other image patches. Ensemble learning methods have been very successful in learning classifiers for object detection.},
address = {Cham},
author = {Wu, Jianxin and Rehg, James M.},
booktitle = {Ensemble Machine Learning: Methods and Applications},
doi = {10.1007/9781441993267_8},
isbn = {9781441993267},
pages = {225--250},
publisher = {Springer International Publishing},
title = {{Object detection}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}660-1},
year = {2012}
}
@article{Chenouard2014,
abstract = {Particle tracking is of key importance for quantitative analysis of intracellular dynamic processes from time-lapse microscopy image data. Because manually detecting and following large numbers of individual particles is not feasible, automated computational methods have been developed for these tasks by many groups. Aiming to perform an objective comparison of methods, we gathered the community and organized an open competition in which participating teams applied their own methods independently to a commonly defined data set including diverse scenarios. Performance was assessed using commonly defined measures. Although no single method performed best across all scenarios, the results revealed clear differences between the various approaches, leading to notable practical conclusions for users and developers. {\textcopyright} 2014 Nature America, Inc.},
author = {Chenouard, Nicolas and Smal, Ihor and {De Chaumont}, Fabrice and Ma{\v{s}}ka, Martin and Sbalzarini, Ivo F. and Gong, Yuanhao and Cardinale, Janick and Carthel, Craig and Coraluppi, Stefano and Winter, Mark and Cohen, Andrew R. and Godinez, William J. and Rohr, Karl and Kalaidzidis, Yannis and Liang, Liang and Duncan, James and Shen, Hongying and Xu, Yingke and Magnusson, Klas E.G. and Jald{\'{e}}n, Joakim and Blau, Helen M. and Paul-Gilloteaux, Perrine and Roudot, Philippe and Kervrann, Charles and Waharte, Fran{\c{c}}ois and Tinevez, Jean Yves and Shorte, Spencer L. and Willemse, Joost and Celler, Katherine and {Van Wezel}, Gilles P. and Dan, Han Wei and Tsai, Yuh Show and {De Sol{\'{o}}rzano}, Carlos Ortiz and Olivo-Marin, Jean Christophe and Meijering, Erik},
doi = {10.1038/nmeth.2808},
issn = {15487091},
journal = {Nature Methods},
month = {mar},
number = {3},
pages = {281--289},
pmid = {24441936},
title = {{Objective comparison of particle tracking methods}},
url = {http://www.nature.com/articles/nmeth.2808},
volume = {11},
year = {2014}
}
@article{Commowick2018,
abstract = {We present a study of multiple sclerosis segmentation algorithms conducted at the international MICCAI 2016 challenge. This challenge was operated using a new open-science computing infrastructure. This allowed for the automatic and independent evaluation of a large range of algorithms in a fair and completely automatic manner. This computing infrastructure was used to evaluate thirteen methods of MS lesions segmentation, exploring a broad range of state-of-theart algorithms, against a high-quality database of 53 MS cases coming from four centers following a common definition of the acquisition protocol. Each case was annotated manually by an unprecedented number of seven different experts. Results of the challenge highlighted that automatic algorithms, including the recent machine learning methods (random forests, deep learning, {\ldots}), are still trailing human expertise on both detection and delineation criteria. In addition, we demonstrate that computing a statistically robust consensus of the algorithms performs closer to human expertise on one score (segmentation) although still trailing on detection scores.},
author = {Commowick, Olivier and Istace, Audrey and Kain, Micha{\"{e}}l and Laurent, Baptiste and Leray, Florent and Simon, Mathieu and Pop, Sorina Camarasu and Girard, Pascal and Am{\'{e}}li, Roxana and Ferr{\'{e}}, Jean Christophe and Kerbrat, Anne and Tourdias, Thomas and Cervenansky, Fr{\'{e}}d{\'{e}}ric and Glatard, Tristan and Beaumont, J{\'{e}}r{\'{e}}my and Doyle, Senan and Forbes, Florence and Knight, Jesse and Khademi, April and Mahbod, Amirreza and Wang, Chunliang and McKinley, Richard and Wagner, Franca and Muschelli, John and Sweeney, Elizabeth and Roura, Eloy and Llad{\'{o}}, Xavier and Santos, Michel M. and Santos, Wellington P. and Silva-Filho, Abel G. and Tomas-Fernandez, Xavier and Urien, H{\'{e}}l{\`{e}}ne and Bloch, Isabelle and Valverde, Sergi and Cabezas, Mariano and Vera-Olmos, Francisco Javier and Malpica, Norberto and Guttmann, Charles and Vukusic, Sandra and Edan, Gilles and Dojat, Michel and Styner, Martin and Warfield, Simon K. and Cotton, Fran{\c{c}}ois and Barillot, Christian},
doi = {10.1038/s41598-018-31911-7},
issn = {20452322},
journal = {Scientific Reports},
month = {dec},
number = {1},
pages = {13650},
title = {{Objective Evaluation of Multiple Sclerosis Lesion Segmentation using a Data Management and Processing Infrastructure}},
url = {http://www.nature.com/articles/s41598-018-31911-7},
volume = {8},
year = {2018}
}
@inproceedings{Wang2017,
abstract = {We present O-CNN, an Octree-based Convolutional Neural Network (CNN) for 3D shape analysis. Built upon the octree representation of 3D shapes, our method takes the average normal vectors of a 3D model sampled in the finest leaf octants as input and performs 3D CNN operations on the octants occupied by the 3D shape surface. We design a novel octree data structure to efficiently store the octant information and CNN features into the graphics memory and execute the entire O-CNN training and evaluation on the GPU. O-CNN supports various CNN structures and works for 3D shapes in different representations. By restraining the computations on the octants occupied by 3D surfaces, the memory and computational costs of the O-CNN grow quadratically as the depth of the octree increases, which makes the 3D CNN feasible for high-resolution 3D models. We compare the performance of the O-CNN with other existing 3D CNN solutions and demonstrate the efficiency and efficacy of O-CNN in three shape analysis tasks, including object classification, shape retrieval, and shape segmentation.},
archivePrefix = {arXiv},
arxivId = {1712.01537},
author = {Wang, Peng Shuai and Liu, Yang and Guo, Yu Xiao and Sun, Chun Yu and Tong, Xin},
booktitle = {ACM Transactions on Graphics},
doi = {10.1145/3072959.3073608},
eprint = {1712.01537},
issn = {15577368},
keywords = {Convolutional neural network,Object classification,Octree,Shape retrieval,Shape segmentation},
number = {4},
pages = {72:1--72:11},
title = {{O-CNN: Octree-based convolutional neural networks for 3D shape analysis}},
url = {http://doi.acm.org/10.1145/3072959.3073608},
volume = {36},
year = {2017}
}
@inproceedings{Riegler2016,
abstract = {We present OctNet, a representation for deep learning with sparse 3D data. In contrast to existing models, our representation enables 3D convolutional networks which are both deep and high resolution. Towards this goal, we exploit the sparsity in the input data to hierarchically partition the space using a set of unbalanced octrees where each leaf node stores a pooled feature representation. This allows to focus memory allocation and computation to the relevant dense regions and enables deeper networks without compromising resolution. We demonstrate the utility of our OctNet representation by analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling.},
annote = {{\_}eprint: 1611.05009},
archivePrefix = {arXiv},
arxivId = {1611.05009},
author = {Riegler, Gernot and Ulusoy, Ali Osman and Geiger, Andreas},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.701},
eprint = {1611.05009},
isbn = {9781538604571},
pages = {6620--6629},
title = {{OctNet: Learning deep 3D representations at high resolutions}},
url = {http://arxiv.org/abs/1611.05009},
volume = {2017-Janua},
year = {2017}
}
@article{Wilhelms1992,
abstract = {The large size of many volume data sets often prevents visualization algorithms from providing interactive rendering. The use of hierarchical data structures can ameliorate this problem by storing summary information to prevent useless exploration of regions of little or no current interest within the volume. This paper discusses research into the use of the octree hierarchical data structure when the regions of current interest can vary during the application, and are not known a priori. Octrees are well suited to the six-sided cell structure of many volumes. A new space-efficient design is introduced for octree representations of volumes whose resolutions are not conveniently a power of two; octrees following this design are called branch-on-need octrees 1992. Also, a caching method is described that essentially passes information between octree neighbors whose visitation times may be quite different, then discards it when its useful life is over. Using the application of octrees to isosurface generation as a focus, space and time comparisons for octree-based versus more traditional “marching” methods are presented. {\textcopyright} 1992, ACM. All rights reserved.},
annote = {Place: New York, NY, USA
Publisher: ACM},
author = {Wilhelms, Jane and {Van Gelder}, Allen},
doi = {10.1145/130881.130882},
issn = {15577368},
journal = {ACM Transactions on Graphics (TOG)},
keywords = {hierarchical spatial enumeration,isosurface extraction,octree,scientific visualization},
number = {3},
pages = {201--227},
title = {{Octrees for Faster Isosurface Generation}},
url = {http://doi.acm.org/10.1145/130881.130882},
volume = {11},
year = {1992}
}
@incollection{Richardt2020,
address = {Cham},
author = {Richardt, Christian},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_808-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Omnidirectional Stereo}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}808-1},
year = {2020}
}
@article{Cordonnier2019,
abstract = {Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: Do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code is publicly available.},
archivePrefix = {arXiv},
arxivId = {1911.03584},
author = {Cordonnier, Jean Baptiste and Loukas, Andreas and Jaggi, Martin},
eprint = {1911.03584},
journal = {arXiv},
month = {nov},
title = {{On the relationship between self-attention and convolutional layers}},
url = {http://arxiv.org/abs/1911.03584},
year = {2019}
}
@article{Su2017,
abstract = {Recent research has revealed that the output of deep neural networks (DNNs) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97{\%} of the natural images in Kaggle CIFAR-10 test dataset and 16.04{\%} of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03{\%} and 22.91{\%} confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.},
archivePrefix = {arXiv},
arxivId = {1710.08864},
author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
doi = {10.1109/TEVC.2019.2890858},
eprint = {1710.08864},
issn = {19410026},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Convolutional neural network,differential evolution (DE),image recognition,information security},
month = {oct},
number = {5},
pages = {828--841},
title = {{One Pixel Attack for Fooling Deep Neural Networks}},
url = {http://arxiv.org/abs/1710.08864 http://dx.doi.org/10.1109/TEVC.2019.2890858},
volume = {23},
year = {2019}
}
@article{Marcus2007,
abstract = {The Open Access Series of Imaging Studies is a series of magnetic resonance imaging data sets that is publicly available for study and analysis. The initial data set consists of a cross-sectional collection of 416 subjects aged 18 to 96 years. One hundred of the included subjects older than 60 years have been clinically diagnosed with very mild to moderate Alzheimer's disease. The subjects are all right-handed and include both men and women. For each subject, three or four individual T1-weighted magnetic resonance imaging scans obtained in single imaging sessions are included. Multiple within-session acquisitions provide extremely high contrast-to-noise ratio, making the data amenable to a wide range of analytic approaches including automated computational analysis. Additionally, a reliability data set is included containing 20 subjects without dementia imaged on a subsequent visit within 90 days of their initial session. Automated calculation of whole-brain volume and estimated total intracranial volume are presented to demonstrate use of the data for measuring differences associated with normal aging and Alzheimer's disease. {\textcopyright} 2007 Massachusetts Institute of Technology.},
author = {Marcus, Daniel S. and Wang, Tracy H. and Parker, Jamie and Csernansky, John G. and Morris, John C. and Buckner, Randy L.},
doi = {10.1162/jocn.2007.19.9.1498},
issn = {0898929X},
journal = {Journal of Cognitive Neuroscience},
month = {sep},
number = {9},
pages = {1498--1507},
pmid = {17714011},
title = {{Open Access Series of Imaging Studies (OASIS): Cross-sectional MRI data in young, middle aged, nondemented, and demented older adults}},
url = {http://www.mitpressjournals.org/doi/10.1162/jocn.2007.19.9.1498},
volume = {19},
year = {2007}
}
@article{Marcus2010,
abstract = {The Open Access Series of Imaging Studies is a series of neuroimaging data sets that are publicly available for study and analysis. The present MRI data set consists of a longitudinal collection of 150 subjects aged 60 to 96 years all acquired on the same scanner using identical sequences. Each subject was scanned on two or more visits, separated by at least 1 year for a total of 373 imaging sessions. Subjects were characterized using the Clinical Dementia Rating (CDR) as either nondemented or with very mild tomild Alzheimer's disease. Seventy-two of the subjects were characterized as nondemented throughout the study. Sixty-four of the included subjects were characterized as demented at the time of their initial visits and remained so for subsequent scans, including 51 individuals with CDR 0.5 similar level of impairment to individuals elsewhere considered to have "mild cognitive impairment." Another 14 subjects were characterized as nondemented at the time of their initial visit (CDR 0) and were subsequently characterized as demented at a later visit (CDR {\textgreater} 0). The subjects were all right-handed and include bothmen (n=62) and women (n = 88). For each scanning session, three or four individual T1-weighted MRI scans were obtained. Multiple withinsession acquisitions provide extremely high contrast to noise, making the data amenable to a wide range of analytic approaches including automated computational analysis. Automated calculation of whole-brain volume is presented to demonstrate use of the data for measuring differences associated with normal aging and Alzheimer's disease. {\textcopyright} 2010 Massachusetts Institute of Technology.},
author = {Marcus, Daniel S. and Fotenos, Anthony F. and Csernansky, John G. and Morris, John C. and Buckner, Randy L.},
doi = {10.1162/jocn.2009.21407},
issn = {0898929X},
journal = {Journal of Cognitive Neuroscience},
month = {dec},
number = {12},
pages = {2677--2684},
title = {{Open access series of imaging studies: Longitudinal MRI data in nondemented and demented older adults}},
url = {http://www.mitpressjournals.org/doi/10.1162/jocn.2009.21407},
volume = {22},
year = {2010}
}
@misc{Stanford2011a,
abstract = {A free and open platform for sharing MRI, MEG, EEG, iEEG, and ECoG data},
author = {Stanford, Stanford Center for Reproducible Neuroscience},
booktitle = {World Wide Web},
title = {{OpenNEURO}},
year = {2011}
}
@misc{Stanford2011,
abstract = {A free and open platform for sharing MRI, MEG, EEG, iEEG, and ECoG data},
author = {Stanford, Stanford Center for Reproducible Neuroscience},
booktitle = {World Wide Web},
title = {{OpenNEURO}},
url = {https://openneuro.org/},
year = {2011}
}
@article{Goode2013,
abstract = {Although widely touted as a replacement for glass slides and microscopes in pathology, digital slides present major challenges in data storage, transmission, processing and interoperability. Since no universal data format is in widespread use for these images today, each vendor defines its own proprietary data formats, analysis tools, viewers and software libraries. This creates issues not only for pathologists, but also for interoperability. In this paper, we present the design and implementation of OpenSlide{\textless}i{\textgreater},{\textless}/i{\textgreater} a vendor-neutral C library for reading and manipulating digital slides of diverse vendor formats. The library is extensible and easily interfaced to various programming languages. An application written to the OpenSlide interface can transparently handle multiple vendor formats. OpenSlide is in use today by many academic and industrial organizations world-wide, including many research sites in the United States that are funded by the National Institutes of Health.},
author = {Satyanarayanan, Mahadev and Goode, Adam and Gilbert, Benjamin and Harkes, Jan and Jukic, Drazen},
doi = {10.4103/2153-3539.119005},
issn = {2153-3539},
journal = {Journal of Pathology Informatics},
number = {1},
pages = {27},
title = {{OpenSlide: A vendor-neutral software foundation for digital pathology}},
volume = {4},
year = {2013}
}
@incollection{Brox2020,
address = {Cham},
author = {Brox, Thomas},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_600-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Optical Flow: Traditional Approaches}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}600-1},
year = {2020}
}
@article{Song2013,
abstract = {Positron emission tomography (PET)-computed tomography (CT) images have been widely used in clinical practice for radiotherapy treatment planning of the radiotherapy. Many existing segmentation approaches only work for a single imaging modality, which suffer from the low spatial resolution in PET or low contrast in CT. In this work, we propose a novel method for the co-segmentation of the tumor in both PET and CT images, which makes use of advantages from each modality: the functionality information from PET and the anatomical structure information from CT. The approach formulates the segmentation problem as a minimization problem of a Markov random field model, which encodes the information from both modalities. The optimization is solved using a graph-cut based method. Two sub-graphs are constructed for the segmentation of the PET and the CT images, respectively. To achieve consistent results in two modalities, an adaptive context cost is enforced by adding context arcs between the two sub-graphs. An optimal solution can be obtained by solving a single maximum flow problem, which leads to simultaneous segmentation of the tumor volumes in both modalities. The proposed algorithm was validated in robust delineation of lung tumors on 23 PET-CT datasets and two head-and-neck cancer subjects. Both qualitative and quantitative results show significant improvement compared to the graph cut methods solely using PET or CT. {\textcopyright} 2012 IEEE.},
author = {Song, Qi and Bai, Junjie and Han, Dongfeng and Bhatia, Sudershan and Sun, Wenqing and Rockey, William and Bayouth, John E. and Buatti, John M. and Wu, Xiaodong},
doi = {10.1109/TMI.2013.2263388},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Context information,Positron emission tomography-computed tomography (,global optimization,graph cut,image segmentation,lung tumor},
month = {sep},
number = {9},
pages = {1685--1697},
pmid = {23693127},
title = {{Optimal Co-segmentation of tumor in PET-CT images with context information}},
volume = {32},
year = {2013}
}
@incollection{Kelly2014,
address = {Cham},
author = {Kelly, Alonzo and Kelly, Alonzo},
booktitle = {Mobile Robotics},
doi = {10.1017/cbo9781139381284.006},
pages = {270--369},
publisher = {Springer International Publishing},
title = {{Optimal Estimation}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}714-1},
year = {2014}
}
@book{Dorronsoro2020,
address = {Cham},
author = {Dorronsoro, Bernab{\'{e}} and Ruiz, Patricia and Carlos, Juan and Torre, De and Urda, Daniel and Eds, El-ghazali Talbi},
doi = {10.1007/978-3-030-41913-4},
editor = {Dorronsoro, Bernab{\'{e}} and Ruiz, Patricia and de la Torre, Juan Carlos and Urda, Daniel and Talbi, El-Ghazali},
isbn = {9783030419127},
number = {March},
pages = {2020},
publisher = {Springer International Publishing},
series = {Communications in Computer and Information Science},
title = {{Optimization and Learning}},
url = {http://link.springer.com/10.1007/978-3-030-41913-4},
volume = {1173},
year = {2020}
}
@article{Singh2002,
abstract = {Designing the dialogue policy of a spoken dialogue system involves many nontrivial choices. This paper presents a reinforcement learning approach for automatically optimizing a dialogue policy, which addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users. We report on the design, construction and empirical evaluation of NJFun, an experimental spoken dialogue system that provides users with access to information about fun things to do in New Jersey. Our results show that by optimizing its performance via reinforcement learning, NJFun measurably improves system performance. {\textcopyright} 2002 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.},
author = {Singh, Satinder and Man, Diane Lit and Kearns, Michael and Walker, Marilyn},
doi = {10.1613/jair.859},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {105--133},
title = {{Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system}},
volume = {16},
year = {2002}
}
@book{Bors2020,
address = {Berkeley, CA},
author = {Bors, Luc and Samajdwer, Ardhendu and van Oosterhout, Mascha},
booktitle = {Oracle Digital Assistant},
doi = {10.1007/978-1-4842-5422-6},
isbn = {978-1-4842-5421-9},
publisher = {Apress},
title = {{Oracle Digital Assistant}},
url = {http://link.springer.com/10.1007/978-1-4842-5422-6},
year = {2020}
}
@inproceedings{Sedaghat2016,
abstract = {Recent work has shown good recognition results in 3D object recognition using 3D convolutional networks. In this paper, we show that the object orientation plays an important role in 3D recognition. More specifically, we argue that objects induce different features in the network under rotation. Thus, we approach the category-level classification task as a multi-task problem, in which the network is trained to predict the pose of the object in addition to the class label as a parallel task. We show that this yields significant improvements in the classification results. We test our suggested architecture on several datasets representing various 3D data sources: LiDAR data, CAD models, and RGB-D images. We report state-of-the-art results on classification as well as significant improvements in precision and speed over the baseline on 3D detection.},
annote = {{\_}eprint: 1604.03351},
archivePrefix = {arXiv},
arxivId = {1604.03351},
author = {Sedaghat, Nima and Zolfaghari, Mohammadreza and Amiri, Ehsan and Brox, Thomas},
booktitle = {British Machine Vision Conference 2017, BMVC 2017},
doi = {10.5244/c.31.97},
eprint = {1604.03351},
isbn = {190172560X},
title = {{Orientation-boosted Voxel nets for 3D object recognition}},
url = {http://arxiv.org/abs/1604.03351},
volume = {abs/1604.0},
year = {2017}
}
@misc{LeaveyP.2019,
author = {{Leavey P.}, Sengupta A Rakheja D Daescu O Arunachalam H B {\&} Mishra R},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/TCIA.2019.BVHJHDAS},
publisher = {The Cancer Imaging Archive},
title = {{Osteosarcoma data from UT Southwestern/UT Dallas for Viable and Necrotic Tumor Assessment}},
url = {https://wiki.cancerimagingarchive.net/x/xwElAw},
year = {2019}
}
@inproceedings{Duggal2016,
abstract = {This paper proposes a method for segmentation of nuclei of single/isolated and overlapping/touching immature white blood cells from microscopic images of B-Lineage acute lymphoblastic leukemia (ALL) prepared from peripheral blood and bone marrow aspirate. We propose deep belief network approach for the segmentation of these nuclei. Simulation results and comparison with some of the existing methods demonstrate the efficacy of the proposed method.},
author = {Duggal, Rahul and Gupta, Anubha and Gupta, Ritu and Wadhwa, Manya and Ahuja, Chirag},
booktitle = {ACM International Conference Proceeding Series},
doi = {10.1145/3009977.3010043},
isbn = {9781450347532},
keywords = {B-ALL,Deep belief network,Machine learning,Overlapping nuclei segmentation},
title = {{Overlapping cell nuclei segmentation in microscopic images using deep belief networks}},
year = {2016}
}
@data{GrandChallengePALM,
author = {Zhang, Huazhu Fu; Fei Li; Jos{\'{e}} Ignacio Orlando; Hrvoje Bogunovi{\'{c}}; Xu Sun; Jingan Liao; Yanwu Xu; Shaochong Zhang; Xiulan},
booktitle = {IEEE Dataport},
doi = {10.21227/55pk-8z03},
publisher = {IEEE Dataport},
title = {{PALM: PAthoLogic Myopia Challenge}},
url = {http://dx.doi.org/10.21227/55pk-8z03},
volume = {[Online]},
year = {2019}
}
@incollection{Cai2018,
abstract = {Automatic pancreas segmentation in radiology images, e.g., computed tomography (CT), and magnetic resonance imaging (MRI), is frequently required by computer-aided screening, diagnosis, and quantitative assessment. Yet, pancreas is a challenging abdominal organ to segment due to the high inter-patient anatomical variability in both shape and volume metrics. Recently, convolutional neural networks (CNN) have demonstrated promising performance on accurate segmentation of pancreas. However, the CNN-based method often suffers from segmentation discontinuity for reasons such as noisy image quality and blurry pancreatic boundary. In this chapter, we first discuss the CNN configurations and training objectives that lead to the state-of-the-art performance on pancreas segmentation. We then present a recurrent neural network (RNN) to address the problem of segmentation spatial inconsistency across adjacent image slices. The RNN takes outputs of the CNN and refines the segmentation by improving the shape smoothness.},
annote = {{\_}eprint: 1803.11303},
archivePrefix = {arXiv},
arxivId = {1803.11303},
author = {Cai, Jinzheng and Lu, Le and Xing, Fuyong and Yang, Lin},
booktitle = {Advances in Computer Vision and Pattern Recognition},
doi = {10.1007/978-3-030-13969-8_1},
eprint = {1803.11303},
issn = {21916594},
keywords = {Computer Science - Computer Vision and Pattern Rec},
pages = {3--21},
title = {{Pancreas Segmentation in CT and MRI via Task-Specific Network Design and Recurrent Neural Contextual Learning}},
year = {2019}
}
@inproceedings{Cai2017a,
abstract = {Deep neural networks have demonstrated very promising performance on accurate segmentation of challenging organs (e.g., pancreas) in abdominal CT and MRI scans. The current deep learning approaches conduct pancreas segmentation by processing sequences of 2D image slices independently through deep, dense per-pixel masking for each image, without explicitly enforcing spatial consistency constraint on segmentation of successive slices. We propose a new convolutional/recurrent neural network architecture to address the contextual learning and segmentation consistency problem. A deep convolutional sub-network is first designed and pre-trained from scratch. The output layer of this network module is then connected to recurrent layers and can be fine-tuned for contextual learning, in an end-to-end manner. Our recurrent sub-network is a type of Long short-term memory (LSTM) network that performs segmentation on an image by integrating its neighboring slice segmentation predictions, in the form of a dependent sequence processing. Additionally, a novel segmentation-direct loss function (named Jaccard Loss) is proposed and deep networks are trained to optimize Jaccard Index (JI) directly. Extensive experiments are conducted to validate our proposed deep models, on quantitative pancreas segmentation using both CT and MRI scans. Our method outperforms the state-of-the-art work on CT [11] and MRI pancreas segmentation [1], respectively.},
address = {Cham},
author = {Cai, Jinzheng and Lu, Le and Xie, Yuanpu and Xing, Fuyong and Yang, Lin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-66179-7_77},
editor = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D Louis and Duchesne, Simon},
isbn = {9783319661780},
issn = {16113349},
pages = {674--682},
publisher = {Springer International Publishing},
title = {{Pancreas segmentation in MRI using graph-based decision fusion on convolutional neural networks}},
volume = {10435 LNCS},
year = {2017}
}
@incollection{Cochran2011,
abstract = {A partially observable Markov decision process (POMDP) is a generalization of a Markov decision process where the states of the model are not completely observable by the decision maker. Noisy observations provide a belief regarding the underlying state, while the decision maker has some control over the progression of the model through the selection of actions. In this article, we introduce POMDPs and discuss the relationship between Markov models and POMDPs. A general POMDP formulation and a wide range of POMDP applications from the literature are also presented.},
address = {Hoboken, NJ, USA},
author = {Cochran, James J. and Cox, Louis A. and Keskinocak, Pinar and Kharoufeh, Jeffrey P. and Smith, J. Cole and Yaylali, Emine and Ivy, Julie S.},
booktitle = {Wiley Encyclopedia of Operations Research and Management Science},
doi = {10.1002/9780470400531.eorms0646},
month = {feb},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Partially Observable MDPs (POMDPS): Introduction and Examples}},
url = {http://doi.wiley.com/10.1002/9780470400531.eorms0646},
year = {2011}
}
@article{Zhang2019b,
abstract = {Diagnostic pathology is the foundation and gold standard for identifying carcinomas. However, high inter-observer variability substantially affects productivity in routine pathology and is especially ubiquitous in diagnostician-deficient medical centres. Despite rapid growth in computer-aided diagnosis (CAD), the application of whole-slide pathology diagnosis remains impractical. Here, we present a novel pathology whole-slide diagnosis method, powered by artificial intelligence, to address the lack of interpretable diagnosis. The proposed method masters the ability to automate the human-like diagnostic reasoning process and translate gigapixels directly to a series of interpretable predictions, providing second opinions and thereby encouraging consensus in clinics. Moreover, using 913 collected examples of whole-slide data representing patients with bladder cancer, we show that our method matches the performance of 17 pathologists in the diagnosis of urothelial carcinoma. We believe that our method provides an innovative and reliable means for making diagnostic suggestions and can be deployed at low cost as next-generation, artificial intelligence-enhanced CAD technology for use in diagnostic pathology.},
author = {Zhang, Zizhao and Chen, Pingjun and McGough, Mason and Xing, Fuyong and Wang, Chunbao and Bui, Marilyn and Xie, Yuanpu and Sapkota, Manish and Cui, Lei and Dhillon, Jasreman and Ahmad, Nazeel and Khalil, Farah K. and Dickinson, Shohreh I. and Shi, Xiaoshuang and Liu, Fujun and Su, Hai and Cai, Jinzheng and Yang, Lin},
doi = {10.1038/s42256-019-0052-1},
journal = {Nature Machine Intelligence},
number = {5},
pages = {236--245},
title = {{Pathologist-level interpretable whole-slide cancer diagnosis with deep learning}},
volume = {1},
year = {2019}
}
@article{He2020,
abstract = {Is it possible to develop an “AI Pathologist" to pass the board-certified examination of the American Board of Pathology? To achieve this goal, the first step is to create a visual question answering (VQA) dataset where the AI agent is presented with a pathology image together with a question and is asked to give the correct answer. Our work makes the first attempt to build such a dataset. Different from creating general-domain VQA datasets where the images are widely accessible and there are many crowdsourcing workers available and capable of generating question-answer pairs, developing a medical VQA dataset is much more challenging. First, due to privacy concerns, pathology images are usually not publicly available. Second, only well-trained pathologists can understand pathology images, but they barely have time to help create datasets for AI research. To address these challenges, we resort to pathology textbooks and online digital libraries. We develop a semi-automated pipeline to extract pathology images and captions from textbooks and generate question-answer pairs from captions using natural language processing. We collect 32,799 open-ended questions from 4,998 pathology images where each question is manually checked to ensure correctness. To our best knowledge, this is the first dataset for pathology VQA. Our dataset will be released publicly to promote research in medical VQA.},
archivePrefix = {arXiv},
arxivId = {2003.10286},
author = {He, Xuehai and Zhang, Yichen and Mou, Luntian and Xing, Eric and Xie, Pengtao},
eprint = {2003.10286},
journal = {arXiv},
keywords = {Dataset,Healthcare,Pathology,Visual question answering},
month = {mar},
title = {{PATHVQA: 30000+ questions for medical visual question answering}},
url = {http://arxiv.org/abs/2003.10286},
year = {2020}
}
@book{Evans2013,
abstract = {Writing blocks are likely to strike any writer, even experienced ones, at sometime or another. Academia has its own challenges which can provoke blocks particular to that environment. Drawing on her knowledge as writer, psychotherapeutic counsellor and university tutor, Kate Evans has put together a book which addresses many of the differing aspects of writing blocks, including looking at their emotional and psychological foundations. With discussion and practical exercises, this volume suggests that an infusion of creative techniques can offer pathways through writing blocks in the academic environment. The case studies provide an in-depth consideration of varying experiences of writing blocks. The book is aimed at students with essays, projects or reports to write, or theses to tackle; as well as academics who are working on articles and books. It will also offer insights for supervisors who wish to support those who are writing and guidance for people running writing groups within academia. Over-all the book encourages a creative, collaborative approach which aims to equip academics for writing within the context of the twenty-first century. "This book offers something for every academic writer, whether budding or experienced. Students struggling with essays and dissertations will find many practical exercises along with invaluable advice. More practised writers will encounter fresh insights{\ldots} I am confident that you, the reader, will enjoy this book, which is itself a model of good writing." Dr Linda Finlay, the Open University, UK.},
address = {Rotterdam},
author = {Evans, Kate},
booktitle = {Pathways through Writing Blocks in the Academic Environment},
doi = {10.1007/978-94-6209-242-6},
isbn = {9789462092426},
pages = {1--148},
publisher = {SensePublishers},
title = {{Pathways through writing blocks in the academic environment}},
url = {http://link.springer.com/10.1007/978-94-6209-242-6},
year = {2013}
}
@article{Gupta2018,
abstract = {Plasma cell segmentation is the first stage of a computer assisted automated diagnostic tool for multiple myeloma (MM). Owing to large variability in biological cell types, a method for one cell type cannot be applied directly on the other cell types. In this paper, we present PCSeg Tool for plasma cell segmentation from microscopic medical images. These images were captured from bone marrow aspirate slides of patients with MM. PCSeg has a robust pipeline consisting of a pre-processing step, the proposed modified multiphase level set method followed by post-processing steps including the watershed and circular Hough transform to segment clusters of cells of interest and to remove unwanted cells. Our modified level set method utilizes prior information about the probability densities of regions of interest (ROIs) in the color spaces and provides a solution to the minimal-partition problem to segment ROIs in one of the level sets of a two-phase level set formulation. PCSeg tool is tested on a number of microscopic images and provides good segmentation results on single cells as well as efficient segmentation of plasma cell clusters.},
author = {Gupta, Anubha and Mallick, Pramit and Sharma, Ojaswa and Gupta, Ritu and Duggal, Rahul},
doi = {10.1371/journal.pone.0207908},
issn = {19326203},
journal = {PLoS ONE},
pmid = {30540767},
title = {{PCSEG: Color model driven probabilistic multiphase level set based tool for plasma cell segmentation in multiple myeloma}},
year = {2018}
}
@misc{Tessa2018,
author = {Tessa, Carlo},
doi = {10.18112/OPENNEURO.DS001354.V1.0.0},
publisher = {Openneuro},
title = {{PD De Novo: Resting State fMRI and Physiological Signals}},
url = {https://openneuro.org/datasets/ds001354/versions/1.0.0},
year = {2018}
}
@online{GrandChallengeODIR-2019,
title = {{Peking University International Competition on Ocular Disease Intelligent Recognition}},
url = {https://odir2019.grand-challenge.org/}
}
@misc{Yorke2019,
author = {Yorke, Afua A. and McDonald, Gary C. and {Solis Jr.}, David and Guerrero., Thomas},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/TCIA.2019.woskq5oo},
publisher = {The Cancer Imaging Archive},
title = {{Pelvic Reference Data}},
url = {https://wiki.cancerimagingarchive.net/x/tQGJAw},
year = {2019}
}
@incollection{Panda2020,
address = {Cham},
author = {Panda, Rameswar and Roy-Chowdhury, Amit K.},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_825-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Person Re-identification: Current Approaches and Future Challenges}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}825-1},
year = {2020}
}
@article{Goldberger2000,
abstract = {The newly inaugurated Research Resource for Complex Physiologic Signals, which was created under the auspices of the National Center for Research Resources of the National Institutes of Health, is intended to stimulate current research and new investigations in the study of cardiovascular and other complex biomedical signals. The resource has 3 interdependent components. PhysioBank is a large and growing archive of well-characterized digital recordings of physiological signals and related data for use by the biomedical research community. It currently includes databases of multiparameter cardiopulmonary, neural, and other biomedical signals from healthy subjects and from patients with a variety of conditions with major public health implications, including life-threatening arrhythmias, congestive heart failure, sleep apnea, neurological disorders, and aging. PhysioToolkit is a library of open-source software for physiological signal processing and analysis, the detection of physiologically significant events using both classic techniques and novel methods based on statistical physics and nonlinear dynamics, the interactive display and characterization of signals, the creation of new databases, the simulation of physiological and other signals, the quantitative evaluation and comparison of analysis methods, and the analysis of nonstationary processes. PhysioNet is an on-line forum for the dissemination and exchange of recorded biomedical signals and open-source software for analyzing them. It provides facilities for the cooperative analysis of data and the evaluation of proposed new algorithms. In addition to providing free electronic access to PhysioBank data and PhysioToolkit software via the World Wide Web (http://www.physionet. org), PhysioNet offers services and training via on-line tutorials to assist users with varying levels of expertise.},
author = {Goldberger, A. L. and Amaral, L. A. and Glass, L. and Hausdorff, J. M. and Ivanov, P. C. and Mark, R. G. and Mietus, J. E. and Moody, G. B. and Peng, C. K. and Stanley, H. E.},
doi = {10.1161/01.cir.101.23.e215},
issn = {15244539},
journal = {Circulation},
month = {jun},
number = {23},
pmid = {10851218},
title = {{PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals.}},
url = {https://www.ahajournals.org/doi/10.1161/01.CIR.101.23.e215},
volume = {101},
year = {2000}
}
@book{Loveday2013,
abstract = {The metabolism and disposition of the bronchodilator, N t butylarterenol (tBA) and its di p toluate ester (bitolterol) were compared in the rat. Radioactivity was preferentially retained in lungs of rats compared with heart and blood after iv medication with tritium labeled bitolterol, but was not retained in tissues after iv medication with [3H]tBA. After oral and iv medication with [3H]bitolterol, fecal radioactivity accounted for 24{\%} of the dose and 65 and 79{\%} of the radioactivity, respectively, was excreted in urine (0-72 hr). In comparison, urine radioactivity after oral and iv medication with [3H]tBA was 43 and 83{\%} of the dose, respectively, and fecal radioactivity accounted for 43 and 23{\%} of the dose, respectively (0-72 hr). Bitolterol was hydrolyzed in vitro to tBA esterases found in various tissues including small intestine, liver, and plasma. Moreover, tBA was a substrate for catecholamine O methyltransferase but not for monoamine oxidase. Similar metabolites were observed in urine samples of rats given either [3H]tBA or [3H]bitolterol. Urine metabolites were identified as free and conjugated forms of both tBA and 3 O methyl tBA.},
address = {Cham},
author = {Shargel, L. and Dorrbecker, S. A. and Levitt, M.},
booktitle = {Drug Metabolism and Disposition},
doi = {10.1007/978-3-642-36530-0},
editor = {{De Marsico}, Maria and {Sanniti di Baja}, Gabriella and Fred, Ana},
isbn = {978-3-642-36529-4},
issn = {00909556},
keywords = {combining classifier,electricity theft,optimum path forest,support vector machine,unbalance class problem,ute},
number = {1},
pages = {65--71},
pmid = {3403},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Physiological disposition and metabolism of N t butylarterenol and its di p toluate ester (bitolterol) in the rat}},
url = {http://link.springer.com/10.1007/978-3-642-36530-0},
volume = {4},
year = {1976}
}
@article{Rajan2019,
abstract = {Pulmonary embolisms (PE) are known to be one of the leading causes for cardiac-related mortality. Due to inherent variabilities in how PE manifests and the cumbersome nature of manual diagnosis, there is growing interest in leveraging AI tools for detecting PE. In this paper, we build a two-stage detection pipeline that is accurate, computationally efficient, robust to variations in PE types and kernels used for CT reconstruction, and most importantly, does not require dense annotations. Given the challenges in acquiring expert annotations in large-scale datasets, our approach produces state-of-the-art results with very sparse emboli contours (at 10mm slice spacing), while using models with significantly lower number of parameters. We achieve AUC scores of 0.94 on the validation set and 0.85 on the test set of highly severe PEs. Using a large, real-world dataset characterized by complex PE types and patients from multiple hospitals, we present an elaborate empirical study and provide guidelines for designing highly generalizable pipelines.},
annote = {{\_}eprint: 1910.02175},
archivePrefix = {arXiv},
arxivId = {1910.02175},
author = {Rajan, Deepta and Beymer, David and Abedin, Shafiqul and Dehghan, Ehsan},
eprint = {1910.02175},
journal = {arXiv},
title = {{Pi-PE: A pipeline for pulmonary embolism detection using sparsely annotated 3D CT images}},
url = {http://arxiv.org/abs/1910.02175},
year = {2019}
}
@article{Born2020a,
abstract = {With the rapid development of COVID-19 into a global pandemic, there is an ever more urgent need for cheap, fast and reliable tools that can assist physicians in diagnosing COVID-19. Medical imaging such as CT can take a key role in complementing conventional diagnostic tools from molecular biology, and, using deep learning techniques, several automatic systems were demonstrated promising performances using CT or X-ray data. Here, we advocate a more prominent role of point-of-care ultrasound imaging to guide COVID-19 detection. Ultrasound is non-invasive and ubiquitous in medical facilities around the globe. Our contribution is threefold. First, we gather a lung ultrasound (POCUS) dataset consisting of (currently) 1103 images (654 COVID-19, 277 bacterial pneumonia and 172 healthy controls), sampled from 64 videos. While this dataset was assembled from various online sources and is by no means exhaustive, it was processed specifically to feed deep learning models and is intended to serve as a starting point for an open-access initiative. Second, we train a deep convolutional neural network (POCOVID-Net) on this 3-class dataset and achieve an accuracy of 89{\%} and, by a majority vote, a video accuracy of 92{\%} . For detecting COVID-19 in particular, the model performs with a sensitivity of 0.96, a specificity of 0.79 and F1-score of 0.92 in a 5-fold cross validation. Third, we provide an open-access web service (POCOVIDScreen) that is available at: https://pocovidscreen.org. The website deploys the predictive model, allowing to perform predictions on ultrasound lung images. In addition, it grants medical staff the option to (bulk) upload their own screenings in order to contribute to the growing public database of pathological lung ultrasound images.},
archivePrefix = {arXiv},
arxivId = {2004.12084},
author = {Born, Jannis and Br{\"{a}}ndle, Gabriel and Cossio, Manuel and Disdier, Marion and Goulet, Julie and Roulin, J{\'{e}}r{\'{e}}mie and Wiedemann, Nina},
eprint = {2004.12084},
journal = {arXiv},
month = {apr},
title = {{POCOVID-net: Automatic detection of COVID-19 from a new lung ultrasound imaging dataset (POCUS)}},
url = {http://arxiv.org/abs/2004.12084},
year = {2020}
}
@inproceedings{Huang2016a,
abstract = {In this paper, we tackle the labeling problem for 3D point clouds. We introduce a 3D point cloud labeling scheme based on 3D Convolutional Neural Network. Our approach minimizes the prior knowledge of the labeling problem and does not require a segmentation step or hand-crafted features as most previous approaches did. Particularly, we present solutions for large data handling during the training and testing process. Experiments performed on the urban point cloud dataset containing 7 categories of objects show the robustness of our approach.},
author = {Jing, Huang and You, Suya},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2016.7900038},
isbn = {9781509048472},
issn = {10514651},
keywords = {3D convolutional neural network,3D point cloud labeling scheme,Labeling,Neural networks,Testing,Three-dimensional displays,Training,Training data,Two dimensional displays,computer vision,data handling,neural nets,object recognition,testing process,training process,urban point cloud dataset},
pages = {2670--2675},
title = {{Point cloud labeling using 3D Convolutional Neural Network}},
volume = {0},
year = {2016}
}
@inproceedings{Li2018,
abstract = {We present a simple and general framework for feature learning from point clouds. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point clouds are irregular and unordered, thus directly convolving kernels against features associated with the points will result in desertion of shape information and variance to point ordering. To address these problems, we propose to learn an X-transformation from the input points to simultaneously promote two causes: the first is the weighting of the input features associated with the points, and the second is the permutation of the points into a latent and potentially canonical order. Element-wise product and sum operations of the typical convolution operator are subsequently applied on the X-transformed features. The proposed method is a generalization of typical CNNs to feature learning from point clouds, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.},
annote = {{\_}eprint: 1801.07791},
archivePrefix = {arXiv},
arxivId = {1801.07791},
author = {Li, Yangyan and Bu, Rui and Sun, Mingchao and Wu, Wei and Di, Xinhan and Chen, Baoquan},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1801.07791},
issn = {10495258},
pages = {820--830},
title = {{PointCNN: Convolution on X-transformed points}},
url = {http://arxiv.org/abs/1801.07791},
volume = {2018-Decem},
year = {2018}
}
@inproceedings{Qi2016,
abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
annote = {{\_}eprint: 1612.00593},
archivePrefix = {arXiv},
arxivId = {1612.00593},
author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.16},
eprint = {1612.00593},
isbn = {9781538604571},
pages = {77--85},
title = {{PointNet: Deep learning on point sets for 3D classification and segmentation}},
url = {http://arxiv.org/abs/1612.00593},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Qi2017,
abstract = {Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.},
annote = {{\_}eprint: 1706.02413},
archivePrefix = {arXiv},
arxivId = {1706.02413},
author = {Qi, Charles R. and Yi, Li and Su, Hao and Guibas, Leonidas J.},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1706.02413},
issn = {10495258},
pages = {5100--5109},
title = {{PointNet++: Deep hierarchical feature learning on point sets in a metric space}},
url = {http://arxiv.org/abs/1706.02413},
volume = {2017-Decem},
year = {2017}
}
@inproceedings{Kohl2004,
abstract = {This paper presents a machine learning approach to optimizing a quadrupedal trot gait for forward speed. Given a parameterized walk designed for a specific robot, we propose using a form of policy gradient reinforcement learning to automatically search the set of possible parameters with the goal of finding the fastest possible walk. We implement and test our approach on a commercially available quadrupedal robot platform, namely the Sony Aibo robot. After about three hours of learning, all on the physical robots and with no human intervention other than to change the batteries, the robots achieved a gait faster than any previously known gait known for the Aibo, significantly outperforming, a variety of existing hand-coded and learned solutions.},
author = {Kohl, Nate and Stone, Peter},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/robot.2004.1307456},
issn = {10504729},
keywords = {Learning Control,Multi Legged Robots,Walking Robots},
title = {{Policy gradient reinforcement learning for fast quadrupedal locomotion}},
year = {2004}
}
@book{Shaik2020,
address = {Berkeley, CA},
author = {Shaik, Baji},
booktitle = {PostgreSQL Configuration},
doi = {10.1007/978-1-4842-5663-3},
isbn = {978-1-4842-5662-6},
publisher = {Apress},
title = {{PostgreSQL Configuration}},
url = {http://link.springer.com/10.1007/978-1-4842-5663-3},
year = {2020}
}
@inproceedings{Snoek2012,
abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a "black art" requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1206.2944},
isbn = {9781627480031},
issn = {10495258},
pages = {2951--2959},
title = {{Practical Bayesian optimization of machine learning algorithms}},
volume = {4},
year = {2012}
}
@inproceedings{Zhong2017a,
abstract = {Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained sequentially to choose component layers. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it performs competitive results in comparison to the hand-crafted state-of-the-art networks on image classification, additionally, the best network generated by BlockQNN achieves 3.54{\%} top-1 error rate on CIFAR-10 which beats all existing auto-generate networks. (2) in the meanwhile, it offers tremendous reduction of the search space in designing networks which only spends 3 days with 32 GPUs, and (3) moreover, it has strong generalizability that the network built on CIFAR also performs well on a larger-scale ImageNet dataset.},
archivePrefix = {arXiv},
arxivId = {1708.05552},
author = {Zhong, Zhao and Yan, Junjie and Wu, Wei and Shao, Jing and Liu, Cheng Lin},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00257},
eprint = {1708.05552},
isbn = {9781538664209},
issn = {10636919},
pages = {2423--2432},
title = {{Practical Block-Wise Neural Network Architecture Generation}},
year = {2018}
}
@book{BergHansen2020,
address = {Berkeley, CA},
author = {{Berg Hansen}, Kim},
booktitle = {Practical Oracle SQL},
doi = {10.1007/978-1-4842-5617-6},
isbn = {978-1-4842-5616-9},
publisher = {Apress},
title = {{Practical Oracle SQL}},
url = {http://link.springer.com/10.1007/978-1-4842-5617-6},
year = {2020}
}
@article{Veta2019,
abstract = {Tumor proliferation is an important biomarker indicative of the prognosis of breast cancer patients. Assessment of tumor proliferation in a clinical setting is a highly subjective and labor-intensive task. Previous efforts to automate tumor proliferation assessment by image analysis only focused on mitosis detection in predefined tumor regions. However, in a real-world scenario, automatic mitosis detection should be performed in whole-slide images (WSIs) and an automatic method should be able to produce a tumor proliferation score given a WSI as input. To address this, we organized the TUmor Proliferation Assessment Challenge 2016 (TUPAC16) on prediction of tumor proliferation scores from WSIs. The challenge dataset consisted of 500 training and 321 testing breast cancer histopathology WSIs. In order to ensure fair and independent evaluation, only the ground truth for the training dataset was provided to the challenge participants. The first task of the challenge was to predict mitotic scores, i.e., to reproduce the manual method of assessing tumor proliferation by a pathologist. The second task was to predict the gene expression based PAM50 proliferation scores from the WSI. The best performing automatic method for the first task achieved a quadratic-weighted Cohen's kappa score of $\kappa$ = 0.567, 95{\%} CI [0.464, 0.671] between the predicted scores and the ground truth. For the second task, the predictions of the top method had a Spearman's correlation coefficient of r = 0.617, 95{\%} CI [0.581 0.651] with the ground truth. This was the first comparison study that investigated tumor proliferation assessment from WSIs. The achieved results are promising given the difficulty of the tasks and weakly-labeled nature of the ground truth. However, further research is needed to improve the practical utility of image analysis methods for this task.},
archivePrefix = {arXiv},
arxivId = {1807.08284},
author = {Veta, Mitko and Heng, Yujing J. and Stathonikos, Nikolas and Bejnordi, Babak Ehteshami and Beca, Francisco and Wollmann, Thomas and Rohr, Karl and Shah, Manan A. and Wang, Dayong and Rousson, Mikael and Hedlund, Martin and Tellez, David and Ciompi, Francesco and Zerhouni, Erwan and Lanyi, David and Viana, Matheus and Kovalev, Vassili and Liauchuk, Vitali and Phoulady, Hady Ahmady and Qaiser, Talha and Graham, Simon and Rajpoot, Nasir and Sj{\"{o}}blom, Erik and Molin, Jesper and Paeng, Kyunghyun and Hwang, Sangheum and Park, Sunggyun and Jia, Zhipeng and Chang, Eric I.Chao and Xu, Yan and Beck, Andrew H. and van Diest, Paul J. and Pluim, Josien P.W.},
doi = {10.1016/j.media.2019.02.012},
eprint = {1807.08284},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Breast cancer,Cancer prognostication,Deep learning,Tumor proliferation},
month = {jul},
pages = {111--121},
title = {{Predicting breast tumor proliferation from whole-slide images: The TUPAC16 challenge}},
url = {http://arxiv.org/abs/1807.08284 http://dx.doi.org/10.1016/j.media.2019.02.012},
volume = {54},
year = {2019}
}
@article{Akkus2017,
abstract = {Several studies have linked codeletion of chromosome arms 1p/19q in low-grade gliomas (LGG) with positive response to treatment and longer progression-free survival. Hence, predicting 1p/19q status is crucial for effective treatment planning of LGG. In this study, we predict the 1p/19q status from MR images using convolutional neural networks (CNN), which could be a non-invasive alternative to surgical biopsy and histopathological analysis. Our method consists of three main steps: image registration, tumor segmentation, and classification of 1p/19q status using CNN. We included a total of 159 LGG with 3 image slices each who had biopsy-proven 1p/19q status (57 non-deleted and 102 codeleted) and preoperative postcontrast-T1 (T1C) and T2 images. We divided our data into training, validation, and test sets. The training data was balanced for equal class probability and was then augmented with iterations of random translational shift, rotation, and horizontal and vertical flips to increase the size of the training set. We shuffled and augmented the training data to counter overfitting in each epoch. Finally, we evaluated several configurations of a multi-scale CNN architecture until training and validation accuracies became consistent. The results of the best performing configuration on the unseen test set were 93.3{\%} (sensitivity), 82.22{\%} (specificity), and 87.7{\%} (accuracy). Multi-scale CNN with their self-learning capability provides promising results for predicting 1p/19q status non-invasively based on T1C and T2 images. Predicting 1p/19q status non-invasively from MR images would allow selecting effective treatment strategies for LGG patients without the need for surgical biopsy.},
author = {Akkus, Zeynettin and Ali, Issa and Sedl{\'{a}}ř, Jiř{\'{i}} and Agrawal, Jay P. and Parney, Ian F. and Giannini, Caterina and Erickson, Bradley J.},
doi = {10.1007/s10278-017-9984-3},
issn = {1618727X},
journal = {Journal of Digital Imaging},
keywords = {1p/19q codeletion,Convolutional neural networks,Low grade gliomas,Therapy response},
month = {aug},
number = {4},
pages = {469--476},
pmid = {28600641},
title = {{Predicting Deletion of Chromosomal Arms 1p/19q in Low-Grade Gliomas from MR Images Using Machine Intelligence}},
url = {http://link.springer.com/10.1007/s10278-017-9984-3},
volume = {30},
year = {2017}
}
@book{Kainz2017,
address = {Cham},
author = {Kainz, Bernhard and Bhatia, Kanwal and Vaillant, Ghislain and Zuluaga, Maria A.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-52280-7},
editor = {Zuluaga, Maria A. and Bhatia, Kanwal and Kainz, Bernhard and Moghari, Mehdi H. and Pace, Danielle F.},
isbn = {9783319522791},
issn = {16113349},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Preface}},
url = {http://link.springer.com/10.1007/978-3-319-52280-7},
volume = {10129 LNCS},
year = {2017}
}
@article{Ogiela2008,
abstract = {This chapter briefly discusses the main stages of image preprocessing. The introduction to this book mentioned that the preprocessing of medical image is subject to certain restrictions and is generally more complex than the processing of other image types [26, 52]. This is why, of the many different techniques and methods for image filtering, we have decided to discuss here only selected ones, most frequently applied to medical images and which have been proven to be suitable for that purpose in numerous practical cases. Their operation will be illustrated with examples of simple procedures aimed at improving the quality of imaging and allowing significant information to be generated for its use at the stages of image interpretation. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
address = {Berlin, Heidelberg},
author = {Ogiela, Marek R. and Tadeusiewicz, Ryszard},
doi = {10.1007/978-3-540-75402-2_4},
isbn = {9783540753995},
issn = {1860949X},
journal = {Studies in Computational Intelligence},
pages = {65--97},
publisher = {Springer Berlin Heidelberg},
title = {{Preprocessing medical images and their overall enhancement}},
url = {https://doi.org/10.1007/978-3-540-75402-2{\_}4},
volume = {84},
year = {2008}
}
@incollection{Tefas2016,
address = {Cham},
author = {Picasso, Princeton},
booktitle = {Ieee Signal Processing Letters},
doi = {10.1201/b17700-1},
isbn = {9781439802847},
number = {2},
pages = {40--42},
publisher = {Springer International Publishing},
title = {{Principal Component Analysis Why Principal Component Analysis ?}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}649-1},
volume = {9},
year = {2002}
}
@article{Tofighi2019,
abstract = {Cell nuclei detection is a challenging research topic because of limitations in cellular image quality and diversity of nuclear morphology, i.e., varying nuclei shapes, sizes, and overlaps between multiple cell nuclei. This has been a topic of enduring interest with promising recent success shown by deep learning methods. These methods train convolutional neural networks (CNNs) with a training set of input images and known, labeled nuclei locations. Many such methods are supplemented by spatial or morphological processing. Using a set of canonical cell nuclei shapes, prepared with the help of a domain expert, we develop a new approach that we call shape priors (SPs) with CNNs (SPs-CNN). We further extend the network to introduce an SP layer and then allowing it to become trainable (i.e., optimizable). We call this network as tunable SP-CNN (TSP-CNN). In summary, we present new network structures that can incorporate "expected behavior" of nucleus shapes via two components: learnable layers that perform the nucleus detection and a fixed processing part that guides the learning with prior information. Analytically, we formulate two new regularization terms that are targeted at: 1) learning the shapes and 2) reducing false positives while simultaneously encouraging detection inside the cell nucleus boundary. Experimental results on two challenging datasets reveal that the proposed SP-CNN and TSP-CNN can outperform the state-of-the-art alternatives.},
archivePrefix = {arXiv},
arxivId = {1901.07061},
author = {Tofighi, Mohammad and Guo, Tiantong and Vanamala, Jairam K.P. and Monga, Vishal},
doi = {10.1109/TMI.2019.2895318},
eprint = {1901.07061},
issn = {1558254X},
journal = {IEEE transactions on medical imaging},
keywords = {Biomedical imaging,Computer architecture,Deep learning,Image edge detection,Image segmentation,Microprocessors,Nucleus detection,Shape,TSP-CNN,biology computing,canonical cell nuclei shapes,cell nuclei detection,cell nucleus boundary,cell nucleus detection,cellular biophysics,cellular image quality,convolutional neural nets,convolutional neural networks,deep learning,deep learning methods,domain expert,fixed processing part,input images,labeled nuclei locations,learnable layers,learnable shapes,learning (artificial intelligence),medical image processing,morphological processing,multiple cell nuclei,network structures,nuclear morphology,nucleus shapes,regularization terms,shape priors,spatial processing,training set,tunable SP-CNN},
month = {sep},
number = {9},
pages = {2047--2058},
pmid = {30703016},
title = {{Prior Information Guided Regularized Deep Learning for Cell Nucleus Detection}},
volume = {38},
year = {2019}
}
@book{Reinkemeyer2020,
address = {Cham},
booktitle = {Process Mining in Action},
doi = {10.1007/978-3-030-40172-6},
editor = {Reinkemeyer, Lars},
isbn = {978-3-030-40171-9},
publisher = {Springer International Publishing},
title = {{Process Mining in Action}},
url = {http://link.springer.com/10.1007/978-3-030-40172-6},
year = {2020}
}
@misc{k8s,
author = {{The Linux Foundation}},
booktitle = {Kubernetes.Io},
title = {{Production-Grade Container Orchestration - Kubernetes}},
url = {https://kubernetes.io/},
year = {2020}
}
@misc{neuromorphometrics2012,
title = {{Products | Neuromorphometrics, Inc.}},
url = {http://www.neuromorphometrics.com/?page{\_}id=23}
}
@misc{Stackexchange-q6988,
booktitle = {Stackexchange},
title = {{programming - How to sort an alphanumeric list - TeX - LaTeX Stack Exchange}},
url = {https://tex.stackexchange.com/questions/6988/how-to-sort-an-alphanumeric-list/7095{\#}7095},
urldate = {2020-07-30}
}
@book{Skalka2005,
address = {Cham},
author = {Skalka, Christian},
booktitle = {IEEE Security and Privacy},
doi = {10.1109/MSP.2005.77},
editor = {M{\"{u}}ller, Peter},
isbn = {978-3-030-44913-1},
issn = {15407993},
number = {3},
pages = {80--83},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Programming languages and systems security}},
url = {http://link.springer.com/10.1007/978-3-030-44914-8},
volume = {3},
year = {2005}
}
@inproceedings{Liu2017a,
abstract = {We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.},
archivePrefix = {arXiv},
arxivId = {1712.00559},
author = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01246-5_2},
eprint = {1712.00559},
isbn = {9783030012458},
issn = {16113349},
pages = {19--35},
title = {{Progressive Neural Architecture Search}},
volume = {11205 LNCS},
year = {2018}
}
@book{Baines2014,
abstract = {Turn your students into scientists who use their knowledge and creativity to solve real-world problems. Each lesson features a step-by-step guide; a summary of recent research; and handouts that are classroom-ready. Learn about the three levels of writing, from a Level 1 quickwrite to a formal, multi-part, Level 3 research paper. Each writing assignment-narrative, persuasive, and informative-includes a detailed rubric that makes grading easy. Students collaborate to contain an outbreak of avian flu, lead a group of people trying to survive under harsh conditions, battle drought in a densely-populated city in the American southwest, research the behavior of animals in the local region, and calculate their own speed, velocity, and momentum. Engaging and demanding, Project-Based Writing in Science helps students to understand and improve the world.},
address = {Rotterdam},
author = {Baines, Lawrence},
booktitle = {Project-Based Writing in Science},
doi = {10.1007/978-94-6209-671-4},
isbn = {9789462096714},
pages = {1--108},
publisher = {SensePublishers},
title = {{Project-based writing in science}},
url = {http://link.springer.com/10.1007/978-94-6209-671-4},
year = {2014}
}
@misc{Litjens2017a,
author = {Litjens, G and Debats, O and Barentsz, J and Karssemeijer, N. and Huisman, Henkjan J.},
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/K9TCIA.2017.MURS5CL},
publisher = {The Cancer Imaging Archive},
title = {{ProstateX Challenge data}},
url = {https://wiki.cancerimagingarchive.net/x/iIFpAQ},
year = {2017}
}
@article{Nibali2017PulmonaryNC,
abstract = {Purpose : Lung cancer has the highest death rate among all cancers in the USA. In this work we focus on improving the ability of computer-aided diagnosis (CAD) systems to predict the malignancy of nodules from cropped CT images of lung nodules. Methods: We evaluate the effectiveness of very deep convolutional neural networks at the task of expert-level lung nodule malignancy classification. Using the state-of-the-art ResNet architecture as our basis, we explore the effect of curriculum learning, transfer learning, and varying network depth on the accuracy of malignancy classification. Results: Due to a lack of public datasets with standardized problem definitions and train/test splits, studies in this area tend to not compare directly against other existing work. This makes it hard to know the relative improvement in the new solution. In contrast, we directly compare our system against two state-of-the-art deep learning systems for nodule classification on the LIDC/IDRI dataset using the same experimental setup and data set. The results show that our system achieves the highest performance in terms of all metrics measured including sensitivity, specificity, precision, AUROC, and accuracy. Conclusions: The proposed method of combining deep residual learning, curriculum learning, and transfer learning translates to high nodule classification accuracy. This reveals a promising new direction for effective pulmonary nodule CAD systems that mirrors the success of recent deep learning advances in other image-based application domains.},
author = {Nibali, Aiden and He, Zhen and Wollersheim, Dennis},
doi = {10.1007/s11548-017-1605-6},
issn = {18616429},
journal = {International Journal of Computer Assisted Radiology and Surgery},
keywords = {CT images,Convolutional neural network,Lung nodule},
month = {oct},
number = {10},
pages = {1799--1808},
pmid = {28501942},
title = {{Pulmonary nodule classification with deep residual networks}},
url = {http://link.springer.com/10.1007/s11548-017-1605-6},
volume = {12},
year = {2017}
}
@article{Oliphant2007,
annote = {{\_}eprint: https://aip.scitation.org/doi/pdf/10.1109/MCSE.2007.58},
author = {Oliphant, Travis E.},
doi = {10.1109/MCSE.2007.58},
issn = {15219615},
journal = {Computing in Science and Engineering},
number = {3},
pages = {10--20},
title = {{Python for scientific computing}},
url = {https://aip.scitation.org/doi/abs/10.1109/MCSE.2007.58},
volume = {9},
year = {2007}
}
@book{Milliken2020,
abstract = {A Ten-Week Bootcamp Approach to Python Programming},
address = {Berkeley, CA},
author = {Milliken, Connor P.},
booktitle = {Python Projects for Beginners},
doi = {10.1007/978-1-4842-5355-7},
isbn = {978-1-4842-5354-0},
publisher = {Apress},
title = {{Python Projects for Beginners}},
url = {http://link.springer.com/10.1007/978-1-4842-5355-7},
year = {2020}
}
@article{Steiner2019,
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
archivePrefix = {arXiv},
arxivId = {1912.01703},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"{o}}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
eprint = {1912.01703},
journal = {arXiv},
title = {{PyTorch: An imperative style, high-performance deep learning library}},
url = {http://arxiv.org/abs/1912.01703},
year = {2019}
}
@article{Ruifrok2001,
abstract = {OBJECTIVE: To develop a flexible method of separation and quantification of immunohistochemical staining by means of color image analysis. STUDY DESIGN: An algorithm was developed to deconvolve the color information acquired with red-green-blue (RGB) cameras and to calculate the contribution of each of the applied stains based on stain-specific RGB absorption. The algorithm was tested using different combinations of diaminobenzidine, hematoxylin and eosin at different staining levels. RESULTS: Quantification of the different stains was not significantly influenced by the combination of multiple stains in a single sample. The color deconvolution algorithm resulted in comparable quantification independent of the stain combinations as long as the histochemical procedures did not influence the amount of stain in the sample due to bleaching because of stain solubility and saturation of staining was prevented. CONCLUSION: This image analysis algorithm provides a robust and flexible method for objective immunohistochemical analysis of samples stained with up to three different stains using a laboratory microscope, standard RGB camera setup and the public domain program NIH Image.},
author = {Ruifrok, A. C. and Johnston, D. A.},
issn = {08846812},
journal = {Analytical and Quantitative Cytology and Histology},
keywords = {Color deconvolution,Color separation,Computer-assisted,Image processing,Immunohistochemistry},
number = {4},
pages = {291--299},
pmid = {11531144},
title = {{Quantification of histochemical staining by color deconvolution}},
volume = {23},
year = {2001}
}
@article{Daducci2014a,
abstract = {Validation is arguably the bottleneck in the diffusion magnetic resonance imaging (MRI) community. This paper evaluates and compares 20 algorithms for recovering the local intra-voxel fiber structure from diffusion MRI data and is based on the results of the 'HARDI reconstruction challenge' organized in the context of the 'ISBI 2012' conference. Evaluated methods encompass a mixture of classical techniques well known in the literature such as diffusion tensor, Q-Ball and diffusion spectrum imaging, algorithms inspired by the recent theory of compressed sensing and also brand new approaches proposed for the first time at this contest. To quantitatively compare the methods under controlled conditions, two datasets with known ground-truth were synthetically generated and two main criteria were used to evaluate the quality of the reconstructions in every voxel: correct assessment of the number of fiber populations and angular accuracy in their orientation. This comparative study investigates the behavior of every algorithm with varying experimental conditions and highlights strengths and weaknesses of each approach. This information can be useful not only for enhancing current algorithms and develop the next generation of reconstruction methods, but also to assist physicians in the choice of the most adequate technique for their studies. {\textcopyright} 1982-2012 IEEE.},
author = {Daducci, Alessandro and Canales-Rodriguez, Erick Jorge and Descoteaux, Maxime and Garyfallidis, Eleftherios and Gur, Yaniv and Lin, Ying Chia and Mani, Merry and Merlet, Sylvain and Paquette, Michael and Ramirez-Manzanares, Alonso and Reisert, Marco and Rodrigues, Paulo Reis and Sepehrband, Farshid and Caruyer, Emmanuel and Choupan, Jeiran and Deriche, Rachid and Jacob, Mathews and Menegaz, Gloria and Prckovska, Vesna and Rivera, Mariano and Wiaux, Yves and Thiran, Jean Philippe},
doi = {10.1109/TMI.2013.2285500},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Diffusion magnetic resonance imaging (dMRI),local reconstruction,quantitative comparison,synthetic data,validation},
month = {feb},
number = {2},
pages = {384--399},
pmid = {24132007},
title = {{Quantitative comparison of reconstruction methods for intra-voxel fiber recovery from diffusion MRI}},
url = {http://ieeexplore.ieee.org/document/6630106/},
volume = {33},
year = {2014}
}
@misc{CPTAC2018,
booktitle = {The Cancer Imaging Archive},
doi = {10.7937/k9/tcia.2018.3rje41q1},
publisher = {The Cancer Imaging Archive},
title = {{Radiology Data from the Clinical Proteomic Tumor Analysis Consortium Glioblastoma Multiforme [CPTAC-GBM] collection [Data set]. The Cancer Imaging Archive.}},
url = {https://wiki.cancerimagingarchive.net/x/gAHUAQ},
year = {2018}
}
@article{Vallieres2017,
abstract = {Quantitative extraction of high-dimensional mineable data from medical images is a process known as radiomics. Radiomics is foreseen as an essential prognostic tool for cancer risk assessment and the quantification of intratumoural heterogeneity. In this work, 1615 radiomic features (quantifying tumour image intensity, shape, texture) extracted from pre-treatment FDG-PET and CT images of 300 patients from four different cohorts were analyzed for the risk assessment of locoregional recurrences (LR) and distant metastases (DM) in head-and-neck cancer. Prediction models combining radiomic and clinical variables were constructed via random forests and imbalance-adjustment strategies using two of the four cohorts. Independent validation of the prediction and prognostic performance of the models was carried out on the other two cohorts (LR: AUC = 0.69 and CI = 0.67; DM: AUC = 0.86 and CI = 0.88). Furthermore, the results obtained via Kaplan-Meier analysis demonstrated the potential of radiomics for assessing the risk of specific tumour outcomes using multiple stratification groups. This could have important clinical impact, notably by allowing for a better personalization of chemo-radiation treatments for head-and-neck cancer patients from different risk groups.},
archivePrefix = {arXiv},
arxivId = {1703.08516},
author = {Valli{\`{e}}res, Martin and Kay-Rivest, Emily and Perrin, L{\'{e}}o Jean and Liem, Xavier and Furstoss, Christophe and Aerts, Hugo J.W.L. and Khaouam, Nader and Nguyen-Tan, Phuc Felix and Wang, Chang Shu and Sultanem, Khalil and Seuntjens, Jan and {El Naqa}, Issam},
doi = {10.1038/s41598-017-10371-5},
eprint = {1703.08516},
issn = {20452322},
journal = {Scientific Reports},
month = {dec},
number = {1},
pages = {10117},
pmid = {28860628},
title = {{Radiomics strategies for risk assessment of tumour failure in head-and-neck cancer}},
url = {http://www.nature.com/articles/s41598-017-10371-5},
volume = {7},
year = {2017}
}
@article{Zhong2017,
abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
annote = {{\_}eprint: 1708.04896},
archivePrefix = {arXiv},
arxivId = {1708.04896},
author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
doi = {10.1609/aaai.v34i07.7000},
eprint = {1708.04896},
issn = {2159-5399},
journal = {arXiv},
title = {{Random erasing data augmentation}},
url = {http://arxiv.org/abs/1708.04896},
year = {2017}
}
@article{Bergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success-they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms. {\textcopyright} 2012 James Bergstra and Yoshua Bengio.},
author = {Bergstra, James and Bengio, Yoshua},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Deep learning,Global optimization,Model selection,Neural networks,Response surface modeling},
number = {1},
pages = {281--305},
title = {{Random search for hyper-parameter optimization}},
volume = {13},
year = {2012}
}
@article{Ju2015,
abstract = {Accurate lung tumor delineation plays an important role in radiotherapy treatment planning. Since the lung tumor has poor boundary in positron emission tomography (PET) images and low contrast in computed tomography (CT) images, segmentation of tumor in the PET and CT images is a challenging task. In this paper, we effectively integrate the two modalities by making fully use of the superior contrast of PET images and superior spatial resolution of CT images. Random walk and graph cut method is integrated to solve the segmentation problem, in which random walk is utilized as an initialization tool to provide object seeds for graph cut segmentation on the PET and CT images. The co-segmentation problem is formulated as an energy minimization problem which is solved by max-flow/min-cut method. A graph, including two sub-graphs and a special link, is constructed, in which one sub-graph is for the PET and another is for CT, and the special link encodes a context term which penalizes the difference of the tumor segmentation on the two modalities. To fully utilize the characteristics of PET and CT images, a novel energy representation is devised. For the PET, a downhill cost and a 3D derivative cost are proposed. For the CT, a shape penalty cost is integrated into the energy function which helps to constrain the tumor region during the segmentation. We validate our algorithm on a data set which consists of 18 PET-CT images. The experimental results indicate that the proposed method is superior to the graph cut method solely using the PET or CT is more accurate compared with the random walk method, random walk co-segmentation method, and non-improved graph cut method.},
author = {Ju, Wei and Xiang, Deihui and Zhang, Bin and Wang, Lirong and Kopriva, Ivica and Chen, Xinjian},
doi = {10.1109/TIP.2015.2488902},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Computed Tomography (CT),Positron Emission Tomography (PET),graph cut,image segmentation,interactive segmentation,lung tumor,prior information,random walk},
number = {12},
pages = {5854--5867},
pmid = {26462198},
title = {{Random Walk and Graph Cut for Co-Segmentation of Lung Tumor on PET-CT Images}},
volume = {24},
year = {2015}
}
@book{Daepp2011,
address = {New York, NY},
author = {Daepp, Ulrich and Gorkin, Pamela},
doi = {10.1007/978-1-4419-9479-0},
isbn = {978-1-4419-9478-3},
publisher = {Springer New York},
series = {Undergraduate Texts in Mathematics},
title = {{Reading, Writing, and Proving}},
url = {http://link.springer.com/10.1007/978-1-4419-9479-0},
year = {2011}
}
@book{Roos2019,
address = {Singapore},
author = {Roos, Cathryn and Roos, Gregory},
doi = {10.1007/978-981-13-7820-1},
isbn = {978-981-13-7819-5},
publisher = {Springer Singapore},
series = {SpringerBriefs in Education},
title = {{Real Science in Clear English}},
url = {http://link.springer.com/10.1007/978-981-13-7820-1},
year = {2019}
}
@book{Magnor2020,
address = {Cham},
doi = {10.1007/978-3-030-41816-8},
editor = {Magnor, Marcus and Sorkine-Hornung, Alexander},
isbn = {978-3-030-41815-1},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Real VR – Immersive Digital Reality}},
url = {http://link.springer.com/10.1007/978-3-030-41816-8},
volume = {11900},
year = {2020}
}
@article{VanRullen2018,
abstract = {Although distinct categories are reliably decoded from fMRI brain responses, it has proved more difficult to distinguish visually similar inputs, such as different faces. Here, we apply a recently developed deep learning system to reconstruct face images from human fMRI. We trained a variational auto-encoder (VAE) neural network using a GAN (Generative Adversarial Network) unsupervised procedure over a large data set of celebrity faces. The auto-encoder latent space provides a meaningful, topologically organized 1024-dimensional description of each image. We then presented several thousand faces to human subjects, and learned a simple linear mapping between the multi-voxel fMRI activation patterns and the 1024 latent dimensions. Finally, we applied this mapping to novel test images, translating fMRI patterns into VAE latent codes, and codes into face reconstructions. The system not only performed robust pairwise decoding ({\textgreater}95{\%} correct), but also accurate gender classification, and even decoded which face was imagined, rather than seen.},
archivePrefix = {arXiv},
arxivId = {1810.03856},
author = {VanRullen, Rufin and Reddy, Leila},
doi = {10.1038/s42003-019-0438-y},
eprint = {1810.03856},
issn = {23993642},
journal = {Communications Biology},
month = {oct},
number = {1},
pmid = {31925027},
title = {{Reconstructing faces from fMRI patterns using deep generative neural networks}},
url = {http://arxiv.org/abs/1810.03856},
volume = {2},
year = {2019}
}
@article{Shillcock2016,
abstract = {Large-scale brain initiatives such as the US BRAIN initiative and the European Human Brain Project aim to marshall a vast amount of data and tools for the purpose of furthering our understanding of brains. Fundamental to this goal is that neuronal morphologies must be seamlessly reconstructed and aggregated on scales up to the whole rodent brain. The experimental labor needed to manually produce this number of digital morphologies is prohibitively large. The BigNeuron initiative is assembling community-generated, open-source, automated reconstruction algorithms into an open platform, and is beginning to generate an increasing flow of high-quality reconstructed neurons. We propose a novel extension of this workflow to use this data stream to generate an unlimited number of statistically equivalent, yet distinct, digital morphologies. This will bring automated processing of reconstructed cells into digital neurons to the wider neuroscience community, and enable a range of morphologically accurate computational models.},
author = {Shillcock, Julian C. and Hawrylycz, Michael and Hill, Sean and Peng, Hanchuan},
doi = {10.1007/s40708-016-0041-7},
issn = {21984026},
journal = {Brain Informatics},
keywords = {Automated reconstruction,BigNeuron,Morphometric analysis,Neuron,Synthesis},
month = {dec},
number = {4},
pages = {205--209},
title = {{Reconstructing the brain: from image stacks to neuron synthesis}},
url = {http://link.springer.com/10.1007/s40708-016-0041-7},
volume = {3},
year = {2016}
}
@article{Wierstra2009,
abstract = {Reinforcement learning for partially observable Markov decision problems (POMDPs) is a challenge as it requires policies with an internal state. Traditional approaches suffer significantly from this shortcoming and usually make strong assumptions on the problem domain such as perfect system models, state-estimators and a Markovian hidden system. Recurrent neural networks (RNNs) offer a natural framework for dealing with policy learning using hidden state and require only few limiting assumptions. As they can be trained well using gradient descent, they are suited for policy gradient approaches. In this paper, we present a policy gradient method, the Recurrent Policy Gradient which constitutes a model-free reinforcement learning method. It is aimed at training limited-memory stochastic policies on problems which require long-term memories of past observations. The approach involves approximating a policy gradient for a recurrent neural network by backpropagating return-weighted characteristic eligibilities through time. Using a "Long Short-Term Memory" RNN architecture, we are able to outperform previous RL methods on three important benchmark tasks. Furthermore, we show that using history-dependent baselines helps reducing estimation variance significantly, thus enabling our approach to tackle more challenging, highly stochastic environments. {\textcopyright} The Author 2009. Published by Oxford University Press. All rights reserved.},
author = {Wierstra, Daan and F{\"{o}}rster, Alexander and Peters, Jan and Schmidhuber, J{\"{u}}rgen},
doi = {10.1093/jigpal/jzp049},
issn = {13670751},
journal = {Logic Journal of the IGPL},
keywords = {Partially Observable Markov Decision Problems (POM,Policy gradient methods,Recurrent neural networks,Reinforcement learning},
number = {5},
pages = {620--634},
title = {{Recurrent policy gradients}},
volume = {18},
year = {2009}
}
@incollection{Rusinkiewicz2020,
address = {Cham},
author = {Rusinkiewicz, Szymon},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_537-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Reflectance Models}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}537-1},
year = {2020}
}
@misc{Orlando2020,
abstract = {Glaucoma is one of the leading causes of irreversible but preventable blindness in working age populations. Color fundus photography (CFP) is the most cost-effective imaging modality to screen for retinal disorders. However, its application to glaucoma has been limited to the computation of a few related biomarkers such as the vertical cup-to-disc ratio. Deep learning approaches, although widely applied for medical image analysis, have not been extensively used for glaucoma assessment due to the limited size of the available data sets. Furthermore, the lack of a standardize benchmark strategy makes difficult to compare existing methods in a uniform way. In order to overcome these issues we set up the Retinal Fundus Glaucoma Challenge, REFUGE (https://refuge.grand-challenge.org), held in conjunction with MICCAI 2018. The challenge consisted of two primary tasks, namely optic disc/cup segmentation and glaucoma classification. As part of REFUGE, we have publicly released a data set of 1200 fundus images with ground truth segmentations and clinical glaucoma labels, currently the largest existing one. We have also built an evaluation framework to ease and ensure fairness in the comparison of different models, encouraging the development of novel techniques in the field. 12 teams qualified and participated in the online challenge. This paper summarizes their methods and analyzes their corresponding results. In particular, we observed that two of the top-ranked teams outperformed two human experts in the glaucoma classification task. Furthermore, the segmentation results were in general consistent with the ground truth annotations, with complementary outcomes that can be further exploited by ensembling the results.},
archivePrefix = {arXiv},
arxivId = {1910.03667},
author = {Orlando, Jos{\'{e}} Ignacio and Fu, Huazhu and {Barbossa Breda}, Jo{\~{a}}o and van Keer, Karel and Bathula, Deepti R. and Diaz-Pinto, Andr{\'{e}}s and Fang, Ruogu and Heng, Pheng Ann and Kim, Jeyoung and Lee, Joon Ho and Lee, Joonseok and Li, Xiaoxiao and Liu, Peng and Lu, Shuai and Murugesan, Balamurali and Naranjo, Valery and Phaye, Sai Samarth R. and Shankaranarayana, Sharath M. and Sikka, Apoorva and Son, Jaemin and van den Hengel, Anton and Wang, Shujun and Wu, Junyan and Wu, Zifeng and Xu, Guanghui and Xu, Yongli and Yin, Pengshuai and Li, Fei and Zhang, Xiulan and Xu, Yanwu and Bogunovi{\'{c}}, Hrvoje},
booktitle = {Medical Image Analysis},
doi = {10.1016/j.media.2019.101570},
eprint = {1910.03667},
issn = {13618423},
keywords = {Deep learning,Fundus photography,Glaucoma,Image classification,Image segmentation},
pmid = {31630011},
title = {{REFUGE Challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs}},
volume = {59},
year = {2020}
}
@incollection{Deguchi2020,
address = {Cham},
author = {Deguchi, Koichiro},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_828-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Regularization}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}828-1},
year = {2020}
}
@article{Real2018,
abstract = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier-AmoebaNet-A-that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9{\%} top-1 / 96.6{\%} top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
archivePrefix = {arXiv},
arxivId = {1802.01548},
author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
doi = {10.1609/aaai.v33i01.33014780},
eprint = {1802.01548},
isbn = {9781577358091},
issn = {2159-5399},
journal = {33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019},
pages = {4780--4789},
title = {{Regularized evolution for image classifier architecture search}},
volume = {33},
year = {2019}
}
@article{Kaelbling1996,
abstract = {This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word "reinforcement." The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
doi = {10.1613/jair.301},
eprint = {9605103},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
month = {apr},
pages = {237--285},
primaryClass = {cs},
title = {{Reinforcement learning: A survey}},
url = {https://arxiv.org/abs/cs/9605103},
volume = {4},
year = {1996}
}
@book{Koelsch2016,
address = {Berkeley, CA},
author = {Koelsch, George},
doi = {10.1007/978-1-4842-2099-3},
isbn = {978-1-4842-2098-6},
publisher = {Apress},
title = {{Requirements Writing for System Engineering}},
url = {http://link.springer.com/10.1007/978-1-4842-2099-3},
year = {2016}
}
@misc{Alkhasli2019a,
author = {Alkhasli, Isabel and Sakreida, Katrin and Mottaghy, Felix M and Binkofski, Ferdinand},
doi = {10.18112/OPENNEURO.DS001832.V1.0.1},
publisher = {Openneuro},
title = {{Resting State - TMS}},
url = {https://openneuro.org/datasets/ds001832/versions/1.0.1},
year = {2019}
}
@online{GrandChallengeREFUGE,
title = {{Retinal Fundus Glaucoma Challenge}},
url = {https://refuge.grand-challenge.org/}
}
@article{Niemeijer2010,
abstract = {The detection of microaneurysms in digital color fundus photographs is a critical first step in automated screening for diabetic retinopathy (DR), a common complication of diabetes. To accomplish this detection numerous methods have been published in the past but none of these was compared with each other on the same data. In this work we present the results of the first international microaneurysm detection competition, organized in the context of the Retinopathy Online Challenge (ROC), a multiyear online competition for various aspects of DR detection. For this competition, we compare the results of five different methods, produced by five different teams of researchers on the same set of data. The evaluation was performed in a uniform manner using an algorithm presented in this work. The set of data used for the competition consisted of 50 training images with available reference standard and 50 test images where the reference standard was witheld by the organizers (M. Niemeijer, B. van Ginneken, and M. D. Abrmoff). The results obtained on the test data was submitted through a website after which standardized evaluation software was used to determine the performance of each of the methods. A human expert detected microaneurysms in the test set to allow comparison with the performance of the automatic methods. The overall results show that microaneurysm detection is a challenging task for both the automatic methods as well as the human expert. There is room for improvement as the best performing system does not reach the performance of the human expert. The data associated with the ROC microaneurysm detection competition will remain publicly available and the website will continue accepting submissions. {\textcopyright} 2006 IEEE.},
author = {Niemeijer, Meindert and {Van Ginneken}, Bram and Cree, Michael J. and Mizutani, Atsushi and Quellec, Gw{\'{e}}nol{\'{e}} and Sanchez, Clara I. and Zhang, Bob and Hornero, Roberto and Lamard, Mathieu and Muramatsu, Chisako and Wu, Xiangqian and Cazuguel, Guy and You, Jane and Mayo, Agust{\'{i}}n and Li, Qin and Hatanaka, Yuji and Cochener, B{\'{e}}atrice and Roux, Christian and Karray, Fakhri and Garcia, Mar{\'{i}}a and Fujita, Hiroshi and Abramoff, Michael D.},
doi = {10.1109/TMI.2009.2033909},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Computer aided detection,Computer aided diagnosis,Diabetic retinopathy,Fundus photographs,Retina,Retinopathy Online Challenge (ROC) competition},
number = {1},
pages = {185--195},
pmid = {19822469},
title = {{Retinopathy online challenge: Automatic detection of microaneurysms in digital color fundus photographs}},
volume = {29},
year = {2010}
}
@article{Bogunovic2019,
abstract = {Retinal swelling due to the accumulation of fluid is associated with the most vision-threatening retinal diseases. Optical coherence tomography (OCT) is the current standard of care in assessing the presence and quantity of retinal fluid and image-guided treatment management. Deep learning methods have made their impact across medical imaging, and many retinal OCT analysis methods have been proposed. However, it is currently not clear how successful they are in interpreting the retinal fluid on OCT, which is due to the lack of standardized benchmarks. To address this, we organized a challenge RETOUCH in conjunction with MICCAI 2017, with eight teams participating. The challenge consisted of two tasks: fluid detection and fluid segmentation. It featured for the first time: all three retinal fluid types, with annotated images provided by two clinical centers, which were acquired with the three most common OCT device vendors from patients with two different retinal diseases. The analysis revealed that in the detection task, the performance on the automated fluid detection was within the inter-grader variability. However, in the segmentation task, fusing the automated methods produced segmentations that were superior to all individual methods, indicating the need for further improvements in the segmentation performance.},
author = {Bogunovic, Hrvoje and Venhuizen, Freerk and Klimscha, Sophie and Apostolopoulos, Stefanos and Bab-Hadiashar, Alireza and Bagci, Ulas and Beg, Mirza Faisal and Bekalo, Loza and Chen, Qiang and Ciller, Carlos and Gopinath, Karthik and Gostar, Amirali K. and Jeon, Kiwan and Ji, Zexuan and Kang, Sung Ho and Koozekanani, Dara D. and Lu, Donghuan and Morley, Dustin and Parhi, Keshab K. and Park, Hyoung Suk and Rashno, Abdolreza and Sarunic, Marinko and Shaikh, Saad and Sivaswamy, Jayanthi and Tennakoon, Ruwan and Yadav, Shivin and {De Zanet}, Sandro and Waldstein, Sebastian M. and Gerendas, Bianca S. and Klaver, Caroline and Sanchez, Clara I. and Schmidt-Erfurth, Ursula},
doi = {10.1109/TMI.2019.2901398},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Evaluation,image classification,image segmentation,optical coherence tomography,retina},
month = {aug},
number = {8},
pages = {1858--1874},
pmid = {30835214},
title = {{RETOUCH: The Retinal OCT Fluid Detection and Segmentation Benchmark and Challenge}},
url = {https://ieeexplore.ieee.org/document/8653407/},
volume = {38},
year = {2019}
}
@inproceedings{Chatfield2014,
abstract = {The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.},
annote = {{\_}eprint: 1405.3531},
archivePrefix = {arXiv},
arxivId = {1405.3531},
author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
booktitle = {BMVC 2014 - Proceedings of the British Machine Vision Conference 2014},
doi = {10.5244/c.28.6},
eprint = {1405.3531},
title = {{Return of the devil in the details: Delving deep into convolutional nets}},
year = {2014}
}
@misc{Minati2009,
abstract = {This comprehensive, pedagogically-oriented review is aimed at a heterogeneous audience representative of the allied disciplines involved in research and patient care. After a foreword on epidemiology, genetics, and risk factors, the amyloid cascade model is introduced and the main neuropathological hallmarks are discussed. The progression of memory, language, visual processing, executive, attentional, and praxis deficits, and of behavioral symptoms is presented. After a summary on neuropsychological assessment, emerging biomarkers from cerebrospinal fluid assays, magnetic resonance imaging, nuclear medicine, and electrophysiology are discussed. Existing treatments are briefly reviewed, followed by an introduction to emerging disease-modifying therapies such as secretase modulators, inhibitors of Abeta aggregation, immunotherapy, inhibitors of tau protein phosphorylation, and delivery of nerve growth factor. {\textcopyright} 2009 Sage Publications.},
author = {Minati, Ludovico and Edginton, Trudi and {Grazia Bruzzone}, Maria and Giaccone, Giorgio},
booktitle = {American Journal of Alzheimer's Disease and other Dementias},
doi = {10.1177/1533317508328602},
issn = {15333175},
keywords = {Alzheimer's disease,Neuroimaging,Neuropathology,Neuropsychological testing,Pharmacotherapy},
number = {2},
pages = {95--121},
pmid = {19116299},
title = {{Reviews: Current concepts in alzheimer's disease: A multidisciplinary review}},
volume = {24},
year = {2009}
}
@incollection{Lin2020,
address = {Cham},
author = {Lin, Tong and Zha, Hongbin},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_801-1},
pages = {1--6},
publisher = {Springer International Publishing},
title = {{Riemannian Manifold}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}801-1},
year = {2020}
}
@inproceedings{lung_cls_1,
abstract = {Risk stratification of lung nodules is a task of primary importance in lung cancer diagnosis. Any improvement in robust and accurate nodule characterization can assist in identifying cancer stage, prognosis, and improving treatment planning. In this study, we propose a 3D Convolutional Neural Network (CNN) based nodule characterization strategy. With a completely 3D approach, we utilize the volumetric information from a CT scan which would be otherwise lost in the conventional 2D CNN based approaches. In order to address the need for a large amount of training data for CNN, we resort to transfer learning to obtain highly discriminative features.Moreover, we also acquire the task dependent feature representation for six high-level nodule attributes and fuse this complementary information via a Multi-task learning (MTL) framework. Finally, we propose to incorporate potential disagreement among radiologists while scoring different nodule attributes in a graph regularized sparsemulti-task learning. We evaluated our proposed approach on one of the largest publicly available lung nodule datasets comprising 1018 scans and obtained state-of-the-art results in regressing the malignancy scores.},
archivePrefix = {arXiv},
arxivId = {1704.08797},
author = {Hussein, Sarfaraz and Cao, Kunlin and Song, Qi and Bagci, Ulas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-59050-9_20},
eprint = {1704.08797},
isbn = {9783319590493},
issn = {16113349},
keywords = {3D convolutional neural network,Computed tomography (CT),Computer-aided diagnosis (CAD),Deep learning,Lung nodule characterization,Multi-task learning,Transfer learning},
month = {apr},
pages = {249--260},
title = {{Risk stratification of lung nodules using 3D CNN-based multi-task learning}},
url = {http://arxiv.org/abs/1704.08797 http://dx.doi.org/10.1007/978-3-319-59050-9{\_}20 http://link.springer.com/10.1007/978-3-319-59050-9{\_}20},
volume = {10265 LNCS},
year = {2017}
}
@incollection{Meer2020,
address = {Cham},
author = {Meer, Peter},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_882-1},
pages = {1--8},
publisher = {Springer International Publishing},
title = {{Robust Estimation Techniques}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}882-1},
year = {2020}
}
@online{GrandChallengeROCC,
author = {Rabbani, Hossein and Rasti, Reza and Kafieh, Rahele},
title = {{ROCC - Retinal OCT Classification Challenge (ROCC)}},
url = {https://rocc.grand-challenge.org/},
year = {2017}
}
@article{Veeling2018,
abstract = {We propose a new model for digital pathology segmentation, based on the observation that histopathology images are inherently symmetric under rotation and reflection. Utilizing recent findings on rotation equivariant CNNs, the proposed model leverages these symmetries in a principled manner. We present a visual analysis showing improved stability on predictions, and demonstrate that exploiting rotation equivariance significantly improves tumor detection performance on a challenging lymph node metastases dataset. We further present a novel derived dataset to enable principled comparison of machine learning models, in combination with an initial benchmark. Through this dataset, the task of histopathology diagnosis becomes accessible as a challenging benchmark for fundamental machine learning research.},
archivePrefix = {arXiv},
arxivId = {1806.03962},
author = {Veeling, Bastiaan S. and Linmans, Jasper and Winkens, Jim and Cohen, Taco and Welling, Max},
doi = {10.1007/978-3-030-00934-2_24},
eprint = {1806.03962},
isbn = {9783030009335},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
month = {jun},
pages = {210--218},
title = {{Rotation equivariant CNNs for digital pathology}},
url = {http://arxiv.org/abs/1806.03962},
volume = {11071 LNCS},
year = {2018}
}
@inproceedings{Spanier2014,
abstract = {We describe a new method for the automatic segmentation of multiple organs of the ventral cavity in CT scans. The method is based on a set of rules that determine the order in which the organs are isolated and segmented. First, the air-containing organs are segmented: the trachea and the lungs. Then, the organs with high blood content: the spleen, the kidneys and the liver, are segmented. Each organ is individually segmented with a generic four-step pipeline procedure. Our method is unique in that it uses the same generic segmentation approach for all organs and in that it relies on the segmentation difficulty of organs to guide the segmentation process. Experimental results on 20 CT scans of the VISCERAL Anatomy2 Challenge training datasets yield an average Dice volume overlap similarity score of 90.95. For the 10 CT scans test datasets, the average Dice scores is 88.5.},
author = {Spanier, Assaf B. and Joskowicz, Leo},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-13972-2_15},
isbn = {9783319139715},
issn = {16113349},
pages = {163--170},
title = {{Rule-based ventral cavity multi-organ automatic segmentation in CT scans}},
volume = {8848},
year = {2014}
}
@article{Munos2016,
abstract = {In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace($\lambda$), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of "off-policyness"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to Q∗ without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q($\lambda$), which was an open problem since 1989. We illustrate the benefits of Retrace($\lambda$) on a standard suite of Atari 2600 games.},
archivePrefix = {arXiv},
arxivId = {1606.02647},
author = {Munos, R{\'{e}}mi and Stepleton, Thomas and Harutyunyan, Anna and Bellemare, Marc G.},
eprint = {1606.02647},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
month = {jun},
pages = {1054--1062},
title = {{Safe and efficient off-policy reinforcement learning}},
url = {http://arxiv.org/abs/1606.02647},
year = {2016}
}
@article{Kasthuri2015,
abstract = {We describe automated technologies to probe the structure of neural tissue at nanometer resolution and use them to generate a saturated reconstruction of a sub-volume of mouse neocortex in which all cellular objects (axons, dendrites, and glia) and many sub-cellular components (synapses, synaptic vesicles, spines, spine apparati, postsynaptic densities, and mitochondria) are rendered and itemized in a database. We explore these data to study physical properties of brain tissue. For example, by tracing the trajectories of all excitatory axons and noting their juxtapositions, both synaptic and non-synaptic, with every dendritic spine we refute the idea that physical proximity is sufficient to predict synaptic connectivity (the so-called Peters' rule). This online minable database provides general access to the intrinsic complexity of the neocortex and enables further data-driven inquiries. Video Abstract},
author = {Kasthuri, Narayanan and Hayworth, Kenneth Jeffrey and Berger, Daniel Raimund and Schalek, Richard Lee and Conchello, Jos{\'{e}} Angel and Knowles-Barley, Seymour and Lee, Dongil and V{\'{a}}zquez-Reina, Amelio and Kaynig, Verena and Jones, Thouis Raymond and Roberts, Mike and Morgan, Josh Lyskowski and Tapia, Juan Carlos and Seung, H. Sebastian and Roncal, William Gray and Vogelstein, Joshua Tzvi and Burns, Randal and Sussman, Daniel Lewis and Priebe, Carey Eldin and Pfister, Hanspeter and Lichtman, Jeff William},
doi = {10.1016/j.cell.2015.06.054},
issn = {10974172},
journal = {Cell},
month = {jul},
number = {3},
pages = {648--661},
pmid = {26232230},
title = {{Saturated Reconstruction of a Volume of Neocortex}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0092867415008247},
volume = {162},
year = {2015}
}
@incollection{Zhou2020,
abstract = {{\textcopyright} Springer International Publishing Switzerland 2015. There are a large number of small and inexpensive single-board computers with Linux operating systems available on the market today. Most of these aim for the consumer and enthusiast market, but can also be used in research and commercial applications. This paper builds on several years of experience with using such computers in student projects, as well as the development of cyber-physical and embedded control systems. A summary of the properties that are key for dependability for selected boards is given in tabulated form. These boards have interesting properties for many embedded and cyber-physical systems, e.g. high-performance, small size and low cost. The use of Linux for operating system means a development environment that is familiar to many developers, and the availability of many libraries and applications. While not suitable for applications were formally proven dependability is necessary, we argue that by actively mitigating some of the potential problems identified in this paper such computers can be used in many applications where high dependability is desirable, especially in combination with low-cost. A solution with redundant single-board computers is presented as a strategy for achieving high dependability. Due to the low cost and small size, this is feasible for applications were redundancy traditionally would be prohibitively too large or costly.},
address = {Cham},
author = {Zhou, Bolei},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_799-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Scene Classification}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}799-1},
year = {2020}
}
@article{Gehlot2020,
abstract = {Acute lymphoblastic leukemia (ALL) is a pervasive pediatric white blood cell cancer across the globe. With the popularity of convolutional neural networks (CNNs), computer-aided diagnosis of cancer has attracted considerable attention. Such tools are easily deployable and are cost-effective. Hence, these can enable extensive coverage of cancer diagnostic facilities. However, the development of such a tool for ALL cancer was challenging so far due to the non-availability of a large training dataset. The visual similarity between the malignant and normal cells adds to the complexity of the problem. This paper discusses the recent release of a large dataset and presents a novel deep learning architecture for the classification of cell images of ALL cancer. The proposed architecture, namely, SDCT-AuxNet$\theta$ is a 2-module framework that utilizes a compact CNN as the main classifier in one module and a Kernel SVM as the auxiliary classifier in the other one. While CNN classifier uses features through bilinear-pooling, spectral-averaged features are used by the auxiliary classifier. Further, this CNN is trained on the stain deconvolved quantity images in the optical density domain instead of the conventional RGB images. A novel test strategy is proposed that exploits both the classifiers for decision making using the confidence scores of their predicted class labels. Elaborate experiments have been carried out on our recently released public dataset of 15114 images of ALL cancer and healthy cells to establish the validity of the proposed methodology that is also robust to subject-level variability. A weighted F1 score of 94.8{\%} is obtained that is best so far on this challenging dataset.},
author = {Gehlot, Shiv and Gupta, Anubha and Gupta, Ritu},
doi = {10.1016/j.media.2020.101661},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {ALL diagnosis,Acute lymphoblastic leukemia,Cell classification,Convolutional neural network,Deep learning},
month = {apr},
pages = {101661},
pmid = {32066066},
title = {{SDCT-AuxNet$\theta$: DCT augmented stain deconvolutional CNN with auxiliary classifier for cancer diagnosis}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S136184152030027X},
volume = {61},
year = {2020}
}
@article{Duggal2017,
abstract = {Convolutional Neural Networks (CNNs) are typically trained in the RGB color space. However, in medical imaging, we believe that pixel stain quantities offer a fundamental view of the interaction between tissues and stain chemicals. Since the optical density (OD) colorspace allows to compute pixel stain quantities from pixel RGB intensities using the Beer-Lambert's law, we propose a stain deconvolutional layer, hereby named as SD-Layer, affixed at the front of CNN that performs two functions: (1) it transforms the input RGB microscopic images to Optical Density (OD) space and (2) this layer deconvolves OD image with the stain basis learned through backpropagation and provides tissue-specific stain absorption quantities as input to the following CNN layers. With the introduction of only nine additional learnable parameters in the proposed SD-Layer, we obtain a considerably improved performance on two standard CNN architectures: AlexNet and T-CNN. Using the T-CNN architecture prefixed with the proposed SD-Layer, we obtain 5-fold cross-validation accuracy of 93.2{\%} in the problem of differentiating malignant immature White Blood Cells (WBCs) from normal immature WBCs for cancer detection.},
author = {Duggal, Rahul and Gupta, Anubha and Gupta, Ritu and Mallick, Pramit},
doi = {10.1007/978-3-319-66179-7_50},
isbn = {9783319661780},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Cancer imaging,Classification,Deep learning,Stain deconvolution},
pages = {435--443},
title = {{SD-Layer: Stain deconvolutional layer for CNNs in medical microscopic imaging}},
volume = {10435 LNCS},
year = {2017}
}
@inproceedings{Tchapmi2017,
abstract = {3D semantic scene labeling is fundamental to agents operating in the real world. In particular, labeling raw 3D point sets from sensors provides fine-grained semantics. Recent works leverage the capabilities of Neural Networks(NNs), but are limited to coarse voxel predictions and do not explicitly enforce global consistency. We present SEGCloud, an end-To-end framework to obtain 3D point-level segmentation that combines the advantages of NNs, trilinear interpolation(TI) and fully connected Conditional Random Fields (FC-CRF). Coarse voxel predictions from a 3D Fully Convolutional NN are transferred back to the raw 3D points via trilinear interpolation. Then the FC-CRF enforces global consistency and provides fine-grained semantics on the points. We implement the latter as a differentiable Recurrent NN to allow joint optimization. We evaluate the framework on two indoor and two outdoor 3D datasets (NYU V2, S3DIS, KITTI, Semantic3D.net), and show performance comparable or superior to the state-of-The-Art on all datasets.},
annote = {{\_}eprint: 1710.07563},
archivePrefix = {arXiv},
arxivId = {1710.07563},
author = {Tchapmi, Lyne and Choy, Christopher and Armeni, Iro and Gwak, Junyoung and Savarese, Silvio},
booktitle = {Proceedings - 2017 International Conference on 3D Vision, 3DV 2017},
doi = {10.1109/3DV.2017.00067},
eprint = {1710.07563},
isbn = {9781538626108},
issn = {2475-7888},
keywords = {3D-Conditional-Random-Fields,3D-Convolutional-Neural-Networks,3D-Point-Clouds,3D-Semantic-Segmentation,RGB-D},
pages = {537--547},
title = {{SEGCloud: Semantic segmentation of 3D point clouds}},
url = {http://arxiv.org/abs/1710.07563},
volume = {abs/1710.0},
year = {2018}
}
@article{Naylor2019,
abstract = {The advent of digital pathology provides us with the challenging opportunity to automatically analyze whole slides of diseased tissue in order to derive quantitative profiles that can be used for diagnosis and prognosis tasks. In particular, for the development of interpretable models, the detection and segmentation of cell nuclei is of the utmost importance. In this paper, we describe a new method to automatically segment nuclei from Haematoxylin and Eosin (HE) stained histopathology data with fully convolutional networks. In particular, we address the problem of segmenting touching nuclei by formulating the segmentation problem as a regression task of the distance map. We demonstrate superior performance of this approach as compared to other approaches using Convolutional Neural Networks.},
author = {Naylor, Peter and La{\'{e}}, Marick and Reyal, Fabien and Walter, Thomas},
doi = {10.1109/TMI.2018.2865709},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Cancer research,deep learning,digital pathology,histopathology,nuclei segmentation},
number = {2},
pages = {448--459},
pmid = {30716022},
title = {{Segmentation of Nuclei in Histopathology Images by Deep Regression of the Distance Map}},
volume = {38},
year = {2019}
}
@inproceedings{Trullo2017,
abstract = {Cancer is one of the leading causes of death worldwide. Radiotherapy is a standard treatment for this condition and the first step of the radiotherapy process is to identify the target volumes to be targeted and the healthy organs at risk (OAR) to be protected. Unlike previous methods for automatic segmentation of OAR that typically use local information and individually segment each OAR, in this paper, we propose a deep learning framework for the joint segmentation of OAR in CT images of the thorax, specifically the heart, esophagus, trachea and the aorta. Making use of Fully Convolutional Networks (FCN), we present several extensions that improve the performance, including a new architecture that allows to use low level features with high level information, effectively combining local and global information for improving the localization accuracy. Finally, by using Conditional Random Fields (specifically the CRF as Recurrent Neural Network model), we are able to account for relationships between the organs to further improve the segmentation results. Experiments demonstrate competitive performance on a dataset of 30 CT scans.},
author = {Trullo, R. and Petitjean, C. and Ruan, S. and Dubray, B. and Nie, D. and Shen, D.},
booktitle = {Proceedings - International Symposium on Biomedical Imaging},
doi = {10.1109/ISBI.2017.7950685},
isbn = {9781509011711},
issn = {19458452},
keywords = {CRF,CRFasRNN,CT Segmentation,Fully Convolutional Networks (FCN)},
pages = {1003--1006},
title = {{Segmentation of Organs at Risk in thoracic CT images using a SharpMask architecture and Conditional Random Fields}},
volume = {2017},
year = {2017}
}
@incollection{Shotton2020,
address = {Cham},
author = {Shotton, Jamie and Kohli, Pushmeet},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_251-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Semantic Image Segmentation: Traditional Approach}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}251-1},
year = {2020}
}
@article{Luc2016,
abstract = {Adversarial training has been shown to produce state of the art results for generative image modeling. In this paper we propose an adversarial training approach to train semantic segmentation models. We train a convolutional semantic segmentation network along with an adversarial network that discriminates segmentation maps coming either from the ground truth or from the segmentation network. The motivation for our approach is that it can detect and correct higher-order inconsistencies between ground truth segmentation maps and the ones produced by the segmentation net. Our experiments show that our adversarial training approach leads to improved accuracy on the Stanford Background and PASCAL VOC 2012 datasets.},
archivePrefix = {arXiv},
arxivId = {1611.08408},
author = {Luc, Pauline and Couprie, Camille and Chintala, Soumith and Verbeek, Jakob},
eprint = {1611.08408},
month = {nov},
title = {{Semantic Segmentation using Adversarial Networks}},
url = {http://arxiv.org/abs/1611.08408},
year = {2016}
}
@inproceedings{Hackel2017,
abstract = {This paper presents a new 3D point cloud classification benchmark data set with over four billion manually labelled points, meant as input for data-hungry (deep) learning methods. We also discuss first submissions to the benchmark that use deep convolutional neural networks (CNNs) as a work horse, which already show remarkable performance improvements over state-of-the-art. CNNs have become the de-facto standard for many tasks in computer vision and machine learning like semantic segmentation or object detection in images, but have no yet led to a true breakthrough for 3D point cloud labelling tasks due to lack of training data. With the massive data set presented in this paper, we aim at closing this data gap to help unleash the full potential of deep learning methods for 3D labelling tasks. Our semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains 8 semantic classes and covers a wide range of urban outdoor scenes: churches, streets, railroad tracks, squares, villages, soccer fields and castles. We describe our labelling interface and show that our data set provides more dense and complete point clouds with much higher overall number of labelled points compared to those already available to the research community. We further provide baseline method descriptions and comparison between methods submitted to our online system. We hope semantic3D.net will pave the way for deep learning methods in 3D point cloud labelling to learn richer, more general 3D representations, and first submissions after only a few months indicate that this might indeed be the case.},
archivePrefix = {arXiv},
arxivId = {1704.03847},
author = {Hackel, T. and Savinov, N. and Ladicky, L. and Wegner, J. D. and Schindler, K. and Pollefeys, M.},
booktitle = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
doi = {10.5194/isprs-annals-IV-1-W1-91-2017},
eprint = {1704.03847},
issn = {21949050},
number = {1W1},
pages = {91--98},
title = {{Semantic3D.Net: a New Large-Scale Point Cloud Classification Benchmark}},
volume = {4},
year = {2017}
}
@inproceedings{Kipf2016,
abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
annote = {{\_}eprint: 1609.02907},
archivePrefix = {arXiv},
arxivId = {1609.02907},
author = {Kipf, Thomas N. and Welling, Max},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1609.02907},
title = {{Semi-supervised classification with graph convolutional networks}},
url = {http://arxiv.org/abs/1609.02907},
volume = {abs/1609.0},
year = {2017}
}
@article{Wang2020,
abstract = {The availability of a large amount of annotated data is critical for many medical image analysis applications, in particular for those relying on deep learning methods which are known to be data-hungry. However, annotated medical data, especially multimodal data, is often scarce and costly to obtain. In this paper, we address the problem of synthesizing multi-parameter magnetic resonance imaging data (i.e. mp-MRI), which typically consists of Apparent Diffusion Coefficient (ADC) and T2-weighted (T2w) images, containing clinically significant (CS) prostate cancer (PCa) via semi-supervised learning and adversarial learning. Specifically, our synthesizer generates mp-MRI data in a sequential manner: first utilizing a decoder to generate an ADC map from a 128-d latent vector, followed by translating the ADC to the T2w image via U-Net. The synthesizer is trained in a semi-supervised manner. In the supervised training process, a limited amount of paired ADC-T2w images and the corresponding ADC encodings are provided and the synthesizer learns the paired relationship by explicitly minimizing the reconstruction losses between synthetic and real images. To avoid overfitting limited ADC encodings, an unlimited amount of random latent vectors and unpaired ADC-T2w Images are utilized in the unsupervised training process for learning the marginal image distributions of real images. To improve the robustness for training the synthesizer, we decompose the difficult task of generating full-size images into several simpler tasks which generate sub-images only. A StitchLayer is then employed to seamlessly fuse sub-images together in an interlaced manner into a full-size image. In addition, to enforce the synthetic images to indeed contain distinguishable CS PCa lesions, we propose to also maximize an auxiliary distance of Jensen-Shannon divergence (JSD) between CS and nonCS images. Experimental results show that our method can effectively synthesize a large variety of mp-MRI images which contain meaningful CS PCa lesions, display a good visual quality and have the correct paired relationship between the two modalities of a pair. Compared to the state-of-the-art methods based on adversarial learning (Liu and Tuzel, 2016; Costa et al., 2017), our method achieves a significant improvement in terms of both visual quality and several popular quantitative evaluation metrics.},
author = {Wang, Zhiwei and Lin, Yi and Cheng, Kwang Ting (Tim) and Yang, Xin},
doi = {10.1016/j.media.2019.101565},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Deep learning,GAN,Generative models,Multimodal image synthesis},
pages = {101565},
pmid = {31630010},
title = {{Semi-supervised mp-MRI data synthesis with StitchLayer and auxiliary distance maximization}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841519301057},
volume = {59},
year = {2020}
}
@inproceedings{Hutter2011,
abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach. {\textcopyright} Springer-Verlag Berlin Heidelberg 2011.},
author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-25566-3_40},
isbn = {9783642255656},
issn = {03029743},
pages = {507--523},
title = {{Sequential model-based optimization for general algorithm configuration}},
volume = {6683 LNCS},
year = {2011}
}
@inproceedings{Zhou2018,
abstract = {Cell nuclei detection and fine-grained classification have been fundamental yet challenging problems in histopathology image analysis. Due to the nuclei tiny size, significant inter/intra-class variances, as well as the inferior image quality, previous automated methods would easily suffer from limited accuracy and robustness. In the meanwhile, existing approaches usually deal with these two tasks independently, which would neglect the close relatedness of them. In this paper, we present a novel method of sibling fully convolutional network with prior objectness interaction (called SFCN-OPI) to tackle the two tasks simultaneously and interactively using a unified end-to-end framework. Specifically, the sibling FCN branches share features in earlier layers while holding respective higher layers for specific tasks. More importantly, the detection branch outputs the objectness prior which dynamically interacts with the fine-grained classification sibling branch during the training and testing processes. With this mechanism, the fine-grained classification successfully focuses on regions with high confidence of nuclei existence and outputs the conditional probability, which in turn benefits the detection through back propagation. Extensive experiments on colon cancer histology images have validated the effectiveness of our proposed SFCN-OPI and our method has outperformed the state-of-the-art methods by a large margin.},
author = {Zhou, Yanning and Dou, Qi and Chen, Hao and Qin, Jing and Heng, Pheng Ann},
booktitle = {arXiv},
title = {{SFCN-OPI: Detection and fine-grained classification of nuclei using sibling FCN with objectness prior interaction}},
year = {2017}
}
@article{sgdr,
abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14{\%} and 16.21{\%}, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
archivePrefix = {arXiv},
arxivId = {1608.03983},
author = {Loshchilov, Ilya and Hutter, Frank},
eprint = {1608.03983},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
month = {aug},
title = {{SGDR: Stochastic Gradient Descent with Warm Restarts}},
url = {http://arxiv.org/abs/1608.03983},
volume = {abs/1608.0},
year = {2016}
}
@incollection{Matsushita2020,
address = {Cham},
author = {Matsushita, Yasuyuki},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_829-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Shape from Shading}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}829-1},
year = {2020}
}
@article{Chang2015,
abstract = {We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.},
annote = {{\_}eprint: 1512.03012},
archivePrefix = {arXiv},
arxivId = {1512.03012},
author = {Chang, Angel X. and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},
eprint = {1512.03012},
journal = {CoRR},
title = {{ShapeNet: An Information-Rich 3D Model Repository}},
url = {http://arxiv.org/abs/1512.03012},
volume = {abs/1512.0},
year = {2015}
}
@incollection{Kang2020,
address = {Cham},
author = {Kang, Sing Bing},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_795-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Shiftable Windows for Stereo}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}795-1},
year = {2020}
}
@misc{Zheng2018,
abstract = {In the early days, content-based image retrieval (CBIR) was studied with global features. Since 2003, image retrieval based on local descriptors (de facto SIFT) has been extensively studied for over a decade due to the advantage of SIFT in dealing with image transformations. Recently, image representations based on the convolutional neural network (CNN) have attracted increasing interest in the community and demonstrated impressive performance. Given this time of rapid evolution, this article provides a comprehensive survey of instance retrieval over the last decade. Two broad categories, SIFT-based and CNN-based methods, are presented. For the former, according to the codebook size, we organize the literature into using large/medium-sized/small codebooks. For the latter, we discuss three lines of methods, i.e., using pre-trained or fine-tuned CNN models, and hybrid methods. The first two perform a single-pass of an image to the network, while the last category employs a patch-based feature extraction scheme. This survey presents milestones in modern instance retrieval, reviews a broad selection of previous works in different categories, and provides insights on the connection between SIFT and CNN-based methods. After analyzing and comparing retrieval performance of different categories on several datasets, we discuss promising directions towards generic and specialized instance retrieval.},
archivePrefix = {arXiv},
arxivId = {1608.01807},
author = {Zheng, Liang and Yang, Yi and Tian, Qi},
booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
doi = {10.1109/TPAMI.2017.2709749},
eprint = {1608.01807},
issn = {01628828},
keywords = {Instance retrieval,SIFT,convolutional neural network,literature survey},
month = {may},
number = {5},
pages = {1224--1244},
publisher = {IEEE Computer Society},
title = {{SIFT Meets CNN: A Decade Survey of Instance Retrieval}},
volume = {40},
year = {2018}
}
@article{Li2019a,
abstract = {Signet ring cell carcinoma is a type of rare adenocarcinoma with poor prognosis. Early detection leads to huge improvement of patients' survival rate. However, pathologists can only visually detect signet ring cells under the microscope. This procedure is not only laborious but also prone to omission. An automatic and accurate signet ring cell detection solution is thus important but has not been investigated before. In this paper, we take the first step to present a semi-supervised learning framework for the signet ring cell detection problem. Self-training is proposed to deal with the challenge of incomplete annotations, and cooperative-training is adapted to explore the unlabeled regions. Combining the two techniques, our semi-supervised learning framework can make better use of both labeled and unlabeled data. Experiments on large real clinical data demonstrate the effectiveness of our design. Our framework achieves accurate signet ring cell detection and can be readily applied in the clinical trails. The dataset will be released soon to facilitate the development of the area.},
archivePrefix = {arXiv},
arxivId = {1907.03954},
author = {Li, Jiahui and Yang, Shuang and Huang, Xiaodi and Da, Qian and Yang, Xiaoqun and Hu, Zhiqiang and Duan, Qi and Wang, Chaofu and Li, Hongsheng},
doi = {10.1007/978-3-030-20351-1_66},
eprint = {1907.03954},
isbn = {9783030203504},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Semi-supervised learning,Signet ring cell benchmark,Signet ring cell detection},
month = {jul},
pages = {842--854},
title = {{Signet Ring Cell Detection with a Semi-supervised Learning Framework}},
url = {http://arxiv.org/abs/1907.03954},
volume = {11492 LNCS},
year = {2019}
}
@misc{SIIM-ISIC-2020,
author = {{International Skin Imaging Collaboration}},
doi = {10.34970/2020-DS01},
publisher = {International Skin Imaging Collaboration},
title = {{SIIM-ISIC 2020 Challenge Dataset}},
url = {https://challenge2020.isic-archive.com/},
year = {2020}
}
@article{Willia1992,
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms. {\textcopyright} 1992, Kluwer Academic Publishers. All rights reserved.},
author = {Willia, Ronald J.},
doi = {10.1023/A:1022672621406},
issn = {15730565},
journal = {Machine Learning},
keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis},
number = {3},
pages = {229--256},
title = {{Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning}},
volume = {8},
year = {1992}
}
@article{Ferdouse2011,
abstract = {Noise problems in the environment have gained attention due to the tremendous growth of technology that has led to noisy engines, heavy machinery, high speed wind buffeting and other noise sources. The problem of controlling the noise level has become the focus of a tremendous amount of research over the years. In last few years various adaptive algorithms are developed for noise cancellation. In this paper we present an implementation of LMS (Least Mean Square), NLMS (Normalized Least Mean Square) and RLS (Recursive Least Square) algorithms on MATLAB platform with the intention to compare their performance in noise cancellation. We simulate the adaptive filter in MATLAB with a noisy tone signal and white noise signal and analyze the performance of algorithms in terms of MSE (Mean Squared Error), percentage noise removal, computational complexity and stability. The obtained results shows that RLS has the best performance but at the cost of large computational complexity and memory requirement.},
annote = {{\_}eprint: 1104.1962},
author = {Thenua, Raj and Agarwal, SK},
journal = {International Journal of Engineering Science and Technology},
keywords = {Adaptive Filter,Computer Science - Other Computer Science,Convergence,DOAJ:Computer Science,DOAJ:Technology and Engineering,Electronic computers. Computer science,FTRLS,GAL,IJCSI,Instruments and machines,Mathematics,Mean Square Error,Noise,Q,QA1-939,QA71-90,QA75.5-76.95,RLS,Science},
number = {9},
pages = {4373--4378},
title = {{Simulation and performance analysis of adaptive filter in noise cancellation}},
volume = {2},
year = {2010}
}
@article{Prastawa2009,
abstract = {Obtaining validation data and comparison metrics for segmentation of magnetic resonance images (MRI) are difficult tasks due to the lack of reliable ground truth. This problem is even more evident for images presenting pathology, which can both alter tissue appearance through infiltration and cause geometric distortions. Systems for generating synthetic images with user-defined degradation by noise and intensity inhomogeneity offer the possibility for testing and comparison of segmentation methods. Such systems do not yet offer simulation of sufficiently realistic looking pathology. This paper presents a system that combines physical and statistical modeling to generate synthetic multi-modal 3D brain MRI with tumor and edema, along with the underlying anatomical ground truth, Main emphasis is placed on simulation of the major effects known for tumor MRI, such as contrast enhancement, local distortion of healthy tissue, infiltrating edema adjacent to tumors, destruction and deformation of fiber tracts, and multi-modal MRI contrast of healthy tissue and pathology. The new method synthesizes pathology in multi-modal MRI and diffusion tensor imaging (DTI) by simulating mass effect, warping and destruction of white matter fibers, and infiltration of brain tissues by tumor cells. We generate synthetic contrast enhanced MR images by simulating the accumulation of contrast agent within the brain. The appearance of the the brain tissue and tumor in MRI is simulated by synthesizing texture images from real MR images. The proposed method is able to generate synthetic ground truth and synthesized MR images with tumor and edema that exhibit comparable segmentation challenges to real tumor MRI. Such image data sets will find use in segmentation reliability studies, comparison and validation of different segmentation methods, training and teaching, or even in evaluating standards for tumor size like the RECIST criteria (response evaluation criteria in solid tumors). {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Prastawa, Marcel and Bullitt, Elizabeth and Gerig, Guido},
doi = {10.1016/j.media.2008.11.002},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Brain MRI,Diffusion tensor imaging,Gold standard,Ground truth,Segmentation validation,Simulation of tumor infiltration,Tumor simulation},
month = {apr},
number = {2},
pages = {297--311},
pmid = {19119055},
title = {{Simulation of brain tumors in MR images for evaluation of segmentation efficacy}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841508001357},
volume = {13},
year = {2009}
}
@article{Wu2016,
abstract = {Understanding 3D object structure from a single image is an important but difficult task in computer vision, mostly due to the lack of 3D object annotations in real images. Previous work tackles this problem by either solving an optimization task given 2D keypoint positions, or training on synthetic data with ground truth 3D information. In this work, we propose 3D INterpreter Network (3D-INN), an endto-end framework which sequentially estimates 2D keypoint heatmaps and 3D object structure, trained on both real 2D-annotated images and synthetic 3D data. This is made possible mainly by two technic al innovations.First, we propose a Projection Layer, which projects estimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3D structural parameters supervised by 2D annotations on real images. Second, heatmaps of keypoints serve as an intermediate representation connecting real and synthetic data, enabling 3D-INN to benefit from the variation and abundance of synthetic 3D objects, without suffering from the difference between the statistics of real and synthesized images due to imperfect rendering. The network achieves state-of-the-art performance on both 2D keypoint estimation and 3D structure recovery. We also show that the recovered 3D information can be used in other vision applications, such as image retrieval.},
annote = {From Duplicate 2 (Single Image 3D Interpreter Network - Wu, Jiajun; Xue, Tianfan; Lim, Joseph J; Tian, Yuandong; Tenenbaum, Joshua B; Torralba, Antonio; Freeman, William T)

{\_}eprint: 1604.08685},
archivePrefix = {arXiv},
arxivId = {1604.08685},
author = {Wu, Jiajun and Xue, Tianfan and Lim, Joseph J. and Tian, Yuandong and Tenenbaum, Joshua B. and Torralba, Antonio and Freeman, William T.},
doi = {10.1007/978-3-319-46466-4_22},
eprint = {1604.08685},
isbn = {9783319464657},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {3D structure,Keypoint estimation,Neural network,Single image 3D reconstruction,Synthetic data},
pages = {365--382},
title = {{Single image 3D interpreter network}},
url = {http://arxiv.org/abs/1604.08685},
volume = {9910 LNCS},
year = {2016}
}
@misc{TCIA-Breast-MRI-NACT-Pilot,
author = {Newitt, David and Hylton, Nola},
doi = {10.7937/K9/TCIA.2016.QHsyhJKy},
pages = {The Cancer Imaging Archive},
publisher = {The Cancer Imaging Archive},
title = {{Single site breast DCE-MRI data and segmentations from patients undergoing neoadjuvant chemotherapy}},
url = {https://wiki.cancerimagingarchive.net/x/ZIhXAQ},
year = {2016}
}
@incollection{Zhang2016,
abstract = {Photoacoustic imaging is a non-ionizing imaging modality that provides contrast consistent with optical imaging techniques while the resolution and penetration depth is similar to ultrasound techniques. In a previous publication Opt. Express 18, 11406 (2010), a technique was introduced to experimentally acquire the imaging operator for a photoacoustic imaging system. While this was an important foundation for future work, we have recently improved the experimental procedure allowing for a more densely populated imaging operator to be acquired. Subsets of the imaging operator were produced by varying the transducer count as well as the measurement space temporal sampling rate. Examination of the matrix rank and the effect of contributing object space singular vectors to image reconstruction were performed. For a PAI system collecting only limited data projections, matrix rank increased linearly with transducer count and measurement space temporal sampling rate. Image reconstruction using a regularized pseudoinverse of the imaging operator was performed on photoacoustic signals from a point source, line source, and an array of point sources derived from the imaging operator. As expected, image quality increased for each object with increasing transducer count and measurement space temporal sampling rate. Using the same approach, but on experimentally sampled photoacoustic signals from a moving point-like source, acquisition, data transfer, reconstruction and image display took 1.4 s using one laser pulse per 3D frame. With relatively simple hardware improvements to data transfer and computation speed, our current imaging results imply that acquisition and display of 3D photoacoustic images at laser repetition rates of 10Hz is easily achieved.},
address = {Cham},
author = {Zhang, Yanchun and Xu, Guandong},
booktitle = {Encyclopedia of Database Systems},
doi = {10.1007/978-1-4899-7993-3_538-2},
pages = {1--3},
publisher = {Springer International Publishing},
title = {{Singular Value Decomposition}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}802-1},
year = {2016}
}
@article{Codella2019,
abstract = {This work summarizes the results of the largest skin image analysis challenge in the world, hosted by the International Skin Imaging Collaboration (ISIC), a global partnership that has organized the world's largest public repository of dermoscopic images of skin. The challenge was hosted in 2018 at the Medical Image Computing and Computer Assisted Intervention (MICCAI) conference in Granada, Spain. The dataset included over 12,500 images across 3 tasks. 900 users registered for data download, 115 submitted to the lesion segmentation task, 25 submitted to the lesion attribute detection task, and 159 submitted to the disease classification task. Novel evaluation protocols were established, including a new test for segmentation algorithm performance, and a test for algorithm ability to generalize. Results show that top segmentation algorithms still fail on over 10{\%} of images on average, and algorithms with equal performance on test data can have different abilities to generalize. This is an important consideration for agencies regulating the growing set of machine learning tools in the healthcare domain, and sets a new standard for future public challenges in healthcare.},
archivePrefix = {arXiv},
arxivId = {1902.03368},
author = {Codella, Noel and Rotemberg, Veronica and Tschandl, Philipp and {Emre Celebi}, M. and Dusza, Stephen and Gutman, David and Helba, Brian and Kalloo, Aadi and Liopyris, Konstantinos and Marchetti, Michael and Kittler, Harald and Halpern, Allan},
eprint = {1902.03368},
journal = {arXiv},
keywords = {Deep learning,Dermoscopy,Melanoma,Skin cancer},
month = {feb},
title = {{Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (ISIC)}},
url = {http://arxiv.org/abs/1902.03368},
year = {2019}
}
@article{Codella2017,
abstract = {This article describes the design, implementation, and results of the latest installment of the dermoscopic image analysis benchmark challenge. The goal is to support research and development of algorithms for automated diagnosis of melanoma, the most lethal skin cancer. The challenge was divided into 3 tasks: lesion segmentation, feature detection, and disease classification. Participation involved 593 registrations, 81 pre-submissions, 46 finalized submissions (including a 4-page manuscript), and approximately 50 attendees, making this the largest standardized and comparative study in this field to date. While the official challenge duration and ranking of participants has concluded, the dataset snapshots remain available for further research and development.},
archivePrefix = {arXiv},
arxivId = {1710.05006},
author = {Codella, Noel C.F. and Gutman, David and Celebi, M. Emre and Helba, Brian and Marchetti, Michael A. and Dusza, Stephen W. and Kalloo, Aadi and Liopyris, Konstantinos and Mishra, Nabin and Kittler, Harald and Halpern, Allan},
doi = {10.1109/ISBI.2018.8363547},
eprint = {1710.05006},
isbn = {9781538636367},
issn = {19458452},
journal = {Proceedings - International Symposium on Biomedical Imaging},
keywords = {Challenge,Dataset,Deep learning,Dermatology,Dermoscopy,Melanoma,Skin cancer},
month = {oct},
pages = {168--172},
title = {{Skin lesion analysis toward melanoma detection: A challenge at the 2017 International symposium on biomedical imaging (ISBI), hosted by the international skin imaging collaboration (ISIC)}},
url = {http://arxiv.org/abs/1710.05006},
volume = {2018-April},
year = {2018}
}
@inproceedings{Brock2017,
abstract = {Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized handdesigned networks.},
archivePrefix = {arXiv},
arxivId = {1708.05344},
author = {Brock, Andrew and Lim, Theodore and Ritchie, J. M. and Weston, Nick},
booktitle = {arXiv},
eprint = {1708.05344},
title = {{SMASH: One-shot model architecture search through hypernetworks}},
volume = {abs/1708.0},
year = {2017}
}
@article{Berrada2018,
abstract = {The top-k error is a common measure of performance in machine learning and computer vision. In practice, top-k classification is typically performed with deep neural networks trained with the cross-entropy loss. Theoretical results indeed suggest that cross-entropy is an optimal learning objective for such a task in the limit of infinite data. In the context of limited and noisy data however, the use of a loss function that is specifically designed for top-k classification can bring significant improvements. Our empirical evidence suggests that the loss function must be smooth and have non-sparse gradients in order to work well with deep neural networks. Consequently, we introduce a family of smoothed loss functions that are suited to top-k optimization via deep learning. The widely used cross-entropy is a special case of our family. Evaluating our smooth loss functions is computationally challenging: a na{\"{i}}ve algorithm would require O(nk) operations, where n is the number of classes. Thanks to a connection to polynomial algebra and a divide- and-conquer approach, we provide an algorithm with a time complexity of O(kn). Furthermore, we present a novel approximation to obtain fast and stable algorithms on GPUs with single floating point precision. We compare the performance of the cross-entropy loss and our margin-based losses in various regimes of noise and data size, for the predominant use case of k = 5. Our investigation reveals that our loss is more robust to noise and overfitting than cross-entropy.},
archivePrefix = {arXiv},
arxivId = {1802.07595},
author = {Berrada, Leonard and Zisserman, Andrew and Kumar, M. Pawan},
eprint = {1802.07595},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
month = {feb},
title = {{Smooth loss functions for deep top-k classification}},
url = {http://arxiv.org/abs/1802.07595},
year = {2018}
}
@misc{Gupta2019a,
author = {Gupta, Anubha and Gupta, Rita},
doi = {10.7937/TCIA.2019.OF2W8LXR},
publisher = {The Cancer Imaging Archive},
title = {{SN-CanData: White Blood Cancer Dataset of B-ALL and MM for Stain Normalization}},
url = {https://wiki.cancerimagingarchive.net/x/EQIlAw},
year = {2019}
}
@inproceedings{Czarnecki2017,
abstract = {At the heart of deep learning we aim to use neural networks as function approximators - training them to produce outputs from inputs in emulation of a ground truth function or data creation process. In many cases we only have access to input-output pairs from the ground truth, however it is becoming more common to have access to derivatives of the target output with respect to the input - for example when the ground truth function is itself a neural network such as in network compression or distillation. Generally these target derivatives are not computed, or are ignored. This paper introduces Sobolev Training for neural networks, which is a method for incorporating these target derivatives in addition the to target values while training. By optimising neural networks to not only approximate the function's outputs but also the function's derivatives we encode additional information about the target function within the parameters of the neural network. Thereby we can improve the quality of our predictors, as well as the data-efficiency and generalization capabilities of our learned function approximation. We provide theoretical justifications for such an approach as well as examples of empirical evidence on three distinct domains: regression on classical optimisation datasets, distilling policies of an agent playing Atari, and on large-scale applications of synthetic gradients. In all three domains the use of Sobolev Training, employing target derivatives in addition to target values, results in models with higher accuracy and stronger generalisation.},
archivePrefix = {arXiv},
arxivId = {1706.04859},
author = {Czarnecki, Wojciech Marian and Osindero, Simon and Jaderberg, Max and Swirszcz, Grzegorz and Pascanu, Razvan},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1706.04859},
issn = {10495258},
pages = {4279--4288},
title = {{Sobolev training for neural networks}},
volume = {2017-Decem},
year = {2017}
}
@article{Ning2015,
abstract = {Diffusion magnetic resonance imaging (dMRI) is the modality of choice for investigating in-vivo white matter connectivity and neural tissue architecture of the brain. The diffusion-weighted signal in dMRI reflects the diffusivity of water molecules in brain tissue and can be utilized to produce image-based biomarkers for clinical research. Due to the constraints on scanning time, a limited number of measurements can be acquired within a clinically feasible scan time. In order to reconstruct the dMRI signal from a discrete set of measurements, a large number of algorithms have been proposed in recent years in conjunction with varying sampling schemes, i.e., with varying b-values and gradient directions. Thus, it is imperative to compare the performance of these reconstruction methods on a single data set to provide appropriate guidelines to neuroscientists on making an informed decision while designing their acquisition protocols. For this purpose, the SPArse Reconstruction Challenge (SPARC) was held along with the workshop on Computational Diffusion MRI (at MICCAI 2014) to validate the performance of multiple reconstruction methods using data acquired from a physical phantom. A total of 16 reconstruction algorithms (9 teams) participated in this community challenge. The goal was to reconstruct single b-value and/or multiple b-value data from a sparse set of measurements. In particular, the aim was to determine an appropriate acquisition protocol (in terms of the number of measurements, b-values) and the analysis method to use for a neuroimaging study. The challenge did not delve on the accuracy of these methods in estimating model specific measures such as fractional anisotropy (FA) or mean diffusivity, but on the accuracy of these methods to fit the data. This paper presents several quantitative results pertaining to each reconstruction algorithm. The conclusions in this paper provide a valuable guideline for choosing a suitable algorithm and the corresponding data-sampling scheme for clinical neuroscience applications.},
author = {Ning, Lipeng and Laun, Frederik and Gur, Yaniv and DiBella, Edward V.R. and Deslauriers-Gauthier, Samuel and Megherbi, Thinhinane and Ghosh, Aurobrata and Zucchelli, Mauro and Menegaz, Gloria and Fick, Rutger and St-Jean, Samuel and Paquette, Michael and Aranda, Ramon and Descoteaux, Maxime and Deriche, Rachid and O'Donnell, Lauren and Rathi, Yogesh},
doi = {10.1016/j.media.2015.10.012},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Angular error,Diffusion MRI,Normalized mean square error,Physical phantom},
month = {dec},
number = {1},
pages = {316--331},
title = {{Sparse Reconstruction Challenge for diffusion MRI: Validation on a physical phantom to determine which acquisition scheme and analysis method to use?}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841515001541},
volume = {26},
year = {2015}
}
@article{S.Rameshkumar2016,
author = {Rameshkumar, S and Thilak, J Anish Jafrin and Suresh, P and Sathishkumar, S and Subramani, N},
doi = {10.15680/IJIRSET.2016.0512161},
issn = {2319-8753},
journal = {International Journal of Innovative Research in Science Engineering and Technology},
keywords = {filter,image denoising,mri,mse,psnr,rmse,wb},
month = {dec},
number = {12},
pages = {21079--21083},
title = {{Speckle Noise Removal in MRI Scan Image Using WB – Filter}},
volume = {5},
year = {2016}
}
@inproceedings{Rippel2015,
abstract = {Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs). We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality. This representation also enables a new form of stochastic regularization by randomized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling. Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training.},
annote = {{\_}eprint: 1506.03767},
archivePrefix = {arXiv},
arxivId = {1506.03767},
author = {Rippel, Oren and Snoek, Jasper and Adams, Ryan P.},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1506.03767},
issn = {10495258},
keywords = {Computer Science - Learning,Statistics - Machine Learning},
pages = {2449--2457},
title = {{Spectral representations for convolutional neural networks}},
volume = {2015-Janua},
year = {2015}
}
@misc{ArmatoIIIS.G.HadjiiskiL.TourassiG.D.DrukkerK.GigerM.L.LiF.2015,
author = {{Armato III, S. G., Hadjiiski, L., Tourassi, G.D., Drukker, K., Giger, M.L., Li, F.}, Redmond and {G., Farahani, K., Kirby, J.S. and Clarke}, L.P.},
booktitle = {Cancer Imaging Arch 10},
doi = {10.7937/K9/TCIA.2015.UZLSU3FL},
pages = {p.K9},
publisher = {The Cancer Imaging Archive},
title = {{SPIE-AAPM-NCI Lung Nodule Classification Challenge}},
url = {https://wiki.cancerimagingarchive.net/x/bImiAQ},
year = {2015}
}
@article{Jamaludin2017,
abstract = {The objective of this work is to automatically produce radiological gradings of spinal lumbar MRIs and also localize the predicted pathologies. We show that this can be achieved via a Convolutional Neural Network (CNN) framework that takes intervertebral disc volumes as inputs and is trained only on disc-specific class labels. Our contributions are: (i) a CNN architecture that predicts multiple gradings at once, and we propose variants of the architecture including using 3D convolutions; (ii) showing that this architecture can be trained using a multi-task loss function without requiring segmentation level annotation; and (iii) a localization method that clearly shows pathological regions in the disc volumes. We compare three visualization methods for the localization. The network is applied to a large corpus of MRI T2 sagittal spinal MRIs (using a standard clinical scan protocol) acquired from multiple machines, and is used to automatically compute disk and vertebra gradings for each MRI. These are: Pfirrmann grading, disc narrowing, upper/lower endplate defects, upper/lower marrow changes, spondylolisthesis, and central canal stenosis. We report near human performances across the eight gradings, and also visualize the evidence for these gradings localized on the original scans.},
author = {Jamaludin, Amir and Kadir, Timor and Zisserman, Andrew},
doi = {10.1016/j.media.2017.07.002},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {MRI analysis,Radiological classification,Spinal MRI},
pages = {63--73},
pmid = {28756059},
title = {{SpineNet: Automated classification and evidence visualization in spinal MRIs}},
url = {http://www.sciencedirect.com/science/article/pii/S136184151730110X},
volume = {41},
year = {2017}
}
@inproceedings{Su2018,
abstract = {We present a network architecture for processing point clouds that directly operates on a collection of points represented as a sparse set of samples in a high-dimensional lattice. Na{\~{A}}ely applying convolutions on this lattice scales poorly, both in terms of memory and computational cost, as the size of the lattice increases. Instead, our network uses sparse bilateral convolutional layers as building blocks. These layers maintain efficiency by using indexing structures to apply convolutions only on occupied parts of the lattice, and allow flexible specifications of the lattice structure enabling hierarchical and spatially-aware feature learning, as well as joint 2D-3D reasoning. Both point-based and image-based representations can be easily incorporated in a network with such layers and the resulting model can be trained in an end-to-end manner. We present results on 3D segmentation tasks where our approach outperforms existing state-of-the-art techniques.},
annote = {{\_}eprint: 1802.08275},
archivePrefix = {arXiv},
arxivId = {1802.08275},
author = {Su, Hang and Jampani, Varun and Sun, Deqing and Maji, Subhransu and Kalogerakis, Evangelos and Yang, Ming Hsuan and Kautz, Jan},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00268},
eprint = {1802.08275},
isbn = {9781538664209},
issn = {10636919},
pages = {2530--2539},
title = {{SPLATNet: Sparse Lattice Networks for Point Cloud Processing}},
url = {http://arxiv.org/abs/1802.08275},
volume = {abs/1802.0},
year = {2018}
}
@article{senet,
abstract = {Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the 'Squeeze-and-Excitation' (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251{\%}, achieving a {\~{}}25{\%} relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.},
author = {Hu, Jie and Shen, Li and Sun, Gang},
doi = {10.1109/CVPR.2018.00745},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {7132--7141},
publisher = {IEEE},
title = {{Squeeze-and-Excitation Networks}},
url = {https://ieeexplore.ieee.org/document/8578843/},
year = {2018}
}
@article{Zhang2016a,
abstract = {Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGANs) aimed at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of a scene based on a given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and the text description as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and multiple discriminators arranged in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.},
archivePrefix = {arXiv},
arxivId = {1710.10916},
author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris N.},
doi = {10.1109/TPAMI.2018.2856256},
eprint = {1710.10916},
issn = {19393539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Generative models,generative adversarial networks (GANs),multi-distribution approximation,multi-stage GANs,photo-realistic image generation,text-to-image synthesis},
month = {dec},
number = {8},
pages = {1947--1962},
pmid = {30010548},
title = {{StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1612.03242},
volume = {41},
year = {2019}
}
@article{Gupta2017,
author = {Gupta, Ritu and Mallick, Pramit and Duggal, Rahul and Gupta, Anubha and Sharma, Ojaswa},
doi = {10.1016/j.clml.2017.03.178},
issn = {21522650},
journal = {Clinical Lymphoma Myeloma and Leukemia},
month = {feb},
number = {1},
pages = {e99},
title = {{Stain Color Normalization and Segmentation of Plasma Cells in Microscopic Images as a Prelude to Development of Computer Assisted Automated Disease Diagnostic Tool in Multiple Myeloma}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S2152265017304688},
volume = {17},
year = {2017}
}
@article{Kuijf2019,
abstract = {Quantification of cerebral white matter hyperintensities (WMH) of presumed vascular origin is of key importance in many neurological research studies. Currently, measurements are often still obtained from manual segmentations on brain MR images, which is a laborious procedure. The automatic WMH segmentation methods exist, but a standardized comparison of the performance of such methods is lacking. We organized a scientific challenge, in which developers could evaluate their methods on a standardized multi-center/-scanner image dataset, giving an objective comparison: the WMH Segmentation Challenge. Sixty T1 + FLAIR images from three MR scanners were released with the manual WMH segmentations for training. A test set of 110 images from five MR scanners was used for evaluation. The segmentation methods had to be containerized and submitted to the challenge organizers. Five evaluation metrics were used to rank the methods: 1) Dice similarity coefficient; 2) modified Hausdorff distance (95th percentile); 3) absolute log-transformed volume difference; 4) sensitivity for detecting individual lesions; and 5) F1-score for individual lesions. In addition, the methods were ranked on their inter-scanner robustness; 20 participants submitted their methods for evaluation. This paper provides a detailed analysis of the results. In brief, there is a cluster of four methods that rank significantly better than the other methods, with one clear winner. The inter-scanner robustness ranking shows that not all the methods generalize to unseen scanners. The challenge remains open for future submissions and provides a public platform for method evaluation.},
archivePrefix = {arXiv},
arxivId = {1904.00682},
author = {Kuijf, Hugo J. and Casamitjana, Adri{\`{a}} and Collins, D. Louis and Dadar, Mahsa and Georgiou, Achilleas and Ghafoorian, Mohsen and Jin, Dakai and Khademi, April and Knight, Jesse and Li, Hongwei and Llad{\'{o}}, Xavier and Biesbroek, J. Matthijs and Luna, Miguel and Mahmood, Qaiser and Mckinley, Richard and Mehrtash, Alireza and Ourselin, Sebastien and Park, Bo Yong and Park, Hyunjin and Park, Sang Hyun and Pezold, Simon and Puybareau, Elodie and {De Bresser}, Jeroen and Rittner, Leticia and Sudre, Carole H. and Valverde, Sergi and Vilaplana, Veronica and Wiest, Roland and Xu, Yongchao and Xu, Ziyue and Zeng, Guodong and Zhang, Jianguo and Zheng, Guoyan and Heinen, Rutger and Chen, Christopher and {Van Der Flier}, Wiesje and Barkhof, Frederik and Viergever, Max A. and Biessels, Geert Jan and Andermatt, Simon and Bento, Mariana and Berseth, Matt and Belyaev, Mikhail and Cardoso, M. Jorge},
doi = {10.1109/TMI.2019.2905770},
eprint = {1904.00682},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Magnetic resonance imaging (MRI),brain,evaluation and performance,segmentation},
month = {nov},
number = {11},
pages = {2556--2568},
title = {{Standardized Assessment of Automatic Segmentation of White Matter Hyperintensities and Results of the WMH Segmentation Challenge}},
url = {https://ieeexplore.ieee.org/document/8669968/},
volume = {38},
year = {2019}
}
@article{Kirisli2013,
abstract = {Though conventional coronary angiography (CCA) has been the standard of reference for diagnosing coronary artery disease in the past decades, computed tomography angiography (CTA) has rapidly emerged, and is nowadays widely used in clinical practice. Here, we introduce a standardized evaluation framework to reliably evaluate and compare the performance of the algorithms devised to detect and quantify the coronary artery stenoses, and to segment the coronary artery lumen in CTA data. The objective of this evaluation framework is to demonstrate the feasibility of dedicated algorithms to: (1) (semi-)automatically detect and quantify stenosis on CTA, in comparison with quantitative coronary angiography (QCA) and CTA consensus reading, and (2) (semi-)automatically segment the coronary lumen on CTA, in comparison with expert's manual annotation. A database consisting of 48 multicenter multivendor cardiac CTA datasets with corresponding reference standards are described and made available. The algorithms from 11 research groups were quantitatively evaluated and compared. The results show that (1) some of the current stenosis detection/quantification algorithms may be used for triage or as a second-reader in clinical practice, and that (2) automatic lumen segmentation is possible with a precision similar to that obtained by experts. The framework is open for new submissions through the website, at http://coronary.bigr.nl/stenoses/. {\textcopyright} 2013 Elsevier B.V.},
author = {Kirişli, H. A. and Schaap, M. and Metz, C. T. and Dharampal, A. S. and Meijboom, W. B. and Papadopoulou, S. L. and Dedic, A. and Nieman, K. and de Graaf, M. A. and Meijs, M. F.L. and Cramer, M. J. and Broersen, A. and Cetin, S. and Eslami, A. and Fl{\'{o}}rez-Valencia, L. and Lor, K. L. and Matuszewski, B. and Melki, I. and Mohr, B. and {\"{O}}ks{\"{u}}z, I. and Shahzad, R. and Wang, C. and Kitslaar, P. H. and Unal, G. and Katouzian, A. and Orkisz, M. and Chen, C. M. and Precioso, F. and Najman, L. and Masood, S. and {\"{U}}nay, D. and van Vliet, L. and Moreno, R. and Goldenberg, R. and Vu{\c{c}}ini, E. and Krestin, G. P. and Niessen, W. J. and {Van Walsum}, T.},
doi = {10.1016/j.media.2013.05.007},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Computed tomography angiography (CTA),Coronary arteries,Standardized evaluation framework,Stenoses detection,Stenoses quantification},
month = {dec},
number = {8},
pages = {859--876},
pmid = {23837963},
title = {{Standardized evaluation framework for evaluating coronary artery stenosis detection, stenosis quantification and lumen segmentation algorithms in computed tomography angiography}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1361841513000819},
volume = {17},
year = {2013}
}
@article{Balocco2014,
abstract = {This paper describes an evaluation framework that allows a standardized and quantitative comparison of IVUS lumen and media segmentation algorithms. This framework has been introduced at the MICCAI 2011 Computing and Visualization for (Intra)Vascular Imaging (CVII) workshop, comparing the results of eight teams that participated.We describe the available data-base comprising of multi-center, multi-vendor and multi-frequency IVUS datasets, their acquisition, the creation of the reference standard and the evaluation measures. The approaches address segmentation of the lumen, the media, or both borders; semi- or fully-automatic operation; and 2-D vs. 3-D methodology. Three performance measures for quantitative analysis have been proposed. The results of the evaluation indicate that segmentation of the vessel lumen and media is possible with an accuracy that is comparable to manual annotation when semi-automatic methods are used, as well as encouraging results can be obtained also in case of fully-automatic segmentation. The analysis performed in this paper also highlights the challenges in IVUS segmentation that remains to be solved. {\textcopyright} 2013 Elsevier Ltd.},
author = {Balocco, Simone and Gatta, Carlo and Ciompi, Francesco and Wahle, Andreas and Radeva, Petia and Carlier, Stephane and Unal, Gozde and Sanidas, Elias and Mauri, Josepa and Carillo, Xavier and Kovarnik, Tomas and Wang, Ching Wei and Chen, Hsiang Chou and Exarchos, Themis P. and Fotiadis, Dimitrios I. and Destrempes, Fran{\c{c}}ois and Cloutier, Guy and Pujol, Oriol and Alberti, Marina and Mendizabal-Ruiz, E. Gerardo and Rivera, Mariano and Aksoy, Timur and Downe, Richard W. and Kakadiaris, Ioannis A.},
doi = {10.1016/j.compmedimag.2013.07.001},
issn = {08956111},
journal = {Computerized Medical Imaging and Graphics},
keywords = {Algorithm comparison,Evaluation framework,IVUS (intravascular ultrasound),Image segmentation},
month = {mar},
number = {2},
pages = {70--90},
title = {{Standardized evaluation methodology and reference database for evaluating IVUS image segmentation}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0895611113001298},
volume = {38},
year = {2014}
}
@article{Bron2015,
abstract = {Algorithms for computer-aided diagnosis of dementia based on structural MRI have demonstrated high performance in the literature, but are difficult to compare as different data sets and methodology were used for evaluation. In addition, it is unclear how the algorithms would perform on previously unseen data, and thus, how they would perform in clinical practice when there is no real opportunity to adapt the algorithm to the data at hand. To address these comparability, generalizability and clinical applicability issues, we organized a grand challenge that aimed to objectively compare algorithms based on a clinically representative multi-center data set. Using clinical practice as the starting point, the goal was to reproduce the clinical diagnosis. Therefore, we evaluated algorithms for multi-class classification of three diagnostic groups: patients with probable Alzheimer's disease, patients with mild cognitive impairment and healthy controls. The diagnosis based on clinical criteria was used as reference standard, as it was the best available reference despite its known limitations. For evaluation, a previously unseen test set was used consisting of 354 T1-weighted MRI scans with the diagnoses blinded. Fifteen research teams participated with a total of 29 algorithms. The algorithms were trained on a small training set (n. =. 30) and optionally on data from other sources (e.g., the Alzheimer's Disease Neuroimaging Initiative, the Australian Imaging Biomarkers and Lifestyle flagship study of aging). The best performing algorithm yielded an accuracy of 63.0{\%} and an area under the receiver-operating-characteristic curve (AUC) of 78.8{\%}. In general, the best performances were achieved using feature extraction based on voxel-based morphometry or a combination of features that included volume, cortical thickness, shape and intensity. The challenge is open for new submissions via the web-based framework: http://caddementia.grand-challenge.org.},
author = {Bron, Esther E. and Smits, Marion and van der Flier, Wiesje M. and Vrenken, Hugo and Barkhof, Frederik and Scheltens, Philip and Papma, Janne M. and Steketee, Rebecca M.E. and {M{\'{e}}ndez Orellana}, Carolina and Meijboom, Rozanna and Pinto, Madalena and Meireles, Joana R. and Garrett, Carolina and Bastos-Leite, Ant{\'{o}}nio J. and Abdulkadir, Ahmed and Ronneberger, Olaf and Amoroso, Nicola and Bellotti, Roberto and C{\'{a}}rdenas-Pe{\~{n}}a, David and {\'{A}}lvarez-Meza, Andr{\'{e}}s M. and Dolph, Chester V. and Iftekharuddin, Khan M. and Eskildsen, Simon F. and Coup{\'{e}}, Pierrick and Fonov, Vladimir S. and Franke, Katja and Gaser, Christian and Ledig, Christian and Guerrero, Ricardo and Tong, Tong and Gray, Katherine R. and Moradi, Elaheh and Tohka, Jussi and Routier, Alexandre and Durrleman, Stanley and Sarica, Alessia and {Di Fatta}, Giuseppe and Sensi, Francesco and Chincarini, Andrea and Smith, Garry M. and Stoyanov, Zhivko V. and S{\o}rensen, Lauge and Nielsen, Mads and Tangaro, Sabina and Inglese, Paolo and Wachinger, Christian and Reuter, Martin and van Swieten, John C. and Niessen, Wiro J. and Klein, Stefan},
doi = {10.1016/j.neuroimage.2015.01.048},
issn = {10959572},
journal = {NeuroImage},
keywords = {Alzheimer's disease,Challenge,Classification,Computer-aided diagnosis,Mild cognitive impairment,Structural MRI},
month = {may},
pages = {562--579},
title = {{Standardized evaluation of algorithms for computer-aided diagnosis of dementia based on structural MRI: The CADDementia challenge}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811915000737},
volume = {111},
year = {2015}
}
@article{Suinesiaputra2018,
abstract = {Statistical shape modeling is a powerful tool for visualizing and quantifying geometric and functional patterns of the heart. After myocardial infarction (MI), the left ventricle typically remodels in response to physiological challenges. Several methods have been proposed in the literature to describe statistical shape changes. Which method best characterizes the left ventricular remodeling after MI is an open research question. A better descriptor of remodeling is expected to provide a more accurate evaluation of disease status in MI patients. We therefore designed a challenge to test shape characterization in MI given a set of three-dimensional left ventricular surface points. The training set comprised 100 MI patients, and 100 asymptomatic volunteers (AV). The challenge was initiated in 2015 at the Statistical Atlases and Computational Models of the Heart workshop, in conjunction with the MICCAI conference. The training set with labels was provided to participants, who were asked to submit the likelihood of MI from a different (validation) set of 200 cases (100 AV and 100 MI). Sensitivity, specificity, accuracy, and area under the receiver operating characteristic curve were used as the outcome measures. The goals of this challenge were to 1) establish a common dataset for evaluating statistical shape modeling algorithms in MI, and 2) test whether statistical shape modeling provides additional information characterizing MI patients over standard clinical measures. Eleven groups with a wide variety of classification and feature extraction approaches participated in this challenge. All methods achieved excellent classification results with accuracy ranges from 0.83 to 0.98. The areas under the receiver operating characteristic curves were all above 0.90. Four methods showed significantly higher performance than standard clinical measures. The dataset and software for evaluation are available from the Cardiac Atlas Project website.11 http://www.cardiacatlas.org.},
author = {Suinesiaputra, Avan and Ablin, Pierre and Alba, Xenia and Alessandrini, Martino and Allen, Jack and Bai, Wenjia and Cimen, Serkan and Claes, Peter and Cowan, Brett R. and Dhooge, Jan and Duchateau, Nicolas and Ehrhardt, Jan and Frangi, Alejandro F. and Gooya, Ali and Grau, Vicente and Lekadir, Karim and Lu, Allen and Mukhopadhyay, Anirban and Oksuz, Ilkay and Parajuli, Nripesh and Pennec, Xavier and Pereanez, Marco and Pinto, Catarina and Piras, Paolo and Rohe, Marc Michel and Rueckert, Daniel and Saring, Dennis and Sermesant, Maxime and Siddiqi, Kaleem and Tabassian, Mahdi and Teresi, Luciano and Tsaftaris, Sotirios A. and Wilms, Matthias and Young, Alistair A. and Zhang, Xingyu and Medrano-Gracia, Pau},
doi = {10.1109/JBHI.2017.2652449},
issn = {21682194},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Cardiac modeling,classification,myocardial infarct,statistical shape analysis},
month = {mar},
number = {2},
pages = {503--515},
pmid = {28103561},
title = {{Statistical Shape Modeling of the Left Ventricle: Myocardial Infarct Classification Challenge}},
url = {https://ieeexplore.ieee.org/document/7820042/},
volume = {22},
year = {2018}
}
@incollection{Theodoridis2015,
abstract = {http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/},
author = {Theodoridis, Sergios},
booktitle = {Machine Learning},
doi = {10.1016/b978-0-12-801522-3.00005-7},
month = {aug},
pages = {161--231},
title = {{Stochastic Gradient Descent}},
url = {https://en.wikipedia.org/wiki/Stochastic{\_}gradient{\_}descent{\#}Extensions{\_}and{\_}variants},
year = {2015}
}
@incollection{Bottou2012,
abstract = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.},
author = {Bottou, L{\'{e}}on},
booktitle = {Neural Networks: Tricks of the Trade - Second Edition},
doi = {10.1007/978-3-642-35289-8_25},
pages = {421--436},
title = {{Stochastic Gradient Descent Tricks}},
url = {https://doi.org/10.1007/978-3-642-35289-8{\_}25},
year = {2012}
}
@article{Vahadane2016,
abstract = {Staining and scanning of tissue samples for microscopic examination is fraught with undesirable color variations arising from differences in raw materials and manufacturing techniques of stain vendors, staining protocols of labs, and color responses of digital scanners. When comparing tissue samples, color normalization and stain separation of the tissue images can be helpful for both pathologists and software. Techniques that are used for natural images fail to utilize structural properties of stained tissue samples and produce undesirable color distortions. The stain concentration cannot be negative. Tissue samples are stained with only a few stains and most tissue regions are characterized by at most one effective stain. We model these physical phenomena that define the tissue structure by first decomposing images in an unsupervised manner into stain density maps that are sparse and non-negative. For a given image, we combine its stain density maps with stain color basis of a pathologist-preferred target image, thus altering only its color while preserving its structure described by the maps. Stain density correlation with ground truth and preference by pathologists were higher for images normalized using our method when compared to other alternatives. We also propose a computationally faster extension of this technique for large whole-slide images that selects an appropriate patch sample instead of using the entire image to compute the stain color basis.},
author = {Vahadane, Abhishek and Peng, Tingying and Sethi, Amit and Albarqouni, Shadi and Wang, Lichao and Baust, Maximilian and Steiger, Katja and Schlitter, Anna Melissa and Esposito, Irene and Navab, Nassir},
doi = {10.1109/TMI.2016.2529665},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Color normalization,Unsupervised stain separation,histopathological images,non-negative matrix factorization,sparse regularization},
number = {8},
pages = {1962--1971},
title = {{Structure-Preserving Color Normalization and Sparse Stain Separation for Histological Images}},
volume = {35},
year = {2016}
}
@incollection{Fisher2020,
abstract = {Sub-pixel estimation is the process of estimating the value of a geometric quantity to better than pixel accuracy, even though the data was originally sampled on an integer pixel quantized space.},
address = {Cham},
author = {Fisher, Robert B.},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_189-1},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Subpixel Estimation}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}189-1},
year = {2020}
}
@incollection{Fukui2020,
address = {Cham},
author = {Fukui, Kazuhiro},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_708-1},
pages = {1--5},
publisher = {Springer International Publishing},
title = {{Subspace Methods}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}708-1},
year = {2020}
}
@article{Cortes1995,
abstract = {In this paper, the optimal margin algorithm is generalized$\backslash$nto non-separable problems by the introduction of slack$\backslash$nvariables in the statement of the optimization problem.},
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1007/bf00994018},
issn = {0885-6125},
journal = {Machine Learning},
number = {3},
pages = {273--297},
title = {{Support-vector networks}},
volume = {20},
year = {1995}
}
@article{Corneanu2016,
abstract = {Facial expressions are an important way through which humans interact socially. Building a system capable of automatically recognizing facial expressions from images and video has been an intense field of study in recent years. Interpreting such expressions remains challenging and much research is needed about the way they relate to human affect. This paper presents a general overview of automatic RGB, 3D, thermal and multimodal facial expression analysis. We define a new taxonomy for the field, encompassing all steps from face detection to facial expression recognition, and describe and classify the state of the art methods accordingly. We also present the important datasets and the bench-marking of most influential methods. We conclude with a general discussion about trends, important questions and future lines of research.},
archivePrefix = {arXiv},
arxivId = {1606.03237},
author = {Corneanu, Ciprian Adrian and Sim{\'{o}}n, Marc Oliu and Cohn, Jeffrey F. and Guerrero, Sergio Escalera},
doi = {10.1109/TPAMI.2016.2515606},
eprint = {1606.03237},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {3D,Facial expression,RGB,affect,emotion recognition,multimodal,thermal},
month = {aug},
number = {8},
pages = {1548--1568},
pmid = {26761193},
publisher = {IEEE Computer Society},
title = {{Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression Recognition: History, Trends, and Affect-Related Applications}},
volume = {38},
year = {2016}
}
@inproceedings{Yi2016,
abstract = {In this paper, we study the problem of semantic annotation on 3D models that are represented as shape graphs. A functional view is taken to represent localized information on graphs, so that annotations such as part segment or keypoint are nothing but 0-1 indicator vertex functions. Compared with images that are 2D grids, shape graphs are irregular and non-isomorphic data structures. To enable the prediction of vertex functions on them by convolutional neural networks, we resort to spectral CNN method that enables weight sharing by parametrizing kernels in the spectral domain spanned by graph Laplacian eigenbases. Under this setting, our network, named SyncSpecCNN, strives to overcome two key challenges: how to share coefficients and conduct multi-scale analysis in different parts of the graph for a single shape, and how to share information across related but different shapes that may be represented by very different graphs. Towards these goals, we introduce a spectral parametrization of dilated convolutional kernels and a spectral transformer network. Experimentally we tested SyncSpecCNN on various tasks, including 3D shape part segmentation and keypoint prediction. State-of-the-art performance has been achieved on all benchmark datasets.},
annote = {{\_}eprint: 1612.00606},
archivePrefix = {arXiv},
arxivId = {1612.00606},
author = {Yi, Li and Su, Hao and Guo, Xingwen and Guibas, Leonidas},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.697},
eprint = {1612.00606},
isbn = {9781538604571},
pages = {6584--6592},
title = {{SyncSpecCNN: Synchronized spectral CNN for 3D shape segmentation}},
url = {http://arxiv.org/abs/1612.00606},
volume = {2017-Janua},
year = {2017}
}
@article{Ger2018a,
abstract = {Purpose: Magnetic resonance imaging (MRI) provides noninvasive evaluation of patient's anatomy without using ionizing radiation. Due to this and the high soft-tissue contrast, MRI use has increased and has potential for use in longitudinal studies where changes in patients' anatomy or tumors at different time points are compared. Deformable image registration can be useful for these studies. Here, we describe two datasets that can be used to evaluate the registration accuracy of systems for MR images, as it cannot be assumed to be the same as that measured on CT images. Acquisition and validation methods: Two sets of images were created to test registration accuracy. (a) A porcine phantom was created by placing ten 0.35-mm gold markers into porcine meat. The porcine phantom was immobilized in a plastic container with movable dividers. T1-weighted, T2-weighted, and CT images were acquired with the porcine phantom compressed in four different ways. The markers were not visible on the MR images, due to the selected voxel size, so they did not interfere with the measured registration accuracy, while the markers were visible on the CT images and were used to identify the known deformation between positions. (b) Synthetic images were created using 28 head and neck squamous cell carcinoma patients who had MR scans pre-, mid-, and postradiotherapy treatment. An inter- and intrapatient variation model was created using these patient scans. Four synthetic pretreatment images were created using the interpatient model, and four synthetic post-treatment images were created for each of the pretreatment images using the intrapatient model. Data format and usage notes: The T1-weighted, T2-weighted, and CT scans of the porcine phantom in the four positions are provided. Four T1-weighted synthetic pretreatment images each with four synthetic post-treatment images, and four T2-weighted synthetic pretreatment images each with four synthetic post-treatment images are provided. Additionally, the applied deformation vector fields to generate the synthetic post-treatment images are provided. The data are available on TCIA under the collection MRI-DIR. Potential applications: The proposed database provides two sets of images (one acquired and one computer generated) for use in evaluating deformable image registration accuracy. T1- and T2-weighted images are available for each technique as the different image contrast in the two types of images may impact the registration accuracy.},
author = {Ger, Rachel B. and Yang, Jinzhong and Ding, Yao and Jacobsen, Megan C. and Cardenas, Carlos E. and Fuller, Clifton D. and Howell, Rebecca M. and Li, Heng and Stafford, R. Jason and Zhou, Shouhao and Court, Laurence E.},
doi = {10.1002/mp.13090},
issn = {00942405},
journal = {Medical Physics},
keywords = {MRI,deformable image registration,deformation,registration},
month = {sep},
number = {9},
pages = {4315--4321},
pmid = {30007075},
title = {{Synthetic head and neck and phantom images for determining deformable image registration accuracy in magnetic resonance imaging}},
url = {http://doi.wiley.com/10.1002/mp.13090},
volume = {45},
year = {2018}
}
@book{Ramage2020,
abstract = {Systems Thinkers presents a biographical history of the field of systems thinking, by examining the life and work of thirty of its major thinkers. It discusses each thinker's key contributions, the way this contribution was expressed in practice and the relationship between their life and ideas. This discussion is supported by an extract from the thinker's own writing, to give a flavour of their work and to give readers a sense of which thinkers are most relevant to their own interests. Systems thinking is necessarily interdisciplinary, so that the thinkers selected come from a wide range of areas - biology, management, physiology, anthropology, chemistry, public policy, sociology and environmental studies among others. Some are core innovators in systems ideas; some have been primarily practitioners who also advanced and popularised systems ideas; others are well-known figures who drew heavily upon systems thinking although it was not their primary discipline. A significant aim of the book is to broaden and deepen the reader's interest in systems writers, providing an appetising 'taster' for each of the 30 thinkers, so that the reader is encouraged to go on to study the published works of the thinkers themselves. Copyright {\textcopyright} 2009 The Open University. All rights reserved.},
address = {London},
author = {Ramage, Magnus and Shipp, Karen},
booktitle = {Systems Thinkers},
doi = {10.1007/978-1-4471-7475-2},
isbn = {978-1-4471-7474-5},
pages = {1--324},
publisher = {Springer London},
title = {{Systems Thinkers}},
url = {http://link.springer.com/10.1007/978-1-4471-7475-2},
year = {2020}
}
@online{TADPOLE,
title = {{TADPOLE - Home}},
url = {https://tadpole.grand-challenge.org/},
urldate = {2020-05-25}
}
@article{Tesau1995,
author = {Tesau, Covid and Tesau, Gerald},
doi = {10.1145/203330.203343},
issn = {15577317},
journal = {Communications of the ACM},
number = {3},
pages = {58--68},
title = {{Temporal Difference Learning and TD-Gammon}},
volume = {38},
year = {1995}
}
@inproceedings{Abadi2016,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, generalpurpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that Tensor-Flow achieves for several real-world applications.},
annote = {{\_}eprint: 1605.08695},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
booktitle = {Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016},
eprint = {1605.08695},
isbn = {9781931971331},
pages = {265--283},
title = {{TensorFlow: A system for large-scale machine learning}},
url = {http://arxiv.org/abs/1605.08695},
volume = {abs/1605.0},
year = {2016}
}
@article{Vallet2015,
abstract = {The objective of the TerraMobilita/iQmulus 3D urban analysis benchmark is to evaluate the current state of the art in urban scene analysis from mobile laser scanning (MLS) at large scale. A very detailed semantic tree for urban scenes is proposed. We call analysis the capacity of a method to separate the points of the scene into these categories (classification), and to separate the different objects of the same type for object classes (detection). A very large ground truth is produced manually in two steps using advanced editing tools developed especially for this benchmark. Based on this ground truth, the benchmark aims at evaluating the classification, detection and segmentation quality of the submitted results.},
author = {Vallet, Bruno and Br{\'{e}}dif, Mathieu and Serna, Andres and Marcotegui, Beatriz and Paparoditis, Nicolas},
doi = {10.1016/j.cag.2015.03.004},
issn = {00978493},
journal = {Computers and Graphics (Pergamon)},
keywords = {Benchmark,Classification,Laser scanning,Mobile mapping,Segmentation,Urban scene},
pages = {126--133},
title = {{TerraMobilita/iQmulus urban point cloud analysis benchmark}},
volume = {49},
year = {2015}
}
@article{Ye2015,
abstract = {This paper analyzes, compares, and contrasts technical challenges, methods, and the performance of text detection and recognition research in color imagery. It summarizes the fundamental problems and enumerates factors that should be considered when addressing these problems. Existing techniques are categorized as either stepwise or integrated and sub-problems are highlighted including text localization, verification, segmentation and recognition. Special issues associated with the enhancement of degraded text and the processing of video text, multi-oriented, perspectively distorted and multilingual text are also addressed. The categories and sub-categories of text are illustrated, benchmark datasets are enumerated, and the performance of the most representative approaches is compared. This review provides a fundamental comparison and analysis of the remaining problems in the field.},
author = {Ye, Qixiang and Doermann, David},
doi = {10.1109/TPAMI.2014.2366765},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Survey,Text Detection,Text Localization,Text Recognition},
month = {jul},
number = {7},
pages = {1480--1500},
publisher = {IEEE Computer Society},
title = {{Text Detection and Recognition in Imagery: A Survey}},
volume = {37},
year = {2015}
}
@article{Mueller2005,
abstract = {With increasing life expectancy in developed countries, the incidence of Alzheimer's disease (AD) and its socioeconomic impact are growing. Increasing knowledge of the mechanisms of AD facilitates the development of treatment strategies aimed at slowing down or preventing neuronal death. AD treatment trials using clinical outcome measures require long observation times and large patient samples. There is increasing evidence that neuroimaging and cerebrospinal fluid and blood biomarkers may provide information that may reduce sample sizes and observation periods. The Alzheimer's Disease Neuroimaging Initiative will help identify clinical, neuroimaging, and biomarker outcome measures that provide the highest power for measurement of longitudinal changes and for prediction of transitions. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
author = {Mueller, Susanne G. and Weiner, Michael W. and Thal, Leon J. and Petersen, Ronald C. and Jack, Clifford and Jagust, William and Trojanowski, John Q. and Toga, Arthur W. and Beckett, Laurel},
doi = {10.1016/j.nic.2005.09.008},
issn = {10525149},
journal = {Neuroimaging Clinics of North America},
number = {4},
pages = {869--877},
pmid = {16443497},
title = {{The Alzheimer's disease neuroimaging initiative}},
volume = {15},
year = {2005}
}
@article{Weiner2017,
abstract = {Introduction The overall goal of the Alzheimer's Disease Neuroimaging Initiative (ADNI) is to validate biomarkers for Alzheimer's disease (AD) clinical trials. ADNI-3, which began on August 1, 2016, is a 5-year renewal of the current ADNI-2 study. Methods ADNI-3 will follow current and additional subjects with normal cognition, mild cognitive impairment, and AD using innovative technologies such as tau imaging, magnetic resonance imaging sequences for connectivity analyses, and a highly automated immunoassay platform and mass spectroscopy approach for cerebrospinal fluid biomarker analysis. A Systems Biology/pathway approach will be used to identify genetic factors for subject selection/enrichment. Amyloid positron emission tomography scanning will be standardized using the Centiloid method. The Brain Health Registry will help recruit subjects and monitor subject cognition. Results Multimodal analyses will provide insight into AD pathophysiology and disease progression. Discussion ADNI-3 will aim to inform AD treatment trials and facilitate development of AD disease-modifying treatments.},
author = {Weiner, Michael W. and Veitch, Dallas P. and Aisen, Paul S. and Beckett, Laurel A. and Cairns, Nigel J. and Green, Robert C. and Harvey, Danielle and Jack, Clifford R. and Jagust, William and Morris, John C. and Petersen, Ronald C. and Salazar, Jennifer and Saykin, Andrew J. and Shaw, Leslie M. and Toga, Arthur W. and Trojanowski, John Q.},
doi = {10.1016/j.jalz.2016.10.006},
issn = {15525279},
journal = {Alzheimer's and Dementia},
keywords = {Alzheimer's disease,Amyloid phenotyping,Brain Health Registry,Centiloid method,Clinical trial biomarkers,Functional connectivity,Tau imaging},
month = {may},
number = {5},
pages = {561--571},
pmid = {27931796},
title = {{The Alzheimer's Disease Neuroimaging Initiative 3: Continued innovation for clinical trial improvement}},
url = {http://doi.wiley.com/10.1016/j.jalz.2016.10.006},
volume = {13},
year = {2017}
}
@article{Marinescu2020,
abstract = {Accurate prediction of progression in subjects at risk of Alzheimer's disease is crucial for enrolling the right subjects in clinical trials. However, a prospective comparison of state-of-the-art algorithms for predicting disease onset and progression is currently lacking. We present the findings of The Alzheimer's Disease Prediction Of Longitudinal Evolution (TADPOLE) Challenge, which compared the performance of 92 algorithms from 33 international teams at predicting the future trajectory of 219 individuals at risk of Alzheimer's disease. Challenge participants were required to make a prediction, for each month of a 5-year future time period, of three key outcomes: clinical diagnosis, Alzheimer's Disease Assessment Scale Cognitive Subdomain (ADAS-Cog13), and total volume of the ventricles. No single submission was best at predicting all three outcomes. For clinical diagnosis and ventricle volume prediction, the best algorithms strongly outperform simple baselines in predictive ability. However, for ADAS-Cog13 no single submitted prediction method was significantly better than random guessing. On a limited, cross-sectional subset of the data emulating clinical trials, performance of best algorithms at predicting clinical diagnosis decreased only slightly (3{\%} error increase) compared to the full longitudinal dataset. Two ensemble methods based on taking the mean and median over all predictions, obtained top scores on almost all tasks. Better than average performance at diagnosis prediction was generally associated with the additional inclusion of features from cerebrospinal fluid (CSF) samples and diffusion tensor imaging (DTI). On the other hand, better performance at ventricle volume prediction was associated with inclusion of summary statistics, such as patient-specific biomarker trends. The submission system remains open via the website https://tadpole.grand-challenge.org, while code for submissions is being collated by TADPOLE SHARE: https://tadpole-share.github.io/. Our work suggests that current prediction algorithms are accurate for biomarkers related to clinical diagnosis and ventricle volume, opening up the possibility of cohort refinement in clinical trials for Alzheimer's disease.},
archivePrefix = {arXiv},
arxivId = {2002.03419},
author = {Marinescu, Razvan V. and Oxtoby, Neil P. and Young, Alexandra L. and Bron, Esther E. and Toga, Arthur W. and Weiner, Michael W. and Barkhof, Frederik and Fox, Nick C. and Eshaghi, Arman and Toni, Tina and Salaterski, Marcin and Lunina, Veronika and Ansart, Manon and Durrleman, Stanley and Lu, Pascal and Iddi, Samuel and Li, Dan and Thompson, Wesley K. and Donohue, Michael C. and Nahon, Aviv and Levy, Yarden and Halbersberg, Dan and Cohen, Mariya and Liao, Huiling and Li, Tengfei and Yu, Kaixian and Zhu, Hongtu and Tamez-Pe{\~{n}}a, Jos{\'{e}} G. and Ismail, Aya and Wood, Timothy and Bravo, Hector Corrada and Nguyen, Minh and Sun, Nanbo and Feng, Jiashi and {Thomas Yeo}, B. T. and Chen, Gang and Qi, Ke and Chen, Shiyang and Qiu, Deqiang and Buciuman, Ionut and Kelner, Alex and Pop, Raluca and Rimocea, Denisa and Ghazi, Mostafa M. and Nielsen, Mads and Ourselin, Sebastien and S{\o}rensen, Lauge and Venkatraghavan, Vikram and Liu, Keli and Rabe, Christina and Manser, Paul and Hill, Steven M. and Howlett, James and Huang, Zhiyue and Kiddle, Steven and Mukherjee, Sach and Rouanet, Ana{\"{i}}s and Taschler, Bernd and Tom, Brian D.M. and White, Simon R. and Faux, Noel and Sedai, Suman and {de Velasco Oriol}, Javier and Clemente, Edgar E.V. and Estrada, Karol and Aksman, Leon and Altmann, Andre and Stonnington, Cynthia M. and Wang, Yalin and Wu, Jianfeng and Devadas, Vivek and Fourrier, Clementine and Raket, Lars Lau and Sotiras, Aristeidis and Erus, Guray and Doshi, Jimit and Davatzikos, Christos and Vogel, Jacob and Doyle, Andrew and Tam, Angela and Diaz-Papkovich, Alex and Jammeh, Emmanuel and Koval, Igor and Moore, Paul and Lyons, Terry J. and Gallacher, John and Tohka, Jussi and Ciszek, Robert and Jedynak, Bruno and Pandya, Kruti and Bilgel, Murat and Engels, William and Cole, Joseph and Golland, Polina and Klein, Stefan and Alexander, Daniel C.},
eprint = {2002.03419},
journal = {arXiv},
month = {feb},
title = {{The Alzheimer's disease prediction of longitudinal evolution (tadpole) challenge: Results after 1 year follow-up}},
url = {http://arxiv.org/abs/2002.03419},
year = {2020}
}
@book{Lantsoght2018,
abstract = {This textbook is a guide to success during the PhD trajectory. The first part of this book takes the reader through all steps of the PhD trajectory, and the second part contains a unique glossary of terms and explanation relevant for PhD candidates. Written in the accessible language of the PhD Talk blogs, the book contains a great deal of practical advice for carrying out research, and presenting one's work. It includes tips and advice from current and former PhD candidates, thus representing a broad range of opinions. The book includes exercises that help PhD candidates get their work kick-started. It covers all steps of a doctoral journey in STEM: getting started in a program, planning the work, the literature review, the research question, experimental work, writing, presenting, online tools, presenting at one's first conference, writing the first journal paper, writing and defending the thesis, and the career after the PhD. Since a PhD trajectory is a deeply personal journey, this book suggests methods PhD candidates can try out, and teaches them how to figure out for themselves which proposed methods work for them, and how to find their own way of doing things.},
address = {Cham},
author = {Lantsoght, Eva O L},
booktitle = {Springer Texts in Education},
doi = {10.1007/978-3-319-77425-1},
isbn = {9783319774244},
issn = {2366-7672},
publisher = {Springer International Publishing},
series = {Springer Texts in Education},
title = {{The A-Z of the PhD Trajectory}},
url = {http://link.springer.com/10.1007/978-3-319-77425-1},
year = {2018}
}
@article{Clark2013,
abstract = {The National Institutes of Health have placed significant emphasis on sharing of research data to support secondary research. Investigators have been encouraged to publish their clinical and imaging data as part of fulfilling their grant obligations. Realizing it was not sufficient to merely ask investigators to publish their collection of imaging and clinical data, the National Cancer Institute (NCI) created the open source National Biomedical Image Archive software package as a mechanism for centralized hosting of cancer related imaging. NCI has contracted with Washington University in Saint Louis to create The Cancer Imaging Archive (TCIA) - an open-source, open-access information resource to support research, development, and educational initiatives utilizing advanced medical imaging of cancer. In its first year of operation, TCIA accumulated 23 collections (3.3 million images). Operating and maintaining a high-availability image archive is a complex challenge involving varied archive-specific resources and driven by the needs of both image submitters and image consumers. Quality archives of any type (traditional library, PubMed, refereed journals) require management and customer service. This paper describes the management tasks and user support model for TCIA. {\textcopyright} 2013 Society for Imaging Informatics in Medicine.},
author = {Clark, Kenneth and Vendt, Bruce and Smith, Kirk and Freymann, John and Kirby, Justin and Koppel, Paul and Moore, Stephen and Phillips, Stanley and Maffitt, David and Pringle, Michael and Tarbox, Lawrence and Prior, Fred},
doi = {10.1007/s10278-013-9622-7},
issn = {08971889},
journal = {Journal of Digital Imaging},
keywords = {Biomedical image analysis,Cancer detection,Cancer imaging,Image archive,NBIA,TCIA},
month = {dec},
number = {6},
pages = {1045--1057},
pmid = {23884657},
title = {{The cancer imaging archive (TCIA): Maintaining and operating a public information repository}},
url = {https://doi.org/10.1007/s10278-013-9622-7 http://link.springer.com/10.1007/s10278-013-9622-7},
volume = {26},
year = {2013}
}
@book{Alley2018,
abstract = {The Craft of Scientific Writing is designed to help scientists and engineers - both professionals already active in the disciplines as well as students preparing to enter the professions - write about their work clearly and effectively. Written for use as a text in courses on scientific writing, the book includes many useful suggestions about approaching a wide variety of writing tasks from journal papers to grant proposals and from emails to formal reports, as well as a concise guide to style and usage appropriate for scientific writing. Also useful for self-study, the book will be an important reference for all scientists and engineers who need to write about their work. With this new and updated fourth edition, while most technical writing texts have gotten larger over the years, this one has streamlined, to provide busy readers with the essence of what distinguishes the style of the best scientific documents. With this new edition, readers will learn not just how to organize information, but how to emphasize the key details of that information. Also, readers will not just learn how to cast their ideas into precise and clear sentences, but how to connect these sentences in an energetic fashion. In the section on language, the new edition goes into much depth about how to make connections between ideas: an important issue that few technical writing texts address. Moreover, the new edition integrates the discussion of illustrations with language because those two aspects of style are so intertwined. Finally, the new edition does a better job of explaining how to make the process of writing more efficient. From a review of the first edition: "A refreshing addition to a genre dominated by English teacher-style textbooks. Instead of listing rules that constrain writers, the book uses examples to lay out the path to successful communication ... Especially helpful (and entertaining) is the chapter on the writing process. Anyone who has spent more time avoiding a writing task than actually doing it will appreciate Alley's tips." -Dr. Ellen Ochoa, Deputy Director of Flight Crew Operations, Johnson Space Center.},
address = {New York, NY},
author = {Alley, Michael},
booktitle = {The Craft of Scientific Writing: Fourth Edition},
doi = {10.1007/978-1-4419-8288-9},
isbn = {9781441982889},
pages = {1--298},
publisher = {Springer New York},
title = {{The craft of scientific writing: Fourth edition}},
url = {http://link.springer.com/10.1007/978-1-4419-8288-9},
year = {2018}
}
@article{Lowekamp2013,
abstract = {SimpleITK is a new interface to the Insight Segmentation and Registration Toolkit (ITK) designed to facilitate rapid prototyping, education and scientific activities via high level programming languages. ITK is a templated C++ library of image processing algorithms and frameworks for biomedical and other applications, and it was designed to be generic, flexible and extensible. Initially, ITK provided a direct wrapping interface to languages such as Python and Tcl through the WrapITK system. Unlike WrapITK, which exposed ITK's complex templated interface, SimpleITK was designed to provide an easy to use and simplified interface to ITK's algorithms. It includes procedural methods, hides ITK's demand driven pipeline, and provides a template-less layer. Also SimpleITK provides practical conveniences such as binary distribution packages and overloaded operators. Our user-friendly design goals dictated a departure from the direct interface wrapping approach of WrapITK, toward a new facade class structure that only exposes the required functionality, hiding ITK's extensive template use. Internally SimpleITK utilizes a manual description of each filter with code-generation and advanced C++ meta-programming to provide the higher-level interface, bringing the capabilities of ITK to a wider audience. SimpleITK is licensed as open source software library under the Apache License Version 2.0 and more information about downloading it can be found at http://www.simpleitk.org. {\textcopyright} 2013 Lowek amp, Chen, Ib{\'{a}}{\~{n}}ez and Blezek.},
author = {Lowekamp, Bradley C. and Chen, David T. and Ib{\'{a}}{\~{n}}ez, Luis and Blezek, Daniel},
doi = {10.3389/fninf.2013.00045},
issn = {16625196},
journal = {Frontiers in Neuroinformatics},
keywords = {Image processing and analysis,Image processing software,Insight toolkit,Segmentation,Software design,Software development},
number = {DEC},
pages = {45},
title = {{The design of simpleITK}},
volume = {7},
year = {2013}
}
@misc{LeCun1998b,
address = {Cambridge, MA, USA},
annote = {Section: Convolutional Networks for Images, Speech, and Time Series},
author = {Andrew, Alex M.},
booktitle = {Kybernetes},
doi = {10.1108/k.1999.28.9.1084.1},
editor = {Arbib, Michael A},
isbn = {0-262-51102-9},
issn = {0368492X},
keywords = {Artificial intelligence,Brain,Cybernetics,Neural networks,Publication},
number = {9},
pages = {1084--1094},
publisher = {MIT Press},
title = {{The Handbook of Brain Theory and Neural Networks}},
url = {http://dl.acm.org/citation.cfm?id=303568.303704},
volume = {28},
year = {1999}
}
@article{Shackman2011,
abstract = {It has been argued that emotion, pain and cognitive control are functionally segregated in distinct subdivisions of the cingulate cortex. However, recent observations encourage a fundamentally different view. Imaging studies demonstrate that negative affect, pain and cognitive control activate an overlapping region of the dorsal cingulate - the anterior midcingulate cortex (aMCC). Anatomical studies reveal that the aMCC constitutes a hub where information about reinforcers can be linked to motor centres responsible for expressing affect and executing goal-directed behaviour. Computational modelling and other kinds of evidence suggest that this intimacy reflects control processes that are common to all three domains. These observations compel a reconsideration of the dorsal cingulate's contribution to negative affect and pain. {\textcopyright} 2011 Macmillan Publishers Limited. All rights reserved.},
author = {Shackman, Alexander J. and Salomons, Tim V. and Slagter, Heleen A. and Fox, Andrew S. and Winter, Jameel J. and Davidson, Richard J.},
doi = {10.1038/nrn2994},
issn = {1471003X},
journal = {Nature Reviews Neuroscience},
month = {mar},
number = {3},
pages = {154--167},
pmid = {21331082},
title = {{The integration of negative affect, pain and cognitive control in the cingulate cortex}},
url = {http://www.nature.com/articles/nrn2994},
volume = {12},
year = {2011}
}
@misc{Ibanez2003,
abstract = {Everything you need to install, use, and extend the Insight Segmentation and Registration Toolikit ITK. Includes detailed examples, installation procedures, and system overview for ITK version 2.4. (The included examples are taken directly from the ITK source code repository and are designed to demonstrate the essential features of the software.) The book comes with a CD-ROM that contains a complete hyperlinked version of the book plus ITK source code, data, Windows binaries, and extensive class documentation. Also includes CMake binaries for managing the ITK build process on a variety of compiler and operating system configurations.},
author = {Ibanez, L and Schroeder, W and Ng, L and Cates, J},
booktitle = {The ITK Software Guide},
doi = {1�930934-15�7},
edition = {First},
isbn = {1930934157},
issn = {10445323},
number = {May},
pages = {804},
pmid = {1000070720},
publisher = {Kitware, Inc.},
title = {{The ITK Software Guide}},
url = {http://www.itk.org/ItkSoftwareGuide.pdf},
volume = {Second},
year = {2005}
}
@misc{Trikha2013,
abstract = {Femtosecond laser-assisted cataract surgery (FLACS) represents a potential paradigm shift in cataract surgery, but it is not without controversy. Advocates of the technology herald FLACS as a revolution that promises superior outcomes and an improved safety profile for patients. Conversely, detractors point to the large financial costs involved and claim that similar results are achievable with conventional small-incision phacoemulsification. This review provides a balanced and comprehensive account of the development of FLACS since its inception. It explains the physiology and mechanics underlying the technology, and critically reviews the outcomes and implications of initial studies. The benefits and limitations of using femtosecond laser accuracy to create corneal incisions, anterior capsulotomy, and lens fragmentation are explored, with reference to the main platforms, which currently offer FLACS. Economic considerations are discussed, in addition to the practicalities associated with the implementation of FLACS in a healthcare setting. The influence on surgical training and skills is considered and possible future applications of the technology introduced. While in its infancy, FLACS sets out the exciting possibility of a new level of precision in cataract surgery. However, further work in the form of large scale, phase 3 randomised controlled trials are required to demonstrate whether its theoretical benefits are significant in practice and worthy of the necessary huge financial investment and system overhaul. Whether it gains widespread acceptance is likely to be influenced by a complex interplay of scientific and socio-economic factors in years to come. {\textcopyright} 2013 Macmillan Publishers Limited All rights reserved 0950-222X/13.},
author = {Trikha, S. and Turnbull, A. M.J. and Morris, R. J. and Anderson, D. F. and Hossain, P.},
booktitle = {Eye (Basingstoke)},
doi = {10.1038/eye.2012.293},
issn = {14765454},
keywords = {capsulorhexis,capsulotomy,cataract surgery,femtosecond,laser,phacoemulsfication},
number = {4},
pages = {461--473},
title = {{The journey to femtosecond laser-assisted cataract surgery: New beginnings or a false dawn?}},
volume = {27},
year = {2013}
}
@article{Heller2019,
abstract = {Characterization of the relationship between a kidney tumor's appearance on cross-sectional imaging and it's treatment outcomes is a promising direction for informing treatement decisions and improving patient outcomes. Unfortunately, the rigorous study of tumor morphology is limited by the laborious and noisy process of making manual radiographic measurements. Semantic segmentation of the tumor and surrounding organ offers a precise quantitative description of that morphology, but it too requires significant manual effort. A large publicly available dataset of high-fidelity semantic segmentations along with clinical context and treatment outcomes could accelerate not only the study of how morphology relates to outcomes, but also the development of automatic semantic segmentation systems which could enable such studies on unprecedented scales. We present the KiTS19 challenge dataset: a collection of segmented CT imaging and treatment outcomes for 300 patients treated with partial or radical nephrectomy between 2010 and 2018. 210 of these cases have been released publicly and the remaining 90 remain private for the objective evaluation of prediction systems developed using the public cases.},
annote = {{\_}eprint: 1904.00445},
archivePrefix = {arXiv},
arxivId = {1904.00445},
author = {Heller, Nicholas and Sathianathen, Niranjan and Kalapara, Arveen and Walczak, Edward and Moore, Keenan and Kaluzniak, Heather and Rosenberg, Joel and Blake, Paul and Rengel, Zachary and Oestreich, Makinna and Dean, Joshua and Tradewell, Michael and Shah, Aneri and Tejpaul, Resha and Edgerton, Zachary and Peterson, Matthew and Raza, Shaneabbas and Regmi, Subodh and Papanikolopoulos, Nikolaos and Weight, Christopher},
eprint = {1904.00445},
journal = {arXiv},
keywords = {Kidney Tumors,Nephrometry,Semantic Segmentation},
title = {{The KiTS19 challenge data: 300 Kidney tumor cases with clinical context, Ct semantic segmentations, and surgical outcomes}},
url = {http://arxiv.org/abs/1904.00445},
year = {2019}
}
@article{liss,
abstract = {Lung computed tomography (CT) imaging signs play important roles in the diagnosis of lung diseases. In this paper, we review the significance of CT imaging signs in disease diagnosis and determine the inclusion criterion of CT scans and CT imaging signs of our database. We develop the software of abnormal regions annotation and design the storage scheme of CT images and annotation data. Then, we present a publicly available database of lung CT imaging signs, called LISS for short, which contains 271 CT scans and 677 abnormal regions in them. The 677 abnormal regions are divided into nine categories of common CT imaging signs of lung disease (CISLs). The ground truth of these CISLs regions and the corresponding categories are provided. Furthermore, to make the database publicly available, all private data in CT scans are eliminated or replaced with provisioned values. The main characteristic of our LISS database is that it is developed from a new perspective of CT imaging signs of lung diseases instead of commonly considered lung nodules. Thus, it is promising to apply to computer-aided detection and diagnosis research and medical education.},
author = {Han, Guanghui and Liu, Xiabi and Han, Feifei and Santika, I. Nyoman Tenaya and Zhao, Yanfeng and Zhao, Xinming and Zhou, Chunwu},
doi = {10.1109/TBME.2014.2363131},
issn = {15582531},
journal = {IEEE Transactions on Biomedical Engineering},
keywords = {CT imaging signs,Computer-aided diagnosis (CAD),lung lesions,medical database,medical education},
number = {2},
pages = {648--656},
pmid = {25330480},
title = {{The LISS - A public database of common imaging signs of lung diseases for computer-aided detection and diagnosis research and medical education}},
volume = {62},
year = {2015}
}
@article{Bilic2019,
abstract = {In this work, we report the set-up and results of the Liver Tumor Segmentation Benchmark (LiTS) organized in conjunction with the IEEE International Symposium on Biomedical Imaging (ISBI) 2017 and International Conference On Medical Image Computing {\&} Computer Assisted Intervention (MICCAI) 2017. Twenty-four valid state-of-the-art liver and liver tumor segmentation algorithms were applied to a set of 131 computed tomography (CT) volumes with different types of tumor contrast levels (hyper-/hypo-intense), abnormalities in tissues (metastasectomie) size and varying amount of lesions. The submitted algorithms have been tested on 70 undisclosed volumes. The dataset is created in collaboration with seven hospitals and research institutions and manually blind reviewed by independent three radiologists. We found that not a single algorithm performed best for liver and tumors. The best liver segmentation algorithm achieved a Dice score of 0.96(MICCAI) whereas for tumor segmentation the best algorithm evaluated at 0.67(ISBI) and 0.70(MICCAI). The LiTS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
annote = {{\_}eprint: 1901.04056},
archivePrefix = {arXiv},
arxivId = {1901.04056},
author = {Bilic1a, Patrick and Christa, Patrick Ferdinand and Vorontsov, Eugene and Chlebusr, Grzegorz and Chenm, Hao and Doum, Qi and Fum, Chi Wing and Hanp, Xiao and Hengm, Pheng Ann and Hesserq, Jrgen and Kadourye, Samuel and Kopczyskiv, Tomasz and Leo, Miao and Lio, Chunming and Lim, Xiaomeng and Lipkova, Jana and Lowengrubn, John and Meiner, Hans and Moltzr, Jan Hendrik and Pale, Chris and Pirauda, Marie and Qim, Xiaojuan and Qil, Jin and Rempera, Markus and Rothq, Karsten and Schenkr, Andrea and Sekuboyinaa, Anjany and Zhouk, Ping and Hulsemeyera, Christian and Beetza, Marcel and Ettlingera, Florian and Gruena, Felix and Kaissisb, Georgios and Lohferb, Fabian and Brarenb, Rickmer and Holchc, Julian and Hofmannc, Felix and Sommerc, Wieland and Heinemannc, Volker and Jacobsd, Colin and Mamanid, Gabriel Efrain Humpire and Ginnekend, Bram Van and Chartrande, Gabriel and Tange, An and Drozdzale, Michal and Kadourye, Samuel and Ben-Cohenf, Avi and Klangf, Eyal and Amitaif, Marianne M. and Konenf, Eli and Greenspanf, Hayit and Moreaug, Johan and Hostettlerg, Alexandre and Solerg, Luc and Vivantih, Refael and Szeskinh, Adi and Lev-Cohainh, Naama and Sosnah, Jacob and Joskowiczh, Leo and Kumarw, Ashnil and Korex, Avinash and Wangy, Chunliang and Fengz, Dagan and Liaa, Fan and Krishnamurthix, Ganapathy and Heab, Jian and Wuaa, Jianrong and Kimx, Jinman and Zhouac, Jinyi and Maad, Jun and Liaa, Junbo and Maninisae, Kevis Kokitsi and Kaluvax, Krishna Chaitanya and Bix, Lei and Khenedx, Mahendra and Beliverae, Miriam and Linaa, Qizhong and Yangad, Xiaoping and Yuanaf, Yading and Chenaa, Yinan and Liad, Yuanqiang and Qius, Yudong and Wuad, Yuli and Menzea, Bjoern},
eprint = {1901.04056},
journal = {arXiv},
keywords = {Benchmark.,CT,Liver,Medical imaging,Segmentation,Tumor},
title = {{The liver tumor segmentation benchmark (LiTS)}},
url = {http://arxiv.org/abs/1901.04056},
volume = {abs/1901.0},
year = {2019}
}
@article{Leuschner2019,
abstract = {Deep Learning approaches for solving Inverse Problems in imaging have become very effective and are demonstrated to be quite competitive in the field. Comparing these approaches is a challenging task since they highly rely on the data and the setup that is used for training. We provide a public dataset of computed tomography images and simulated low-dose measurements suitable for training this kind of methods. With the LoDoPaB-CT Dataset we aim to create a benchmark that allows for a fair comparison. It contains over 40 000 scan slices from around 800 patients selected from the LIDC/IDRI Database. In this paper we describe how we processed the original slices and how we simulated the measurements. We also include first baseline results.},
archivePrefix = {arXiv},
arxivId = {1910.01113},
author = {Leuschner, Johannes and Schmidt, Maximilian and Baguer, Daniel Otero and Maa{\ss}, Peter},
eprint = {1910.01113},
journal = {arXiv},
month = {oct},
title = {{The LoDoPaB-CT dataset: A benchmark dataset for low-dose ct reconstruction methods}},
url = {http://arxiv.org/abs/1910.01113},
year = {2019}
}
@inproceedings{Berman2017,
abstract = {The Jaccard index, also referred to as the intersection-over-union score, is commonly employed in the evaluation of image segmentation results given its perceptual qualities, scale invariance - which lends appropriate relevance to small objects, and appropriate counting of false negatives, in comparison to per-pixel losses. We present a method for direct optimization of the mean intersection-over-union loss in neural networks, in the context of semantic image segmentation, based on the convex Lov{\~{A}}¡sz extension of submodular losses. The loss is shown to perform better with respect to the Jaccard index measure than the traditionally used cross-entropy loss. We show quantitative and qualitative differences between optimizing the Jaccard index per image versus optimizing the Jaccard index taken over an entire dataset. We evaluate the impact of our method in a semantic segmentation pipeline and show substantially improved intersection-over-union segmentation scores on the Pascal VOC and Cityscapes datasets using state-of-the-art deep learning segmentation architectures.},
annote = {{\_}eprint: 1705.08790},
archivePrefix = {arXiv},
arxivId = {1705.08790},
author = {Berman, Maxim and Triki, Amal Rannen and Blaschko, Matthew B.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00464},
eprint = {1705.08790},
isbn = {9781538664209},
issn = {10636919},
pages = {4413--4421},
title = {{The Lovasz-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks}},
year = {2018}
}
@article{Armato2011,
abstract = {Purpose: The development of computer-aided diagnostic (CAD) methods for lung nodule detection, classification, and quantitative assessment can be facilitated through a well-characterized repository of computed tomography (CT) scans. The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) completed such a database, establishing a publicly available reference for the medical imaging research community. Initiated by the National Cancer Institute (NCI), further advanced by the Foundation for the National Institutes of Health (FNIH), and accompanied by the Food and Drug Administration (FDA) through active participation, this public-private partnership demonstrates the success of a consortium founded on a consensus-based process. Methods: Seven academic centers and eight medical imaging companies collaborated to identify, address, and resolve challenging organizational, technical, and clinical issues to provide a solid foundation for a robust database. The LIDC/IDRI Database contains 1018 cases, each of which includes images from a clinical thoracic CT scan and an associated XML file that records the results of a two-phase image annotation process performed by four experienced thoracic radiologists. In the initial blinded-read phase, each radiologist independently reviewed each CT scan and marked lesions belonging to one of three categories (" nodule≥3 mm," " nodule{\textless}3 mm," and "non- nodule≥3 mm "). In the subsequent unblinded-read phase, each radiologist independently reviewed their own marks along with the anonymized marks of the three other radiologists to render a final opinion. The goal of this process was to identify as completely as possible all lung nodules in each CT scan without requiring forced consensus. Results: The Database contains 7371 lesions marked "nodule" by at least one radiologist. 2669 of these lesions were marked " nodul≥3 mm " by at least one radiologist, of which 928 (34.7{\%}) received such marks from all four radiologists. These 2669 lesions include nodule outlines and subjective nodule characteristic ratings. Conclusions: The LIDC/IDRI Database is expected to provide an essential medical imaging research resource to spur CAD development, validation, and dissemination in clinical practice. {\textcopyright} 2011 U.S. Government.},
author = {Armato, Samuel G. and McLennan, Geoffrey and Bidaut, Luc and McNitt-Gray, Michael F. and Meyer, Charles R. and Reeves, Anthony P. and Zhao, Binsheng and Aberle, Denise R. and Henschke, Claudia I. and Hoffman, Eric A. and Kazerooni, Ella A. and MacMahon, Heber and {Van Beek}, Edwin J.R. and Yankelevitz, David and Biancardi, Alberto M. and Bland, Peyton H. and Brown, Matthew S. and Engelmann, Roger M. and Laderach, Gary E. and Max, Daniel and Pais, Richard C. and Qing, David P.Y. and Roberts, Rachael Y. and Smith, Amanda R. and Starkey, Adam and Batra, Poonam and Caligiuri, Philip and Farooqi, Ali and Gladish, Gregory W. and Jude, C. Matilda and Munden, Reginald F. and Petkovska, Iva and Quint, Leslie E. and Schwartz, Lawrence H. and Sundaram, Baskaran and Dodd, Lori E. and Fenimore, Charles and Gur, David and Petrick, Nicholas and Freymann, John and Kirby, Justin and Hughes, Brian and {Vande Casteele}, Alessi and Gupte, Sangeeta and Sallam, Maha and Heath, Michael D. and Kuhn, Michael H. and Dharaiya, Ekta and Burns, Richard and Fryd, David S. and Salganicoff, Marcos and Anand, Vikram and Shreter, Uri and Vastagh, Stephen and Croft, Barbara Y. and Clarke, Laurence P.},
doi = {10.1118/1.3528204},
issn = {00942405},
journal = {Medical Physics},
keywords = {computed tomography (CT),computer-aided diagnosis (CAD),interobserver variability,lung nodule,thoracic imaging},
month = {jan},
number = {2},
pages = {915--931},
pmid = {21452728},
title = {{The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI): A completed reference database of lung nodules on CT scans}},
url = {http://doi.wiley.com/10.1118/1.3528204},
volume = {38},
year = {2011}
}
@misc{MIMIC-CXR,
author = {Johnson, Alistair E W and Pollard, Tom and Mark, Roger and Berkowitz, Seth and Horng, Steven},
doi = {10.13026/C2JT1Q},
publisher = {physionet.org},
title = {{The MIMIC-CXR Database}},
url = {https://physionet.org/content/mimic-cxr/},
year = {2019}
}
@article{Menze2015,
abstract = {In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients - manually annotated by up to four raters - and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74{\%}-85{\%}), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
author = {Menze, Bjoern H. and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Yuliya and Porz, Nicole and Slotboom, Johannes and Wiest, Roland and Lanczi, Levente and Gerstner, Elizabeth and Weber, Marc Andr{\'{e}} and Arbel, Tal and Avants, Brian B. and Ayache, Nicholas and Buendia, Patricia and Collins, D. Louis and Cordier, Nicolas and Corso, Jason J. and Criminisi, Antonio and Das, Tilak and Delingette, Herv{\'{e}} and Demiralp, {\c{C}}ağatay and Durst, Christopher R. and Dojat, Michel and Doyle, Senan and Festa, Joana and Forbes, Florence and Geremia, Ezequiel and Glocker, Ben and Golland, Polina and Guo, Xiaotao and Hamamci, Andac and Iftekharuddin, Khan M. and Jena, Raj and John, Nigel M. and Konukoglu, Ender and Lashkari, Danial and Mariz, Jos{\'{e}} Ant{\'{o}}nio and Meier, Raphael and Pereira, S{\'{e}}rgio and Precup, Doina and Price, Stephen J. and Raviv, Tammy Riklin and Reza, Syed M.S. and Ryan, Michael and Sarikaya, Duygu and Schwartz, Lawrence and Shin, Hoo Chang and Shotton, Jamie and Silva, Carlos A. and Sousa, Nuno and Subbanna, Nagesh K. and Szekely, Gabor and Taylor, Thomas J. and Thomas, Owen M. and Tustison, Nicholas J. and Unal, Gozde and Vasseur, Flor and Wintermark, Max and Ye, Dong Hye and Zhao, Liang and Zhao, Binsheng and Zikic, Darko and Prastawa, Marcel and Reyes, Mauricio and {Van Leemput}, Koen},
doi = {10.1109/TMI.2014.2377694},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Benchmark,Brain,Image segmentation,MRI,Oncology/tumor},
month = {oct},
number = {10},
pages = {1993--2024},
pmid = {25494501},
title = {{The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)}},
url = {http://ieeexplore.ieee.org/document/6975210/},
volume = {34},
year = {2015}
}
@article{VanDerWalt2011,
abstract = {In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts. {\textcopyright} 2011 IEEE.},
archivePrefix = {arXiv},
arxivId = {1102.1523},
author = {{Van Der Walt}, St{\'{e}}fan and Colbert, S Chris and Varoquaux, Ga{\"{e}}l},
doi = {10.1109/MCSE.2011.37},
eprint = {1102.1523},
issn = {15219615},
journal = {Computing in Science and Engineering},
keywords = {NumPy,Python,numerical computations,programming libraries,scientific programming},
month = {mar},
number = {2},
pages = {22--30},
title = {{The NumPy array: A structure for efficient numerical computation}},
volume = {13},
year = {2011}
}
@book{Taulli2020,
address = {Berkeley, CA},
author = {Taulli, Tom},
booktitle = {The Robotic Process Automation Handbook},
doi = {10.1007/978-1-4842-5729-6},
isbn = {9781484257289},
pages = {2--3},
publisher = {Apress},
title = {{The Robotic Process Automation Handbook}},
url = {https://doi.org/10.1007/978-1-4842-5729-6},
year = {2020}
}
@article{Maloney2010,
abstract = {Scratch is a visual programming environment that allows users (primarily ages 8 to 16) to learn computer programming while working on personally meaningful projects such as animated stories and games. A key design goal of Scratch is to support self-directed learning through tinkering and collaboration with peers. This article explores how the Scratch programming language and environment support this goal. {\textcopyright} 2010 ACM.},
author = {Maloney, John and Resnick, Mitchel and Rusk, Natalie and Silverman, Brian and Eastmond, Evelyn},
doi = {10.1145/1868358.1868363},
issn = {1946-6226},
journal = {ACM Transactions on Computing Education},
keywords = {Programming environment,Programming language,Scratch,Visual programming language},
number = {4},
pages = {16},
title = {{The scratch programming language and environment}},
volume = {10},
year = {2010}
}
@book{Parija2018,
abstract = {Thesis writing is one of the most crucial rites of passage in the postgraduate study period. Yet, for many, it remains the most arduous one as well. What is meant to be an exercise in gaining in-depth knowledge of research methodology often ends up as a slapdash production from the part of the harried graduate student. The differ- ence, we believe, lies in a proper and methodical approach to thesis/dissertation writing. Right from the beginning of the research project, one must be clear with the topic, the objectives, and the methods. While choosing a topic, one must keep in mind several factors such as the appropriateness, the feasibility, and, above all, one's interest in that particular area. The importance of securing clearances from the appropriate monitoring bodies at the correct time cannot be stressed enough. This exercise also gives the students an invaluable opportunity to learn various aspects of statistics, and this will go a long way in their future careers.},
address = {Singapore},
booktitle = {Thesis Writing for Master's and Ph.D. Program},
doi = {10.1007/978-981-13-0890-1},
editor = {Parija, Subhash Chandra and Kate, Vikram},
isbn = {978-981-13-0889-5},
publisher = {Springer Singapore},
title = {{Thesis Writing for Master's and Ph.D. Program}},
url = {http://link.springer.com/10.1007/978-981-13-0890-1},
year = {2018}
}
@article{Li2018ThoracicDI,
abstract = {Accurate identification and localization of abnormalities from radiology images play an integral part in clinical diagnosis and treatment planning. Building a highly accurate prediction model for these tasks usually requires a large number of images manually annotated with labels and finding sites of abnormalities. In reality, however, such annotated data are expensive to acquire, especially the ones with location annotations. We need methods that can work well with only a small amount of location annotations. To address this challenge, we present a unified approach that simultaneously performs disease identification and localization through the same underlying model for all images. We demonstrate that our approach can effectively leverage both class information as well as limited location annotation, and significantly outperforms the comparative reference baseline in both classification and localization tasks.},
author = {Li, Zhe and Wang, Chong and Han, Mei and Xue, Yuan and Wei, Wei and Li, Li Jia and Fei-Fei, Li},
doi = {10.1109/CVPR.2018.00865},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {8290--8299},
publisher = {IEEE},
title = {{Thoracic Disease Identification and Localization with Limited Supervision}},
url = {https://ieeexplore.ieee.org/document/8578963/},
year = {2018}
}
@misc{M2019,
abstract = {This paper proposes, a novel computer based approach for benign or malignant assessment of thyroid nodules in ultrasound images. Ultrasound imaging is one of the frequently used diagnosis tool to detect and classify abnormalities of the thyroid gland. The proposed approach comprises of four important stages: pre-processing, segmentation, feature extraction and classification. Rayleigh trimmed anisotropic diffusion filter is used for pre-processing. A watershed algorithm is used to segment the nodule region. An artificial neural network (ANN) and support vector machine (SVM) classifiers are employed for the classification task, utilizing feature vectors derived from gray level co-occurrence (GLCM) features. The classification results are evaluated with the use of accuracy, sensitivity and specificity. It is derived that SVM classifier provides better result than ANN for discriminating benign and malignant nodules, obtaining accuracy 92.5{\%}, sensitivity 96.66{\%} and specificity 80{\%}.},
author = {M, Gireesha H},
booktitle = {Article in International Journal of Engineering and Technical Research},
doi = {10.5281/ZENODO.3715942},
keywords = {ANN,Anisotropic Diffusion,Computer Aided Diagnosis(CAD),GLCM,SVM,Thyroid Ultrasound,Watershed Segmentation},
pages = {1--15},
publisher = {Zenodo},
title = {{Thyroid Nodule Segmentation and Classification in Ultrasound Images}},
url = {https://www.researchgate.net/publication/332013003},
year = {2019}
}
@article{Li2018b,
abstract = {Aim: Early detection and correct diagnosis of lung cancer are the most important steps in improving patient outcome. This study aims to assess which deep learning models perform best in lung cancer diagnosis. Methods: Non-small cell lung carcinoma and small cell lung carcinoma biopsy specimens were consecutively obtained and stained. The specimen slides were diagnosed by two experienced pathologists (over 20 years). Several deep learning models were trained to discriminate cancer and non-cancer biopsies. Result: Deep learning models give reasonable AUC from 0.8810 to 0.9119. Conclusion: The deep learning analysis could help to speed up the detection process for the whole-slide image (WSI) and keep the comparable detection rate with human observer.},
archivePrefix = {arXiv},
arxivId = {1803.05471},
author = {Li, Zhang and Hu, Zheyu and Xu, Jiaolong and Tan, Tao and Chen, Hui and Duan, Zhi and Liu, Ping and Tang, Jun and Cai, Guoping and Ouyang, Quchang and Tang, Yuling and Litjens, Geert and Li, Qiang},
eprint = {1803.05471},
journal = {arXiv},
keywords = {Artificial intelligence,Classification,Convolutional Neural Networks,Deep learning,Diagnosis,Lung cancer},
month = {mar},
title = {{Title: Computer-aided diagnosis of lung carcinoma using deep learning – a pilot study}},
url = {http://arxiv.org/abs/1803.05471},
year = {2018}
}
@article{Zela2018,
abstract = {While existing work on neural architecture search (NAS) tunes hyperparameters in a separate post-processing step, we demonstrate that architectural choices and other hyperparameter settings interact in a way that can render this separation suboptimal. Likewise, we demonstrate that the common practice of using very few epochs during the main NAS and much larger numbers of epochs during a post-processing step is inefficient due to little correlation in the relative rankings for these two training regimes. To combat both of these problems, we propose to use a recent combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search.},
archivePrefix = {arXiv},
arxivId = {1807.06906},
author = {Zela, Arber and Klein, Aaron and Falkner, Stefan and Hutter, Frank},
eprint = {1807.06906},
journal = {arXiv},
keywords = {Bayesian Optimization,Hyperparameter Optimization,Neural Architecture Search,Object Recognition},
title = {{Towards automated deep learning: Efficient joint neural architecture and hyperparameter search}},
url = {http://arxiv.org/abs/1807.06906},
volume = {abs/1807.0},
year = {2018}
}
@incollection{Mendoza2016,
abstract = {Recent advances in AutoML have led to automated tools that can compete with machine learning experts on supervised learning tasks. In this work, we present two versions of Auto-Net, which provide automatically-tuned deep neural networks without any human intervention. The first version, Auto-Net 1.0, builds upon ideas from the competition-winning system Auto-sklearn by using the Bayesian Optimization method SMAC and uses Lasagne as the underlying deep learning (DL) library. The more recent Auto-Net 2.0 builds upon a recent combination of Bayesian Optimization and HyperBand, called BOHB, and uses PyTorch as DL library. To the best of our knowledge, Auto-Net 1.0 was the first automatically-tuned neural network to win competition datasets against human experts (as part of the first AutoML challenge). Further empirical results show that ensembling Auto-Net 1.0 with Auto-sklearn can perform better than either approach alone, and that Auto-Net 2.0 can perform better yet. 7.1},
author = {Mendoza, Hector and Klein, Aaron and Feurer, Matthias and Springenberg, Jost Tobias and Urban, Matthias and Burkart, Michael and Dippel, Maximilian and Lindauer, Marius and Hutter, Frank},
booktitle = {AutoML@ICML},
doi = {10.1007/978-3-030-05318-5_7},
pages = {135--149},
title = {{Towards Automatically-Tuned Deep Neural Networks}},
year = {2019}
}
@article{Choi2016,
abstract = {Network quantization is one of network compression techniques to reduce the redundancy of deep neural networks. It reduces the number of distinct network parameter values by quantization in order to save the storage for them. In this paper, we design network quantization schemes that minimize the performance loss due to quantization given a compression ratio constraint. We analyze the quantitative relation of quantization errors to the neural network loss function and identify that the Hessian-weighted distortion measure is locally the right objective function for the optimization of network quantization. As a result, Hessian-weighted k-means clustering is proposed for clustering network parameters to quantize. When optimal variable-length binary codes, e.g., Huffman codes, are employed for further compression, we derive that the network quantization problem can be related to the entropy-constrained scalar quantization (ECSQ) problem in information theory and consequently propose two solutions of ECSQ for network quantization, i.e., uniform quantization and an iterative solution similar to Lloyd's algorithm. Finally, using the simple uniform quantization followed by Huffman coding, we show from our experiments that the compression ratios of 51.25, 22.17 and 40.65 are achievable for LeNet, 32-layer ResNet and AlexNet, respectively.},
archivePrefix = {arXiv},
arxivId = {1612.01543},
author = {Choi, Yoojin and El-Khamy, Mostafa and Lee, Jungwon},
eprint = {1612.01543},
journal = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
month = {dec},
title = {{Towards the limit of network quantization}},
url = {http://arxiv.org/abs/1612.01543},
year = {2017}
}
@article{Cote2013,
abstract = {We have developed the Tractometer: an online evaluation and validation system for tractography processing pipelines. One can now evaluate the results of more than 57,000 fiber tracking outputs using different acquisition settings (b-value, averaging), different local estimation techniques (tensor, q-ball, spherical deconvolution) and different tracking parameters (masking, seeding, maximum curvature, step size). At this stage, the system is solely based on a revised FiberCup analysis, but we hope that the community will get involved and provide us with new phantoms, new algorithms, third party libraries and new geometrical metrics, to name a few. We believe that the new connectivity analysis and tractography characteristics proposed can highlight limits of the algorithms and contribute in solving open questions in fiber tracking: from raw data to connectivity analysis. Overall, we show that (i) averaging improves quality of tractography, (ii) sharp angular ODF profiles helps tractography, (iii) seeding and multi-seeding has a large impact on tractography outputs and must be used with care, and (iv) deterministic tractography produces less invalid tracts which leads to better connectivity results than probabilistic tractography. {\textcopyright} 2013 Elsevier B.V.},
author = {C{\^{o}}t{\'{e}}, Marc Alexandre and Girard, Gabriel and Bor{\'{e}}, Arnaud and Garyfallidis, Eleftherios and Houde, Jean Christophe and Descoteaux, Maxime},
doi = {10.1016/j.media.2013.03.009},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {Connectivity analysis,Diffusion MRI,Tractography,Validation},
month = {oct},
number = {7},
pages = {844--857},
pmid = {23706753},
title = {{Tractometer: Towards validation of tractography pipelines}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23706753},
volume = {17},
year = {2013}
}
@inproceedings{Boser1992,
abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
booktitle = {Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory},
doi = {10.1145/130385.130401},
isbn = {089791497X},
pages = {144--152},
publisher = {ACM Press},
title = {{Training algorithm for optimal margin classifiers}},
year = {1992}
}
@incollection{Yang2020,
abstract = {Transfer learning deals with how systems can quickly adapt themselves to new situations, tasks and environments. It gives machine learning systems the ability to leverage auxiliary data and models to help solve target problems when there is only a small amount of data available. This makes such systems more reliable and robust, keeping the machine learning model faced with unforeseeable changes from deviating too much from expected performance. At an enterprise level, transfer learning allows knowledge to be reused so experience gained once can be repeatedly applied to the real world. For example, a pre-trained model that takes account of user privacy can be downloaded and adapted at the edge of a computer network. This self-contained, comprehensive reference text describes the standard algorithms and demonstrates how these are used in different transfer learning paradigms. It offers a solid grounding for newcomers as well as new insights for seasoned researchers and developers.},
address = {Cham},
author = {Yang, Qiang and Zhang, Yu and Dai, Wenyuan and Pan, Sinno Jialin},
booktitle = {Transfer Learning},
doi = {10.1017/9781139061773},
pages = {1--4},
publisher = {Springer International Publishing},
title = {{Transfer Learning}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}837-1},
year = {2020}
}
@article{Khan2021,
abstract = {Astounding results from transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. This has led to exciting progress on a number of tasks while requiring minimal inductive biases in the model design. This survey aims to provide a comprehensive overview of the transformer models in the computer vision discipline and assumes little to no prior background in the field. We start with an introduction to fundamental concepts behind the success of transformer models i.e., self-supervision and self-attention. Transformer architectures leverage self-attention mechanisms to encode long-range dependencies in the input domain which makes them highly expressive. Since they assume minimal prior knowledge about the structure of the problem, self-supervision using pretext tasks is applied to pre-train transformer models on large-scale (unlabelled) datasets. The learned representations are then fine-tuned on the downstream tasks, typically leading to excellent performance due to the generalization and expressivity of encoded features. We cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering and visual reasoning), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.},
archivePrefix = {arXiv},
arxivId = {2101.01169},
author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
eprint = {2101.01169},
month = {jan},
title = {{Transformers in Vision: A Survey}},
url = {http://arxiv.org/abs/2101.01169},
year = {2021}
}
@article{Mogadala2019,
abstract = {Integration of vision and language tasks has seen a significant growth in the recent times due to surge of interest from multi-disciplinary communities such as deep learning, computer vision, and natural language processing. In this survey, we focus on ten different vision and language integration tasks in terms of their problem formulation, methods, existing datasets, evaluation measures, and comparison of results achieved with the corresponding state-of-the-art methods. This goes beyond earlier surveys which are either task-specific or concentrate only on one type of visual content i.e., image or video. We then conclude the survey by discussing some possible future directions for integration of vision and language research.},
archivePrefix = {arXiv},
arxivId = {1907.09358},
author = {Mogadala, Aditya and Kalimuthu, Marimuthu and Klakow, Dietrich},
eprint = {1907.09358},
journal = {arXiv},
month = {jul},
title = {{Trends in integration of vision and language research: A survey of tasks, datasets, and methods}},
url = {http://arxiv.org/abs/1907.09358},
year = {2019}
}
@inproceedings{Schulman2015,
abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael and Abbeel, Pieter},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1502.05477},
isbn = {9781510810587},
pages = {1889--1897},
title = {{Trust region policy optimization}},
volume = {3},
year = {2015}
}
@article{Kuo2016,
abstract = {This work attempts to address two fundamental questions about the structure of the convolutional neural networks (CNN): (1) why a nonlinear activation function is essential at the filter output of all intermediate layers? (2) what is the advantage of the two-layer cascade system over the one-layer system? A mathematical model called the “REctified-COrrelations on a Sphere” (RECOS) is proposed to answer these two questions. After the CNN training process, the converged filter weights define a set of anchor vectors in the RECOS model. Anchor vectors represent the frequently occurring patterns (or the spectral components). The necessity of rectification is explained using the RECOS model. Then, the behavior of a two-layer RECOS system is analyzed and compared with its one-layer counterpart. The LeNet-5 and the MNIST dataset are used to illustrate discussion points. Finally, the RECOS model is generalized to a multilayer system with the AlexNet as an example.},
annote = {{\_}eprint: 1609.04112},
archivePrefix = {arXiv},
arxivId = {1609.04112},
author = {Kuo, C. C.Jay},
doi = {10.1016/j.jvcir.2016.11.003},
eprint = {1609.04112},
issn = {10959076},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Convolutional neural network (CNN),MNIST dataset,Nonlinear activation,RECOS model,Rectified linear unit (ReLU)},
pages = {406--413},
title = {{Understanding convolutional neural networks with a mathematical model}},
url = {http://arxiv.org/abs/1609.04112},
volume = {41},
year = {2016}
}
@article{Zhanga,
author = {Zhang, Liang and Fan, Zhonghao and Li, Yuehan and Zhu, Guangming and Li, Ping and Lu, Xiaoyuan and Shen, Peiyi and Afaq, Syed},
title = {{U-net based analysis of MRI for Alzheimer ' s disease diagnosis}},
year = {2019}
}
@incollection{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
annote = {{\_}eprint: 1505.04597},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
isbn = {9783319245737},
issn = {16113349},
pages = {234--241},
title = {{U-net: Convolutional networks for biomedical image segmentation}},
url = {http://link.springer.com/10.1007/978-3-319-24574-4{\_}28},
volume = {9351},
year = {2015}
}
@inproceedings{Boulch2017,
abstract = {In this work, we describe a new, general, and efficient method for unstructured point cloud labeling. As the question of efficiently using deep Convolutional Neural Networks (CNNs) on 3D data is still a pending issue, we propose a framework which applies CNNs on multiple 2D image views (or snapshots) of the point cloud. The approach consists in three core ideas. (i) We pick many suitable snapshots of the point cloud. We generate two types of images: a Red-Green-Blue (RGB) view and a depth composite view containing geometric features. (ii) We then perform a pixel-wise labeling of each pair of 2D snapshots using fully convolutional networks. Different architectures are tested to achieve a profitable fusion of our heterogeneous inputs. (iii) Finally, we perform fast back-projection of the label predictions in the 3D space using efficient buffering to label every 3D point. Experiments show that our method is suitable for various types of point clouds such as Lidar or photogrammetric data.},
annote = {ISSN: 1997-0471},
author = {Boulch, A. and {Le Saux}, B. and Audebert, N.},
booktitle = {Eurographics Workshop on 3D Object Retrieval, EG 3DOR},
doi = {10.2312/3dor.20171047},
editor = {Pratikakis, Ioannis and Dupont, Florent and Ovsjanikov, Maks},
isbn = {9783038680307},
issn = {19970471},
pages = {17--24},
publisher = {The Eurographics Association},
title = {{Unstructured point cloud semantic labeling using deep segmentation networks}},
volume = {2017-April},
year = {2017}
}
@book{Leordeanu2020,
address = {Cham},
author = {Leordeanu, Marius},
doi = {10.1007/978-3-030-42128-1},
isbn = {9783030421274},
publisher = {Springer International Publishing},
series = {Advances in Computer Vision and Pattern Recognition},
title = {{Unsupervised Learning in Space and Time}},
url = {http://www.springer.com/series/4205},
year = {2020}
}
@article{Tartaglione2020,
abstract = {The possibility to use widespread and simple chest X-ray (CXR) imaging for early screening of COVID-19 patients is attracting much interest from both the clinical and the AI community. In this study we provide insights and also raise warnings on what is reasonable to expect by applying deep learning to COVID classification of CXR images. We provide a methodological guide and critical reading of an extensive set of statistical results that can be obtained using currently available datasets. In particular, we take the challenge posed by current small size COVID data and show how significant can be the bias introduced by transfer-learning using larger public non-COVID CXR datasets. We also contribute by providing results on a medium size COVID CXR dataset, just collected by one of the major emergency hospitals in Northern Italy during the peak of the COVID pandemic. These novel data allow us to contribute to validate the generalization capacity of preliminary results circulating in the scientific community. Our conclusions shed some light into the possibility to effectively discriminate COVID using CXR.},
archivePrefix = {arXiv},
arxivId = {2004.05405},
author = {Tartaglione, Enzo and Barbano, Carlo Alberto and Berzovini, Claudio and Calandri, Marco and Grangetto, Marco},
doi = {10.3390/ijerph17186933},
eprint = {2004.05405},
issn = {16604601},
journal = {International Journal of Environmental Research and Public Health},
keywords = {COVID-19,Chest X-ray,Classification,Deep learning},
month = {apr},
number = {18},
pages = {1--17},
pmid = {32971995},
title = {{Unveiling COVID-19 from chest x-ray with deep learning: A hurdles race with small data}},
url = {http://arxiv.org/abs/2004.05405 http://dx.doi.org/10.3390/ijerph17186933},
volume = {17},
year = {2020}
}
@article{Setio2017,
abstract = {Automatic detection of pulmonary nodules in thoracic computed tomography (CT) scans has been an active area of research for the last two decades. However, there have only been few studies that provide a comparative performance evaluation of different systems on a common database. We have therefore set up the LUNA16 challenge, an objective evaluation framework for automatic nodule detection algorithms using the largest publicly available reference database of chest CT scans, the LIDC-IDRI data set. In LUNA16, participants develop their algorithm and upload their predictions on 888 CT scans in one of the two tracks: 1) the complete nodule detection track where a complete CAD system should be developed, or 2) the false positive reduction track where a provided set of nodule candidates should be classified. This paper describes the setup of LUNA16 and presents the results of the challenge so far. Moreover, the impact of combining individual systems on the detection performance was also investigated. It was observed that the leading solutions employed convolutional networks and used the provided set of nodule candidates. The combination of these solutions achieved an excellent sensitivity of over 95{\%} at fewer than 1.0 false positives per scan. This highlights the potential of combining algorithms to improve the detection performance. Our observer study with four expert readers has shown that the best system detects nodules that were missed by expert readers who originally annotated the LIDC-IDRI data. We released this set of additional nodules for further development of CAD systems.},
archivePrefix = {arXiv},
arxivId = {1612.08012},
author = {Setio, Arnaud Arindra Adiyoso and Traverso, Alberto and de Bel, Thomas and Berens, Moira S.N. and van den Bogaard, Cas and Cerello, Piergiorgio and Chen, Hao and Dou, Qi and Fantacci, Maria Evelina and Geurts, Bram and van der Gugten, Robbert and Heng, Pheng Ann and Jansen, Bart and de Kaste, Michael M.J. and Kotov, Valentin and Lin, Jack Yu Hung and Manders, Jeroen T.M.C. and S{\'{o}}{\~{n}}ora-Mengana, Alexander and Garc{\'{i}}a-Naranjo, Juan Carlos and Papavasileiou, Evgenia and Prokop, Mathias and Saletta, Marco and Schaefer-Prokop, Cornelia M. and Scholten, Ernst T. and Scholten, Luuk and Snoeren, Miranda M. and Torres, Ernesto Lopez and Vandemeulebroucke, Jef and Walasek, Nicole and Zuidhof, Guido C.A. and van Ginneken, Bram and Jacobs, Colin},
doi = {10.1016/j.media.2017.06.015},
eprint = {1612.08012},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Computed tomography,Computer-aided detection,Convolutional networks,Deep learning,Medical image challenges,Pulmonary nodules},
month = {dec},
pages = {1--13},
pmid = {28732268},
title = {{Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: The LUNA16 challenge}},
url = {http://arxiv.org/abs/1612.08012 http://dx.doi.org/10.1016/j.media.2017.06.015},
volume = {42},
year = {2017}
}
@inproceedings{Sharma2016,
abstract = {With the advent of affordable depth sensors, 3D capture becomes more and more ubiquitous and already has made its way into commercial products. Yet, capturing the geometry or complete shapes of everyday objects using scanning devices (e.g. Kinect) still comes with several challenges that result in noise or even incomplete shapes. Recent success in deep learning has shown how to learn complex shape distributions in a data-driven way from large scale 3D CAD Model collections and to utilize them for 3D processing on volumetric representations and thereby circumventing problems of topology and tessellation. Prior work has shown encouraging results on problems ranging from shape completion to recognition.We provide an analysis of such approaches and discover that training as well as the resulting representation are strongly and unnecessarily tied to the notion of object labels. Thus, we propose a full convolutional volumetric auto encoder that learns volumetric representation from noisy data by estimating the voxel occupancy grids. The proposed method outperforms prior work on challenging tasks like denoising and shape completion. We also show that the obtained deep embedding gives competitive performance when used for classification and promising results for shape interpolation.},
annote = {{\_}eprint: 1604.03755},
archivePrefix = {arXiv},
arxivId = {1604.03755},
author = {Sharma, Abhishek and Grau, Oliver and Fritz, Mario},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-49409-8_20},
eprint = {1604.03755},
isbn = {9783319494081},
issn = {16113349},
keywords = {3D deep learning,Denoising auto-encoder,Shape blending,Shape completion},
pages = {236--250},
title = {{VConv-DAE: Deep volumetric shape learning without object labels}},
url = {http://arxiv.org/abs/1604.03755},
volume = {9915 LNCS},
year = {2016}
}
@article{Sekuboyina2020,
abstract = {In this paper we report the challenge set-up and results of the Large Scale Vertebrae Segmentation Challenge (VerSe) organized in conjunction with the MICCAI 2019. The challenge consisted of two tasks, vertebrae labelling and vertebrae segmentation. For this a total of 160 multidetector CT scan cohort closely resembling clinical setting was prepared and was annotated at a voxel-level by a human-machine hybrid algorithm. In this paper we also present the annotation protocol and the algorithm that aided the medical experts in the annotation process. Eleven fully automated algorithms were benchmarked on this data with the best performing algorithm achieving a vertebrae identification rate of 95{\%} and a Dice coefficient of 90{\%}. VerSe'19 is an open-call challenge at its image data along with the annotations and evaluation tools will continue to be publicly accessible through its online portal.},
archivePrefix = {arXiv},
arxivId = {2001.09193},
author = {Sekuboyina, Anjany and Bayat, Amirhossein and Husseini, Malek E. and L{\"{o}}ffler, Maximilian and Rempfler, Markus and Kuka{\v{c}}ka, Jan and Tetteh, Giles and Valentinitsch, Alexander and Payer, Christian and Urschler, Martin and Chen, Maodong and Cheng, Dalong and Lessmann, Nikolas and Hu, Yujin and Wang, Tianfu and Yang, Dong and Xu, Daguang and Ambellan, Felix and Zachow, Stefan and Jiang, Tao and Ma, Xinjun and Angerman, Christoph and Wang, Xin and Wei, Qingyue and Brown, Kevin and Wolf, Matthias and Kirszenberg, Alexandre and Puybareau, {\'{E}}lodie and Menze, Bj{\"{o}}rn H. and Kirschke, S.},
eprint = {2001.09193},
journal = {arXiv},
keywords = {Computed tomography,Spine segmentation,Vertebrae detection and localization,Vertebrae labelling,Vertebrae segmentation},
month = {jan},
title = {{VerSe: A vertebrae labelling and segmentation benchmark}},
url = {http://arxiv.org/abs/2001.09193},
year = {2020}
}
@inproceedings{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
annote = {{\_}eprint: 1409.1556},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1409.1556},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2015}
}
@article{Geiger2013,
abstract = {We present a novel dataset captured from a VW station wagon for use in mobile robotics and autonomous driving research. In total, we recorded 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system. The scenarios are diverse, capturing real-world traffic situations, and range from freeways over rural areas to inner-city scenes with many static and dynamic objects. Our data is calibrated, synchronized and timestamped, and we provide the rectified and raw image sequences. Our dataset also contains object labels in the form of 3D tracklets, and we provide online benchmarks for stereo, optical flow, object detection and other tasks. This paper describes our recording platform, the data format and the utilities that we provide. {\textcopyright} The Author(s) 2013.},
author = {Geiger, A. and Lenz, P. and Stiller, C. and Urtasun, R.},
doi = {10.1177/0278364913491297},
issn = {02783649},
journal = {International Journal of Robotics Research},
keywords = {Dataset,GPS,KITTI,SLAM,autonomous driving,benchmarks,cameras,computer vision,field robotics,laser,mobile robotics,object detection,optical flow,stereo,tracking},
number = {11},
pages = {1231--1237},
title = {{Vision meets robotics: The KITTI dataset}},
volume = {32},
year = {2013}
}
@misc{Bouget2017,
abstract = {In recent years, tremendous progress has been made in surgical practice for example with Minimally Invasive Surgery (MIS). To overcome challenges coming from deported eye-to-hand manipulation, robotic and computer-assisted systems have been developed. Having real-time knowledge of the pose of surgical tools with respect to the surgical camera and underlying anatomy is a key ingredient for such systems. In this paper, we present a review of the literature dealing with vision-based and marker-less surgical tool detection. This paper includes three primary contributions: (1) identification and analysis of data-sets used for developing and testing detection algorithms, (2) in-depth comparison of surgical tool detection methods from the feature extraction process to the model learning strategy and highlight existing shortcomings, and (3) analysis of validation techniques employed to obtain detection performance results and establish comparison between surgical tool detectors. The papers included in the review were selected through PubMed and Google Scholar searches using the keywords: “surgical tool detection”, “surgical tool tracking”, “surgical instrument detection” and “surgical instrument tracking” limiting results to the year range 2000 2015. Our study shows that despite significant progress over the years, the lack of established surgical tool data-sets, and reference format for performance assessment and method ranking is preventing faster improvement.},
author = {Bouget, David and Allan, Max and Stoyanov, Danail and Jannin, Pierre},
booktitle = {Medical Image Analysis},
doi = {10.1016/j.media.2016.09.003},
issn = {13618423},
keywords = {Data-set,Endoscopic/microscopic images,Object detection,Tool detection,Validation},
pages = {633--654},
pmid = {27744253},
title = {{Vision-based and marker-less surgical tool detection and tracking: a review of the literature}},
volume = {35},
year = {2017}
}
@article{Hohman2019,
abstract = {Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W's and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.},
archivePrefix = {arXiv},
arxivId = {1801.06889},
author = {Hohman, Fred and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
doi = {10.1109/TVCG.2018.2843369},
eprint = {1801.06889},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Deep learning,information visualization,neural networks,visual analytics},
number = {8},
pages = {2674--2693},
title = {{Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers}},
url = {http://www.sciencedirect.com/science/article/pii/S1361841519300829},
volume = {25},
year = {2019}
}
@article{Miyawaki2008,
abstract = {Perceptual experience consists of an enormous number of possible states. Previous fMRI studies have predicted a perceptual state by classifying brain activity into prespecified categories. Constraint-free visual image reconstruction is more challenging, as it is impractical to specify brain activity for all possible images. In this study, we reconstructed visual images by combining local image bases of multiple scales, whose contrasts were independently decoded from fMRI activity by automatically selecting relevant voxels and exploiting their correlated patterns. Binary-contrast, 10 × 10-patch images (2100 possible states) were accurately reconstructed without any image prior on a single trial or volume basis by measuring brain activity only for several hundred random images. Reconstruction was also used to identify the presented image among millions of candidates. The results suggest that our approach provides an effective means to read out complex perceptual states from brain activity while discovering information representation in multivoxel patterns. {\textcopyright} 2008 Elsevier Inc. All rights reserved.},
author = {Miyawaki, Yoichi and Uchida, Hajime and Yamashita, Okito and aki Sato, Masa and Morito, Yusuke and Tanabe, Hiroki C. and Sadato, Norihiro and Kamitani, Yukiyasu},
doi = {10.1016/j.neuron.2008.11.004},
issn = {08966273},
journal = {Neuron},
keywords = {SYSNEURO},
month = {dec},
number = {5},
pages = {915--929},
pmid = {19081384},
title = {{Visual Image Reconstruction from Human Brain Activity using a Combination of Multiscale Local Image Decoders}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627308009586},
volume = {60},
year = {2008}
}
@misc{Zhang2018,
abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, interpretability is always Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of a low interpretability of their black-box representations. We believe that high model interpretability may help people break several bottlenecks of deep learning, e.g., learning from a few annotations, learning via human–computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
annote = {{\_}eprint: 1802.00614},
archivePrefix = {arXiv},
arxivId = {1802.00614},
author = {shi Zhang, Quan and chun Zhu, Song},
booktitle = {Frontiers of Information Technology and Electronic Engineering},
doi = {10.1631/FITEE.1700808},
eprint = {1802.00614},
issn = {20959230},
keywords = {Artificial intelligence,Deep learning,Interpretable model},
number = {1},
pages = {27--39},
title = {{Visual interpretability for deep learning: a survey}},
volume = {19},
year = {2018}
}
@article{Wu2016,
abstract = {Visual Question Answering (VQA) is a challenging task that has received increasing attention from both the computer vision and the natural language processing communities. Given an image and a question in natural language, it requires reasoning over visual elements of the image and general knowledge to infer the correct answer. In the first part of this survey, we examine the state of the art by comparing modern approaches to the problem. We classify methods by their mechanism to connect the visual and textual modalities. In particular, we examine the common approach of combining convolutional and recurrent neural networks to map images and questions to a common feature space. We also discuss memory-augmented and modular architectures that interface with structured knowledge bases. In the second part of this survey, we review the datasets available for training and evaluating VQA systems. The various datatsets contain questions at different levels of complexity, which require different capabilities and types of reasoning. We examine in depth the question/answer pairs from the Visual Genome project, and evaluate the relevance of the structured annotations of images with scene graphs for VQA. Finally, we discuss promising future directions for the field, in particular the connection to structured knowledge bases and the use of natural language processing models.},
archivePrefix = {arXiv},
arxivId = {1607.05910},
author = {Wu, Qi and Teney, Damien and Wang, Peng and Shen, Chunhua and Dick, Anthony and van den Hengel, Anton},
doi = {10.1016/j.cviu.2017.05.001},
eprint = {1607.05910},
issn = {1090235X},
journal = {Computer Vision and Image Understanding},
keywords = {Knowledge bases,Natural language processing,Recurrent neural networks,Visual question answering},
month = {jul},
pages = {21--40},
title = {{Visual question answering: A survey of methods and datasets}},
url = {http://arxiv.org/abs/1607.05910},
volume = {163},
year = {2017}
}
@article{Smeulders2014,
abstract = {There is a large variety of trackers, which have been proposed in the literature during the last two decades with some mixed success. Object tracking in realistic scenarios is a difficult problem, therefore, it remains a most active area of research in computer vision. A good tracker should perform well in a large number of videos involving illumination changes, occlusion, clutter, camera motion, low contrast, specularities, and at least six more aspects. However, the performance of proposed trackers have been evaluated typically on less than ten videos, or on the special purpose datasets. In this paper, we aim to evaluate trackers systematically and experimentally on 315 video fragments covering above aspects. We selected a set of nineteen trackers to include a wide variety of algorithms often cited in literature, supplemented with trackers appearing in 2010 and 2011 for which the code was publicly available. We demonstrate that trackers can be evaluated objectively by survival curves, Kaplan Meier statistics, and Grubs testing. We find that in the evaluation practice the F-score is as effective as the object tracking accuracy (OTA) score. The analysis under a large variety of circumstances provides objective insight into the strengths and weaknesses of trackers. {\textcopyright} 2014 IEEE.},
author = {Smeulders, Arnold W.M. and Chu, Dung M. and Cucchiara, Rita and Calderara, Simone and Dehghan, Afshin and Shah, Mubarak},
doi = {10.1109/TPAMI.2013.230},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Camera surveillance,Computer vision,Image processing,Object tracking,Tracking dataset,Tracking evaluation,Video understanding},
number = {7},
pages = {1442--1468},
publisher = {IEEE Computer Society},
title = {{Visual tracking: An experimental survey}},
volume = {36},
year = {2014}
}
@inproceedings{Milletari2016,
abstract = {Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.},
annote = {{\_}eprint: 1606.04797},
archivePrefix = {arXiv},
arxivId = {1606.04797},
author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed Ahmad},
booktitle = {Proceedings - 2016 4th International Conference on 3D Vision, 3DV 2016},
doi = {10.1109/3DV.2016.79},
eprint = {1606.04797},
isbn = {9781509054077},
keywords = {Deep learning,convolutional neural networks,machine learning,prostate,segmentation},
pages = {565--571},
title = {{V-Net: Fully convolutional neural networks for volumetric medical image segmentation}},
year = {2016}
}
@inproceedings{Engelcke2017,
abstract = {This paper proposes a computationally efficient approach to detecting objects natively in 3D point clouds using convolutional neural networks (CNNs). In particular, this is achieved by leveraging a feature-centric voting scheme to implement novel convolutional layers which explicitly exploit the sparsity encountered in the input. To this end, we examine the trade-off between accuracy and speed for different architectures and additionally propose to use an L1 penalty on the filter activations to further encourage sparsity in the intermediate representations. To the best of our knowledge, this is the first work to propose sparse convolutional layers and L1 regularisation for efficient large-scale processing of 3D data. We demonstrate the efficacy of our approach on the KITTI object detection benchmark and show that VoteSDeep models with as few as three layers outperform the previous state of the art in both laser and laser-vision based approaches by margins of up to 40{\%} while remaining highly competitive in terms of processing time.},
archivePrefix = {arXiv},
arxivId = {1609.06666},
author = {Engelcke, Martin and Rao, Dushyant and Wang, Dominic Zeng and Tong, Chi Hay and Posner, Ingmar},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2017.7989161},
eprint = {1609.06666},
isbn = {9781509046331},
issn = {10504729},
pages = {1355--1361},
title = {{Vote3Deep: Fast object detection in 3D point clouds using efficient convolutional neural networks}},
url = {https://doi.org/10.1109/ICRA.2017.7989161},
year = {2017}
}
@inproceedings{Maturana2015,
abstract = {Robust object recognition is a crucial skill for robots operating autonomously in real world environments. Range sensors such as LiDAR and RGBD cameras are increasingly found in modern robotic systems, providing a rich source of 3D information that can aid in this task. However, many current systems do not fully utilize this information and have trouble efficiently dealing with large amounts of point cloud data. In this paper, we propose VoxNet, an architecture to tackle this problem by integrating a volumetric Occupancy Grid representation with a supervised 3D Convolutional Neural Network (3D CNN). We evaluate our approach on publicly available benchmarks using LiDAR, RGBD, and CAD data. VoxNet achieves accuracy beyond the state of the art while labeling hundreds of instances per second.},
author = {Maturana, Daniel and Scherer, Sebastian},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2015.7353481},
isbn = {9781479999941},
issn = {21530866},
pages = {922--928},
title = {{VoxNet: A 3D Convolutional Neural Network for real-time object recognition}},
url = {https://doi.org/10.1109/IROS.2015.7353481},
volume = {2015-Decem},
year = {2015}
}
@inproceedings{Wang2018WeaklySL,
abstract = {Histopathology image analysis serves as the gold standard for cancer diagnosis. Efficient and precise diagnosis is quite critical for the subsequent therapeutic treatment of patients. So far, computer-aided diagnosis has not been widely applied in pathological field yet as currently well-addressed tasks are only the tip of the iceberg. Whole slide image (WSI) classification is a quite challenging problem. First, the scarcity of annotations heavily impedes the pace of developing effective approaches. Pixelwise delineated annotations on WSIs are time consuming and tedious, which poses difficulties in building a large-scale training dataset. In addition, a variety of heterogeneous patterns of tumor existing in high magnification field are actually the major obstacle. Furthermore, a gigapixel scale WSI cannot be directly analyzed due to the immeasurable computational cost. How to design the weakly supervised learning methods to maximize the use of available WSI-level labels that can be readily obtained in clinical practice is quite appealing. To overcome these challenges, we present a weakly supervised approach in this article for fast and effective classification on the whole slide lung cancer images. Our method first takes advantage of a patch-based fully convolutional network (FCN) to retrieve discriminative blocks and provides representative deep features with high efficiency. Then, different context-aware block selection and feature aggregation strategies are explored to generate globally holistic WSI descriptor which is ultimately fed into a random forest (RF) classifier for the image-level prediction. To the best of our knowledge, this is the first study to exploit the potential of image-level labels along with some coarse annotations for weakly supervised learning. A large-scale lung cancer WSI dataset is constructed in this article for evaluation, which validates the effectiveness and feasibility of the proposed method. Extensive experiments demonstrate the superior performance of our method that surpasses the state-of-the-art approaches by a significant margin with an accuracy of 97.3{\%}. In addition, our method also achieves the best performance on the public lung cancer WSIs dataset from The Cancer Genome Atlas (TCGA). We highlight that a small number of coarse annotations can contribute to further accuracy improvement. We believe that weakly supervised learning methods have great potential to assist pathologists in histology image diagnosis in the near future.},
author = {Wang, Xi and Chen, Hao and Gan, Caixia and Lin, Huangjing and Dou, Qi and Tsougenis, Efstratios and Huang, Qitao and Cai, Muyan and Heng, Pheng Ann},
booktitle = {IEEE Transactions on Cybernetics},
doi = {10.1109/TCYB.2019.2935141},
issn = {21682275},
keywords = {Deep learning,histology image analysis,weakly supervised learning,whole slide images (WSIs)},
month = {sep},
number = {9},
pages = {3950--3962},
pmid = {31484154},
title = {{Weakly Supervised Deep Learning for Whole Slide Lung Cancer Image Analysis}},
url = {https://ieeexplore.ieee.org/document/8822590/},
volume = {50},
year = {2020}
}
@article{Li2019,
abstract = {Developing new deep learning methods for medical image analysis is a prevalent research topic in machine learning. In this paper, we propose a deep learning scheme with a novel loss function for weakly supervised breast cancer diagnosis. According to the Nottingham Grading System, mitotic count plays an important role in breast cancer diagnosis and grading. To determine the cancer grade, pathologists usually need to manually count mitosis from a great deal of histopathology images, which is a very tedious and time-consuming task. This paper proposes an automatic method for detecting mitosis. We regard the mitosis detection task as a semantic segmentation problem and use a deep fully convolutional network to address it. Different from conventional training data used in semantic segmentation system, the training label of mitosis data is usually in the format of centroid pixel, rather than all the pixels belonging to a mitosis. The centroid label is a kind of weak label, which is much easier to annotate and can save the effort of pathologists a lot. However, technically this weak label is not sufficient for training a mitosis segmentation model. To tackle this problem, we expand the single-pixel label to a novel label with concentric circles, where the inside circle is a mitotic region and the ring around the inside circle is a “middle ground”. During the training stage, we do not compute the loss of the ring region because it may have the presence of both mitotic and non-mitotic pixels. This new loss termed as “concentric loss” is able to make the semantic segmentation network be trained with the weakly annotated mitosis data. On the generated segmentation map from the segmentation model, we filter out low confidence and obtain mitotic cells. On the challenging ICPR 2014 MITOSIS dataset and AMIDA13 dataset, we achieve a 0.562 F-score and 0.673 F-score respectively, outperforming all previous approaches significantly. On the latest TUPAC16 dataset, we obtain a F-score of 0.669, which is also the state-of-the-art result. The excellent results quantitatively demonstrate the effectiveness of the proposed mitosis segmentation network with the concentric loss. All of our code has been made publicly available at https://github.com/ChaoLi977/SegMitos{\_}mitosis{\_}detection.},
author = {Li, Chao and Wang, Xinggang and Liu, Wenyu and Latecki, Longin Jan and Wang, Bo and Huang, Junzhou},
doi = {10.1016/j.media.2019.01.013},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Breast cancer grading,Fully convolutional network,Mitosis detection,Weakly supervised learning},
pages = {165--178},
pmid = {30798116},
title = {{Weakly supervised mitosis detection in breast histopathology images using concentric loss}},
volume = {53},
year = {2019}
}
@book{Mardan2019,
address = {Berkeley, CA},
author = {Mardan, Azat},
booktitle = {Write Your Way To Success},
doi = {10.1007/978-1-4842-3970-4},
isbn = {978-1-4842-3969-8},
publisher = {Apress},
title = {{Write Your Way To Success}},
url = {http://link.springer.com/10.1007/978-1-4842-3970-4},
year = {2019}
}
@book{Blair2016,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
address = {Rotterdam},
author = {Blair, Lorrie},
booktitle = {Writing a Graduate Thesis or Dissertation},
doi = {10.1007/978-94-6300-426-8},
isbn = {978-94-6300-426-8},
publisher = {SensePublishers},
title = {{Writing a Graduate Thesis or Dissertation}},
url = {http://link.springer.com/10.1007/978-94-6300-426-8},
year = {2016}
}
@book{Parija2017,
abstract = {This book covers all essential aspects of writing scientific research articles, presenting eighteen carefully selected titles that offer essential, "must-know" content on how to write high-quality articles. The book also addresses other, rarely discussed areas of scientific writing including dealing with rejected manuscripts, the reviewer's perspective as to what they expect in a scientific article, plagiarism, copyright issues, and ethical standards in publishing scientific papers. Simplicity is the book's hallmark, and it aims to provide an accessible, comprehensive and essential resource for those seeking guidance on how to publish their research work. The importance of publishing research work cannot be overemphasized. However, a major limitation in publishing work in a scientific journal is the lack of information on or experience with scientific writing and publishing. Young faculty and trainees who are starting their research career are in need of a comprehensive guide that provides all essential components of scientific writing and aids them in getting their research work published.},
address = {Singapore},
author = {Parija, Subhash Chandra and Kate, Vikram},
booktitle = {Writing and Publishing a Scientific Research Paper},
doi = {10.1007/978-981-10-4720-6},
editor = {Parija, Subhash Chandra and Kate, Vikram},
isbn = {9789811047206},
pages = {1--195},
publisher = {Springer Singapore},
title = {{Writing and publishing a scientific research paper}},
url = {http://link.springer.com/10.1007/978-981-10-4720-6},
year = {2017}
}
@book{Englander2014,
abstract = {This book provides a comprehensive review of the current knowledge on writing and publishing scientific research papers and the social contexts. It deals with both English and non-Anglophone science writers, and presents a global perspective and an international focus. The book collects and synthesizes research from a range of disciplines, including applied linguistics, the sociology of science, sociolinguistics, bibliometrics, composition studies, and science education. This multidisciplinary approach helps the reader gain a solid understanding of the subject. Divided into three parts, the book considers the context of scientific papers, the text itself, and the people involved. It explains how the typical sections of scientific papers are structured. Standard English scientific writing style is also compared with science papers written in other languages. The book discusses the strengths and challenges faced by people with different degrees of science writing expertise and the role of journal editors and reviewers. The Rise of English as the Language of Science -- Measuring the Impact of Articles, Journals and Nations -- English Competence, Funds for Research, and Publishing Success -- Collaborations, Teams and Networks -- The Scientific Research Article and the Creation of Science -- Varieties of Science Texts -- Structure of the Research Article in the Creation of Knowledge -- Writing the Five Principal Sections: Abstract, Introduction, Methods, Results and Discussion -- Variations in Different Languages and Cultures -- Graduate Students Becoming Scientists -- Novice Scientists and Expert Scientists -- English-Speaking Scientists and Multilingual Scientists -- Gatekeepers, Guardians and Allies -- Afterword: Negotiating Research Article Writing and Publication.},
address = {Dordrecht},
author = {Mu, Congjun},
booktitle = {English for Specific Purposes},
doi = {10.1016/j.esp.2014.12.004},
isbn = {9789400777149},
issn = {08894906},
pages = {76--78},
publisher = {Springer Netherlands},
series = {SpringerBriefs in Education},
title = {{Writing and publishing science research papers in english: A global perspective}},
url = {http://link.springer.com/10.1007/978-94-007-7714-9},
volume = {39},
year = {2015}
}
@book{Zobel2014,
abstract = {The elements of good writing are an essential part of success in science. With comprehensive practical help for students and experienced researchers, Writing for Computer Science: - Gives extensive guidance for writing style and editing; - Presents sound practice for graphs, figures, and tables; - Guides the presentation of mathematics, algorithms and experiments; - Shows how to assemble research materials into a technical paper; - Offers guidelines and advice on spoken presentations. This second edition contains detailed new material on research methods, the how-to of being a scientist, including: - Development of ideas into research programs; -Design and evaluation of experiments; - How to search for, read, evaluate, and referee other research; - Research ethics and the qualities that separate good and bad science. Writing for Computer Science is not only an introduction to the doing and describing of research, but is a handy reference for working scientists in computing and mathematical sciences.},
address = {London},
author = {Zobel, Justin},
booktitle = {Writing for Computer Science},
doi = {10.1007/978-1-4471-6639-9},
isbn = {978-1-4471-6638-2},
publisher = {Springer London},
title = {{Writing for Computer Science}},
url = {http://link.springer.com/10.1007/978-1-4471-6639-9},
year = {2014}
}
@book{RenckJalongo2016,
address = {Cham},
author = {Haight, Kristina},
booktitle = {Home Healthcare Nurse},
doi = {10.1097/00004045-198805000-00015},
isbn = {978-3-319-31648-2},
issn = {15390713},
number = {3},
pages = {39},
publisher = {Springer International Publishing},
series = {Springer Texts in Education},
title = {{Writing for Publication}},
url = {http://link.springer.com/10.1007/978-3-319-31650-5},
volume = {6},
year = {1988}
}
@book{Myatt2017,
abstract = {This book demonstrates how to develop and engage in successful academic collaborations that are both practical and sustainable across campuses and within local communities. Authored by experienced writing program administrators, this edited collection includes a wide range of information addressing collaborative partnerships and projects, theoretical explorations of collaborative praxis, and strategies for sustaining collaborative initiatives. Contributors offer case studies of writing program collaborations and honestly address both the challenges of academic collaboration and the hallmarks of successful partnerships.},
address = {New York},
author = {Myatt, Alice Johnston and Gaillet, Lyn{\'{e}}e Lewis},
booktitle = {Writing Program and Writing Center Collaborations: Transcending Boundaries},
doi = {10.1057/978-1-137-59932-2},
editor = {Myatt, Alice Johnston and Gaillet, Lyn{\'{e}}e Lewis},
isbn = {9781137599322},
pages = {1--275},
publisher = {Palgrave Macmillan US},
title = {{Writing program and writing center collaborations: Transcending boundaries}},
url = {http://link.springer.com/10.1057/978-1-137-59932-2},
year = {2016}
}
@book{Greenshields2017,
abstract = {W. Greenshields, Writing the Structures of the Subject,
The Palgrave Lacan Series, DOI 10.1007/978-3-319-47533-2{\_}3

This book examines and explores Jacques Lacan's controversial topologisation of psychoanalysis, and seeks to persuade the reader that this enterprise was necessary and important. In providing both an introduction to a fundamental component of Lacan's theories, as well as readings of texts that have been largely ignored, it provides a thorough critical interpretation of his work. Will Greenshields argues that Lacan achieved his most pedagogically clear and successful presentations of his most essential and notoriously complex concepts – such as structure, the subject and the real – through the deployment of topology. The book will help readers to better understand Lacan, and also those concepts that have become prevalent in various intellectual discourses such as contemporary continental philosophy, politics and the study of ideology, and literary or cultural criticism.


},
address = {Cham},
author = {Greenshields, Will},
booktitle = {Writing the Structures of the Subject},
doi = {10.1007/978-3-319-47533-2},
isbn = {978-3-319-47532-5},
publisher = {Springer International Publishing},
title = {{Writing the Structures of the Subject}},
url = {http://link.springer.com/10.1007/978-3-319-47533-2},
year = {2017}
}
@article{Yang2019,
abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.},
annote = {{\_}eprint: 1906.08237},
archivePrefix = {arXiv},
arxivId = {1906.08237},
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
eprint = {1906.08237},
journal = {arXiv},
title = {{XLNet: Generalized autoregressive pretraining for language understanding}},
url = {http://arxiv.org/abs/1906.08237},
year = {2019}
}
@incollection{Lampert2020,
address = {Cham},
author = {Lampert, Christoph H.},
booktitle = {Computer Vision},
doi = {10.1007/978-3-030-03243-2_874-1},
pages = {1--3},
publisher = {Springer International Publishing},
title = {{Zero-Shot Learning}},
url = {http://link.springer.com/10.1007/978-3-030-03243-2{\_}874-1},
year = {2020}
}
@article{Rezaei2020,
abstract = {The challenge of learning a new concept without receiving any examples beforehand is called zero-shot learning (ZSL). One of the major issues in deep learning based methodologies is the requirement of feeding a vast amount of annotated and labelled images by a human to train the network model. ZSL is known for having minimal human intervention by relying only on previously known concepts and auxiliary information. It is an ever-growing research area since it has human-like characteristics in learning new concepts, which makes it applicable in many real-world scenarios, from autonomous vehicles to surveillance systems to medical imaging and COVID-19 CT scan-based diagnosis. In this paper, we present the definition of the problem, we review over fundamentals, and the challenging steps of Zero-shot learning, including recent categories of solutions, motivations behind each approach, and their advantages over other categories. Inspired from different settings and extensions, we have a broaden solution called one/few-shot learning. We then review thorough datasets, the variety of splits, and the evaluation protocols proposed so far. Finally, we discuss the recent applications and possible future directions of ZSL. We aim to convey a useful intuition through this paper towards the goal of handling computer vision learning tasks more similar to the way humans learn.},
archivePrefix = {arXiv},
arxivId = {2004.14143},
author = {Rezaei, Mahdi and Shahidi, Mahsa},
doi = {10.1016/j.ibmed.2020.100005},
eprint = {2004.14143},
issn = {26665212},
journal = {arXiv},
keywords = {Autonomous Vehicles,COVID-19 Pandemic,Deep Learning,Machine Learning,Semantic Embedding,Supervised Annotation,Zero-shot learning},
month = {apr},
title = {{Zero-Shot Learning and Its Applications From Autonomous Vehicles to Covid-19 Diagnosis: A Review}},
url = {http://arxiv.org/abs/2004.14143},
year = {2020}
}
@article{Stoeckel1970,
author = {Stoeckel, H. and Stober, B.},
issn = {00443387},
journal = {Zeitschrift fur praktische Anasthesie und Wiederbelebung},
month = {aug},
number = {4},
pages = {237--250},
pmid = {4256233},
title = {{Zur Problematik der Massivtransfusion mit ACD-Blut.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/4256233},
volume = {5},
year = {1970}
}
