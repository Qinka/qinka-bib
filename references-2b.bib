
@inproceedings{cai_efficient_2017,
	title = {Efficient {Architecture} {Search} by {Network} {Transformation}},
	booktitle = {{AAAI}},
	author = {Cai, Han and Chen, Tianyao and Zhang, Weinan and Yu, Yong and Wang, Jun},
	year = {2017}
}

@article{brock_smash_2017,
	title = {{SMASH}: {One}-{Shot} {Model} {Architecture} {Search} through {HyperNetworks}},
	volume = {abs/1708.05344},
	journal = {ArXiv},
	author = {Brock, Andrew and Lim, Theodore and Ritchie, James M. and Weston, Nick},
	year = {2017}
}

@article{zoph_neural_2016,
	title = {Neural {Architecture} {Search} with {Reinforcement} {Learning}},
	volume = {abs/1611.01578},
	journal = {ArXiv},
	author = {Zoph, Barret and Le, Quoc V.},
	year = {2016}
}

@article{pham_efficient_2018,
	title = {Efficient {Neural} {Architecture} {Search} via {Parameter} {Sharing}},
	volume = {abs/1802.03268},
	journal = {ArXiv},
	author = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
	year = {2018}
}

@article{liu_progressive_2017,
	title = {Progressive {Neural} {Architecture} {Search}},
	volume = {abs/1712.00559},
	journal = {ArXiv},
	author = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin L.},
	year = {2017}
}

@article{liu_darts_2018,
	title = {{DARTS}: {Differentiable} {Architecture} {Search}},
	volume = {abs/1806.09055},
	journal = {ArXiv},
	author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
	year = {2018}
}

@article{zhong_practical_2017,
	title = {Practical {Block}-{Wise} {Neural} {Network} {Architecture} {Generation}},
	journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	author = {Zhong, Zhao and Yan, Junjie and Wu, Wei and Shao, Jing and Liu, Cheng-Lin},
	year = {2017},
	pages = {2423--2432}
}

@article{zoph_learning_2017,
	title = {Learning {Transferable} {Architectures} for {Scalable} {Image} {Recognition}},
	journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	year = {2017},
	pages = {8697--8710}
}

@article{liu_hierarchical_2017,
	title = {Hierarchical {Representations} for {Efficient} {Architecture} {Search}},
	volume = {abs/1711.00436},
	journal = {ArXiv},
	author = {Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
	year = {2017}
}

@inproceedings{jin_auto-keras_2018,
	title = {Auto-{Keras}: {An} {Efficient} {Neural} {Architecture} {Search} {System}},
	booktitle = {{KDD}},
	author = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
	year = {2018}
}

@inproceedings{wei_network_2016,
	title = {Network {Morphism}},
	booktitle = {{ICML}},
	author = {Wei, Tao and Wang, Changhu and Rui, Yong and Chen, Chang Wen},
	year = {2016}
}

@article{chen_net2net_2015,
	title = {{Net2Net}: {Accelerating} {Learning} via {Knowledge} {Transfer}},
	volume = {abs/1511.05641},
	journal = {CoRR},
	author = {Chen, Tianqi and Goodfellow, Ian J. and Shlens, Jonathon},
	year = {2015}
}

@article{baker_designing_2016,
	title = {Designing {Neural} {Network} {Architectures} using {Reinforcement} {Learning}},
	volume = {abs/1611.02167},
	journal = {ArXiv},
	author = {Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
	year = {2016}
}

@inproceedings{real_regularized_2018,
	title = {Regularized {Evolution} for {Image} {Classifier} {Architecture} {Search}},
	booktitle = {{AAAI}},
	author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
	year = {2018}
}

@inproceedings{real_large-scale_2017,
	title = {Large-{Scale} {Evolution} of {Image} {Classifiers}},
	booktitle = {{ICML}},
	author = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc V. and Kurakin, Alexey},
	year = {2017}
}

@article{stanley_evolving_2001,
	title = {Evolving {Neural} {Networks} through {Augmenting} {Topologies}},
	volume = {10},
	journal = {Evolutionary Computation},
	author = {Stanley, Kenneth O. and Miikkulainen, Risto},
	year = {2001},
	pages = {99--127}
}

@article{klein_fast_2016,
	title = {Fast {Bayesian} {Optimization} of {Machine} {Learning} {Hyperparameters} on {Large} {Datasets}},
	volume = {abs/1605.07079},
	journal = {ArXiv},
	author = {Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
	year = {2016}
}

@article{zela_towards_2018,
	title = {Towards {Automated} {Deep} {Learning}: {Efficient} {Joint} {Neural} {Architecture} and {Hyperparameter} {Search}},
	volume = {abs/1807.06906},
	journal = {ArXiv},
	author = {Zela, Arber and Klein, Aaron and Falkner, Stefan and Hutter, Frank},
	year = {2018}
}

@inproceedings{mendoza_towards_2016,
	title = {Towards {Automatically}-{Tuned} {Neural} {Networks}},
	booktitle = {{AutoML}@{ICML}},
	author = {Mendoza, Hector and Klein, Aaron and Feurer, Matthias and Springenberg, Jost Tobias and Hutter, Frank},
	year = {2016}
}

@article{bergstra_random_2012,
	title = {Random search for hyper-parameter optimization},
	volume = {13},
	number = {1},
	journal = {Journal of Machine Learning Research},
	author = {Bergstra, James and Bengio, Yoshua},
	year = {2012},
	pages = {281--305}
}

@article{snoek_practical_2012,
	title = {Practical {Bayesian} {Optimization} of {Machine} {Learning} {Algorithms}},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	year = {2012},
	pages = {2951--2959}
}

@article{hutter_sequential_2011,
	title = {Sequential model-based optimization for general algorithm configuration},
	author = {Hutter, Frank and Hoos, Holger H and Leytonbrown, Kevin},
	year = {2011},
	pages = {507--523}
}

@article{flynn_deepstereo_2015,
	title = {{DeepStereo}: {Learning} to {Predict} {New} {Views} from the {World}'s {Imagery}},
	volume = {abs/1506.06825},
	url = {http://arxiv.org/abs/1506.06825},
	journal = {CoRR},
	author = {Flynn, John and Neulander, Ivan and Philbin, James and Snavely, Noah},
	year = {2015},
	note = {\_eprint: 1506.06825}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	volume = {abs/1512.03385},
	url = {http://arxiv.org/abs/1512.03385},
	journal = {CoRR},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
	note = {\_eprint: 1512.03385}
}

@article{ren_faster_2015,
	title = {Faster {R}-{CNN}: {Towards} {Real}-{Time} {Object} {Detection} with {Region} {Proposal} {Networks}},
	volume = {abs/1506.01497},
	url = {http://arxiv.org/abs/1506.01497},
	journal = {CoRR},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross B. and Sun, Jian},
	year = {2015},
	note = {\_eprint: 1506.01497}
}

@article{su_multi-view_2015,
	title = {Multi-view {Convolutional} {Neural} {Networks} for {3D} {Shape} {Recognition}},
	volume = {abs/1505.00880},
	url = {http://arxiv.org/abs/1505.00880},
	journal = {CoRR},
	author = {Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik G.},
	year = {2015},
	note = {\_eprint: 1505.00880}
}

@article{tatarchenko_single-view_2015,
	title = {Single-view to {Multi}-view: {Reconstructing} {Unseen} {Views} with a {Convolutional} {Network}},
	volume = {abs/1511.06702},
	url = {http://arxiv.org/abs/1511.06702},
	journal = {CoRR},
	author = {Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},
	year = {2015},
	note = {\_eprint: 1511.06702}
}

@article{wu_single_2016,
	title = {Single {Image} {3D} {Interpreter} {Network}},
	volume = {abs/1604.08685},
	url = {http://arxiv.org/abs/1604.08685},
	journal = {CoRR},
	author = {Wu, Jiajun and Xue, Tianfan and Lim, Joseph J. and Tian, Yuandong and Tenenbaum, Joshua B. and Torralba, Antonio and Freeman, William T.},
	year = {2016},
	note = {\_eprint: 1604.08685}
}

@article{xie_deep3d_2016,
	title = {{Deep3D}: {Fully} {Automatic} {2D}-to-{3D} {Video} {Conversion} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {abs/1604.03650},
	url = {http://arxiv.org/abs/1604.03650},
	journal = {CoRR},
	author = {Xie, Junyuan and Girshick, Ross B. and Farhadi, Ali},
	year = {2016},
	note = {\_eprint: 1604.03650}
}

@article{zamir_generic_2017,
	title = {Generic {3D} {Representation} via {Pose} {Estimation} and {Matching}},
	volume = {abs/1710.08247},
	url = {http://arxiv.org/abs/1710.08247},
	journal = {CoRR},
	author = {Zamir, Amir Roshan and Wekel, Tilman and Agrawal, Pulkit and Wei, Colin and Malik, Jitendra and Savarese, Silvio},
	year = {2017},
	note = {\_eprint: 1710.08247}
}

@incollection{lecun_generalization_1989,
	title = {Generalization and network design strategies},
	booktitle = {Connectionism in perspective},
	publisher = {Elsevier},
	author = {Lecun, Yann},
	editor = {Pfeifer, R. and Schreter, Z. and Fogelman, F. and Steels, L.},
	year = {1989}
}

@article{ghiasi_laplacian_2016,
	title = {Laplacian {Reconstruction} and {Refinement} for {Semantic} {Segmentation}},
	volume = {abs/1605.02264},
	url = {http://arxiv.org/abs/1605.02264},
	journal = {CoRR},
	author = {Ghiasi, Golnaz and Fowlkes, Charless C.},
	year = {2016},
	note = {\_eprint: 1605.02264}
}

@article{wang_o-cnn_2017,
	title = {O-{CNN}: octree-based convolutional neural networks for {3D} shape analysis},
	volume = {36},
	url = {http://doi.acm.org/10.1145/3072959.3073608},
	doi = {10.1145/3072959.3073608},
	number = {4},
	journal = {ACM Trans. Graph.},
	author = {Wang, Peng-Shuai and Liu, Yang and Guo, Yu-Xiao and Sun, Chun-Yu and Tong, Xin},
	year = {2017},
	pages = {72:1--72:11}
}

@article{riegler_octnet_2016,
	title = {{OctNet}: {Learning} {Deep} {3D} {Representations} at {High} {Resolutions}},
	volume = {abs/1611.05009},
	url = {http://arxiv.org/abs/1611.05009},
	journal = {CoRR},
	author = {Riegler, Gernot and Ulusoy, Ali Osman and Geiger, Andreas},
	year = {2016},
	note = {\_eprint: 1611.05009}
}

@inproceedings{huang_point_2016,
	title = {Point cloud labeling using {3D} {Convolutional} {Neural} {Network}},
	doi = {10.1109/ICPR.2016.7900038},
	booktitle = {2016 23rd {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Huang, Jing and You, Suya},
	year = {2016},
	keywords = {3D convolutional neural network, 3D point cloud labeling scheme, computer vision, data handling, Labeling, neural nets, Neural networks, object recognition, Testing, testing process, Three-dimensional displays, Training, Training data, training process, Two dimensional displays, urban point cloud dataset},
	pages = {2670--2675}
}

@inproceedings{pratikakis_unstructured_2017,
	title = {Unstructured {Point} {Cloud} {Semantic} {Labeling} {Using} {Deep} {Segmentation} {Networks}},
	isbn = {978-3-03868-030-7},
	doi = {10.2312/3dor.20171047},
	booktitle = {Eurographics {Workshop} on {3D} {Object} {Retrieval}},
	publisher = {The Eurographics Association},
	author = {Boulch, Alexandre and Saux, Bertrand Le and Audebert, Nicolas},
	editor = {Pratikakis, Ioannis and Dupont, Florent and Ovsjanikov, Maks},
	year = {2017},
	note = {ISSN: 1997-0471}
}

@article{kamnitsas_efficient_2016,
	title = {Efficient {Multi}-{Scale} {3D} {CNN} with {Fully} {Connected} {CRF} for {Accurate} {Brain} {Lesion} {Segmentation}},
	volume = {abs/1603.05959},
	url = {http://arxiv.org/abs/1603.05959},
	journal = {CoRR},
	author = {Kamnitsas, Konstantinos and Ledig, Christian and Newcombe, Virginia F. J. and Simpson, Joanna P. and Kane, Andrew D. and Menon, David K. and Rueckert, Daniel and Glocker, Ben},
	year = {2016},
	note = {\_eprint: 1603.05959}
}

@article{klokov_escape_2017,
	title = {Escape from {Cells}: {Deep} {Kd}-{Networks} for {The} {Recognition} of {3D} {Point} {Cloud} {Models}},
	volume = {abs/1704.01222},
	url = {http://arxiv.org/abs/1704.01222},
	journal = {CoRR},
	author = {Klokov, Roman and Lempitsky, Victor S.},
	year = {2017},
	note = {\_eprint: 1704.01222}
}

@inproceedings{kim_3d_2013,
	title = {{3D} {Scene} {Understanding} by {Voxel}-{CRF}},
	url = {https://doi.org/10.1109/ICCV.2013.180},
	doi = {10.1109/ICCV.2013.180},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision}, {ICCV} 2013, {Sydney}, {Australia}, {December} 1-8, 2013},
	author = {Kim, Byung-soo and Kohli, Pushmeet and Savarese, Silvio},
	year = {2013},
	pages = {1425--1432}
}

@book{noauthor_ieee_2013,
	title = {{IEEE} {International} {Conference} on {Computer} {Vision}, {ICCV} 2013, {Sydney}, {Australia}, {December} 1-8, 2013},
	isbn = {978-1-4799-2839-2},
	url = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6750807},
	publisher = {IEEE Computer Society},
	year = {2013}
}

@inproceedings{maturana_voxnet_2015,
	title = {{VoxNet}: {A} {3D} {Convolutional} {Neural} {Network} for real-time object recognition},
	url = {https://doi.org/10.1109/IROS.2015.7353481},
	doi = {10.1109/IROS.2015.7353481},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}, {IROS} 2015, {Hamburg}, {Germany}, {September} 28 - {October} 2, 2015},
	author = {Maturana, Daniel and Scherer, Sebastian},
	year = {2015},
	pages = {922--928}
}

@incollection{tran_deep_2016,
	address = {United States},
	title = {Deep {End2End} {Voxel2Voxel} {Prediction}},
	abstract = {Over the last few years deep learning methods have emerged as one of the most prominent approaches for video analysis. However, so far their most successful applications have been in the area of video classification and detection, i.e., problems involving the prediction of a single class label or a handful of output variables per video. Furthermore, while deep networks are commonly recognized as the best models to use in these domains, there is a widespread perception that in order to yield successful results they often require time-consuming architecture search, manual tweaking of parameters and computationally intensive preprocessing or post-processing methods. In this paper we challenge these views by presenting a deep 3D convolutional architecture trained end to end to perform voxel-level prediction, i.e., to output a variable at every voxel of the video. Most importantly, we show that the same exact architecture can be used to achieve competitive results on three widely different voxel-prediction tasks: video semantic segmentation, optical flow estimation, and video coloring. The three networks learned on these problems are trained from raw video without any form of preprocessing and their outputs do not require post-processing to achieve outstanding performance. Thus, they offer an efficient alternative to traditional and much more computationally expensive methods in these video domains.},
	booktitle = {Proceedings - 29th {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}, {CVPRW} 2016},
	publisher = {IEEE Computer Society},
	author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
	year = {2016},
	doi = {10.1109/CVPRW.2016.57},
	pages = {402--409}
}

@article{brock_generative_2016,
	title = {Generative and {Discriminative} {Voxel} {Modeling} with {Convolutional} {Neural} {Networks}},
	volume = {abs/1608.04236},
	url = {http://arxiv.org/abs/1608.04236},
	journal = {CoRR},
	author = {Brock, André and Lim, Theodore and Ritchie, James M. and Weston, Nick},
	year = {2016},
	note = {\_eprint: 1608.04236}
}

@inproceedings{silberman_indoor_2012,
	address = {Berlin, Heidelberg},
	series = {{ECCV}'12},
	title = {Indoor {Segmentation} and {Support} {Inference} from {RGBD} {Images}},
	isbn = {978-3-642-33714-7},
	url = {http://dx.doi.org/10.1007/978-3-642-33715-4_54},
	doi = {10.1007/978-3-642-33715-4_54},
	booktitle = {Proceedings of the 12th {European} {Conference} on {Computer} {Vision} - {Volume} {Part} {V}},
	publisher = {Springer-Verlag},
	author = {Silberman, Nathan and Hoiem, Derek and Kohli, Pushmeet and Fergus, Rob},
	year = {2012},
	note = {event-place: Florence, Italy},
	pages = {746--760}
}

@article{meagher_geometric_1982,
	title = {Geometric {Modeling} {Using} {Octree}-{Encoding}},
	volume = {19},
	journal = {Computer Graphics and Image Processing},
	author = {Meagher, Donald},
	year = {1982},
	pages = {129--147}
}

@article{wilhelms_octrees_1992,
	title = {Octrees for {Faster} {Isosurface} {Generation}},
	volume = {11},
	issn = {0730-0301},
	url = {http://doi.acm.org/10.1145/130881.130882},
	doi = {10.1145/130881.130882},
	number = {3},
	journal = {ACM Trans. Graph.},
	author = {Wilhelms, Jane and Van Gelder, Allen},
	year = {1992},
	note = {Place: New York, NY, USA
Publisher: ACM},
	keywords = {hierarchical spatial enumeration, isosurface extraction, octree, scientific visualization},
	pages = {201--227}
}

@article{wu_3d_2014,
	title = {{3D} for 2.{5D} {Object} {Recognition} and {Next}-{Best}-{View} {Prediction}},
	volume = {abs/1406.5670},
	url = {http://arxiv.org/abs/1406.5670},
	journal = {CoRR},
	author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Tang, Xiaoou and Xiao, Jianxiong},
	year = {2014},
	note = {\_eprint: 1406.5670},
	file = {全文:C\:\\Users\\qinka\\Zotero\\storage\\NSJWSBK8\\Wu 等。 - 2014 - 3D for 2.5D Object Recognition and Next-Best-View .pdf:application/pdf}
}

@article{sharma_vconv-dae_2016,
	title = {{VConv}-{DAE}: {Deep} {Volumetric} {Shape} {Learning} {Without} {Object} {Labels}},
	volume = {abs/1604.03755},
	url = {http://arxiv.org/abs/1604.03755},
	journal = {CoRR},
	author = {Sharma, Abhishek and Grau, Oliver and Fritz, Mario},
	year = {2016},
	note = {\_eprint: 1604.03755}
}

@article{esteves_3d_2017,
	title = {{3D} object classification and retrieval with {Spherical} {CNNs}},
	volume = {abs/1711.06721},
	url = {http://arxiv.org/abs/1711.06721},
	journal = {CoRR},
	author = {Esteves, Carlos and Allen-Blanchette, Christine and Makadia, Ameesh and Daniilidis, Kostas},
	year = {2017},
	note = {\_eprint: 1711.06721}
}

@article{sedaghat_orientation-boosted_2016,
	title = {Orientation-boosted {Voxel} {Nets} for {3D} {Object} {Recognition}},
	volume = {abs/1604.03351},
	url = {http://arxiv.org/abs/1604.03351},
	journal = {CoRR},
	author = {Sedaghat, Nima and Zolfaghari, Mohammadreza and Brox, Thomas},
	year = {2016},
	note = {\_eprint: 1604.03351}
}

@article{song_semantic_2017,
	title = {Semantic {Scene} {Completion} from a {Single} {Depth} {Image}},
	journal = {IEEE Conference on Computer Vision and Pattern Recognition},
	author = {Song, Shuran and Yu, Fisher and Zeng, Andy and Chang, Angel X and Savva, Manolis and Funkhouser, Thomas},
	year = {2017}
}

@article{tchapmi_segcloud_2017,
	title = {{SEGCloud}: {Semantic} {Segmentation} of {3D} {Point} {Clouds}},
	volume = {abs/1710.07563},
	url = {http://arxiv.org/abs/1710.07563},
	journal = {CoRR},
	author = {Tchapmi, Lyne P. and Choy, Christopher B. and Armeni, Iro and Gwak, JunYoung and Savarese, Silvio},
	year = {2017},
	note = {\_eprint: 1710.07563}
}

@article{xu_spidercnn_2018,
	title = {{SpiderCNN}: {Deep} {Learning} on {Point} {Sets} with {Parameterized} {Convolutional} {Filters}},
	journal = {ArXiv e-prints},
	author = {Xu, Y. and Fan, T. a@articleDBLP:journals/tog/WangLGST17, author = Peng-Shuai Wang {and} Yang Liu {and} Yu-Xiao Guo {and} Chun-Yu Sun {and} Xin Tong, title = O-CNN: octree-based convolutional neural networks for 3D shape analysis, journal = ACM Trans. Graph., volume = 36, number = 4, pages = 72:1–72:11, year = 2017, url = http://doi.acm.org/10.1145/3072959.3073608, doi = 10.1145/3072959.3073608, timestamp = Sun, 06 Aug 2017 17:40:31 +0200, biburl = http://dblp.org/rec/bib/journals/tog/WangLGST17, bibsource = dblp computer science bibliography, http://dblp.org @articleDBLP:journals/corr/RieglerUG16, author = Gernot Riegler {and} Ali Osman Ulusoy {and} Andreas Geiger, title = OctNet: Learning Deep 3D Representations at High Resolutions, journal = CoRR, volume = abs/1611.05009, year = 2016, url = http://arxiv.org/abs/1611.05009, archivePrefix = arXiv, eprint = 1611.05009, timestamp = Wed, 07 Jun 2017 14:40:54 +0200, biburl = http://dblp.org/rec/bib/journals/corr/RieglerUG16, bibsource = dblp computer science bibliography, http://dblp.org @INPROCEEDINGS7900038, author=Jing Huang {and} Suya You, booktitle=2016 23rd International Conference on Pattern Recognition (ICPR), title=Point cloud labeling using 3D Convolutional Neural Network, year=2016, volume=, number=, pages=2670-2675, keywords=computer vision;data handling;neural nets;object recognition;3D convolutional neural network;3D point cloud labeling scheme;computer vision;data handling;object recognition;testing process;training process;urban point cloud dataset;Labeling;Neural networks;Testing;Three-dimensional displays;Training;Training data;Two dimensional displays, doi=10.1109/ICPR.2016.7900038, ISSN=, month=Dec, @inproceedings 3dor.20171047, booktitle = Eurographics Workshop on 3D Object Retrieval, editor = Ioannis Pratikakis {and} Florent Dupont {and} Maks Ovsjanikov, title = Unstructured Point Cloud Semantic Labeling Using Deep Segmentation Networks, author = Boulch, Alexandre {and} Saux, Bertrand Le {and} Audebert, Nicolas, year = 2017, publisher = The Eurographics Association, ISSN = 1997-0471, ISBN = 978-3-03868-030-7, DOI = 10.2312/3dor.20171047 @articleDBLP:journals/corr/EngelckeRWTP16, author = Martin Engelcke {and} Dushyant Rao {and} Dominic Zeng Wang {and} Chi Hay Tong {and} Ingmar Posner, title = Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks, journal = CoRR, volume = abs/1609.06666, year = 2016, url = http://arxiv.org/abs/1609.06666, archivePrefix = arXiv, eprint = 1609.06666, timestamp = Wed, 07 Jun 2017 14:41:41 +0200, biburl = http://dblp.org/rec/bib/journals/corr/EngelckeRWTP16, bibsource = dblp computer science bibliography, http://dblp.org @proceedingsDBLP:conf/icra/2017, title = 2017 IEEE International Conference on Robotics {and} Automation, ICRA 2017, Singapore, Singapore, May 29 - June 3, 2017, publisher = IEEE, year = 2017, url = http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=7960754, isbn = 978-1-5090-4633-1, timestamp = Wed, 26 Jul 2017 15:16:56 +0200, biburl = http://dblp.org/rec/bib/conf/icra/2017, bibsource = dblp computer science bibliography, http://dblp.org @articleDBLP:journals/corr/KamnitsasLNSKMR16, author = Konstantinos Kamnitsas {and} Christian Ledig {and} Virginia F. J. Newcombe {and} Joanna P. Simpson {and} Andrew D. Kane {and} David K. Menon {and} Daniel Rueckert {and} Ben Glocker, title = Efficient Multi-Scale 3D CNN with Fully Connected CRF for Accurate Brain Lesion Segmentation, journal = CoRR, volume = abs/1603.05959, year = 2016, url = http://arxiv.org/abs/1603.05959, archivePrefix = arXiv, eprint = 1603.05959, timestamp = Wed, 07 Jun 2017 14:42:35 +0200, biburl = http://dblp.org/rec/bib/journals/corr/KamnitsasLNSKMR16, bibsource = dblp computer science bibliography, http://dblp.org @articleDBLP:journals/corr/KlokovL17, author = Roman Klokov {and} Victor S. Lempitsky, title = Escape from Cells: Deep Kd-Networks for The Recognition of 3D Point Cloud Models, journal = CoRR, volume = abs/1704.01222, year = 2017, url = http://arxiv.org/abs/1704.01222, archivePrefix = arXiv, eprint = 1704.01222, timestamp = Wed, 07 Jun 2017 14:40:16 +0200, biburl = http://dblp.org/rec/bib/journals/corr/KlokovL17, bibsource = dblp computer science bibliography, http://dblp.org @inproceedingsDBLP:conf/iccv/KimKS13, author = Byung-soo Kim {and} Pushmeet Kohli {and} Silvio Savarese, title = 3D Scene Understanding by Voxel-CRF, booktitle = IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013, pages = 1425–1432, year = 2013, crossref = DBLP:conf/iccv/2013, url = https://doi.org/10.1109/ICCV.2013.180, doi = 10.1109/ICCV.2013.180, timestamp = Wed, 24 May 2017 08:31:04 +0200, biburl = http://dblp.org/rec/bib/conf/iccv/KimKS13, bibsource = dblp computer science bibliography, http://dblp.org @proceedingsDBLP:conf/iccv/2013, title = IEEE International Conference on Computer Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013, publisher = IEEE Computer Society, year = 2013, url = http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6750807, isbn = 978-1-4799-2839-2, timestamp = Fri, 19 Feb 2016 11:45:57 +0100, biburl = http://dblp.org/rec/bib/conf/iccv/2013, bibsource = dblp computer science bibliography, http://dblp.org @inproceedingsDBLP:conf/iros/MaturanaS15, author = Daniel Maturana {and} Sebastian Scherer, title = VoxNet: A 3D Convolutional Neural Network for real-time object recognition, booktitle = 2015 IEEE/RSJ International Conference on Intelligent Robots {and} Systems, IROS 2015, Hamburg, Germany, September 28 - October 2, 2015, pages = 922–928, year = 2015, crossref = DBLP:conf/iros/2015, url = https://doi.org/10.1109/IROS.2015.7353481, doi = 10.1109/IROS.2015.7353481, timestamp = Mon, 22 May 2017 17:11:27 +0200, biburl = http://dblp.org/rec/bib/conf/iros/MaturanaS15, bibsource = dblp computer science bibliography, http://dblp.org @proceedingsDBLP:conf/iros/2015, title = 2015 IEEE/RSJ International Conference on Intelligent Robots {and} Systems, IROS 2015, Hamburg, Germany, September 28 - October 2, 2015, publisher = IEEE, year = 2015, url = http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=7347169, isbn = 978-1-4799-9994-1, timestamp = Mon, 04 Jan 2016 12:12:56 +0100, biburl = http://dblp.org/rec/bib/conf/iros/2015, bibsource = dblp computer science bibliography, http://dblp.org @inbook765ece793c4e4a05ac7b7cd9f4d4da66, title = "Deep End2End Voxel2Voxel Prediction", abstract = "Over the last few years deep learning methods have emerged as one of the most prominent approaches for video analysis. However, so far their most successful applications have been in the area of video classification {and} detection, i.e., problems involving the prediction of a single class label or a handful of output variables per video. Furthermore, while deep networks are commonly recognized as the best models to use in these domains, there is a widespread perception that in order to yield successful results they often require time-consuming architecture search, manual tweaking of parameters {and} computationally intensive preprocessing or post-processing methods. In this paper we challenge these views by presenting a deep 3D convolutional architecture trained end to end to perform voxel-level prediction, i.e., to output a variable at every voxel of the video. Most importantly, we show that the same exact architecture can be used to achieve competitive results on three widely different voxel-prediction tasks: video semantic segmentation, optical flow estimation, {and} video coloring. The three networks learned on these problems are trained from raw video without any form of preprocessing {and} their outputs do not require post-processing to achieve outstanding performance. Thus, they offer an efficient alternative to traditional {and} much more computationally expensive methods in these video domains.", author = "Du Tran {and} Lubomir Bourdev {and} Rob Fergus {and} Lorenzo Torresani {and} Manohar Paluri", year = "2016", month = "12", doi = "10.1109/CVPRW.2016.57", pages = "402–409", booktitle = "Proceedings - 29th IEEE Conference on Computer Vision {and} Pattern Recognition Workshops, CVPRW 2016", publisher = "IEEE Computer Society", address = "United States", @articleDBLP:journals/corr/BrockLRW16, author = André Brock {and} Theodore Lim {and} James M. Ritchie {and} Nick Weston, title = Generative {and} Discriminative Voxel Modeling with Convolutional Neural Networks, journal = CoRR, volume = abs/1608.04236, year = 2016, url = http://arxiv.org/abs/1608.04236, archivePrefix = arXiv, eprint = 1608.04236, timestamp = Wed, 07 Jun 2017 14:42:45 +0200, biburl = http://dblp.org/rec/bib/journals/corr/BrockLRW16, bibsource = dblp computer science bibliography, http://dblp.org @inproceedingsSilberman:2012:ISS:2403138.2403195, author = Silberman, Nathan {and} Hoiem, Derek {and} Kohli, Pushmeet {and} Fergus, Rob, title = Indoor Segmentation {and} Support Inference from RGBD Images, booktitle = Proceedings of the 12th European Conference on Computer Vision - Volume Part V, series = ECCV'12, year = 2012, isbn = 978-3-642-33714-7, location = Florence, Italy, pages = 746–760, numpages = 15, url = http://dx.doi.org/10.1007/978-3-642-33715-4\_54, doi = 10.1007/978-3-642-33715-4\_54, acmid = 2403195, publisher = Springer-Verlag, address = Berlin, Heidelberg, @articleMeagherOctree, author = Meagher, Donald, year = 1982, month = 06, pages = 129-147, title = Geometric Modeling Using Octree-Encoding, volume = 19, booktitle = Computer Graphics {and} Image Processing @articleWilhelms:1992:OFI:130881.130882, author = Wilhelms, Jane {and} Van Gelder, Allen, title = Octrees for Faster Isosurface Generation, journal = ACM Trans. Graph., issue\_date = July 1992, volume = 11, number = 3, month = jul, year = 1992, issn = 0730-0301, pages = 201–227, numpages = 27, url = http://doi.acm.org/10.1145/130881.130882, doi = 10.1145/130881.130882, acmid = 130882, publisher = ACM, address = New York, NY, USA, keywords = hierarchical spatial enumeration, isosurface extraction, octree, scientific visualization, @articleDBLP:journals/corr/WuSKTX14, author = Zhirong Wu {and} Shuran Song {and} Aditya Khosla {and} Xiaoou Tang {and} Jianxiong Xiao, title = 3D for 2.5D Object Recognition {and} Next-Best-View Prediction, journal = CoRR, volume = abs/1406.5670, year = 2014, url = http://arxiv.org/abs/1406.5670, archivePrefix = arXiv, eprint = 1406.5670, timestamp = Wed, 07 Jun 2017 14:42:52 +0200, biburl = http://dblp.org/rec/bib/journals/corr/WuSKTX14, bibsource = dblp computer science bibliography, http://dblp.org @articleDBLP:journals/corr/SharmaGF16, author = Abhishek Sharma {and} Oliver Grau {and} Mario Fritz, title = VConv-DAE: Deep Volumetric Shape Learning Without Object Labels, journal = CoRR, volume = abs/1604.03755, year = 2016, url = http://arxiv.org/abs/1604.03755, archivePrefix = arXiv, eprint = 1604.03755, timestamp = Wed, 07 Jun 2017 14:43:02 +0200, biburl = http://dblp.org/rec/bib/journals/corr/SharmaGF16, bibsource = dblp computer science bibliography, http://dblp.org @articleDBLP:journals/corr/GirdharFRG16, author = Rohit Girdhar {and} David F. Fouhey {and} Mikel Rodriguez {and} Abhinav Gupta, title = Learning a Predictable {and} Generative Vector Representation for Objects, journal = CoRR, volume = abs/1603.08637, year = 2016, url = http://arxiv.org/abs/1603.08637, archivePrefix = arXiv, eprint = 1603.08637, timestamp = Wed, 07 Jun 2017 14:40:15 +0200, biburl = http://dblp.org/rec/bib/journals/corr/GirdharFRG16, bibsource = dblp computer science bibliography, http://dblp.org @articleDBLP:journals/corr/ChoyXGCS16, author = Christopher Bongsoo Choy {and} Danfei Xu {and} JunYoung Gwak {and} Kevin Chen {and} Silvio Savarese, title = 3D-R2N2: A Unified Approach for Single {and} Multi-view 3D Object Reconstruction, journal = CoRR, volume = abs/1604.00449, year = 2016, url = http://arxiv.org/abs/1604.00449, archivePrefix = arXiv, eprint = 1604.00449, timestamp = Wed, 07 Jun 2017 14:42:32 +0200, biburl = http://dblp.org/rec/bib/journals/corr/ChoyXGCS16, bibsource = dblp computer science bibliography, http://dblp.org @articleDBLP:journals/corr/MilletariNA16, author = Fausto Milletari {and} Nassir Navab {and} Seyed-Ahmad Ahmadi, title = V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation, journal = CoRR, volume = abs/1606.04797, year = 2016, url = http://arxiv.org/abs/1606.04797, archivePrefix = arXiv, eprint = 1606.04797, timestamp = Wed, 07 Jun 2017 14:40:44 +0200, biburl = http://dblp.org/rec/bib/journals/corr/MilletariNA16, bibsource = dblp computer science bibliography, http://dblp.org @articleDBLP:journals/corr/SongX15, author = Shuran Song {and} Jianxiong Xiao, title = Deep Sliding Shapes for Amodal 3D Object Detection in RGB-D Images, journal = CoRR, volume = abs/1511.02300, year = 2015, url = http://arxiv.org/abs/1511.02300, archivePrefix = arXiv, eprint = 1511.02300, timestamp = Wed, 07 Jun 2017 14:42:51 +0200, biburl = http://dblp.org/rec/bib/journals/corr/SongX15, bibsource = dblp computer science bibliography, http://dblp.org @articleDBLP:journals/corr/CicekALBR16, author = Özgün Çiçek {and} Ahmed Abdulkadir {and} Soeren S. Lienkamp {and} Thomas Brox {and} Olaf Ronneberger, title = 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation, journal = CoRR, volume = abs/1606.06650, year = 2016, url = http://arxiv.org/abs/1606.06650, archivePrefix = arXiv, eprint = 1606.06650, timestamp = Wed, 07 Jun 2017 14:41:35 +0200, biburl = http://dblp.org/rec/bib/journals/corr/CicekALBR16, bibsource = dblp computer science bibliography, http://dblp.org @articleDBLP:journals/corr/Graham14, author = Benjamin Graham, title = Spatially-sparse convolutional neural networks, journal = CoRR, volume = abs/1409.6070, year = 2014, url = http://arxiv.org/abs/1409.6070, archivePrefix = arXiv, eprint = 1409.6070, timestamp = Wed, 07 Jun 2017 14:41:28 +0200, biburl = http://dblp.org/rec/bib/journals/corr/Graham14, bibsource = dblp computer science bibliography, http://dblp.org @articleDBLP:journals/corr/Graham15, author = Ben Graham, title = Sparse 3D convolutional neural networks, journal = CoRR, volume = abs/1505.02890, year = 2015, url = http://arxiv.org/abs/1505.02890, archivePrefix = arXiv, eprint = 1505.02890, timestamp = Wed, 07 Jun 2017 14:42:35 +0200, biburl = http://dblp.org/rec/bib/journals/corr/Graham15, bibsource = dblp computer science bibliography, http://dblp.org @articleDBLP:journals/corr/KiefelJG15, author = Martin Kiefel {and} Varun Jampani {and} Peter V. Gehler, title = Sparse Convolutional Networks using the Permutohedral Lattice, journal = CoRR, volume = abs/1503.04949, year = 2015, url = http://arxiv.org/abs/1503.04949, archivePrefix = arXiv, eprint = 1503.04949, timestamp = Wed, 07 Jun 2017 14:41:32 +0200, biburl = http://dblp.org/rec/bib/journals/corr/KiefelJG15, bibsource = dblp computer science bibliography, http://dblp.org @articleDBLP:journals/corr/LiPSQG16, author = Yangyan Li {and} Sören Pirk {and} Hao Su {and} Charles Ruizhongtai Qi {and} Leonidas J. Guibas, title = FPNN: Field Probing Neural Networks for 3D Data, journal = CoRR, volume = abs/1605.06240, year = 2016, url = http://arxiv.org/abs/1605.06240, archivePrefix = arXiv, eprint = 1605.06240, timestamp = Wed, 07 Jun 2017 14:40:47 +0200, biburl = http://dblp.org/rec/bib/journals/corr/LiPSQG16, bibsource = dblp computer science bibliography, http://dblp.org @articleDBLP:journals/corr/abs-1711-06721, author = Carlos Esteves {and} Christine Allen-Blanchette {and} Ameesh Makadia {and} Kostas Daniilidis, title = 3D object classification {and} retrieval with Spherical CNNs, journal = CoRR, volume = abs/1711.06721, year = 2017, url = http://arxiv.org/abs/1711.06721, archivePrefix = arXiv, eprint = 1711.06721, timestamp = Sun, 03 Dec 2017 12:38:15 +0100, biburl = https://dblp.org/rec/bib/journals/corr/abs-1711-06721, bibsource = dblp computer science bibliography, https://dblp.org @articleDBLP:journals/corr/AlvarZB16, author = Nima Sedaghat {and} Mohammadreza Zolfaghari {and} Thomas Brox, title = Orientation-boosted Voxel Nets for 3D Object Recognition, journal = CoRR, volume = abs/1604.03351, year = 2016, url = http://arxiv.org/abs/1604.03351, archivePrefix = arXiv, eprint = 1604.03351, timestamp = Wed, 07 Jun 2017 14:40:18 +0200, biburl = https://dblp.org/rec/bib/journals/corr/AlvarZB16, bibsource = dblp computer science bibliography, https://dblp.org @articlesong2016ssc, title= Semantic Scene Completion from a Single Depth Image, author= Song, Shuran {and} Yu, Fisher {and} Zeng, Andy {and} Chang, Angel X {and} Savva, Manolis {and} Funkhouser, Thomas, journal=IEEE Conference on Computer Vision {and} Pattern Recognition, year=2017 @articleDBLP:journals/corr/abs-1710-07563, author = Lyne P. Tchapmi {and} Christopher B. Choy {and} Iro Armeni {and} JunYoung Gwak {and} Silvio Savarese, title = SEGCloud: Semantic Segmentation of 3D Point Clouds, journal = CoRR, volume = abs/1710.07563, year = 2017, url = http://arxiv.org/abs/1710.07563, archivePrefix = arXiv, eprint = 1710.07563, timestamp = Wed, 01 Nov 2017 19:05:43 +0100, biburl = http://dblp.org/rec/bib/journals/corr/abs-1710-07563, bibsource = dblp computer science bibliography, http://dblp.org @ARTICLE2018arXiv180311527X, author = Xu, Y. {and} Fan, T. {and} Xu, M. {and} Zeng, L. {and} Qiao, Y., title = "SpiderCNN: Deep Learning on Point Sets with Parameterized Convolutional Filters", journal = ArXiv e-prints, archivePrefix = "arXiv", eprint = 1803.11527, primaryClass = "cs.CV", keywords = Computer Science - Computer Vision {and} Pattern Recognition, year = 2018, month = mar, adsurl = http://adsabs.harvard.edu/abs/2018arXiv180311527X, adsnote = Provided by the SAO/NASA Astrophysics Data System @articleDBLP:journals/corr/abs-1802-08275, author = Hang Su {and} Varun Jampani {and} Deqing Sun {and} Subhransu Maji {and} Evangelos Kalogerakis {and} Ming-Hsuan Yang {and} Jan Kautz, title = SPLATNet: Sparse Lattice Networks for Point Cloud Processing, journal = CoRR, volume = abs/1802.08275, year = 2018, url = http://arxiv.org/abs/1802.08275, archivePrefix = arXiv, eprint = 1802.08275, timestamp = Mon, 26 Mar 2018 12:54:00 +0200, biburl = https://dblp.org/rec/bib/journals/corr/abs-1802-08275, bibsource = dblp computer science bibliography, https://dblp.org @ARTICLE2018arXiv180311385S, author = Shao, T. {and} Yang, Y. {and} Weng, Y. {and} Hou, Q. {and} Zhou, K., title = "H-CNN: Spatial Hashing Based CNN for 3D Shape Analysis", journal = ArXiv e-prints, archivePrefix = "arXiv", eprint = 1803.11385, primaryClass = "cs.GR", keywords = Computer Science - Graphics, year = 2018, month = mar, adsurl = http://adsabs.harvard.edu/abs/2018arXiv180311385S, adsnote = Provided by the SAO/NASA Astrophysics Data System @ARTICLE2018arXiv180311303C, author = Cai, J. {and} Lu, L. {and} Xing, F. {and} Yang, L., title = "Pancreas Segmentation in CT {and} MRI Images via Domain Specific Network Designing {and} Recurrent Neural Contextual Learning", journal = ArXiv e-prints, archivePrefix = "arXiv", eprint = 1803.11303, primaryClass = "cs.CV", keywords = Computer Science - Computer Vision {and} Pattern Recognition, year = 2018, month = mar, adsurl = http://adsabs.harvard.edu/abs/2018arXiv180311303C, adsnote = Provided by the SAO/NASA Astrophysics Data System @articleDBLP:journals/corr/QiSMG16, author = Charles Ruizhongtai Qi {and} Hao Su {and} Kaichun Mo {and} Leonidas J. Guibas, title = PointNet: Deep Learning on Point Sets for 3D Classification {and} Segmentation, journal = CoRR, volume = abs/1612.00593, year = 2016, url = http://arxiv.org/abs/1612.00593, archivePrefix = arXiv, eprint = 1612.00593, timestamp = Wed, 07 Jun 2017 14:43:06 +0200, biburl = https://dblp.org/rec/bib/journals/corr/QiSMG16, bibsource = dblp computer science bibliography, https://dblp.org @articleDBLP:journals/corr/QiYSG17, author = Charles Ruizhongtai Qi {and} Li Yi {and} Hao Su {and} Leonidas J. Guibas, title = PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space, journal = CoRR, volume = abs/1706.02413, year = 2017, url = http://arxiv.org/abs/1706.02413, archivePrefix = arXiv, eprint = 1706.02413, timestamp = Mon, 03 Jul 2017 13:29:02 +0200, biburl = https://dblp.org/rec/bib/journals/corr/QiYSG17, bibsource = dblp computer science bibliography, https://dblp.org @articleDBLP:journals/corr/abs-1801-07791, author = Yangyan Li {and} Rui Bu {and} Mingchao Sun {and} Baoquan Chen, title = PointCNN, journal = CoRR, volume = abs/1801.07791, year = 2018, url = http://arxiv.org/abs/1801.07791, archivePrefix = arXiv, eprint = 1801.07791, timestamp = Fri, 02 Feb 2018 14:20:25 +0100, biburl = https://dblp.org/rec/bib/journals/corr/abs-1801-07791, bibsource = dblp computer science bibliography, https://dblp.org @articleDBLP:journals/corr/YiSGG16, author = Li Yi {and} Hao Su {and} Xingwen Guo {and} Leonidas J. Guibas, title = SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation, journal = CoRR, volume = abs/1612.00606, year = 2016, url = http://arxiv.org/abs/1612.00606, archivePrefix = arXiv, eprint = 1612.00606, timestamp = Wed, 07 Jun 2017 14:41:52 +0200, biburl = https://dblp.org/rec/bib/journals/corr/YiSGG16, bibsource = dblp computer science bibliography, https://dblp.org @ARTICLE2015arXiv150603767R, author = Rippel, O. {and} Snoek, J. {and} Adams, R. P., title = "Spectral Representations for Convolutional Neural Networks", journal = ArXiv e-prints, archivePrefix = "arXiv", eprint = 1506.03767, primaryClass = "stat.ML", keywords = Statistics - Machine Learning, Computer Science - Learning, year = 2015, month = jun, adsurl = http://adsabs.harvard.edu/abs/2015arXiv150603767R, adsnote = Provided by the SAO/NASA Astrophysics Data System @articleDBLP:journals/corr/abs-1711-06721, author = Carlos Esteves {and} Christine Allen-Blanchette {and} Ameesh Makadia {and} Kostas Daniilidis, title = 3D object classification {and} retrieval with Spherical CNNs, journal = CoRR, volume = abs/1711.06721, year = 2017, url = http://arxiv.org/abs/1711.06721, archivePrefix = arXiv, eprint = 1711.06721, timestamp = Sun, 03 Dec 2017 12:38:15 +0100, biburl = https://dblp.org/rec/bib/journals/corr/abs-1711-06721, bibsource = dblp computer science bibliography, https://dblp.org @inproceedingsDBLP:conf/icra/EngelckeRWTP17, author = Martin Engelcke {and} Dushyant Rao {and} Dominic Zeng Wang {and} Chi Hay Tong {and} Ingmar Posner, title = Vote3Deep: Fast object detection in 3D point clouds using efficient convolutional neural networks, booktitle = 2017 IEEE International Conference on Robotics {and} Automation, ICRA 2017, Singapore, Singapore, May 29 - June 3, 2017, pages = 1355–1361, year = 2017, crossref = DBLP:conf/icra/2017, url = https://doi.org/10.1109/ICRA.2017.7989161, doi = 10.1109/ICRA.2017.7989161, timestamp = Wed, 26 Jul 2017 15:17:30 +0200, biburl = https://dblp.org/rec/bib/conf/icra/EngelckeRWTP17, bibsource = dblp computer science bibliography, https://dblp.org Xu, M. and Zeng, L. and Qiao, Y.},
	year = {2018},
	note = {\_eprint: 1803.11527},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{su_splatnet_2018,
	title = {{SPLATNet}: {Sparse} {Lattice} {Networks} for {Point} {Cloud} {Processing}},
	volume = {abs/1802.08275},
	url = {http://arxiv.org/abs/1802.08275},
	journal = {CoRR},
	author = {Su, Hang and Jampani, Varun and Sun, Deqing and Maji, Subhransu and Kalogerakis, Evangelos and Yang, Ming-Hsuan and Kautz, Jan},
	year = {2018},
	note = {\_eprint: 1802.08275}
}

@article{shao_h-cnn_2018,
	title = {H-{CNN}: {Spatial} {Hashing} {Based} {CNN} for {3D} {Shape} {Analysis}},
	journal = {ArXiv e-prints},
	author = {Shao, T. and Yang, Y. and Weng, Y. and Hou, Q. and Zhou, K.},
	year = {2018},
	note = {\_eprint: 1803.11385},
	keywords = {Computer Science - Graphics}
}

@article{cai_pancreas_2018,
	title = {Pancreas {Segmentation} in {CT} and {MRI} {Images} via {Domain} {Specific} {Network} {Designing} and {Recurrent} {Neural} {Contextual} {Learning}},
	journal = {ArXiv e-prints},
	author = {Cai, J. and Lu, L. and Xing, F. and Yang, L.},
	year = {2018},
	note = {\_eprint: 1803.11303},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{qi_pointnet_2016,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	volume = {abs/1612.00593},
	url = {http://arxiv.org/abs/1612.00593},
	journal = {CoRR},
	author = {Qi, Charles Ruizhongtai and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	year = {2016},
	note = {\_eprint: 1612.00593}
}

@article{qi_pointnet_2017,
	title = {{PointNet}++: {Deep} {Hierarchical} {Feature} {Learning} on {Point} {Sets} in a {Metric} {Space}},
	volume = {abs/1706.02413},
	url = {http://arxiv.org/abs/1706.02413},
	journal = {CoRR},
	author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	year = {2017},
	note = {\_eprint: 1706.02413}
}

@article{li_pointcnn_2018,
	title = {{PointCNN}},
	volume = {abs/1801.07791},
	url = {http://arxiv.org/abs/1801.07791},
	journal = {CoRR},
	author = {Li, Yangyan and Bu, Rui and Sun, Mingchao and Chen, Baoquan},
	year = {2018},
	note = {\_eprint: 1801.07791}
}

@article{yi_syncspeccnn_2016,
	title = {{SyncSpecCNN}: {Synchronized} {Spectral} {CNN} for {3D} {Shape} {Segmentation}},
	volume = {abs/1612.00606},
	url = {http://arxiv.org/abs/1612.00606},
	journal = {CoRR},
	author = {Yi, Li and Su, Hao and Guo, Xingwen and Guibas, Leonidas J.},
	year = {2016},
	note = {\_eprint: 1612.00606}
}

@article{rippel_spectral_2015,
	title = {Spectral {Representations} for {Convolutional} {Neural} {Networks}},
	journal = {ArXiv e-prints},
	author = {Rippel, O. and Snoek, J. and Adams, R. P.},
	year = {2015},
	note = {\_eprint: 1506.03767},
	keywords = {Computer Science - Learning, Statistics - Machine Learning}
}

@inproceedings{engelcke_vote3deep_2017,
	title = {{Vote3Deep}: {Fast} object detection in {3D} point clouds using efficient convolutional neural networks},
	url = {https://doi.org/10.1109/ICRA.2017.7989161},
	doi = {10.1109/ICRA.2017.7989161},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation}, {ICRA} 2017, {Singapore}, {Singapore}, {May} 29 - {June} 3, 2017},
	author = {Engelcke, Martin and Rao, Dushyant and Wang, Dominic Zeng and Tong, Chi Hay and Posner, Ingmar},
	year = {2017},
	pages = {1355--1361}
}

@article{shorten_survey_2019,
	title = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-019-0197-0},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	number = {1},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	year = {2019},
	pages = {60}
}

@book{chatfield_return_2014,
	title = {Return of the {Devil} in the {Details}: {Delving} {Deep} into {Convolutional} {Nets}},
	author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	year = {2014},
	note = {\_eprint: 1405.3531}
}

@inproceedings{jurio_comparison_2010,
	address = {Berlin, Heidelberg},
	title = {A {Comparison} {Study} of {Different} {Color} {Spaces} in {Clustering} {Based} {Image} {Segmentation}},
	isbn = {978-3-642-14058-7},
	abstract = {In this work we carry out a comparison study between different color spaces in clustering-based image segmentation. We use two similar clustering algorithms, one based on the entropy and the other on the ignorance. The study involves four color spaces and, in all cases, each pixel is represented by the values of the color channels in that space. Our purpose is to identify the best color representation, if there is any, when using this kind of clustering algorithms.},
	booktitle = {Information {Processing} and {Management} of {Uncertainty} in {Knowledge}-{Based} {Systems}. {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Jurio, Aranzazu and Pagola, Miguel and Galar, Mikel and Lopez-Molina, Carlos and Paternain, Daniel},
	editor = {Hüllermeier, Eyke and Kruse, Rudolf and Hoffmann, Frank},
	year = {2010},
	pages = {532--541}
}

@book{summers_improved_2018,
	title = {Improved {Mixed}-{Example} {Data} {Augmentation}},
	author = {Summers, Cecilia and Dinneen, Michael J.},
	year = {2018},
	note = {\_eprint: 1805.11272}
}

@book{zhong_random_2017,
	title = {Random {Erasing} {Data} {Augmentation}},
	author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
	year = {2017},
	note = {\_eprint: 1708.04896}
}

@article{takahashi_data_2019,
	title = {Data {Augmentation} using {Random} {Image} {Cropping} and {Patching} for {Deep} {CNNs}},
	issn = {1558-2205},
	url = {http://dx.doi.org/10.1109/TCSVT.2019.2935128},
	doi = {10.1109/tcsvt.2019.2935128},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Takahashi, Ryo and Matsubara, Takashi and Uehara, Kuniaki},
	year = {2019},
	note = {Publisher: Institute of Electrical and Electronics Engineers (IEEE)},
	pages = {1--1}
}

@inproceedings{madani_chest_2018,
	title = {Chest x-ray generation and data augmentation for cardiovascular abnormality classification},
	volume = {10574},
	url = {https://doi.org/10.1117/12.2293971},
	doi = {10.1117/12.2293971},
	booktitle = {Medical {Imaging} 2018: {Image} {Processing}},
	publisher = {SPIE},
	author = {Madani, Ali and Moradi, Mehdi and Karargyris, Alexandros and Syeda-Mahmood, Tanveer},
	editor = {Angelini, Elsa D. and Landman, Bennett A.},
	year = {2018},
	note = {Backup Publisher: International Society for Optics and Photonics},
	keywords = {Convolutional networks, data augmentation, Generative adversarial networks},
	pages = {415 -- 420}
}

@article{goodfellow_challenges_2015,
	title = {Challenges in representation learning: {A} report on three machine learning contests},
	volume = {64},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608014002159},
	doi = {https://doi.org/10.1016/j.neunet.2014.09.005},
	abstract = {The ICML 2013 Workshop on Challenges in Representation Learning11http://deeplearning.net/icml2013-workshop-competition. focused on three challenges: the black box learning challenge, the facial expression recognition challenge, and the multimodal learning challenge. We describe the datasets created for these challenges and summarize the results of the competitions. We provide suggestions for organizers of future challenges and some comments on what kind of knowledge can be gained from machine learning competitions.},
	journal = {Neural Networks},
	author = {Goodfellow, Ian J. and Erhan, Dumitru and Carrier, Pierre Luc and Courville, Aaron and Mirza, Mehdi and Hamner, Ben and Cukierski, Will and Tang, Yichuan and Thaler, David and Lee, Dong-Hyun and Zhou, Yingbo and Ramaiah, Chetan and Feng, Fangxiang and Li, Ruifan and Wang, Xiaojie and Athanasakis, Dimitris and Shawe-Taylor, John and Milakov, Maxim and Park, John and Ionescu, Radu and Popescu, Marius and Grozea, Cristian and Bergstra, James and Xie, Jingjing and Romaszko, Lukasz and Xu, Bing and Chuang, Zhang and Bengio, Yoshua},
	year = {2015},
	keywords = {Competition, Dataset, Representation learning},
	pages = {59 -- 63}
}

@article{vallet_terramobilitaiqmulus_2015,
	title = {{TerraMobilita}/{iQmulus} {Urban} {Point} {Cloud} {Analysis} {Benchmark}},
	volume = {49},
	journal = {Computers \& Graphics},
	author = {Vallet, Bruno and Brédif, Mathieu and Serna, Andrés and Marcotegui, B and Paparoditis, Nicolas},
	year = {2015}
}

@article{geiger_vision_2013,
	title = {Vision meets {Robotics}: {The} {KITTI} {Dataset}},
	journal = {International Journal of Robotics Research (IJRR)},
	author = {Geiger, Andreas and Lenz, Philip and Stiller, Christoph and Urtasun, Raquel},
	year = {2013}
}

@article{chang_shapenet_2015,
	title = {{ShapeNet}: {An} {Information}-{Rich} {3D} {Model} {Repository}},
	volume = {abs/1512.03012},
	url = {http://arxiv.org/abs/1512.03012},
	journal = {CoRR},
	author = {Chang, Angel X. and Funkhouser, Thomas A. and Guibas, Leonidas J. and Hanrahan, Pat and Huang, Qi-Xing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher},
	year = {2015},
	note = {\_eprint: 1512.03012}
}

@article{wu_3d_2014-1,
	title = {{3D} {ShapeNets}: {A} {Deep} {Representation} for {Volumetric} {Shapes}},
	journal = {ArXiv e-prints},
	author = {Wu, Z. and Song, S. and Khosla, A. and Yu, F. and Zhang, L. and Tang, X. and Xiao, J.},
	year = {2014},
	note = {\_eprint: 1406.5670},
	keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{hackel_semantic3dnet_2017,
	title = {{SEMANTIC3D}.{NET}: {A} new large-scale point cloud classification benchmark},
	volume = {IV-1-W1},
	booktitle = {{ISPRS} {Annals} of the {Photogrammetry}, {Remote} {Sensing} and {Spatial} {Information} {Sciences}},
	author = {Hackel, Timo and Savinov, N. and Ladicky, L. and Wegner, Jan D. and Schindler, K. and Pollefeys, M.},
	year = {2017},
	pages = {91--98}
}

@article{armeni_joint_2017,
	title = {Joint {2D}-{3D}-{Semantic} {Data} for {Indoor} {Scene} {Understanding}},
	volume = {abs/1702.01105},
	url = {http://arxiv.org/abs/1702.01105},
	journal = {CoRR},
	author = {Armeni, Iro and Sax, Sasha and Zamir, Amir Roshan and Savarese, Silvio},
	year = {2017},
	note = {\_eprint: 1702.01105},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics}
}

@article{kumar_dataset_2017,
	title = {A {Dataset} and a {Technique} for {Generalized} {Nuclear} {Segmentation} for {Computational} {Pathology}},
	volume = {36},
	doi = {10.1109/TMI.2017.2677499},
	number = {7},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Kumar, N. and Verma, R. and Sharma, S. and Bhargava, S. and Vahadane, A. and Sethi, A.},
	month = jul,
	year = {2017},
	keywords = {object recognition, Training, Algorithms, Annotation, biological organs, biological tissues, biomedical optical imaging, boundaries, Cell Nucleus, cellular biophysics, chromatin-sparse, computational pathology, Computer-Assisted, conventional image processing techniques, crowded nuclei, dataset, deep learning, digital microscopic tissue images, disease states, diseases, Diseases, feature extraction, generalized nuclear segmentation, H\&E-stained images, hematoxylin and eosin-stained tissue images, high-quality feature extraction, Humans, image classification, image classification problems, Image color analysis, Image Processing, image segmentation, Image segmentation, learning (artificial intelligence), Machine learning, Machine Learning, machine learning algorithms, machine learning-based segmentation, Measurement, medical image processing, nuclear appearances, nuclear boundaries, nuclear morphometrics, nuclear segmentation, nuclei, object-level errors, optical microscopy, organs, Otsu thresholding, overlapping nuclei, Pathology, pixel-level errors, right out-of-the-box, segmentation technique, Staining and Labeling, watershed segmentation},
	pages = {1550--1560}
}

@inproceedings{vahadane_learning_2016,
	title = {Learning based super-resolution of histological images},
	doi = {10.1109/ISBI.2016.7493391},
	booktitle = {2016 {IEEE} 13th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Vahadane, A. and Kumar, N. and Sethi, A.},
	month = apr,
	year = {2016},
	keywords = {Testing, Training, Artificial neural networks, histological image, Image edge detection, Image reconstruction, Image resolution, Image super-resolution, neural network},
	pages = {816--819}
}

@article{setio_validation_2016,
	title = {Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: {The} {LUNA16} challenge},
	volume = {42},
	doi = {10.1016/j.media.2017.06.015},
	journal = {Medical Image Analysis},
	author = {Setio, Arnaud and Traverso, Alberto and Bel, Thomas and Berens, Moira and Bogaard, Cas and Cerello, Piergiorgio and Chen, Hao and Dou, Qi and Fantacci, Maria and Geurts, Bram and Gugten, Robbert and Heng, Pheng and Jansen, Bart and Kaste, Michael and Kotov, Valentin and Lin, Jack and Manders, Jeroen and Sónora-Mengana, Alexander and Naranjo, Juan Carlos and Papavasileiou, Evgenia},
	year = {2016}
}

@article{mueller_alzheimers_2005,
	title = {The {Alzheimer}'s {Disease} {Neuroimaging} {Initiative}},
	volume = {15},
	number = {4},
	journal = {Neuroimaging Clinics of North America},
	author = {Mueller, Susanne G and Weiner, Michael W and Thal, Leon J and Petersen, Ronald Carl and Jack, Clifford R Jr and Jagust, William J and Trojanowski, John Q and Toga, Arthur W and Beckett, Laurel A},
	year = {2005},
	pages = {869--877}
}

@book{armato_iii_samuel_g_data_2015,
	title = {Data {From} {LIDC}-{IDRI}},
	url = {https://wiki.cancerimagingarchive.net/x/rgAe},
	publisher = {The Cancer Imaging Archive},
	author = {{Armato III, Samuel G.} and McLennan, Geoffrey and Bidaut, Luc and McNitt-Gray, Michael F. and Meyer, Charles R. and Reeves, Anthony P. and Zhao, Binsheng and Aberle, Denise R. and Henschke, Claudia I. and Hoffman, Eric A. and Kazerooni, Ella A. and MacMahon, Heber and Van Beek, Edwin J.R. and Yankelevitz, David and Biancardi, Alberto M. and Bland, Peyton H. and Brown, Matthew S. and Engelmann, Roger M. and Laderach, Gary E. and Max, Daniel and Pais, Richard C. and Qing, David P.Y. and Roberts, Rachael Y. and Smith, Amanda R. and Starkey, Adam and Batra, Poonam and Caligiuri, Philip and Farooqi, Ali and Gladish, Gregory W. and Jude, C. Matilda and Munden, Reginald F. and Petkovska, Iva and Quint, Leslie E. and Schwartz, Lawrence H. and Sundaram, Baskaran and Dodd, Lori E. and Fenimore, Charles and Gur, David and Petrick, Nicholas and Freymann, John and Kirby, Justin and Hughes, Brian and Casteele, Alessi Vande and Gupte, Sangeeta and Sallam, Maha and Heath, Michael D. and Kuhn, Michael H. and Dharaiya, Ekta and Burns, Richard and Fryd, David S. and Salganicoff, Marcos and Anand, Vikram and Shreter, Uri and Vastagh, Stephen and Croft, Barbara Y. and Clarke, Laurence P.},
	year = {2015},
	doi = {10.7937/K9/TCIA.2015.LO9QL9SX}
}

@article{clark_cancer_2013,
	title = {The {Cancer} {Imaging} {Archive} ({TCIA}): {Maintaining} and {Operating} a {Public} {Information} {Repository}},
	volume = {26},
	issn = {1618-727X},
	url = {https://doi.org/10.1007/s10278-013-9622-7},
	doi = {10.1007/s10278-013-9622-7},
	abstract = {The National Institutes of Health have placed significant emphasis on sharing of research data to support secondary research. Investigators have been encouraged to publish their clinical and imaging data as part of fulfilling their grant obligations. Realizing it was not sufficient to merely ask investigators to publish their collection of imaging and clinical data, the National Cancer Institute (NCI) created the open source National Biomedical Image Archive software package as a mechanism for centralized hosting of cancer related imaging. NCI has contracted with Washington University in Saint Louis to create The Cancer Imaging Archive (TCIA)–an open-source, open-access information resource to support research, development, and educational initiatives utilizing advanced medical imaging of cancer. In its first year of operation, TCIA accumulated 23 collections (3.3 million images). Operating and maintaining a high-availability image archive is a complex challenge involving varied archive-specific resources and driven by the needs of both image submitters and image consumers. Quality archives of any type (traditional library, PubMed, refereed journals) require management and customer service. This paper describes the management tasks and user support model for TCIA.},
	number = {6},
	journal = {Journal of Digital Imaging},
	author = {Clark, Kenneth and Vendt, Bruce and Smith, Kirk and Freymann, John and Kirby, Justin and Koppel, Paul and Moore, Stephen and Phillips, Stanley and Maffitt, David and Pringle, Michael and Tarbox, Lawrence and Prior, Fred},
	year = {2013},
	pages = {1045--1057}
}

@article{rister_ct_2018,
	title = {{CT} organ segmentation using {GPU} data augmentation, unsupervised labels and {IOU} loss},
	volume = {abs/1811.11226},
	url = {http://arxiv.org/abs/1811.11226},
	journal = {CoRR},
	author = {Rister, Blaine and Yi, Darvin and Shivakumar, Kaushik and Nobashi, Tomomi and Rubin, Daniel L.},
	year = {2018},
	note = {\_eprint: 1811.11226}
}

@article{bilic_liver_2019,
	title = {The {Liver} {Tumor} {Segmentation} {Benchmark} ({LiTS})},
	volume = {abs/1901.04056},
	url = {http://arxiv.org/abs/1901.04056},
	journal = {CoRR},
	author = {Bilic, Patrick and Christ, Patrick Ferdinand and Vorontsov, Eugene and Chlebus, Grzegorz and Chen, Hao and Dou, Qi and Fu, Chi-Wing and Han, Xiao and Heng, Pheng-Ann and Hesser, Jürgen and Kadoury, Samuel and Konopczynski, Tomasz K. and Le, Miao and Li, Chunming and Li, Xiaomeng and Lipková, Jana and Lowengrub, John S. and Meine, Hans and Moltz, Jan Hendrik and Pal, Chris and Piraud, Marie and Qi, Xiaojuan and Qi, Jin and Rempfler, Markus and Roth, Karsten and Schenk, Andrea and Sekuboyina, Anjany and Zhou, Ping and Hülsemeyer, Christian and Beetz, Marcel and Ettlinger, Florian and Grün, Felix and Kaissis, Georgios and Lohöfer, Fabian and Braren, Rickmer and Holch, Julian and Hofmann, Felix and Sommer, Wieland H. and Heinemann, Volker and Jacobs, Colin and Mamani, Gabriel Efrain Humpire and Ginneken, Bram van and Chartrand, Gabriel and Tang, An and Drozdzal, Michal and Ben-Cohen, Avi and Klang, Eyal and Amitai, Michal Marianne and Konen, Eli and Greenspan, Hayit and Moreau, Johan and Hostettler, Alexandre and Soler, Luc and Vivanti, Refael and Szeskin, Adi and Lev-Cohain, Naama and Sosna, Jacob and Joskowicz, Leo and Menze, Bjoern H.},
	year = {2019},
	note = {\_eprint: 1901.04056}
}

@book{heller_kits19_2019,
	title = {The {KiTS19} {Challenge} {Data}: 300 {Kidney} {Tumor} {Cases} with {Clinical} {Context}, {CT} {Semantic} {Segmentations}, and {Surgical} {Outcomes}},
	author = {Heller, Nicholas and Sathianathen, Niranjan and Kalapara, Arveen and Walczak, Edward and Moore, Keenan and Kaluzniak, Heather and Rosenberg, Joel and Blake, Paul and Rengel, Zachary and Oestreich, Makinna and Dean, Joshua and Tradewell, Michael and Shah, Aneri and Tejpaul, Resha and Edgerton, Zachary and Peterson, Matthew and Raza, Shaneabbas and Regmi, Subodh and Papanikolopoulos, Nikolaos and Weight, Christopher},
	year = {2019},
	note = {\_eprint: 1904.00445}
}

@article{rusu_co-registration_2017,
	title = {Co-registration of pre-operative {CT} with ex vivo surgically excised ground glass nodules to define spatial extent of invasive adenocarcinoma on in vivo imaging: a proof-of-concept study},
	volume = {27},
	issn = {1432-1084},
	url = {https://doi.org/10.1007/s00330-017-4813-0},
	doi = {10.1007/s00330-017-4813-0},
	abstract = {To develop an approach for radiology-pathology fusion of ex vivo histology of surgically excised pulmonary nodules with pre-operative CT, to radiologically map spatial extent of the invasive adenocarcinomatous component of the nodule.},
	number = {10},
	journal = {European Radiology},
	author = {Rusu, Mirabela and Rajiah, Prabhakar and Gilkeson, Robert and Yang, Michael and Donatelli, Christopher and Thawani, Rajat and Jacono, Frank J. and Linden, Philip and Madabhushi, Anant},
	year = {2017},
	pages = {4209--4217}
}

@article{aerts_decoding_2014,
	title = {Decoding tumour phenotype by noninvasive imaging using a quantitative radiomics approach},
	volume = {5},
	issn = {2041-1723},
	url = {https://doi.org/10.1038/ncomms5006},
	doi = {10.1038/ncomms5006},
	abstract = {Human cancers exhibit strong phenotypic differences that can be visualized noninvasively by medical imaging. Radiomics refers to the comprehensive quantification of tumour phenotypes by applying a large number of quantitative image features. Here we present a radiomic analysis of 440 features quantifying tumour image intensity, shape and texture, which are extracted from computed tomography data of 1,019 patients with lung or head-and-neck cancer. We find that a large number of radiomic features have prognostic power in independent data sets of lung and head-and-neck cancer patients, many of which were not identified as significant before. Radiogenomics analysis reveals that a prognostic radiomic signature, capturing intratumour heterogeneity, is associated with underlying gene-expression patterns. These data suggest that radiomics identifies a general prognostic phenotype existing in both lung and head-and-neck cancer. This may have a clinical impact as imaging is routinely used in clinical practice, providing an unprecedented opportunity to improve decision-support in cancer treatment at low cost.},
	number = {1},
	journal = {Nature Communications},
	author = {Aerts, Hugo J. W. L. and Velazquez, Emmanuel Rios and Leijenaar, Ralph T. H. and Parmar, Chintan and Grossmann, Patrick and Carvalho, Sara and Bussink, Johan and Monshouwer, René and Haibe-Kains, Benjamin and Rietveld, Derek and Hoebers, Frank and Rietbergen, Michelle M. and Leemans, C. René and Dekker, Andre and Quackenbush, John and Gillies, Robert J. and Lambin, Philippe},
	year = {2014},
	pages = {4006}
}

@article{singanamalli_identifying_2016,
	title = {Identifying in vivo {DCE} {MRI} markers associated with microvessel architecture and gleason grades of prostate cancer},
	volume = {43},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jmri.24975},
	doi = {10.1002/jmri.24975},
	abstract = {Background To identify computer extracted in vivo dynamic contrast enhanced (DCE) MRI markers associated with quantitative histomorphometric (QH) characteristics of microvessels and Gleason scores (GS) in prostate cancer. Methods This study considered retrospective data from 23 biopsy confirmed prostate cancer patients who underwent 3 Tesla multiparametric MRI before radical prostatectomy (RP). Representative slices from RP specimens were stained with vascular marker CD31. Tumor extent was mapped from RP sections onto DCE MRI using nonlinear registration methods. Seventy-seven microvessel QH features and 18 DCE MRI kinetic features were extracted and evaluated for their ability to distinguish low from intermediate and high GS. The effect of temporal sampling on kinetic features was assessed and correlations between those robust to temporal resolution and microvessel features discriminative of GS were examined. Results A total of 12 microvessel architectural features were discriminative of low and intermediate/high grade tumors with area under the receiver operating characteristic curve (AUC) {\textgreater} 0.7. These features were most highly correlated with mean washout gradient (WG) (max rho = −0.62). Independent analysis revealed WG to be moderately robust to temporal resolution (intraclass correlation coefficient [ICC] = 0.63) and WG variance, which was poorly correlated with microvessel features, to be predictive of low grade tumors (AUC = 0.77). Enhancement ratio was the most robust (ICC = 0.96) and discriminative (AUC = 0.78) kinetic feature but was moderately correlated with microvessel features (max rho = −0.52). Conclusion Computer extracted features of prostate DCE MRI appear to be correlated with microvessel architecture and may be discriminative of low versus intermediate and high GS. J. MAGN. RESON. IMAGING 2016;43:149–158.},
	number = {1},
	journal = {Journal of Magnetic Resonance Imaging},
	author = {Singanamalli, Asha and Rusu, Mirabela and Sparks, Rachel E. and Shih, Natalie N.C. and Ziober, Amy and Wang, Li-Ping and Tomaszewski, John and Rosen, Mark and Feldman, Michael and Madabhushi, Anant},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jmri.24975},
	keywords = {DCE MRI, Gleason grades, imaging biomarkers, microvessel architecture, prostate cancer, quantitative histomorphometry},
	pages = {149--158}
}

@article{kuo_understanding_2016,
	title = {Understanding {Convolutional} {Neural} {Networks} with {A} {Mathematical} {Model}},
	volume = {abs/1609.04112},
	url = {http://arxiv.org/abs/1609.04112},
	journal = {CoRR},
	author = {Kuo, C.-C. Jay},
	year = {2016},
	note = {\_eprint: 1609.04112}
}

@article{huang_densely_2016,
	title = {Densely {Connected} {Convolutional} {Networks}},
	volume = {abs/1608.06993},
	url = {http://arxiv.org/abs/1608.06993},
	journal = {CoRR},
	author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q.},
	year = {2016},
	note = {\_eprint: 1608.06993}
}

@article{mou_tbcnn_2014,
	title = {{TBCNN}: {A} {Tree}-{Based} {Convolutional} {Neural} {Network} for {Programming} {Language} {Processing}},
	volume = {abs/1409.5718},
	url = {http://arxiv.org/abs/1409.5718},
	journal = {CoRR},
	author = {Mou, Lili and Li, Ge and Jin, Zhi and Zhang, Lu and Wang, Tao},
	year = {2014},
	note = {\_eprint: 1409.5718}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105}
}

@article{bronstein_geometric_2016,
	title = {Geometric deep learning: going beyond {Euclidean} data},
	volume = {abs/1611.08097},
	url = {http://arxiv.org/abs/1611.08097},
	journal = {CoRR},
	author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
	year = {2016},
	note = {\_eprint: 1611.08097}
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Yann and Bottou, Leon and Bengio, Yoshua and Haffner, Patrick},
	year = {1998},
	pages = {2278--2324}
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016}
}

@incollection{lecun_handbook_1998,
	address = {Cambridge, MA, USA},
	title = {The {Handbook} of {Brain} {Theory} and {Neural} {Networks}},
	isbn = {0-262-51102-9},
	url = {http://dl.acm.org/citation.cfm?id=303568.303704},
	publisher = {MIT Press},
	author = {LeCun, Yann and Bengio, Yoshua},
	editor = {Arbib, Michael A.},
	year = {1998},
	note = {Section: Convolutional Networks for Images, Speech, and Time Series},
	pages = {255--258}
}

@book{szegedy_inception-v4_2016,
	title = {Inception-v4, {Inception}-{ResNet} and the {Impact} of {Residual} {Connections} on {Learning}},
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
	year = {2016},
	note = {\_eprint: 1602.07261}
}

@book{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	year = {2015},
	note = {\_eprint: 1505.04597}
}

@article{kipf_semi-supervised_2016,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	volume = {abs/1609.02907},
	url = {http://arxiv.org/abs/1609.02907},
	journal = {CoRR},
	author = {Kipf, Thomas N. and Welling, Max},
	year = {2016},
	note = {\_eprint: 1609.02907}
}

@article{simonovsky_dynamic_2017,
	title = {Dynamic {Edge}-{Conditioned} {Filters} in {Convolutional} {Neural} {Networks} on {Graphs}},
	volume = {abs/1704.02901},
	url = {http://arxiv.org/abs/1704.02901},
	journal = {CoRR},
	author = {Simonovsky, Martin and Komodakis, Nikos},
	year = {2017},
	note = {\_eprint: 1704.02901}
}

@article{wang_dynamic_2018,
	title = {Dynamic {Graph} {CNN} for {Learning} on {Point} {Clouds}},
	volume = {abs/1801.07829},
	url = {http://arxiv.org/abs/1801.07829},
	journal = {CoRR},
	author = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E. and Bronstein, Michael M. and Solomon, Justin M.},
	year = {2018},
	note = {\_eprint: 1801.07829}
}

@book{sugiyama_graphic_2013,
	title = {Graphic {Machine} {Learning}},
	publisher = {Posts Telecom Press and Kodansha LTD.},
	author = {Sugiyama, Masashi},
	translator = {X, Yongwei},
	year = {2013}
}

@book{flach_machine_2012,
	title = {Machine {Learning}: {The} {Art} and {Science} of {Algorithms} that {Make} {Sense} of {Data}(first edition)},
	publisher = {Posts Telecom Press and Cambridge University Press},
	author = {Flach, Peter},
	translator = {Duan, Fei},
	year = {2012}
}

@phdthesis{hsu_practical_2016,
	title = {A {Practical} {Guide} to {Support} {Vector} {Classification}},
	school = {Department of Computer Science National Taiwan University, Taipei 106, Taiwan},
	author = {Hsu, Chih-Wei and {Chih-Chung} and Len, Chih-Jen},
	month = may,
	year = {2016}
}

@misc{noauthor_support_2017,
	title = {Support vector machine},
	url = {https://en.wikipedia.org/wiki/Support_vector_machine},
	year = {2017}
}

@book{haykin_neural_2011,
	title = {Neural {Network} and {Learning} {Machines} (third edition)},
	publisher = {China Machine Press and Pearson Education},
	author = {Haykin, Simon},
	translator = {Shen, Furao and Xu, Ye and Zheng, Jun and Chao, Jin},
	year = {2011}
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	number = {4},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	year = {1943},
	pages = {115--133}
}

@book{mitchell_machine_1997,
	address = {New York},
	title = {Machine {Learning}},
	isbn = {978-0-07-042807-2},
	abstract = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. This exciting addition to the McGraw-Hill Series in Computer Science focuses on the concepts and techniques that contribute to the rapidly changing field of machine learning—including probability and statistics, artificial intelligence, and neural networks—unifying them all in a logical and coherent manner.},
	publisher = {McGraw-Hill},
	author = {Mitchell, Tom M.},
	year = {1997},
	keywords = {01624 105 book shelf ai learn algorithm}
}

@inproceedings{boser_training_1992,
	title = {A {Training} {Algorithm} for {Optimal} {Margin} {Classifiers}},
	booktitle = {Proceedings of the 5th {Annual} {ACM} {Workshop} on {Computational} {Learning} {Theory}},
	publisher = {ACM Press},
	author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
	year = {1992},
	pages = {144--152}
}

@article{cortes_support-vector_1995,
	title = {Support-{Vector} {Networks}},
	volume = {20},
	number = {3},
	journal = {Machine Learning},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	year = {1995},
	pages = {273--297}
}

@article{lafferty_conditional_2001,
	title = {Conditional {Random} {Fields}: {Probabilistic} {Models} for {Segmenting} and {Labeling} {Sequence} {Data}},
	author = {Lafferty, John and Mccallum, Andrew and Pereira, Fernando},
	year = {2001},
	pages = {282--289}
}

@article{greig_exact_1989,
	title = {Exact {Maximum} {A} {Posteriori} {Estimation} for {Binary} {Images}},
	volume = {51},
	doi = {10.1111/j.2517-6161.1989.tb01764.x},
	journal = {Journal of the Royal Statistical Society, Series B},
	author = {Greig, D.M. and Porteous, B.T. and Seheult, Allan},
	year = {1989},
	pages = {271--279}
}

@article{dice_measures_1945,
	title = {Measures of the {Amount} of {Ecologic} {Association} {Between} {Species}},
	volume = {26},
	number = {3},
	journal = {Ecology},
	author = {Dice, Lee R},
	year = {1945},
	pages = {297--302}
}

@article{dolz_hyperdense-net_2018,
	title = {{HyperDense}-{Net}: {A} hyper-densely connected {CNN} for multi-modal image segmentation},
	volume = {abs/1804.02967},
	url = {http://arxiv.org/abs/1804.02967},
	journal = {CoRR},
	author = {Dolz, Jose and Gopinath, Karthik and Yuan, Jing and Lombaert, Herve and Desrosiers, Christian and Ayed, Ismail Ben},
	year = {2018},
	note = {\_eprint: 1804.02967}
}

@article{kumar_co-learning_2018,
	title = {Co-{Learning} {Feature} {Fusion} {Maps} from {PET}-{CT} {Images} of {Lung} {Cancer}},
	journal = {ArXiv e-prints},
	author = {Kumar, Ashnil and Fulham, Michael and Feng, Dagan and Kim, Jinman},
	year = {2018},
	note = {\_eprint: 1810.02492},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {arXiv:1810.02492}
}

@article{mahmood_multimodal_2018,
	title = {Multimodal {Densenet}},
	journal = {ArXiv e-prints},
	author = {Mahmood, F. and Yang, Z. and Ashley, T. and Durr, N. J.},
	year = {2018},
	note = {\_eprint: 1811.07407},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning}
}

@article{song_optimal_2013,
	title = {Optimal {Co}-{Segmentation} of {Tumor} in {PET}-{CT} {Images} {With} {Context} {Information}},
	volume = {32},
	issn = {0278-0062},
	doi = {10.1109/TMI.2013.2263388},
	abstract = {Positron emission tomography (PET)-computed tomography (CT) images have been widely used in clinical practice for radiotherapy treatment planning of the radiotherapy. Many existing segmentation approaches only work for a single imaging modality, which suffer from the low spatial resolution in PET or low contrast in CT. In this work, we propose a novel method for the co-segmentation of the tumor in both PET and CT images, which makes use of advantages from each modality: the functionality information from PET and the anatomical structure information from CT. The approach formulates the segmentation problem as a minimization problem of a Markov random field model, which encodes the information from both modalities. The optimization is solved using a graph-cut based method. Two sub-graphs are constructed for the segmentation of the PET and the CT images, respectively. To achieve consistent results in two modalities, an adaptive context cost is enforced by adding context arcs between the two sub-graphs. An optimal solution can be obtained by solving a single maximum flow problem, which leads to simultaneous segmentation of the tumor volumes in both modalities. The proposed algorithm was validated in robust delineation of lung tumors on 23 PET-CT datasets and two head-and-neck cancer subjects. Both qualitative and quantitative results show significant improvement compared to the graph cut methods solely using PET or CT.},
	number = {9},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Song, Q. and Bai, J. and Han, D. and Bhatia, S. and Sun, W. and Rockey, W. and Bayouth, J. E. and Buatti, J. M. and Wu, X.},
	month = sep,
	year = {2013},
	keywords = {Algorithms, Computer-Assisted, Humans, Image Processing, image segmentation, Image segmentation, medical image processing, 23 PET-CT dataset, adaptive context cost, anatomical structure information, cancer, Computational modeling, Computed tomography, computerised tomography, Context, context arcs, context information, Context information, Databases, Factual, functionality information, global optimization, graph cut, graph theory, graph-cut based method, Head and Neck Neoplasms, head-and-neck cancer subject, image resolution, low spatial resolution, lung, lung tumor, Lungs, Markov Chains, Markov processes, Markov random field model, minimisation, minimization problem, optimal solution, PET-CT Image, positron emission tomography, Positron emission tomography, Positron emission tomography-computed tomography (PET-CT), positron emission tomography-computed tomography image, Positron-Emission Tomography, radiotherapy treatment planning, single imaging modality, single maximum flow problem, subgraph, Tomography, tumor optimal cosegmentation, tumor volume simultaneous segmentation, Tumors, tumours, X-Ray Computed},
	pages = {1685--1697}
}

@inproceedings{zhong_3d_2017,
	title = {{3D} {Alpha} {Matting} {Based} {Co}-segmentation of {Tumors} on {PET}-{CT} {Images}},
	booktitle = {{CMMI}/{RAMBO}/{SWITCH}@{MICCAI}},
	author = {Zhong, Zisha and Kim, Yusung and Buatti, John M. and Wu, Xiaodong},
	year = {2017}
}

@inproceedings{zhong_3d_2018,
	title = {{3D} fully convolutional networks for co-segmentation of tumors on {PET}-{CT} images},
	doi = {10.1109/ISBI.2018.8363561},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	author = {Zhong, Z. and Kim, Y. and Zhou, L. and Plichta, K. and Allen, B. and Buatti, J. and Wu, X.},
	month = apr,
	year = {2018},
	note = {ISSN: 1945-8452},
	keywords = {Three-dimensional displays, deep learning, image classification, image segmentation, Image segmentation, learning (artificial intelligence), medical image processing, cancer, Computed tomography, computerised tomography, graph cut, lung, positron emission tomography, Tumors, tumours, 3D fully convolutional networks, automated accurate tumor delineation, Biomedical imaging, biomedical MRI, cancer diagnosis, co-segmentation, co-segmentation model, computed tomography, critical diagnostic information, dual-modality imaging, final tumor segmentation results, fully convolutional networks, Lung, lung cancer patients, lung tumor segmentation, PET-CT images, PET-CT scans, probability maps, semantic segmentation framework, tumor reading},
	pages = {228--231}
}

@article{han_globally_2011,
	title = {Globally {Optimal} {Tumor} {Segmentation} in {PET}-{CT} {Images}: {A} {Graph}-{Based} {Co}-segmentation {Method}},
	volume = {22},
	doi = {10.1007/978-3-642-22092-0_21},
	journal = {Information processing in medical imaging : proceedings of the ... conference},
	author = {Han, Dongfeng and Bayouth, John and Song, Qi and Taurani, Aakant and Sonka, Milan and Buatti, John and Wu, Xiaodong},
	year = {2011},
	pages = {245--56}
}

@article{tran_learning_2014,
	title = {Learning {Spatiotemporal} {Features} with {3D} {Convolutional} {Networks}},
	journal = {ArXiv e-prints},
	author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
	year = {2014},
	note = {\_eprint: 1412.0767},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {arXiv:1412.0767}
}

@article{ju_random_2015,
	title = {Random {Walk} and {Graph} {Cut} for {Co}-{Segmentation} of {Lung} {Tumor} on {PET}-{CT} {Images}},
	volume = {24},
	issn = {1057-7149},
	doi = {10.1109/TIP.2015.2488902},
	abstract = {Accurate lung tumor delineation plays an important role in radiotherapy treatment planning. Since the lung tumor has poor boundary in positron emission tomography (PET) images and low contrast in computed tomography (CT) images, segmentation of tumor in the PET and CT images is a challenging task. In this paper, we effectively integrate the two modalities by making fully use of the superior contrast of PET images and superior spatial resolution of CT images. Random walk and graph cut method is integrated to solve the segmentation problem, in which random walk is utilized as an initialization tool to provide object seeds for graph cut segmentation on the PET and CT images. The co-segmentation problem is formulated as an energy minimization problem which is solved by max-flow/min-cut method. A graph, including two sub-graphs and a special link, is constructed, in which one sub-graph is for the PET and another is for CT, and the special link encodes a context term which penalizes the difference of the tumor segmentation on the two modalities. To fully utilize the characteristics of PET and CT images, a novel energy representation is devised. For the PET, a downhill cost and a 3D derivative cost are proposed. For the CT, a shape penalty cost is integrated into the energy function which helps to constrain the tumor region during the segmentation. We validate our algorithm on a data set which consists of 18 PET-CT images. The experimental results indicate that the proposed method is superior to the graph cut method solely using the PET or CT is more accurate compared with the random walk method, random walk co-segmentation method, and non-improved graph cut method.},
	number = {12},
	journal = {IEEE Transactions on Image Processing},
	author = {Ju, W. and Xiang, D. and Zhang, B. and Wang, L. and Kopriva, I. and Chen, X.},
	year = {2015},
	keywords = {Three-dimensional displays, Algorithms, Computer-Assisted, Humans, Image Processing, image segmentation, Image segmentation, medical image processing, Computed tomography, computerised tomography, Context, Databases, Factual, graph cut, lung, lung tumor, Lungs, minimisation, positron emission tomography, Positron emission tomography, Positron-Emission Tomography, Tomography, Tumors, tumours, X-Ray Computed, 3D derivative cost, Carcinoma, computed tomography (CT), Computed Tomography (CT), energy function, energy minimization problem, energy representation, graph cut segmentation method, interactive segmentation, Lung Neoplasms, max-flow-min-cut method, Non-Small-Cell Lung, PET-CT imaging, positron emission tomography (PET), Positron Emission Tomography (PET), prior information, random walk, random walk cosegmentation method, shape penalty, spatial resolution, special link encodes, tumor segmentation},
	pages = {5854--5867}
}

@article{minati_current_2009,
	title = {Current concepts in {Alzheimer}'s disease: a multidisciplinary review.},
	volume = {24},
	number = {2},
	journal = {American Journal of Alzheimers Disease and Other Dementias},
	author = {Minati, Ludovico and Edginton, Trudi and Bruzzone, Maria Grazia and Giaccone, Giorgio},
	year = {2009},
	pages = {95--121}
}

@article{pereira_brain_2016,
	title = {Brain {Tumor} {Segmentation} {Using} {Convolutional} {Neural} {Networks} in {MRI} {Images}},
	volume = {35},
	doi = {10.1109/TMI.2016.2538465},
	number = {5},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Pereira, S. and Pinto, A. and Alves, V. and Silva, C. A.},
	year = {2016},
	keywords = {Training, data augmentation, Computer-Assisted, deep learning, Humans, image segmentation, Image segmentation, Machine Learning, medical image processing, cancer, Context, Tumors, tumours, biomedical MRI, automatic segmentation, automatic segmentation methods, brain, Brain modeling, Brain Neoplasms, Brain tumor, brain tumor segmentation, clinical practice, CNN-based segmentation methods, convolutional neural networks, Dice similarity coefficient metrics, glioma, Glioma, gliomas, Image Interpretation, imaging technique, intensity normalization, Kernel, kernels, magnetic resonance imaging, Magnetic resonance imaging, Magnetic Resonance Imaging, manual segmentation, MRI images, Neural Networks (Computer), neurophysiology, on-site BRATS 2015 Challenge, oncological patients, online evaluation platform, precise quantitative measurements, preprocessing step, quality-of-life, reliable segmentation methods, spatial variability, structural variability},
	pages = {1240--1251}
}

@article{litjens_deep_2016,
	title = {Deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis},
	volume = {6},
	doi = {10.1038/srep26286},
	journal = {Scientific Reports},
	author = {Litjens, Geert and Sánchez, Clara and Timofeeva, Nadya and Hermsen, Meyke and Nagtegaal, Iris and Kovacs, Iringo and Hulsbergen-van de Kaa, Christina and Bult, Peter and Ginneken, Bram and van der Laak, Jeroen},
	year = {2016},
	pages = {26286}
}

@article{hosny_deep_2018,
	title = {Deep learning for lung cancer prognostication: {A} retrospective multi-cohort radiomics study},
	volume = {15},
	url = {https://doi.org/10.1371/journal.pmed.1002711},
	doi = {10.1371/journal.pmed.1002711},
	abstract = {Hugo Aerts and colleagues evaluate the ability of deep learning networks to extract relevant features from computed tomography lung cancer images and stratify patients into low and high mortality risk groups.},
	number = {11},
	journal = {PLOS Medicine},
	author = {Hosny, Ahmed and Parmar, Chintan and Coroller, Thibaud P. and Grossmann, Patrick and Zeleznik, Roman and Kumar, Avnish and Bussink, Johan and Gillies, Robert J. and Mak, Raymond H. and Aerts, Hugo J. W. L.},
	year = {2018},
	note = {Publisher: Public Library of Science},
	pages = {1--25}
}

@article{litjens_survey_2017,
	title = {A survey on deep learning in medical image analysis},
	volume = {42},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841517301135},
	doi = {https://doi.org/10.1016/j.media.2017.07.005},
	abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
	journal = {Medical Image Analysis},
	author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and Laak, Jeroen A. W. M. van der and Ginneken, Bram van and Sánchez, Clara I.},
	year = {2017},
	keywords = {Convolutional neural networks, Deep learning, Medical imaging, Survey},
	pages = {60 -- 88}
}

@article{yi_generative_2019,
	title = {Generative adversarial network in medical imaging: {A} review},
	volume = {58},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841518308430},
	doi = {https://doi.org/10.1016/j.media.2019.101552},
	abstract = {Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.},
	journal = {Medical Image Analysis},
	author = {Yi, Xin and Walia, Ekta and Babyn, Paul},
	year = {2019},
	keywords = {Deep learning, Medical imaging, Generative adversarial network, Generative model, Review},
	pages = {101552}
}

@article{wang_semi-supervised_2020,
	title = {Semi-supervised mp-{MRI} data synthesis with {StitchLayer} and auxiliary distance maximization},
	volume = {59},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841519301057},
	doi = {https://doi.org/10.1016/j.media.2019.101565},
	abstract = {The availability of a large amount of annotated data is critical for many medical image analysis applications, in particular for those relying on deep learning methods which are known to be data-hungry. However, annotated medical data, especially multimodal data, is often scarce and costly to obtain. In this paper, we address the problem of synthesizing multi-parameter magnetic resonance imaging data (i.e. mp-MRI), which typically consists of Apparent Diffusion Coefficient (ADC) and T2-weighted (T2w) images, containing clinically significant (CS) prostate cancer (PCa) via semi-supervised learning and adversarial learning. Specifically, our synthesizer generates mp-MRI data in a sequential manner: first utilizing a decoder to generate an ADC map from a 128-d latent vector, followed by translating the ADC to the T2w image via U-Net. The synthesizer is trained in a semi-supervised manner. In the supervised training process, a limited amount of paired ADC-T2w images and the corresponding ADC encodings are provided and the synthesizer learns the paired relationship by explicitly minimizing the reconstruction losses between synthetic and real images. To avoid overfitting limited ADC encodings, an unlimited amount of random latent vectors and unpaired ADC-T2w Images are utilized in the unsupervised training process for learning the marginal image distributions of real images. To improve the robustness for training the synthesizer, we decompose the difficult task of generating full-size images into several simpler tasks which generate sub-images only. A StitchLayer is then employed to seamlessly fuse sub-images together in an interlaced manner into a full-size image. In addition, to enforce the synthetic images to indeed contain distinguishable CS PCa lesions, we propose to also maximize an auxiliary distance of Jensen-Shannon divergence (JSD) between CS and nonCS images. Experimental results show that our method can effectively synthesize a large variety of mp-MRI images which contain meaningful CS PCa lesions, display a good visual quality and have the correct paired relationship between the two modalities of a pair. Compared to the state-of-the-art methods based on adversarial learning (Liu and Tuzel, 2016; Costa et al., 2017), our method achieves a significant improvement in terms of both visual quality and several popular quantitative evaluation metrics.},
	journal = {Medical Image Analysis},
	author = {Wang, Zhiwei and Lin, Yi and Cheng, Kwang-Ting (Tim) and Yang, Xin},
	year = {2020},
	keywords = {Deep learning, GAN, Generative models, Multimodal image synthesis},
	pages = {101565}
}

@article{swiderska-chadaj_learning_2019,
	title = {Learning to detect lymphocytes in immunohistochemistry with deep learning},
	volume = {58},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841519300829},
	doi = {https://doi.org/10.1016/j.media.2019.101547},
	abstract = {The immune system is of critical importance in the development of cancer. The evasion of destruction by the immune system is one of the emerging hallmarks of cancer. We have built a dataset of 171,166 manually annotated CD3+ and CD8+ cells, which we used to train deep learning algorithms for automatic detection of lymphocytes in histopathology images to better quantify immune response. Moreover, we investigate the effectiveness of four deep learning based methods when different subcompartments of the whole-slide image are considered: normal tissue areas, areas with immune cell clusters, and areas containing artifacts. We have compared the proposed methods in breast, colon and prostate cancer tissue slides collected from nine different medical centers. Finally, we report the results of an observer study on lymphocyte quantification, which involved four pathologists from different medical centers, and compare their performance with the automatic detection. The results give insights on the applicability of the proposed methods for clinical use. U-Net obtained the highest performance with an F1-score of 0.78 and the highest agreement with manual evaluation (κ=0.72), whereas the average pathologists agreement with reference standard was κ=0.64. The test set and the automatic evaluation procedure are publicly available at lyon19.grand-challenge.org.},
	journal = {Medical Image Analysis},
	author = {Swiderska-Chadaj, Zaneta and Pinckaers, Hans and Rijthoven, Mart van and Balkenhol, Maschenka and Melnikova, Margarita and Geessink, Oscar and Manson, Quirine and Sherman, Mark and Polonia, Antonio and Parry, Jeremy and Abubakar, Mustapha and Litjens, Geert and Laak, Jeroen van der and Ciompi, Francesco},
	year = {2019},
	keywords = {Deep learning, Computational pathology, Immune cell detection, Immunohistochemistry},
	pages = {101547}
}

@article{jamaludin_spinenet_2017,
	title = {{SpineNet}: {Automated} classification and evidence visualization in spinal {MRIs}},
	volume = {41},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S136184151730110X},
	doi = {https://doi.org/10.1016/j.media.2017.07.002},
	abstract = {The objective of this work is to automatically produce radiological gradings of spinal lumbar MRIs and also localize the predicted pathologies. We show that this can be achieved via a Convolutional Neural Network (CNN) framework that takes intervertebral disc volumes as inputs and is trained only on disc-specific class labels. Our contributions are: (i) a CNN architecture that predicts multiple gradings at once, and we propose variants of the architecture including using 3D convolutions; (ii) showing that this architecture can be trained using a multi-task loss function without requiring segmentation level annotation; and (iii) a localization method that clearly shows pathological regions in the disc volumes. We compare three visualization methods for the localization. The network is applied to a large corpus of MRI T2 sagittal spinal MRIs (using a standard clinical scan protocol) acquired from multiple machines, and is used to automatically compute disk and vertebra gradings for each MRI. These are: Pfirrmann grading, disc narrowing, upper/lower endplate defects, upper/lower marrow changes, spondylolisthesis, and central canal stenosis. We report near human performances across the eight gradings, and also visualize the evidence for these gradings localized on the original scans.},
	journal = {Medical Image Analysis},
	author = {Jamaludin, Amir and Kadir, Timor and Zisserman, Andrew},
	year = {2017},
	keywords = {MRI analysis, Radiological classification, Spinal MRI},
	pages = {63 -- 73}
}

@article{hohman_visual_2019,
	title = {Visual {Analytics} in {Deep} {Learning}: {An} {Interrogative} {Survey} for the {Next} {Frontiers}},
	volume = {25},
	doi = {10.1109/TVCG.2018.2843369},
	number = {8},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Hohman, F. and Kahng, M. and Pienta, R. and Chau, D. H.},
	year = {2019},
	keywords = {Neural networks, Machine learning, Computational modeling, Deep learning, Conferences, Data visualization, information visualization, neural networks, visual analytics, Visual analytics},
	pages = {2674--2693}
}

@article{liu_automated_2019,
	title = {Automated detection and classification of thyroid nodules in ultrasound images using clinical-knowledge-guided convolutional neural networks},
	volume = {58},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841519300970},
	doi = {https://doi.org/10.1016/j.media.2019.101555},
	abstract = {Accurate diagnosis of thyroid nodules using ultrasonography is a valuable but tough task even for experienced radiologists, considering both benign and malignant nodules have heterogeneous appearances. Computer-aided diagnosis (CAD) methods could potentially provide objective suggestions to assist radiologists. However, the performance of existing learning-based approaches is still limited, for direct application of general learning models often ignores critical domain knowledge related to the specific nodule diagnosis. In this study, we propose a novel deep-learning-based CAD system, guided by task-specific prior knowledge, for automated nodule detection and classification in ultrasound images. Our proposed CAD system consists of two stages. First, a multi-scale region-based detection network is designed to learn pyramidal features for detecting nodules at different feature scales. The region proposals are constrained by the prior knowledge about size and shape distributions of real nodules. Then, a multi-branch classification network is proposed to integrate multi-view diagnosis-oriented features, in which each network branch captures and enhances one specific group of characteristics that were generally used by radiologists. We evaluated and compared our method with the state-of-the-art CAD methods and experienced radiologists on two datasets, i.e. Dataset I and Dataset II. The detection and diagnostic accuracy on Dataset I were 97.5\% and 97.1\%, respectively. Besides, our CAD system also achieved better performance than experienced radiologists on Dataset II, with improvements of accuracy for 8\%. The experimental results demonstrate that our proposed method is effective in the discrimination of thyroid nodules.},
	journal = {Medical Image Analysis},
	author = {Liu, Tianjiao and Guo, Qianqian and Lian, Chunfeng and Ren, Xuhua and Liang, Shujun and Yu, Jing and Niu, Lijuan and Sun, Weidong and Shen, Dinggang},
	year = {2019},
	keywords = {Convolutional neural networks, Clinical knowledge, Thyroid nodule, Ultrasound image},
	pages = {101555}
}

@article{jimenez-carretero_graph-cut_2019,
	title = {A graph-cut approach for pulmonary artery-vein segmentation in noncontrast {CT} images},
	volume = {52},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841518308740},
	doi = {https://doi.org/10.1016/j.media.2018.11.011},
	abstract = {Lung vessel segmentation has been widely explored by the biomedical image processing community; however, the differentiation of arterial from venous irrigation is still a challenge. Pulmonary artery–vein (AV) segmentation using computed tomography (CT) is growing in importance owing to its undeniable utility in multiple cardiopulmonary pathological states, especially those implying vascular remodelling, allowing the study of both flow systems separately. We present a new framework to approach the separation of tree-like structures using local information and a specifically designed graph-cut methodology that ensures connectivity as well as the spatial and directional consistency of the derived subtrees. This framework has been applied to the pulmonary AV classification using a random forest (RF) pre-classifier to exploit the local anatomical differences of arteries and veins. The evaluation of the system was performed using 192 bronchopulmonary segment phantoms, 48 anthropomorphic pulmonary CT phantoms, and 26 lungs from noncontrast CT images with precise voxel-based reference standards obtained by manually labelling the vessel trees. The experiments reveal a relevant improvement in the accuracy ( ∼ 20\%) of the vessel particle classification with the proposed framework with respect to using only the pre-classification based on local information applied to the whole area of the lung under study. The results demonstrated the accurate differentiation between arteries and veins in both clinical and synthetic cases, specifically when the image quality can guarantee a good airway segmentation, which opens a huge range of possibilities in the clinical study of cardiopulmonary diseases.},
	journal = {Medical Image Analysis},
	author = {Jimenez-Carretero, Daniel and Bermejo-Peláez, David and Nardelli, Pietro and Fraga, Patricia and Fraile, Eduardo and Estépar, Raúl San José and Ledesma-Carbayo, Maria J.},
	year = {2019},
	keywords = {Lung, Arteries, Artery-vein segmentation, Graph-cuts, Noncontrast CT, Phantoms, Random forest, Veins},
	pages = {144 -- 159}
}

@article{qaiser_fast_2019,
	title = {Fast and accurate tumor segmentation of histology images using persistent homology and deep convolutional features},
	volume = {55},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841518302688},
	doi = {https://doi.org/10.1016/j.media.2019.03.014},
	abstract = {Tumor segmentation in whole-slide images of histology slides is an important step towards computer-assisted diagnosis. In this work, we propose a tumor segmentation framework based on the novel concept of persistent homology profiles (PHPs). For a given image patch, the homology profiles are derived by efficient computation of persistent homology, which is an algebraic tool from homology theory. We propose an efficient way of computing topological persistence of an image, alternative to simplicial homology. The PHPs are devised to distinguish tumor regions from their normal counterparts by modeling the atypical characteristics of tumor nuclei. We propose two variants of our method for tumor segmentation: one that targets speed without compromising accuracy and the other that targets higher accuracy. The fast version is based on a selection of exemplar image patches from a convolution neural network (CNN) and patch classification by quantifying the divergence between the PHPs of exemplars and the input image patch. Detailed comparative evaluation shows that the proposed algorithm is significantly faster than competing algorithms while achieving comparable results. The accurate version combines the PHPs and high-level CNN features and employs a multi-stage ensemble strategy for image patch labeling. Experimental results demonstrate that the combination of PHPs and CNN features outperform competing algorithms. This study is performed on two independently collected colorectal datasets containing adenoma, adenocarcinoma, signet, and healthy cases. Collectively, the accurate tumor segmentation produces the highest average patch-level F1-score, as compared with competing algorithms, on malignant and healthy cases from both the datasets. Overall the proposed framework highlights the utility of persistent homology for histopathology image analysis.},
	journal = {Medical Image Analysis},
	author = {Qaiser, Talha and Tsang, Yee-Wah and Taniyama, Daiki and Sakamoto, Naoya and Nakane, Kazuaki and Epstein, David and Rajpoot, Nasir},
	year = {2019},
	keywords = {Deep learning, Computational pathology, Colorectal (colon) cancer, Histology image analysis, Persistent homology, Tumor segmentation},
	pages = {1 -- 14}
}

@article{song_multi-layer_2019,
	title = {Multi-layer boosting sparse convolutional model for generalized nuclear segmentation from histopathology images},
	volume = {176},
	issn = {0950-7051},
	url = {http://www.sciencedirect.com/science/article/pii/S095070511930156X},
	doi = {https://doi.org/10.1016/j.knosys.2019.03.031},
	abstract = {It is a challenging problem to achieve generalized nuclear segmentation in digital histopathology images. Existing techniques, using either handcrafted features in learning-based models or traditional image analysis-based approaches, do not effectively tackle the challenging cases, such as crowded nuclei, chromatin-sparse, and heavy background clutter. In contrast, deep networks have achieved state-of-the-art performance in modeling various nuclear appearances. However, their success is limited due to the size of the considered networks. We solve these problems by reformulating nuclear segmentation in terms of a cascade 2-class classification problem and propose a multi-layer boosting sparse convolutional (ML-BSC) model. In the proposed ML-BSC model, discriminative probabilistic binary decision trees (PBDTs) are designed as weak learners in each layer to cope with challenging cases. A sparsity-constrained cascade structure enables the ML-BSC model to improve representation learning. Comparing to the existing techniques, our method can accurately separate individual nuclei in complex histopathology images, and it is more robust against chromatin-sparse and heavy background clutter. An evaluation carried out using three disparate datasets demonstrates the superiority of our method over the state-of-the-art supervised approaches in terms of segmentation accuracy.},
	journal = {Knowledge-Based Systems},
	author = {Song, Jie and Xiao, Liang and Molaei, Mohsen and Lian, Zhichao},
	year = {2019},
	keywords = {Representation learning, Cascade classification, Multi-layer boosting sparse convolutional model, Nucleus segmentation, Probabilistic binary decision tree},
	pages = {40 -- 53}
}

@article{tofighi_prior_2019,
	title = {Prior {Information} {Guided} {Regularized} {Deep} {Learning} for {Cell} {Nucleus} {Detection}},
	volume = {38},
	doi = {10.1109/TMI.2019.2895318},
	number = {9},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Tofighi, M. and Guo, T. and Vanamala, J. K. P. and Monga, V.},
	month = sep,
	year = {2019},
	keywords = {cellular biophysics, deep learning, Image segmentation, learning (artificial intelligence), medical image processing, Image edge detection, Biomedical imaging, convolutional neural networks, Deep learning, biology computing, canonical cell nuclei shapes, cell nuclei detection, cell nucleus boundary, cell nucleus detection, cellular image quality, Computer architecture, convolutional neural nets, deep learning methods, domain expert, fixed processing part, input images, labeled nuclei locations, learnable layers, learnable shapes, Microprocessors, morphological processing, multiple cell nuclei, network structures, nuclear morphology, Nucleus detection, nucleus shapes, regularization terms, Shape, shape priors, spatial processing, training set, TSP-CNN, tunable SP-CNN},
	pages = {2047--2058}
}

@article{naylor_segmentation_2019,
	title = {Segmentation of {Nuclei} in {Histopathology} {Images} by {Deep} {Regression} of the {Distance} {Map}},
	volume = {38},
	doi = {10.1109/TMI.2018.2865709},
	number = {2},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Naylor, P. and Laé, M. and Reyal, F. and Walter, T.},
	year = {2019},
	keywords = {neural nets, biological tissues, cellular biophysics, deep learning, diseases, image segmentation, Image segmentation, medical image processing, Pathology, cancer, Tumors, fully convolutional networks, Computer architecture, Biology, Cancer, Cancer research, cell nuclei, deep regression, digital pathology, diseased tissue, distance map, Haematoxylin and Eosin stained histopathology data, histopathology, histopathology data, histopathology images, interpretable models, nuclei segmentation, patient diagnosis, prognosis tasks, quantitative profiles, regression task, segmentation problem, Task analysis},
	pages = {448--459}
}

@inproceedings{dou_3d_2016,
	address = {Cham},
	title = {{3D} {Deeply} {Supervised} {Network} for {Automatic} {Liver} {Segmentation} from {CT} {Volumes}},
	isbn = {978-3-319-46723-8},
	abstract = {Automatic liver segmentation from CT volumes is a crucial prerequisite yet challenging task for computer-aided hepatic disease diagnosis and treatment. In this paper, we present a novel 3D deeply supervised network (3D DSN) to address this challenging task. The proposed 3D DSN takes advantage of a fully convolutional architecture which performs efficient end-to-end learning and inference. More importantly, we introduce a deep supervision mechanism during the learning process to combat potential optimization difficulties, and thus the model can acquire a much faster convergence rate and more powerful discrimination capability. On top of the high-quality score map produced by the 3D DSN, a conditional random field model is further employed to obtain refined segmentation results. We evaluated our framework on the public MICCAI-SLiver07 dataset. Extensive experiments demonstrated that our method achieves competitive segmentation results to state-of-the-art approaches with a much faster processing speed.},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2016},
	publisher = {Springer International Publishing},
	author = {Dou, Qi and Chen, Hao and Jin, Yueming and Yu, Lequan and Qin, Jing and Heng, Pheng-Ann},
	editor = {Ourselin, Sebastien and Joskowicz, Leo and Sabuncu, Mert R. and Unal, Gozde and Wells, William},
	year = {2016},
	pages = {149--157}
}

@article{lu_automatic_2017,
	title = {Automatic {3D} liver location and segmentation via convolutional neural network and graph cut},
	volume = {12},
	issn = {1861-6429},
	url = {https://doi.org/10.1007/s11548-016-1467-3},
	doi = {10.1007/s11548-016-1467-3},
	abstract = {Segmentation of the liver from abdominal computed tomography (CT) images is an essential step in some computer-assisted clinical interventions, such as surgery planning for living donor liver transplant, radiotherapy and volume measurement. In this work, we develop a deep learning algorithm with graph cut refinement to automatically segment the liver in CT scans.},
	number = {2},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Lu, Fang and Wu, Fa and Hu, Peijun and Peng, Zhiyi and Kong, Dexing},
	year = {2017},
	pages = {171--182}
}

@inproceedings{zhou_fixed-point_2017,
	address = {Cham},
	title = {A {Fixed}-{Point} {Model} for {Pancreas} {Segmentation} in {Abdominal} {CT} {Scans}},
	isbn = {978-3-319-66182-7},
	abstract = {Deep neural networks have been widely adopted for automatic organ segmentation from abdominal CT scans. However, the segmentation accuracy of some small organs (e.g., the pancreas) is sometimes below satisfaction, arguably because deep networks are easily disrupted by the complex and variable background regions which occupies a large fraction of the input volume. In this paper, we formulate this problem into a fixed-point model which uses a predicted segmentation mask to shrink the input region. This is motivated by the fact that a smaller input region often leads to more accurate segmentation. In the training process, we use the ground-truth annotation to generate accurate input regions and optimize network weights. On the testing stage, we fix the network parameters and update the segmentation results in an iterative manner. We evaluate our approach on the NIH pancreas segmentation dataset, and outperform the state-of-the-art by more than \$\$4{\textbackslash}backslash\%\$\$, measured by the average Dice-Sørensen Coefficient (DSC). In addition, we report \$\$62.43{\textbackslash}backslash\%\$\$DSC in the worst case, which guarantees the reliability of our approach in clinical applications.},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} − {MICCAI} 2017},
	publisher = {Springer International Publishing},
	author = {Zhou, Yuyin and Xie, Lingxi and Shen, Wei and Wang, Yan and Fishman, Elliot K. and Yuille, Alan L.},
	editor = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D. Louis and Duchesne, Simon},
	year = {2017},
	pages = {693--701}
}

@inproceedings{cai_pancreas_2017,
	address = {Cham},
	title = {Pancreas {Segmentation} in {MRI} {Using} {Graph}-{Based} {Decision} {Fusion} on {Convolutional} {Neural} {Networks}},
	isbn = {978-3-319-66179-7},
	abstract = {Deep neural networks have demonstrated very promising performance on accurate segmentation of challenging organs (e.g., pancreas) in abdominal CT and MRI scans. The current deep learning approaches conduct pancreas segmentation by processing sequences of 2D image slices independently through deep, dense per-pixel masking for each image, without explicitly enforcing spatial consistency constraint on segmentation of successive slices. We propose a new convolutional/recurrent neural network architecture to address the contextual learning and segmentation consistency problem. A deep convolutional sub-network is first designed and pre-trained from scratch. The output layer of this network module is then connected to recurrent layers and can be fine-tuned for contextual learning, in an end-to-end manner. Our recurrent sub-network is a type of Long short-term memory (LSTM) network that performs segmentation on an image by integrating its neighboring slice segmentation predictions, in the form of a dependent sequence processing. Additionally, a novel segmentation-direct loss function (named Jaccard Loss) is proposed and deep networks are trained to optimize Jaccard Index (JI) directly. Extensive experiments are conducted to validate our proposed deep models, on quantitative pancreas segmentation using both CT and MRI scans. Our method outperforms the state-of-the-art work on CT [11] and MRI pancreas segmentation [1], respectively.},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} − {MICCAI} 2017},
	publisher = {Springer International Publishing},
	author = {Cai, Jinzheng and Lu, Le and Xie, Yuanpu and Xing, Fuyong and Yang, Lin},
	editor = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D. Louis and Duchesne, Simon},
	year = {2017},
	pages = {674--682}
}

@inproceedings{khagi_alzheimers_2019,
	title = {Alzheimer's disease {Classification} from {Brain} {MRI} based on transfer learning from {CNN}},
	doi = {10.1109/BMEiCON.2018.8609974},
	author = {Khagi, Bijen and Lee, Chung and Kwon, Goo-Rak},
	year = {2019}
}

@inproceedings{khvostikov_3d_2018,
	title = {{3D} {CNN}-based classification using {sMRI} and {MD}-{DTI} images for {Alzheimer} disease studies},
	author = {Khvostikov, Alexander and Aderghal, Karim and Benois-Pineau, Jenny and Krylov, Andrey and Catheline, Gwenaelle},
	year = {2018},
	note = {\_eprint: 1801.05968}
}

@inproceedings{khvostikov_classification_2017,
	title = {Classification methods on different imaging modalities for {Alzheimer} disease studies},
	author = {Khvostikov, Alexander and Benois-Pineau, Jenny and Krylov, Andrey and Catheline, Gwenaelle},
	year = {2017}
}

@article{zhao_deep_2018,
	title = {A deep learning model integrating {FCNNs} and {CRFs} for brain tumor segmentation},
	volume = {43},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S136184151730141X},
	doi = {https://doi.org/10.1016/j.media.2017.10.002},
	abstract = {Accurate and reliable brain tumor segmentation is a critical component in cancer diagnosis, treatment planning, and treatment outcome evaluation. Build upon successful deep learning techniques, a novel brain tumor segmentation method is developed by integrating fully convolutional neural networks (FCNNs) and Conditional Random Fields (CRFs) in a unified framework to obtain segmentation results with appearance and spatial consistency. We train a deep learning based segmentation model using 2D image patches and image slices in following steps: 1) training FCNNs using image patches; 2) training CRFs as Recurrent Neural Networks (CRF-RNN) using image slices with parameters of FCNNs fixed; and 3) fine-tuning the FCNNs and the CRF-RNN using image slices. Particularly, we train 3 segmentation models using 2D image patches and slices obtained in axial, coronal and sagittal views respectively, and combine them to segment brain tumors using a voting based fusion strategy. Our method could segment brain images slice-by-slice, much faster than those based on image patches. We have evaluated our method based on imaging data provided by the Multimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015 and BRATS 2016. The experimental results have demonstrated that our method could build a segmentation model with Flair, T1c, and T2 scans and achieve competitive performance as those built with Flair, T1, T1c, and T2 scans.},
	journal = {Medical Image Analysis},
	author = {Zhao, Xiaomei and Wu, Yihong and Song, Guidong and Li, Zhenye and Zhang, Yazhuo and Fan, Yong},
	year = {2018},
	keywords = {Deep learning, Brain tumor segmentation, Conditional random fields, Fully convolutional neural networks},
	pages = {98 -- 111}
}

@article{graham_xy_2018,
	title = {{XY} {Network} for {Nuclear} {Segmentation} in {Multi}-{Tissue} {Histology} {Images}},
	volume = {abs/1812.06499},
	url = {http://arxiv.org/abs/1812.06499},
	journal = {CoRR},
	author = {Graham, Simon and Vu, Quoc Dang and Raza, Shan e Ahmed and Kwak, Jin Tae and Rajpoot, Nasir M.},
	year = {2018},
	note = {\_eprint: 1812.06499}
}

@unpublished{zhang_u-net_nodate,
	title = {U-net based analysis of {MRI} for {Alzheimer}’s disease diagnosis {\textbackslash}{Huge} todo, and this paper is under publishing},
	author = {Zhang, Liang and Fan, Zhonghao}
}

@unpublished{zhang_block_nodate,
	title = {Block {Level} {Skip} {Connections} across {Cascaded} {V}-{Net} for {Multi}-organ {Segmentation} {\textbackslash}{Huge} todo, and this paper is under publishing},
	author = {Zhang, Liang and Zhang, Jiaming}
}

@unpublished{zhang_unknow_nodate,
	title = {unknow},
	author = {Zhang, Liang and Kong, Xiangwen}
}

@incollection{bottou_stochastic_2012,
	title = {Stochastic {Gradient} {Descent} {Tricks}},
	url = {https://doi.org/10.1007/978-3-642-35289-8_25},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade} - {Second} {Edition}},
	author = {Bottou, Léon},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_25},
	pages = {421--436}
}

@misc{noauthor_stochastic_2017,
	title = {Stochastic gradient descent},
	url = {https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Extensions_and_variants},
	month = aug,
	year = {2017}
}

@article{srivastava_dropout_2014,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	volume = {15},
	url = {http://jmlr.org/papers/v15/srivastava14a.html},
	journal = {Journal of Machine Learning Research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958}
}

@article{ioffe_batch_2015,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	volume = {abs/1502.03167},
	url = {http://arxiv.org/abs/1502.03167},
	journal = {CoRR},
	author = {Ioffe, Sergey and Szegedy, Christian},
	year = {2015},
	note = {\_eprint: 1502.03167}
}

@article{ruder_overview_2016,
	title = {An overview of gradient descent optimization algorithms},
	volume = {abs/1609.04747},
	url = {http://arxiv.org/abs/1609.04747},
	journal = {CoRR},
	author = {Ruder, Sebastian},
	year = {2016},
	note = {\_eprint: 1609.04747}
}

@article{werbos_beyond_1974,
	title = {Beyond regression : new tools for prediction and analysis in the behavioral sciences /},
	author = {Werbos, Paul and J. (Paul John, Paul},
	year = {1974}
}

@incollection{rumelhart_neurocomputing_1988,
	address = {Cambridge, MA, USA},
	title = {Neurocomputing: {Foundations} of {Research}},
	isbn = {0-262-01097-6},
	url = {http://dl.acm.org/citation.cfm?id=65669.104451},
	publisher = {MIT Press},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	editor = {Anderson, James A. and Rosenfeld, Edward},
	year = {1988},
	note = {Section: Learning Representations by Back-propagating Errors},
	pages = {696--699}
}

@inproceedings{hutter_automl_2016,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{AutoML} 2016 {Workshop} {Proceedings}: {Proceedings} of the {Workshop} on {Automatic} {Machine} {Learning}, 24 {June} 2016, {New} {York}, {New} {York}, {USA}},
	volume = {64},
	language = {English},
	publisher = {Proceedings of Machine Learning Research},
	author = {Hutter, F. and Kotthoff, L. and Vanschoren, J.},
	year = {2016}
}

@article{zhu_deeplung_2018,
	title = {{DeepLung}: {Deep} {3D} {Dual} {Path} {Nets} for {Automated} {Pulmonary} {Nodule} {Detection} and {Classification}},
	author = {Zhu, Wentao and Liu, Chaochun and Fan, Wei and Xie, Xiaohui},
	year = {2018},
	note = {\_eprint: 1801.09555}
}

@book{milletari_v-net_2016,
	title = {V-{Net}: {Fully} {Convolutional} {Neural} {Networks} for {Volumetric} {Medical} {Image} {Segmentation}},
	author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	year = {2016},
	note = {\_eprint: 1606.04797}
}

@article{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
	note = {\_eprint: 1502.01852}
}

@book{berman_lovasz-softmax_2017,
	title = {The {Lovász}-{Softmax} loss: {A} tractable surrogate for the optimization of the intersection-over-union measure in neural networks},
	author = {Berman, Maxim and Triki, Amal Rannen and Blaschko, Matthew B.},
	year = {2017},
	note = {\_eprint: 1705.08790}
}

@inproceedings{trullo_segmentation_2017,
	title = {Segmentation of {Organs} at {Risk} in thoracic {CT} images using a {SharpMask} architecture and {Conditional} {Random} {Fields}},
	volume = {2017},
	doi = {10.1109/ISBI.2017.7950685},
	booktitle = {Proceedings. {IEEE} {International} {Symposium} on {Biomedical} {Imaging}},
	author = {Trullo, R. and Petitjean, Caroline and Ruan, Su and Dubray, Bernard and Nie, D. and Shen, D.},
	year = {2017},
	pages = {1003--1006}
}

@book{simonyan_very_2014,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2014},
	note = {\_eprint: 1409.1556}
}

@article{girshick_fast_2015,
	title = {Fast {R}-{CNN}},
	author = {Girshick, Ross},
	year = {2015},
	note = {\_eprint: 1504.08083}
}

@article{he_mask_2017,
	title = {Mask {R}-{CNN}},
	journal = {arXiv: Computer Vision and Pattern Recognition},
	author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
	year = {2017}
}

@article{chen_dual_2017,
	title = {Dual {Path} {Networks}},
	journal = {arXiv: Computer Vision and Pattern Recognition},
	author = {Chen, Yunpeng and Li, Jianan and Xiao, Huaxin and Jin, Xiaojie and Yan, Shuicheng and Feng, Jiashi},
	year = {2017}
}

@article{devlin_bert_2018,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2018},
	note = {\_eprint: 1810.04805}
}

@book{yang_xlnet_2019,
	title = {{XLNet}: {Generalized} {Autoregressive} {Pretraining} for {Language} {Understanding}},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	year = {2019},
	note = {\_eprint: 1906.08237}
}

@book{skibbe_marmonet_2019,
	title = {{MarmoNet}: a pipeline for automated projection mapping of the common marmoset brain from whole-brain serial two-photon tomography},
	author = {Skibbe, Henrik and Watakabe, Akiya and Nakae, Ken and Gutierrez, Carlos Enrique and Tsukada, Hiromichi and Hata, Junichi and Kawase, Takashi and Gong, Rui and Woodward, Alexander and Doya, Kenji and Okano, Hideyuki and Yamamori, Tetsuo and Ishii, Shin},
	year = {2019},
	note = {\_eprint: 1908.00876}
}

@book{zhang_leveraging_2019,
	title = {Leveraging {Vision} {Reconstruction} {Pipelines} for {Satellite} {Imagery}},
	author = {Zhang, Kai and Sun, Jin and Snavely, Noah},
	year = {2019},
	note = {\_eprint: 1910.02989}
}

@book{rajan_pi-pe_2019,
	title = {Pi-{PE}: {A} {Pipeline} for {Pulmonary} {Embolism} {Detection} using {Sparsely} {Annotated} {3D} {CT} {Images}},
	author = {Rajan, Deepta and Beymer, David and Abedin, Shafiqul and Dehghan, Ehsan},
	year = {2019},
	note = {\_eprint: 1910.02175}
}

@book{rajchl_neuronet_2018,
	title = {{NeuroNet}: {Fast} and {Robust} {Reproduction} of {Multiple} {Brain} {Image} {Segmentation} {Pipelines}},
	author = {Rajchl, Martin and Pawlowski, Nick and Rueckert, Daniel and Matthews, Paul M. and Glocker, Ben},
	year = {2018},
	note = {\_eprint: 1806.04224}
}

@article{zhang_computer_2017,
	title = {A {Computer} {Vision} {Pipeline} for {Automated} {Determination} of {Cardiac} {Structure} and {Function} and {Detection} of {Disease} by {Two}-{Dimensional} {Echocardiography}},
	author = {Zhang, Jeffrey and Gajjala, Sravani and Agrawal, Pulkit and Tison, Geoffrey H. and Hallock, Laura A. and Beussink-Nelson, Lauren and Fan, Eugene and Aras, Mandar A. and Jordan, ChaRandle and Fleischmann, Kirsten E. and Melisko, Michelle and Qasim, Atif and Efros, Alexei and Shah, Sanjiv J. and Bajcsy, Ruzena and Deo, Rahul C.},
	year = {2017},
	note = {\_eprint: 1706.07342}
}

@book{crankshaw_inferline_2018,
	title = {{InferLine}: {ML} {Inference} {Pipeline} {Composition} {Framework}},
	author = {Crankshaw, Daniel and Sela, Gur-Eyal and Zumar, Corey and Mo, Xiangxi and Gonzalez, Joseph E. and Stoica, Ion and Tumanov, Alexey},
	year = {2018},
	note = {\_eprint: 1812.01776}
}

@inproceedings{lee_survey_2015,
	title = {A survey of medical image processing tools},
	doi = {10.1109/ICSECS.2015.7333105},
	booktitle = {2015 4th {International} {Conference} on {Software} {Engineering} and {Computer} {Systems} ({ICSECS})},
	author = {Lee, L. and Liew, S.},
	year = {2015},
	keywords = {computer vision, Image segmentation, medical image processing, Biomedical image processing, clinical study, diagnostic radiography, graphical schematic diagram, image processing, Medical diagnostic imaging, medical image processing software tool, medical image processing tools, operating systems, pipelined processors, radiation therapy, radiographic techniques, radiotherapy preparation, Software, software tools, tools component, treatment planning},
	pages = {171--176}
}

@article{radul_functional_2001,
	title = {Functional {Representations} of {Lawson} {Monads}},
	volume = {9},
	journal = {Applied Categorical Structures},
	author = {Radul, Taras},
	year = {2001},
	pages = {457--463}
}

@techreport{marlow_haskell_2010,
	title = {Haskell 2010 {Language} {Report}},
	author = {Marlow, Simon},
	year = {2010}
}

@article{chen_end--end_2018,
	title = {An {End}-to-end {Approach} to {Semantic} {Segmentation} with {3D} {CNN} and {Posterior}-{CRF} in {Medical} {Images}},
	author = {Chen, Shuai and Bruijne, Marleen de},
	year = {2018},
	note = {\_eprint: 1811.03549}
}

@article{yao_survey_2017,
	title = {A {Survey} on {Pre}-{Processing} in {Image} {Matting}},
	volume = {32},
	issn = {1860-4749},
	url = {https://doi.org/10.1007/s11390-017-1709-z},
	doi = {10.1007/s11390-017-1709-z},
	abstract = {Pre-processing is an important step in digital image matting, which aims to classify more accurate foreground and background pixels from the unknown region of the input three-region mask (Trimap). This step has no relation with the well-known matting equation and only compares color differences between the current unknown pixel and those known pixels. These newly classified pure pixels are then fed to the matting process as samples to improve the quality of the final matte. However, in the research field of image matting, the importance of pre-processing step is still blurry. Moreover, there are no corresponding review articles for this step, and the quantitative comparison of Trimap and alpha mattes after this step still remains unsolved. In this paper, the necessity and the importance of pre-processing step in image matting are firstly discussed in details. Next, current pre-processing methods are introduced by using the following two categories: static thresholding methods and dynamic thresholding methods. Analyses and experimental results show that static thresholding methods, especially the most popular iterative method, can make accurate pixel classifications in those general Trimaps with relatively fewer unknown pixels. However, in a much larger Trimap, there methods are limited by the conservative color and spatial thresholds. In contrast, dynamic thresholding methods can make much aggressive classifications on much difficult cases, but still strongly suffer from noises and false classifications. In addition, the sharp boundary detector is further discussed as a prior of pure pixels. Finally, summaries and a more effective approach are presented for pre-processing compared with the existing methods.},
	number = {1},
	journal = {Journal of Computer Science and Technology},
	author = {Yao, Gui-Lin},
	year = {2017},
	pages = {122--138}
}

@article{jeyavathana_survey_2016,
	title = {A {Survey}: {Analysis} on {Pre}-processing and {Segmentation} {Techniques} for {Medical} {Images}},
	journal = {International Journal of Research and Scientific Innovation (IJRSI)},
	author = {Jeyavathana, R and Ramasamy, Balasubramanian and Pandian, Anbarasa},
	year = {2016}
}

@book{ferdouse_simulation_2011,
	title = {Simulation and {Performance} {Analysis} of {Adaptive} {Filtering} {Algorithms} in {Noise} {Cancellation}},
	author = {Ferdouse, Lilatul and Akhter, Nasrin and Nipa, Tamanna Haque and Jaigirdar, Fariha Tasmin},
	year = {2011},
	note = {\_eprint: 1104.1962}
}

@incollection{ogiela_preprocessing_2008,
	address = {Berlin, Heidelberg},
	title = {Preprocessing medical images and their overall enhancement},
	isbn = {978-3-540-75402-2},
	url = {https://doi.org/10.1007/978-3-540-75402-2_4},
	abstract = {This chapter briefly discusses the main stages of image preprocessing. The introduction to this book mentioned that the preprocessing of medical image is subject to certain restrictions and is generally more complex than the processing of other image types [26, 52]. This is why, of the many different techniques and methods for image filtering, we have decided to discuss here only selected ones, most frequently applied to medical images and which have been proven to be suitable for that purpose in numerous practical cases. Their operation will be illustrated with examples of simple procedures aimed at improving the quality of imaging and allowing significant information to be generated for its use at the stages of image interpretation.},
	booktitle = {Modern {Computational} {Intelligence} {Methods} for the {Interpretation} of {Medical} {Images}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ogiela, Marek R. and Tadeusiewicz, Ryszard},
	year = {2008},
	doi = {10.1007/978-3-540-75402-2_4},
	pages = {65--97}
}

@article{tustison_n4itk_2010,
	title = {{N4ITK}: {Improved} {N3} {Bias} {Correction}},
	volume = {29},
	doi = {10.1109/TMI.2010.2046908},
	number = {6},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Tustison, N. J. and Avants, B. B. and Cook, P. A. and Zheng, Y. and Egan, A. and Yushkevich, P. A. and Gee, J. C.},
	month = jun,
	year = {2010},
	keywords = {Testing, Algorithms, Computer-Assisted, Humans, image segmentation, medical image processing, lung, Lungs, biomedical MRI, brain, Brain modeling, Image Interpretation, Magnetic Resonance Imaging, Approximation algorithms, Artifacts, Availability, B-spline approximation, B-spline least-squares fitting, bias correction, bias field, Brain, Documentation, hierarchical optimization scheme, image analysis, Image databases, Image Enhancement, inhomogeneity, lung image data, N3, N4ITK, nonparametric nonuniform intensity normalization, Reproducibility of Results, Robustness, Sensitivity and Specificity, Spline},
	pages = {1310--1320}
}

@article{srameshkumar_speckle_2016,
	title = {Speckle {Noise} {Removal} in {MRI} {Scan} {Image} {Using} {WB} – {Filter}},
	volume = {5},
	issn = {2319-8753},
	number = {12},
	journal = {International Journal of Innovative Research in Science Engineering and Technology},
	author = {{S.Rameshkumar} and Thilak, J. Anish Jafrin and {Dr.P.Suresh} and {S.Sathishkumar} and {N.Subramani}},
	month = dec,
	year = {2016}
}

@article{ssenthilraja_noise_2014,
	title = {Noise {Reduction} in {Computed} {Tomography} {Image} {Using} {WB} – {Filter}},
	volume = {5},
	issn = {2229-5518},
	number = {3},
	journal = {International Journal of Scientific \& Engineering Research},
	author = {{S.Senthilraja} and {Dr.P.Suresh} and {Dr.M.Suganthi}},
	month = mar,
	year = {2014}
}

@article{ruifrok_quantification_2001,
	title = {Quantification of histochemical staining by color deconvolution},
	volume = {23},
	number = {4},
	journal = {Analytical and Quantitative Cytology and Histology},
	author = {Ruifrok, Arnout C C and Johnston, Dennis A},
	year = {2001},
	pages = {291--299}
}

@article{magee_colour_2009,
	title = {Colour {Normalisation} in {Digital} {Histopathology} {Images}},
	journal = {Proc Optical Tissue Image analysis in Microscopy, Histopathology and Endoscopy (MICCAI Workshop)},
	author = {Magee, Derek and Treanor, Darren and Crellin, Doreen and Shires, Michael and Smith, Katherine and Mohee, Kevin and Quirke, Philip},
	year = {2009}
}

@article{reinhard_color_2001,
	title = {Color {Transfer} between {Images}},
	volume = {21},
	doi = {10.1109/38.946629},
	journal = {IEEE Computer Graphics and Applications},
	author = {Reinhard, Erik and Ashikhmin, Michael and Gooch, Bruce and Shirley, Peter},
	year = {2001},
	pages = {34--41}
}

@article{vahadane_structure-preserving_2016,
	title = {Structure-{Preserving} {Color} {Normalization} and {Sparse} {Stain} {Separation} for {Histological} {Images}},
	volume = {35},
	number = {8},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Vahadane, Abhishek and Peng, Tingying and Sethi, Amit and Albarqouni, Shadi and Wang, Lichao and Baust, Maximilian and Steiger, Katja and Schlitter, Anna Melissa and Esposito, Irene and Navab, Nassir},
	year = {2016},
	pages = {1962--1971}
}

@article{he_automl_2019,
	title = {{AutoML}: {A} {Survey} of the {State}-of-the-{Art}},
	author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
	year = {2019},
	note = {\_eprint: 1908.00709}
}

@article{jia_caffe_2014,
	title = {Caffe: {Convolutional} {Architecture} for {Fast} {Feature} {Embedding}},
	journal = {arXiv preprint arXiv:1408.5093},
	author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
	year = {2014}
}

@article{abadi_tensorflow_2016,
	title = {{TensorFlow}: {A} system for large-scale machine learning},
	volume = {abs/1605.08695},
	url = {http://arxiv.org/abs/1605.08695},
	journal = {CoRR},
	author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek Gordon and Steiner, Benoit and Tucker, Paul A. and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zhang, Xiaoqiang},
	year = {2016},
	note = {\_eprint: 1605.08695}
}

@article{gibson_niftynet_2018,
	title = {{NiftyNet}: a deep-learning platform for medical imaging},
	volume = {158},
	issn = {0169-2607},
	url = {http://www.sciencedirect.com/science/article/pii/S0169260717311823},
	doi = {https://doi.org/10.1016/j.cmpb.2018.01.025},
	abstract = {Background and objectives Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. Established deep-learning platforms are flexible but do not provide specific functionality for medical image analysis and adapting them for this domain of application requires substantial implementation effort. Consequently, there has been substantial duplication of effort and incompatible infrastructure developed across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon. Methods The NiftyNet infrastructure provides a modular deep-learning pipeline for a range of medical imaging applications including segmentation, regression, image generation and representation learning applications. Components of the NiftyNet pipeline including data loading, data augmentation, network architectures, loss functions and evaluation metrics are tailored to, and take advantage of, the idiosyncracies of medical image analysis and computer-assisted intervention. NiftyNet is built on the TensorFlow framework and supports features such as TensorBoard visualization of 2D and 3D images and computational graphs by default. Results We present three illustrative medical image analysis applications built using NiftyNet infrastructure: (1) segmentation of multiple abdominal organs from computed tomography; (2) image regression to predict computed tomography attenuation maps from brain magnetic resonance images; and (3) generation of simulated ultrasound images for specified anatomical poses. Conclusions The NiftyNet infrastructure enables researchers to rapidly develop and distribute deep learning solutions for segmentation, regression, image generation and representation learning applications, or extend the platform to new applications.},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Gibson, Eli and Li, Wenqi and Sudre, Carole and Fidon, Lucas and Shakir, Dzhoshkun I. and Wang, Guotai and Eaton-Rosen, Zach and Gray, Robert and Doel, Tom and Hu, Yipeng and Whyntie, Tom and Nachev, Parashkev and Modat, Marc and Barratt, Dean C. and Ourselin, Sébastien and Cardoso, M. Jorge and Vercauteren, Tom},
	year = {2018},
	keywords = {Deep learning, Generative adversarial network, Convolutional neural network, Image regression, Medical image analysis, Segmentation},
	pages = {113 -- 122}
}

@article{goode_openslide_2013,
	title = {{OpenSlide}: {A} vendor-neutral software foundation for digital pathology},
	volume = {4},
	number = {1},
	journal = {Journal of Pathology Informatics},
	author = {Goode, Adam and Gilbert, Benjamin and Harkes, Jan and Jukic, Drazen M and Satyanarayanan, Mahadev},
	year = {2013},
	pages = {27--27}
}

@article{lowekamp_design_2013,
	title = {The design of {simpleITK}},
	volume = {7},
	doi = {10.3389/fninf.2013.00045},
	journal = {Frontiers in neuroinformatics},
	author = {Lowekamp, Bradley and Chen, David and Ibanez, Luis and Blezek, Daniel},
	year = {2013},
	pages = {45}
}

@article{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017}
}

@article{steiner_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	author = {Steiner, Benoit and Devito, Zachary and Chintala, Soumith and Gross, Sam and Paszke, Adam and Massa, Francisco and Lerer, Adam and Chanan, Gregory and Lin, Zeming and Yang, Edward and {others}},
	year = {2019}
}

@book{ibanez_itk_2003,
	edition = {First},
	title = {The {ITK} {Software} {Guide}},
	publisher = {Kitware, Inc.},
	author = {Ibanez, L. and Schroeder, W. and Ng, L. and Cates, J.},
	year = {2003}
}

@article{avants_reproducible_2011,
	title = {A reproducible evaluation of {ANTs} similarity metric performance in brain image registration},
	volume = {54},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811910012061},
	doi = {https://doi.org/10.1016/j.neuroimage.2010.09.025},
	abstract = {The United States National Institutes of Health (NIH) commit significant support to open-source data and software resources in order to foment reproducibility in the biomedical imaging sciences. Here, we report and evaluate a recent product of this commitment: Advanced Neuroimaging Tools (ANTs), which is approaching its 2.0 release. The ANTs open source software library consists of a suite of state-of-the-art image registration, segmentation and template building tools for quantitative morphometric analysis. In this work, we use ANTs to quantify, for the first time, the impact of similarity metrics on the affine and deformable components of a template-based normalization study. We detail the ANTs implementation of three similarity metrics: squared intensity difference, a new and faster cross-correlation, and voxel-wise mutual information. We then use two-fold cross-validation to compare their performance on openly available, manually labeled, T1-weighted MRI brain image data of 40 subjects (UCLA's LPBA40 dataset). We report evaluation results on cortical and whole brain labels for both the affine and deformable components of the registration. Results indicate that the best ANTs methods are competitive with existing brain extraction results (Jaccard=0.958) and cortical labeling approaches. Mutual information affine mapping combined with cross-correlation diffeomorphic mapping gave the best cortical labeling results (Jaccard=0.669±0.022). Furthermore, our two-fold cross-validation allows us to quantify the similarity of templates derived from different subgroups. Our open code, data and evaluation scripts set performance benchmark parameters for this state-of-the-art toolkit. This is the first study to use a consistent transformation framework to provide a reproducible evaluation of the isolated effect of the similarity metric on optimal template construction and brain labeling.},
	number = {3},
	journal = {NeuroImage},
	author = {Avants, Brian B. and Tustison, Nicholas J. and Song, Gang and Cook, Philip A. and Klein, Arno and Gee, James C.},
	year = {2011},
	pages = {2033 -- 2044}
}

@article{jenkinson_fsl_2012,
	title = {{FSL}},
	volume = {62},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811911010603},
	doi = {https://doi.org/10.1016/j.neuroimage.2011.09.015},
	abstract = {FSL (the FMRIB Software Library) is a comprehensive library of analysis tools for functional, structural and diffusion MRI brain imaging data, written mainly by members of the Analysis Group, FMRIB, Oxford. For this NeuroImage special issue on “20 years of fMRI” we have been asked to write about the history, developments and current status of FSL. We also include some descriptions of parts of FSL that are not well covered in the existing literature. We hope that some of this content might be of interest to users of FSL, and also maybe to new research groups considering creating, releasing and supporting new software packages for brain image analysis.},
	number = {2},
	journal = {NeuroImage},
	author = {Jenkinson, Mark and Beckmann, Christian F. and Behrens, Timothy E. J. and Woolrich, Mark W. and Smith, Stephen M.},
	year = {2012},
	keywords = {Software, FSL},
	pages = {782 -- 790}
}

@article{oliphant_python_2007,
	title = {Python for {Scientific} {Computing}},
	volume = {9},
	url = {https://aip.scitation.org/doi/abs/10.1109/MCSE.2007.58},
	doi = {10.1109/MCSE.2007.58},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Oliphant, Travis E.},
	year = {2007},
	note = {\_eprint: https://aip.scitation.org/doi/pdf/10.1109/MCSE.2007.58},
	pages = {10--20}
}

@article{walt_numpy_2011,
	title = {The {NumPy} {Array}: {A} {Structure} for {Efficient} {Numerical} {Computation}},
	volume = {13},
	doi = {10.1109/MCSE.2011.37},
	number = {2},
	journal = {Computing in Science Engineering},
	author = {Walt, S. van der and Colbert, S. C. and Varoquaux, G.},
	month = mar,
	year = {2011},
	keywords = {Arrays, Computational efficiency, data structures, Finite element methods, high level language, high level languages, mathematics computing, numerical analysis, Numerical analysis, numerical computation, numerical computations, numerical data, NumPy, numpy array, Performance evaluation, programming libraries, Python, Python programming language, Resource management, scientific programming, Vector quantization},
	pages = {22--30}
}

@article{beers_deepneuro_2018,
	title = {{DeepNeuro}: an open-source deep learning toolbox for neuroimaging},
	author = {Beers, Andrew and Brown, James and Chang, Ken and Hoebel, Katharina and Gerstner, Elizabeth and Rosen, Bruce and Kalpathy-Cramer, Jayashree},
	year = {2018},
	note = {\_eprint: 1808.04589}
}

@article{fischl_freesurfer_2012,
	title = {{FreeSurfer}},
	volume = {62},
	issn = {1095-9572 (Electronic) 1053-8119 (Linking)},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3685476/},
	doi = {10.1016/j.neuroimage.2012.01.021},
	abstract = {FreeSurfer is a suite of tools for the analysis of neuroimaging data that provides an array of algorithms to quantify the functional, connectional and structural properties of the human brain. It has evolved from a package primarily aimed at generating surface representations of the cerebral cortex into one that automatically creates models of most macroscopically visible structures in the human brain given any reasonable T1-weighted input image. It is freely available, runs on a wide variety of hardware and software platforms, and is open source.},
	number = {2},
	journal = {Neuroimage},
	author = {Fischl, B.},
	year = {2012},
	keywords = {Humans, Image Processing, *Algorithms, 20th Century, 21st Century, Brain Mapping/*history/methods, Brain/anatomy \& histology, Computer-Assisted/*history/methods, History, Magnetic Resonance Imaging/*history/methods, Software/*history},
	pages = {774--81}
}

@article{maloney_scratch_2010,
	title = {The {Scratch} {Programming} {Language} and {Environment}},
	volume = {10},
	doi = {10.1145/1868358.1868363},
	journal = {ACM Transactions on Computing Education (TOCE)},
	author = {Maloney, John and Resnick, Mitchel and Rusk, Natalie and Silverman, Brian and Eastmond, Evelyn},
	year = {2010},
	pages = {16}
}

@article{chetlur_cudnn_2014,
	title = {{cuDNN}: {Efficient} {Primitives} for {Deep} {Learning}},
	journal = {arXiv: Neural and Evolutionary Computing},
	author = {Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan D and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
	year = {2014}
}

@article{henschel_fastsurfer_2019,
	title = {{FastSurfer} – {A} fast and accurate deep learning based neuroimaging pipeline},
	author = {Henschel, Leonie and Conjeti, Sailesh and Estrada, Santiago and Diers, Kersten and Fischl, Bruce and Reuter, Martin},
	year = {2019},
	note = {\_eprint: 1910.03866}
}

@book{muller_miscnn_2019,
	title = {{MIScnn}: {A} {Framework} for {Medical} {Image} {Segmentation} with {Convolutional} {Neural} {Networks} and {Deep} {Learning}},
	author = {Müller, Dominik and Kramer, Frank},
	year = {2019},
	note = {\_eprint: 1910.09308}
}

@article{yong_survey_2012,
	title = {A {Survey} of {Visualization} {Tools} in {Medical} {Imaging}},
	volume = {56},
	issn = {1877-0428},
	url = {http://www.sciencedirect.com/science/article/pii/S187704281204116X},
	doi = {https://doi.org/10.1016/j.sbspro.2012.09.654},
	abstract = {More than 30 students from university campus participated in the Development of Biomedical Image Processing Software Package for New Learners Survey investigating the use of software package for processing and editing image. The survey was available online for six months. Facts and opinions were sought to learn the general information, interactive image processing tool, non-interactive (automatic) tool, current status and future of image processing package tool. Composed of 19 questions, the survey built a comprehensive picture of the software package, programming language, workflow of the tool and captured the attitudes of the respondents. Result shows that MATLAB was difficult to use but it was viewed in high regard however. The result of this study is expected to be beneficial and able to assist users on effective image processing and analysis in a newly developed software package.},
	journal = {Procedia - Social and Behavioral Sciences},
	author = {Yong, Ching Yee and Chew, Kim Mey and Mahmood, Nasrul Humaimi and Ariffin, Ismail},
	year = {2012},
	keywords = {Medical imaging, Image editting, Image processing, Software package, Visualisation tools},
	pages = {265 -- 271}
}

@book{zhang_visual_2018,
	title = {Visual {Interpretability} for {Deep} {Learning}: a {Survey}},
	author = {Zhang, Quanshi and Zhu, Song-Chun},
	year = {2018},
	note = {\_eprint: 1802.00614}
}

@article{swiderska-chadaj_learnicytes_2019,
	title = {Learnicytes in immunohistochemistry with deep learning},
	volume = {58},
	issn = {1361-8415},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841519300829},
	doi = {https://doi.org/10.1016/j.media.2019.101547},
	abstract = {The immune system is of critical importance in the development of cancer. The evasion of destruction by the immune system is one of the emerging hallmarks of cancer. We have built a dataset of 171,166 manually annotated CD3+ and CD8+ cells, which we used to train deep learning algorithms for automatic detection of lymphocytes in histopathology images to better quantify immune response. Moreover, we investigate the effectiveness of four deep learning based methods when different subcompartments of the whole-slide image are considered: normal tissue areas, areas with immune cell clusters, and areas containing artifacts. We have compared the proposed methods in breast, colon and prostate cancer tissue slides collected from nine different medical centers. Finally, we report the results of an observer study on lymphocyte quantification, which involved four pathologists from different medical centers, and compare their performance with the automatic detection. The results give insights on the applicability of the proposed methods for clinical use. U-Net obtained the highest performance with an F1-score of 0.78 and the highest agreement with manual evaluation (κ=0.72), whereas the average pathologists agreement with reference standard was κ=0.64. The test set and the automatic evaluation procedure are publicly available at lyon19.grand-challenge.org.},
	journal = {Medical Image Analysis},
	author = {Swiderska-Chadaj, Zaneta and Pinckaers, Hans and Rijthoven, Mart van and Balkenhol, Maschenka and Melnikova, Margarita and Geessink, Oscar and Manson, Quirine and Sherman, Mark and Polonia, Antonio and Parry, Jeremy and Abubakar, Mustapha and Litjens, Geert and Laak, Jeroen van der and Ciompi, Francesco},
	year = {2019},
	keywords = {Deep learning, Computational pathology, Immune cell detection, Immunohistochemistry},
	pages = {101547}
}