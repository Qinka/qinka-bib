@article{DBLP:journals/corr/abs-1804-02967,
  author    = {Jose Dolz and
               Karthik Gopinath and
               Jing Yuan and
               Herve Lombaert and
               Christian Desrosiers and
               Ismail Ben Ayed},
  title     = {HyperDense-Net: {A} hyper-densely connected {CNN} for multi-modal
               image segmentation},
  journal   = {CoRR},
  volume    = {abs/1804.02967},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.02967},
  archivePrefix = {arXiv},
  eprint    = {1804.02967},
  timestamp = {Mon, 13 Aug 2018 16:48:02 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1804-02967},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{2018arXiv181002492K,
       author = {{Kumar}, Ashnil and {Fulham}, Michael and {Feng}, Dagan and {Kim},
        Jinman},
        title = "{Co-Learning Feature Fusion Maps from PET-CT Images of Lung Cancer}",
      journal = {ArXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2018,
        month = Oct,
          eid = {arXiv:1810.02492},
        pages = {arXiv:1810.02492},
archivePrefix = {arXiv},
       eprint = {1810.02492},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/#abs/2018arXiv181002492K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2018arXiv181107407M,
   author = {{Mahmood}, F. and {Yang}, Z. and {Ashley}, T. and {Durr}, N.~J.
	},
    title = "{Multimodal Densenet}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1811.07407},
 primaryClass = "cs.CV",
 keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
     year = 2018,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181107407M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{Zhong20183DFC,
  title={3D fully convolutional networks for co-segmentation of tumors on PET-CT images},
  author={Zisha Zhong and Yusung Kim and Leixin Zhou and Kristin A. Plichta and Bryan Allen and John M. Buatti and Xiaodong Wu},
  journal={2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)},
  year={2018},
  pages={228-231}
}
@ARTICLE{6516899,
author={Q. Song and J. Bai and D. Han and S. Bhatia and W. Sun and W. Rockey and J. E. Bayouth and J. M. Buatti and X. Wu},
journal={IEEE Transactions on Medical Imaging},
title={Optimal Co-Segmentation of Tumor in PET-CT Images With Context Information},
year={2013},
volume={32},
number={9},
pages={1685-1697},
abstract={Positron emission tomography (PET)-computed tomography (CT) images have been widely used in clinical practice for radiotherapy treatment planning of the radiotherapy. Many existing segmentation approaches only work for a single imaging modality, which suffer from the low spatial resolution in PET or low contrast in CT. In this work, we propose a novel method for the co-segmentation of the tumor in both PET and CT images, which makes use of advantages from each modality: the functionality information from PET and the anatomical structure information from CT. The approach formulates the segmentation problem as a minimization problem of a Markov random field model, which encodes the information from both modalities. The optimization is solved using a graph-cut based method. Two sub-graphs are constructed for the segmentation of the PET and the CT images, respectively. To achieve consistent results in two modalities, an adaptive context cost is enforced by adding context arcs between the two sub-graphs. An optimal solution can be obtained by solving a single maximum flow problem, which leads to simultaneous segmentation of the tumor volumes in both modalities. The proposed algorithm was validated in robust delineation of lung tumors on 23 PET-CT datasets and two head-and-neck cancer subjects. Both qualitative and quantitative results show significant improvement compared to the graph cut methods solely using PET or CT.},
keywords={cancer;computerised tomography;graph theory;image resolution;image segmentation;lung;Markov processes;medical image processing;minimisation;positron emission tomography;tumours;tumor optimal cosegmentation;PET-CT Image;context information;positron emission tomography-computed tomography image;radiotherapy treatment planning;single imaging modality;low spatial resolution;functionality information;anatomical structure information;minimization problem;Markov random field model;graph-cut based method;adaptive context cost;context arcs;subgraph;optimal solution;single maximum flow problem;tumor volume simultaneous segmentation;lung tumor;23 PET-CT dataset;head-and-neck cancer subject;Computed tomography;Positron emission tomography;Tumors;Image segmentation;Context;Lungs;Computational modeling;Context information;global optimization;graph cut;image segmentation;lung tumor;Positron emission tomography-computed tomography (PET-CT);Algorithms;Databases, Factual;Head and Neck Neoplasms;Humans;Image Processing, Computer-Assisted;Markov Chains;Positron-Emission Tomography;Tomography, X-Ray Computed},
doi={10.1109/TMI.2013.2263388},
ISSN={0278-0062},
month={Sept},}
@inproceedings{Zhong20173DAM,
	title={3D Alpha Matting Based Co-segmentation of Tumors on PET-CT Images},
	author={Zisha Zhong and Yusung Kim and John M. Buatti and Xiaodong Wu},
	booktitle={CMMI/RAMBO/SWITCH@MICCAI},
	year={2017}
}
@INPROCEEDINGS{8363561,
author={Z. Zhong and Y. Kim and L. Zhou and K. Plichta and B. Allen and J. Buatti and X. Wu},
booktitle={2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)},
title={3D fully convolutional networks for co-segmentation of tumors on PET-CT images},
year={2018},
volume={},
number={},
pages={228-231},
keywords={biomedical MRI;cancer;computerised tomography;image classification;image segmentation;learning (artificial intelligence);lung;medical image processing;positron emission tomography;tumours;graph cut;co-segmentation model;final tumor segmentation results;lung cancer patients;3D fully convolutional networks;PET-CT images;positron emission tomography;computed tomography;dual-modality imaging;critical diagnostic information;automated accurate tumor delineation;tumor reading;semantic segmentation framework;probability maps;PET-CT scans;cancer diagnosis;Tumors;Computed tomography;Image segmentation;Biomedical imaging;Three-dimensional displays;Lung;image segmentation;lung tumor segmentation;co-segmentation;fully convolutional networks;deep learning},
doi={10.1109/ISBI.2018.8363561},
ISSN={1945-8452},
month={April},}
@article{Han2011petct,
author = {Han, Dongfeng and Bayouth, John and Song, Qi and Taurani, Aakant and Sonka, Milan and Buatti, John and Wu, Xiaodong},
year = {2011},
month = {07},
pages = {245-56},
title = {Globally Optimal Tumor Segmentation in PET-CT Images: A Graph-Based Co-segmentation Method},
volume = {22},
journal = {Information processing in medical imaging : proceedings of the ... conference},
doi = {10.1007/978-3-642-22092-0_21}
}
@ARTICLE{2014arXiv1412.0767T,
       author = {{Tran}, Du and {Bourdev}, Lubomir and {Fergus}, Rob and {Torresani},
        Lorenzo and {Paluri}, Manohar},
        title = "{Learning Spatiotemporal Features with 3D Convolutional Networks}",
      journal = {ArXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2014,
        month = Dec,
          eid = {arXiv:1412.0767},
        pages = {arXiv:1412.0767},
archivePrefix = {arXiv},
       eprint = {1412.0767},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/#abs/2014arXiv1412.0767T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{7294713,
author={W. Ju and D. Xiang and B. Zhang and L. Wang and I. Kopriva and X. Chen},
journal={IEEE Transactions on Image Processing},
title={Random Walk and Graph Cut for Co-Segmentation of Lung Tumor on PET-CT Images},
year={2015},
volume={24},
number={12},
pages={5854-5867},
abstract={Accurate lung tumor delineation plays an important role in radiotherapy treatment planning. Since the lung tumor has poor boundary in positron emission tomography (PET) images and low contrast in computed tomography (CT) images, segmentation of tumor in the PET and CT images is a challenging task. In this paper, we effectively integrate the two modalities by making fully use of the superior contrast of PET images and superior spatial resolution of CT images. Random walk and graph cut method is integrated to solve the segmentation problem, in which random walk is utilized as an initialization tool to provide object seeds for graph cut segmentation on the PET and CT images. The co-segmentation problem is formulated as an energy minimization problem which is solved by max-flow/min-cut method. A graph, including two sub-graphs and a special link, is constructed, in which one sub-graph is for the PET and another is for CT, and the special link encodes a context term which penalizes the difference of the tumor segmentation on the two modalities. To fully utilize the characteristics of PET and CT images, a novel energy representation is devised. For the PET, a downhill cost and a 3D derivative cost are proposed. For the CT, a shape penalty cost is integrated into the energy function which helps to constrain the tumor region during the segmentation. We validate our algorithm on a data set which consists of 18 PET-CT images. The experimental results indicate that the proposed method is superior to the graph cut method solely using the PET or CT is more accurate compared with the random walk method, random walk co-segmentation method, and non-improved graph cut method.},
keywords={computerised tomography;image segmentation;lung;medical image processing;minimisation;positron emission tomography;tumours;lung tumor;PET-CT imaging;positron emission tomography;spatial resolution;graph cut segmentation method;energy minimization problem;max-flow-min-cut method;special link encodes;tumor segmentation;energy representation;3D derivative cost;shape penalty;energy function;random walk cosegmentation method;Positron emission tomography;Tumors;Computed tomography;Image segmentation;Lungs;Context;Three-dimensional displays;image segmentation;interactive segmentation;graph cut;random walk;prior information;lung tumor;Positron Emission Tomography (PET);Computed Tomography (CT);Image segmentation;interactive segmentation;graph cut;random walk;prior information;lung tumor;positron emission tomography (PET);computed tomography (CT);Algorithms;Carcinoma, Non-Small-Cell Lung;Databases, Factual;Humans;Image Processing, Computer-Assisted;Lung Neoplasms;Positron-Emission Tomography;Tomography, X-Ray Computed},
doi={10.1109/TIP.2015.2488902},
ISSN={1057-7149},
month={Dec},}

@article{minati2009current,
title={Current concepts in Alzheimer's disease: a multidisciplinary review.},
author={Minati, Ludovico and Edginton, Trudi and Bruzzone, Maria Grazia and Giaccone, Giorgio},
journal={American Journal of Alzheimers Disease and Other Dementias},
volume={24},
number={2},
pages={95--121},
year={2009}}
@ARTICLE{7426413,
author={S. {Pereira} and A. {Pinto} and V. {Alves} and C. A. {Silva}},
journal={IEEE Transactions on Medical Imaging},
title={Brain Tumor Segmentation Using Convolutional Neural Networks in MRI Images},
year={2016},
volume={35},
number={5},
pages={1240-1251},
keywords={biomedical MRI;brain;cancer;image segmentation;medical image processing;neurophysiology;tumours;brain tumor segmentation;convolutional neural networks;MRI images;gliomas;quality-of-life;oncological patients;magnetic resonance imaging;imaging technique;manual segmentation;precise quantitative measurements;clinical practice;automatic segmentation methods;reliable segmentation methods;spatial variability;structural variability;automatic segmentation;kernels;intensity normalization;preprocessing step;CNN-based segmentation methods;data augmentation;Dice similarity coefficient metrics;online evaluation platform;on-site BRATS 2015 Challenge;Tumors;Image segmentation;Magnetic resonance imaging;Kernel;Training;Brain modeling;Context;Brain tumor;brain tumor segmentation;convolutional neural networks;deep learning;glioma;magnetic resonance imaging;Brain Neoplasms;Glioma;Humans;Image Interpretation, Computer-Assisted;Machine Learning;Magnetic Resonance Imaging;Neural Networks (Computer)},
doi={10.1109/TMI.2016.2538465},
ISSN={},
month={May},}
@article{Litjens2016,
author = {Litjens, Geert and Sánchez, Clara and Timofeeva, Nadya and Hermsen, Meyke and Nagtegaal, Iris and Kovacs, Iringo and Hulsbergen-van de Kaa, Christina and Bult, Peter and Ginneken, Bram and van der Laak, Jeroen},
year = {2016},
month = {05},
pages = {26286},
title = {Deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis},
volume = {6},
journal = {Scientific Reports},
doi = {10.1038/srep26286}
}
@article{10.1371/journal.pmed.1002711,
    author = {Hosny, Ahmed AND Parmar, Chintan AND Coroller, Thibaud P. AND Grossmann, Patrick AND Zeleznik, Roman AND Kumar, Avnish AND Bussink, Johan AND Gillies, Robert J. AND Mak, Raymond H. AND Aerts, Hugo J. W. L.},
    journal = {PLOS Medicine},
    publisher = {Public Library of Science},
    title = {Deep learning for lung cancer prognostication: A retrospective multi-cohort radiomics study},
    year = {2018},
    month = {11},
    volume = {15},
    url = {https://doi.org/10.1371/journal.pmed.1002711},
    pages = {1-25},
    abstract = {Hugo Aerts and colleagues evaluate the ability of deep learning networks to extract relevant features from computed tomography lung cancer images and stratify patients into low and high mortality risk groups.},
    number = {11},
    doi = {10.1371/journal.pmed.1002711}
}

@article{LITJENS201760,
title = "A survey on deep learning in medical image analysis",
journal = "Medical Image Analysis",
volume = "42",
pages = "60 - 88",
year = "2017",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2017.07.005",
url = "http://www.sciencedirect.com/science/article/pii/S1361841517301135",
author = "Geert Litjens and Thijs Kooi and Babak Ehteshami Bejnordi and Arnaud Arindra Adiyoso Setio and Francesco Ciompi and Mohsen Ghafoorian and Jeroen A.W.M. van der Laak and Bram van Ginneken and Clara I. Sánchez",
keywords = "Deep learning, Convolutional neural networks, Medical imaging, Survey",
abstract = "Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research."
}
@article{YI2019101552,
title = "Generative adversarial network in medical imaging: A review",
journal = "Medical Image Analysis",
volume = "58",
pages = "101552",
year = "2019",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2019.101552",
url = "http://www.sciencedirect.com/science/article/pii/S1361841518308430",
author = "Xin Yi and Ekta Walia and Paul Babyn",
keywords = "Deep learning, Generative adversarial network, Generative model, Medical imaging, Review",
abstract = "Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique."
}
@article{WANG2020101565,
title = "Semi-supervised mp-MRI data synthesis with StitchLayer and auxiliary distance maximization",
journal = "Medical Image Analysis",
volume = "59",
pages = "101565",
year = "2020",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2019.101565",
url = "http://www.sciencedirect.com/science/article/pii/S1361841519301057",
author = "Zhiwei Wang and Yi Lin and Kwang-Ting (Tim) Cheng and Xin Yang",
keywords = "Generative models, GAN, Multimodal image synthesis, Deep learning",
abstract = "The availability of a large amount of annotated data is critical for many medical image analysis applications, in particular for those relying on deep learning methods which are known to be data-hungry. However, annotated medical data, especially multimodal data, is often scarce and costly to obtain. In this paper, we address the problem of synthesizing multi-parameter magnetic resonance imaging data (i.e. mp-MRI), which typically consists of Apparent Diffusion Coefficient (ADC) and T2-weighted (T2w) images, containing clinically significant (CS) prostate cancer (PCa) via semi-supervised learning and adversarial learning. Specifically, our synthesizer generates mp-MRI data in a sequential manner: first utilizing a decoder to generate an ADC map from a 128-d latent vector, followed by translating the ADC to the T2w image via U-Net. The synthesizer is trained in a semi-supervised manner. In the supervised training process, a limited amount of paired ADC-T2w images and the corresponding ADC encodings are provided and the synthesizer learns the paired relationship by explicitly minimizing the reconstruction losses between synthetic and real images. To avoid overfitting limited ADC encodings, an unlimited amount of random latent vectors and unpaired ADC-T2w Images are utilized in the unsupervised training process for learning the marginal image distributions of real images. To improve the robustness for training the synthesizer, we decompose the difficult task of generating full-size images into several simpler tasks which generate sub-images only. A StitchLayer is then employed to seamlessly fuse sub-images together in an interlaced manner into a full-size image. In addition, to enforce the synthetic images to indeed contain distinguishable CS PCa lesions, we propose to also maximize an auxiliary distance of Jensen-Shannon divergence (JSD) between CS and nonCS images. Experimental results show that our method can effectively synthesize a large variety of mp-MRI images which contain meaningful CS PCa lesions, display a good visual quality and have the correct paired relationship between the two modalities of a pair. Compared to the state-of-the-art methods based on adversarial learning (Liu and Tuzel, 2016; Costa et al., 2017), our method achieves a significant improvement in terms of both visual quality and several popular quantitative evaluation metrics."
}
@article{SWIDERSKACHADAJ2019101547,
title = "Learning to detect lymphocytes in immunohistochemistry with deep learning",
journal = "Medical Image Analysis",
volume = "58",
pages = "101547",
year = "2019",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2019.101547",
url = "http://www.sciencedirect.com/science/article/pii/S1361841519300829",
author = "Zaneta Swiderska-Chadaj and Hans Pinckaers and Mart van Rijthoven and Maschenka Balkenhol and Margarita Melnikova and Oscar Geessink and Quirine Manson and Mark Sherman and Antonio Polonia and Jeremy Parry and Mustapha Abubakar and Geert Litjens and Jeroen van der Laak and Francesco Ciompi",
keywords = "Deep learning, Immune cell detection, Computational pathology, Immunohistochemistry",
abstract = "The immune system is of critical importance in the development of cancer. The evasion of destruction by the immune system is one of the emerging hallmarks of cancer. We have built a dataset of 171,166 manually annotated CD3+ and CD8+ cells, which we used to train deep learning algorithms for automatic detection of lymphocytes in histopathology images to better quantify immune response. Moreover, we investigate the effectiveness of four deep learning based methods when different subcompartments of the whole-slide image are considered: normal tissue areas, areas with immune cell clusters, and areas containing artifacts. We have compared the proposed methods in breast, colon and prostate cancer tissue slides collected from nine different medical centers. Finally, we report the results of an observer study on lymphocyte quantification, which involved four pathologists from different medical centers, and compare their performance with the automatic detection. The results give insights on the applicability of the proposed methods for clinical use. U-Net obtained the highest performance with an F1-score of 0.78 and the highest agreement with manual evaluation (κ=0.72), whereas the average pathologists agreement with reference standard was κ=0.64. The test set and the automatic evaluation procedure are publicly available at lyon19.grand-challenge.org."
}
@article{JAMALUDIN201763,
title = "SpineNet: Automated classification and evidence visualization in spinal MRIs",
journal = "Medical Image Analysis",
volume = "41",
pages = "63 - 73",
year = "2017",
note = "Special Issue on the 2016 Conference on Medical Image Computing and Computer Assisted Intervention (Analog to MICCAI 2015)",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2017.07.002",
url = "http://www.sciencedirect.com/science/article/pii/S136184151730110X",
author = "Amir Jamaludin and Timor Kadir and Andrew Zisserman",
keywords = "MRI analysis, Radiological classification, Spinal MRI",
abstract = "The objective of this work is to automatically produce radiological gradings of spinal lumbar MRIs and also localize the predicted pathologies. We show that this can be achieved via a Convolutional Neural Network (CNN) framework that takes intervertebral disc volumes as inputs and is trained only on disc-specific class labels. Our contributions are: (i) a CNN architecture that predicts multiple gradings at once, and we propose variants of the architecture including using 3D convolutions; (ii) showing that this architecture can be trained using a multi-task loss function without requiring segmentation level annotation; and (iii) a localization method that clearly shows pathological regions in the disc volumes. We compare three visualization methods for the localization. The network is applied to a large corpus of MRI T2 sagittal spinal MRIs (using a standard clinical scan protocol) acquired from multiple machines, and is used to automatically compute disk and vertebra gradings for each MRI. These are: Pfirrmann grading, disc narrowing, upper/lower endplate defects, upper/lower marrow changes, spondylolisthesis, and central canal stenosis. We report near human performances across the eight gradings, and also visualize the evidence for these gradings localized on the original scans."
}
@article{JAMALUDIN201763,
title = "SpineNet: Automated classification and evidence visualization in spinal MRIs",
journal = "Medical Image Analysis",
volume = "41",
pages = "63 - 73",
year = "2017",
note = "Special Issue on the 2016 Conference on Medical Image Computing and Computer Assisted Intervention (Analog to MICCAI 2015)",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2017.07.002",
url = "http://www.sciencedirect.com/science/article/pii/S136184151730110X",
author = "Amir Jamaludin and Timor Kadir and Andrew Zisserman",
keywords = "MRI analysis, Radiological classification, Spinal MRI",
abstract = "The objective of this work is to automatically produce radiological gradings of spinal lumbar MRIs and also localize the predicted pathologies. We show that this can be achieved via a Convolutional Neural Network (CNN) framework that takes intervertebral disc volumes as inputs and is trained only on disc-specific class labels. Our contributions are: (i) a CNN architecture that predicts multiple gradings at once, and we propose variants of the architecture including using 3D convolutions; (ii) showing that this architecture can be trained using a multi-task loss function without requiring segmentation level annotation; and (iii) a localization method that clearly shows pathological regions in the disc volumes. We compare three visualization methods for the localization. The network is applied to a large corpus of MRI T2 sagittal spinal MRIs (using a standard clinical scan protocol) acquired from multiple machines, and is used to automatically compute disk and vertebra gradings for each MRI. These are: Pfirrmann grading, disc narrowing, upper/lower endplate defects, upper/lower marrow changes, spondylolisthesis, and central canal stenosis. We report near human performances across the eight gradings, and also visualize the evidence for these gradings localized on the original scans."
}
@ARTICLE{8371286,
author={F. {Hohman} and M. {Kahng} and R. {Pienta} and D. H. {Chau}},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers},
year={2019},
volume={25},
number={8},
pages={2674-2693},
keywords={Machine learning;Conferences;Visual analytics;Data visualization;Neural networks;Computational modeling;Deep learning;visual analytics;information visualization;neural networks},
doi={10.1109/TVCG.2018.2843369},
ISSN={},
month={Aug},}
@article{LIU2019101555,
title = "Automated detection and classification of thyroid nodules in ultrasound images using clinical-knowledge-guided convolutional neural networks",
journal = "Medical Image Analysis",
volume = "58",
pages = "101555",
year = "2019",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2019.101555",
url = "http://www.sciencedirect.com/science/article/pii/S1361841519300970",
author = "Tianjiao Liu and Qianqian Guo and Chunfeng Lian and Xuhua Ren and Shujun Liang and Jing Yu and Lijuan Niu and Weidong Sun and Dinggang Shen",
keywords = "Ultrasound image, Thyroid nodule, Convolutional neural networks, Clinical knowledge",
abstract = "Accurate diagnosis of thyroid nodules using ultrasonography is a valuable but tough task even for experienced radiologists, considering both benign and malignant nodules have heterogeneous appearances. Computer-aided diagnosis (CAD) methods could potentially provide objective suggestions to assist radiologists. However, the performance of existing learning-based approaches is still limited, for direct application of general learning models often ignores critical domain knowledge related to the specific nodule diagnosis. In this study, we propose a novel deep-learning-based CAD system, guided by task-specific prior knowledge, for automated nodule detection and classification in ultrasound images. Our proposed CAD system consists of two stages. First, a multi-scale region-based detection network is designed to learn pyramidal features for detecting nodules at different feature scales. The region proposals are constrained by the prior knowledge about size and shape distributions of real nodules. Then, a multi-branch classification network is proposed to integrate multi-view diagnosis-oriented features, in which each network branch captures and enhances one specific group of characteristics that were generally used by radiologists. We evaluated and compared our method with the state-of-the-art CAD methods and experienced radiologists on two datasets, i.e. Dataset I and Dataset II. The detection and diagnostic accuracy on Dataset I were 97.5% and 97.1%, respectively. Besides, our CAD system also achieved better performance than experienced radiologists on Dataset II, with improvements of accuracy for 8%. The experimental results demonstrate that our proposed method is effective in the discrimination of thyroid nodules."
}
@article{JIMENEZCARRETERO2019144,
title = "A graph-cut approach for pulmonary artery-vein segmentation in noncontrast CT images",
journal = "Medical Image Analysis",
volume = "52",
pages = "144 - 159",
year = "2019",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2018.11.011",
url = "http://www.sciencedirect.com/science/article/pii/S1361841518308740",
author = "Daniel Jimenez-Carretero and David Bermejo-Peláez and Pietro Nardelli and Patricia Fraga and Eduardo Fraile and Raúl San José Estépar and Maria J Ledesma-Carbayo",
keywords = "Artery-vein segmentation, Lung, Graph-cuts, Random forest, Arteries, Veins, Noncontrast CT, Phantoms",
abstract = "Lung vessel segmentation has been widely explored by the biomedical image processing community; however, the differentiation of arterial from venous irrigation is still a challenge. Pulmonary artery–vein (AV) segmentation using computed tomography (CT) is growing in importance owing to its undeniable utility in multiple cardiopulmonary pathological states, especially those implying vascular remodelling, allowing the study of both flow systems separately. We present a new framework to approach the separation of tree-like structures using local information and a specifically designed graph-cut methodology that ensures connectivity as well as the spatial and directional consistency of the derived subtrees. This framework has been applied to the pulmonary AV classification using a random forest (RF) pre-classifier to exploit the local anatomical differences of arteries and veins. The evaluation of the system was performed using 192 bronchopulmonary segment phantoms, 48 anthropomorphic pulmonary CT phantoms, and 26 lungs from noncontrast CT images with precise voxel-based reference standards obtained by manually labelling the vessel trees. The experiments reveal a relevant improvement in the accuracy ( ∼ 20%) of the vessel particle classification with the proposed framework with respect to using only the pre-classification based on local information applied to the whole area of the lung under study. The results demonstrated the accurate differentiation between arteries and veins in both clinical and synthetic cases, specifically when the image quality can guarantee a good airway segmentation, which opens a huge range of possibilities in the clinical study of cardiopulmonary diseases."
}
@article{QAISER20191,
title = "Fast and accurate tumor segmentation of histology images using persistent homology and deep convolutional features",
journal = "Medical Image Analysis",
volume = "55",
pages = "1 - 14",
year = "2019",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2019.03.014",
url = "http://www.sciencedirect.com/science/article/pii/S1361841518302688",
author = "Talha Qaiser and Yee-Wah Tsang and Daiki Taniyama and Naoya Sakamoto and Kazuaki Nakane and David Epstein and Nasir Rajpoot",
keywords = "Tumor segmentation, Persistent homology, Deep learning, Histology image analysis, Computational pathology, Colorectal (colon) cancer",
abstract = "Tumor segmentation in whole-slide images of histology slides is an important step towards computer-assisted diagnosis. In this work, we propose a tumor segmentation framework based on the novel concept of persistent homology profiles (PHPs). For a given image patch, the homology profiles are derived by efficient computation of persistent homology, which is an algebraic tool from homology theory. We propose an efficient way of computing topological persistence of an image, alternative to simplicial homology. The PHPs are devised to distinguish tumor regions from their normal counterparts by modeling the atypical characteristics of tumor nuclei. We propose two variants of our method for tumor segmentation: one that targets speed without compromising accuracy and the other that targets higher accuracy. The fast version is based on a selection of exemplar image patches from a convolution neural network (CNN) and patch classification by quantifying the divergence between the PHPs of exemplars and the input image patch. Detailed comparative evaluation shows that the proposed algorithm is significantly faster than competing algorithms while achieving comparable results. The accurate version combines the PHPs and high-level CNN features and employs a multi-stage ensemble strategy for image patch labeling. Experimental results demonstrate that the combination of PHPs and CNN features outperform competing algorithms. This study is performed on two independently collected colorectal datasets containing adenoma, adenocarcinoma, signet, and healthy cases. Collectively, the accurate tumor segmentation produces the highest average patch-level F1-score, as compared with competing algorithms, on malignant and healthy cases from both the datasets. Overall the proposed framework highlights the utility of persistent homology for histopathology image analysis."
}
@article{SONG201940,
title = "Multi-layer boosting sparse convolutional model for generalized nuclear segmentation from histopathology images",
journal = "Knowledge-Based Systems",
volume = "176",
pages = "40 - 53",
year = "2019",
issn = "0950-7051",
doi = "https://doi.org/10.1016/j.knosys.2019.03.031",
url = "http://www.sciencedirect.com/science/article/pii/S095070511930156X",
author = "Jie Song and Liang Xiao and Mohsen Molaei and Zhichao Lian",
keywords = "Nucleus segmentation, Cascade classification, Multi-layer boosting sparse convolutional model, Probabilistic binary decision tree, Representation learning",
abstract = "It is a challenging problem to achieve generalized nuclear segmentation in digital histopathology images. Existing techniques, using either handcrafted features in learning-based models or traditional image analysis-based approaches, do not effectively tackle the challenging cases, such as crowded nuclei, chromatin-sparse, and heavy background clutter. In contrast, deep networks have achieved state-of-the-art performance in modeling various nuclear appearances. However, their success is limited due to the size of the considered networks. We solve these problems by reformulating nuclear segmentation in terms of a cascade 2-class classification problem and propose a multi-layer boosting sparse convolutional (ML-BSC) model. In the proposed ML-BSC model, discriminative probabilistic binary decision trees (PBDTs) are designed as weak learners in each layer to cope with challenging cases. A sparsity-constrained cascade structure enables the ML-BSC model to improve representation learning. Comparing to the existing techniques, our method can accurately separate individual nuclei in complex histopathology images, and it is more robust against chromatin-sparse and heavy background clutter. An evaluation carried out using three disparate datasets demonstrates the superiority of our method over the state-of-the-art supervised approaches in terms of segmentation accuracy."
}
@ARTICLE{8626549,
author={M. {Tofighi} and T. {Guo} and J. K. P. {Vanamala} and V. {Monga}},
journal={IEEE Transactions on Medical Imaging},
title={Prior Information Guided Regularized Deep Learning for Cell Nucleus Detection},
year={2019},
volume={38},
number={9},
pages={2047-2058},
keywords={biology computing;cellular biophysics;convolutional neural nets;learning (artificial intelligence);medical image processing;nucleus shapes;learnable layers;fixed processing part;regularization terms;cell nucleus boundary;TSP-CNN;cell nucleus detection;cell nuclei detection;cellular image quality;nuclear morphology;multiple cell nuclei;deep learning methods;convolutional neural networks;training set;input images;labeled nuclei locations;spatial processing;morphological processing;canonical cell nuclei shapes;domain expert;shape priors;tunable SP-CNN;network structures;Shape;Image edge detection;Computer architecture;Microprocessors;Deep learning;Biomedical imaging;Image segmentation;Nucleus detection;deep learning;convolutional neural networks;shape priors;learnable shapes},
doi={10.1109/TMI.2019.2895318},
ISSN={},
month={Sep.},}
@ARTICLE{8438559,
author={P. {Naylor} and M. {Laé} and F. {Reyal} and T. {Walter}},
journal={IEEE Transactions on Medical Imaging},
title={Segmentation of Nuclei in Histopathology Images by Deep Regression of the Distance Map},
year={2019},
volume={38},
number={2},
pages={448-459},
keywords={biological tissues;cancer;cellular biophysics;diseases;image segmentation;medical image processing;neural nets;patient diagnosis;histopathology images;deep regression;distance map;digital pathology;diseased tissue;quantitative profiles;prognosis tasks;interpretable models;cell nuclei;histopathology data;fully convolutional networks;segmentation problem;regression task;Haematoxylin and Eosin stained histopathology data;Image segmentation;Cancer;Pathology;Task analysis;Biology;Tumors;Computer architecture;Cancer research;deep learning;digital pathology;histopathology;nuclei segmentation},
doi={10.1109/TMI.2018.2865709},
ISSN={},
month={Feb},}
@InProceedings{10.1007/978-3-319-46723-8_18,
author="Dou, Qi
and Chen, Hao
and Jin, Yueming
and Yu, Lequan
and Qin, Jing
and Heng, Pheng-Ann",
editor="Ourselin, Sebastien
and Joskowicz, Leo
and Sabuncu, Mert R.
and Unal, Gozde
and Wells, William",
title="3D Deeply Supervised Network for Automatic Liver Segmentation from CT Volumes",
booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="149--157",
abstract="Automatic liver segmentation from CT volumes is a crucial prerequisite yet challenging task for computer-aided hepatic disease diagnosis and treatment. In this paper, we present a novel 3D deeply supervised network (3D DSN) to address this challenging task. The proposed 3D DSN takes advantage of a fully convolutional architecture which performs efficient end-to-end learning and inference. More importantly, we introduce a deep supervision mechanism during the learning process to combat potential optimization difficulties, and thus the model can acquire a much faster convergence rate and more powerful discrimination capability. On top of the high-quality score map produced by the 3D DSN, a conditional random field model is further employed to obtain refined segmentation results. We evaluated our framework on the public MICCAI-SLiver07 dataset. Extensive experiments demonstrated that our method achieves competitive segmentation results to state-of-the-art approaches with a much faster processing speed.",
isbn="978-3-319-46723-8"
}
@Article{Lu2017,
author="Lu, Fang
and Wu, Fa
and Hu, Peijun
and Peng, Zhiyi
and Kong, Dexing",
title="Automatic 3D liver location and segmentation via convolutional neural network and graph cut",
journal="International Journal of Computer Assisted Radiology and Surgery",
year="2017",
month="Feb",
day="01",
volume="12",
number="2",
pages="171--182",
abstract="Segmentation of the liver from abdominal computed tomography (CT) images is an essential step in some computer-assisted clinical interventions, such as surgery planning for living donor liver transplant, radiotherapy and volume measurement. In this work, we develop a deep learning algorithm with graph cut refinement to automatically segment the liver in CT scans.",
issn="1861-6429",
doi="10.1007/s11548-016-1467-3",
url="https://doi.org/10.1007/s11548-016-1467-3"
}
@InProceedings{10.1007/978-3-319-66182-7_79,
author="Zhou, Yuyin
and Xie, Lingxi
and Shen, Wei
and Wang, Yan
and Fishman, Elliot K.
and Yuille, Alan L.",
editor="Descoteaux, Maxime
and Maier-Hein, Lena
and Franz, Alfred
and Jannin, Pierre
and Collins, D. Louis
and Duchesne, Simon",
title="A Fixed-Point Model for Pancreas Segmentation in Abdominal CT Scans",
booktitle="Medical Image Computing and Computer Assisted Intervention − MICCAI 2017",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="693--701",
abstract="Deep neural networks have been widely adopted for automatic organ segmentation from abdominal CT scans. However, the segmentation accuracy of some small organs (e.g., the pancreas) is sometimes below satisfaction, arguably because deep networks are easily disrupted by the complex and variable background regions which occupies a large fraction of the input volume. In this paper, we formulate this problem into a fixed-point model which uses a predicted segmentation mask to shrink the input region. This is motivated by the fact that a smaller input region often leads to more accurate segmentation. In the training process, we use the ground-truth annotation to generate accurate input regions and optimize network weights. On the testing stage, we fix the network parameters and update the segmentation results in an iterative manner. We evaluate our approach on the NIH pancreas segmentation dataset, and outperform the state-of-the-art by more than {\$}{\$}4{\backslash}{\%}{\$}{\$}, measured by the average Dice-S{\o}rensen Coefficient (DSC). In addition, we report {\$}{\$}62.43{\backslash}{\%}{\$}{\$}DSC in the worst case, which guarantees the reliability of our approach in clinical applications.",
isbn="978-3-319-66182-7"
}
@InProceedings{10.1007/978-3-319-66179-7_77,
author="Cai, Jinzheng
and Lu, Le
and Xie, Yuanpu
and Xing, Fuyong
and Yang, Lin",
editor="Descoteaux, Maxime
and Maier-Hein, Lena
and Franz, Alfred
and Jannin, Pierre
and Collins, D. Louis
and Duchesne, Simon",
title="Pancreas Segmentation in MRI Using Graph-Based Decision Fusion on Convolutional Neural Networks",
booktitle="Medical Image Computing and Computer Assisted Intervention − MICCAI 2017",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="674--682",
abstract="Deep neural networks have demonstrated very promising performance on accurate segmentation of challenging organs (e.g., pancreas) in abdominal CT and MRI scans. The current deep learning approaches conduct pancreas segmentation by processing sequences of 2D image slices independently through deep, dense per-pixel masking for each image, without explicitly enforcing spatial consistency constraint on segmentation of successive slices. We propose a new convolutional/recurrent neural network architecture to address the contextual learning and segmentation consistency problem. A deep convolutional sub-network is first designed and pre-trained from scratch. The output layer of this network module is then connected to recurrent layers and can be fine-tuned for contextual learning, in an end-to-end manner. Our recurrent sub-network is a type of Long short-term memory (LSTM) network that performs segmentation on an image by integrating its neighboring slice segmentation predictions, in the form of a dependent sequence processing. Additionally, a novel segmentation-direct loss function (named Jaccard Loss) is proposed and deep networks are trained to optimize Jaccard Index (JI) directly. Extensive experiments are conducted to validate our proposed deep models, on quantitative pancreas segmentation using both CT and MRI scans. Our method outperforms the state-of-the-art work on CT [11] and MRI pancreas segmentation [1], respectively.",
isbn="978-3-319-66179-7"
}
@inproceedings{Khagi2019,
author = {Khagi, Bijen and Lee, Chung and Kwon, Goo-Rak},
year = {2019},
month = {01},
pages = {},
title = {Alzheimer's disease Classification from Brain MRI based on transfer learning from CNN},
doi = {10.1109/BMEiCON.2018.8609974}
}
@misc{alex20183d,
    title={3D CNN-based classification using sMRI and MD-DTI images for Alzheimer disease studies},
    author={Alexander Khvostikov and Karim Aderghal and Jenny Benois-Pineau and Andrey Krylov and Gwenaelle Catheline},
    year={2018},
    eprint={1801.05968},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
@inproceedings{Khvostikov2017ad,
author = {Khvostikov, Alexander and Benois-Pineau, Jenny and Krylov, Andrey and Catheline, Gwenaelle},
year = {2017},
month = {09},
pages = {},
title = {Classification methods on different imaging modalities for Alzheimer disease studies}
}
@article{ZHAO201898,
title = "A deep learning model integrating FCNNs and CRFs for brain tumor segmentation",
journal = "Medical Image Analysis",
volume = "43",
pages = "98 - 111",
year = "2018",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2017.10.002",
url = "http://www.sciencedirect.com/science/article/pii/S136184151730141X",
author = "Xiaomei Zhao and Yihong Wu and Guidong Song and Zhenye Li and Yazhuo Zhang and Yong Fan",
keywords = "Brain tumor segmentation, Fully convolutional neural networks, Conditional random fields, Deep learning",
abstract = "Accurate and reliable brain tumor segmentation is a critical component in cancer diagnosis, treatment planning, and treatment outcome evaluation. Build upon successful deep learning techniques, a novel brain tumor segmentation method is developed by integrating fully convolutional neural networks (FCNNs) and Conditional Random Fields (CRFs) in a unified framework to obtain segmentation results with appearance and spatial consistency. We train a deep learning based segmentation model using 2D image patches and image slices in following steps: 1) training FCNNs using image patches; 2) training CRFs as Recurrent Neural Networks (CRF-RNN) using image slices with parameters of FCNNs fixed; and 3) fine-tuning the FCNNs and the CRF-RNN using image slices. Particularly, we train 3 segmentation models using 2D image patches and slices obtained in axial, coronal and sagittal views respectively, and combine them to segment brain tumors using a voting based fusion strategy. Our method could segment brain images slice-by-slice, much faster than those based on image patches. We have evaluated our method based on imaging data provided by the Multimodal Brain Tumor Image Segmentation Challenge (BRATS) 2013, BRATS 2015 and BRATS 2016. The experimental results have demonstrated that our method could build a segmentation model with Flair, T1c, and T2 scans and achieve competitive performance as those built with Flair, T1, T1c, and T2 scans."
}














@unpublished{zhang-fan-2019-ad,
title={U-net based analysis of MRI for Alzheimer’s disease diagnosis {\Huge todo, and this paper is under publishing}},
author={Liang {Zhang} and Zhonghao {Fan}},
note={Unpublished},
}
@unpublished{zhang-zhang-2019-organ,
title={Block Level Skip Connections across Cascaded V-Net for Multi-organ Segmentation {\Huge todo, and this paper is under publishing}},
author={Liang Zhang and Jiaming Zhang},
note={Unpublished},
}
@unpublished{zhang-kong-2019-lpdc,
title={unknow},
author={Liang {Zhang} and Xiangwen {Kong}},
note={Unpublished},
}
