@article{DBLP:journals/corr/abs-1804-02967,
  author    = {Jose Dolz and
               Karthik Gopinath and
               Jing Yuan and
               Herve Lombaert and
               Christian Desrosiers and
               Ismail Ben Ayed},
  title     = {HyperDense-Net: {A} hyper-densely connected {CNN} for multi-modal
               image segmentation},
  journal   = {CoRR},
  volume    = {abs/1804.02967},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.02967},
  archivePrefix = {arXiv},
  eprint    = {1804.02967},
  timestamp = {Mon, 13 Aug 2018 16:48:02 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1804-02967},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{2018arXiv181002492K,
       author = {{Kumar}, Ashnil and {Fulham}, Michael and {Feng}, Dagan and {Kim},
        Jinman},
        title = "{Co-Learning Feature Fusion Maps from PET-CT Images of Lung Cancer}",
      journal = {ArXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2018,
        month = Oct,
          eid = {arXiv:1810.02492},
        pages = {arXiv:1810.02492},
archivePrefix = {arXiv},
       eprint = {1810.02492},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/#abs/2018arXiv181002492K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{2018arXiv181107407M,
   author = {{Mahmood}, F. and {Yang}, Z. and {Ashley}, T. and {Durr}, N.~J.
	},
    title = "{Multimodal Densenet}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1811.07407},
 primaryClass = "cs.CV",
 keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
     year = 2018,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181107407M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{Zhong20183DFC,
  title={3D fully convolutional networks for co-segmentation of tumors on PET-CT images},
  author={Zisha Zhong and Yusung Kim and Leixin Zhou and Kristin A. Plichta and Bryan Allen and John M. Buatti and Xiaodong Wu},
  journal={2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)},
  year={2018},
  pages={228-231}
}
@ARTICLE{6516899,
author={Q. Song and J. Bai and D. Han and S. Bhatia and W. Sun and W. Rockey and J. E. Bayouth and J. M. Buatti and X. Wu},
journal={IEEE Transactions on Medical Imaging},
title={Optimal Co-Segmentation of Tumor in PET-CT Images With Context Information},
year={2013},
volume={32},
number={9},
pages={1685-1697},
abstract={Positron emission tomography (PET)-computed tomography (CT) images have been widely used in clinical practice for radiotherapy treatment planning of the radiotherapy. Many existing segmentation approaches only work for a single imaging modality, which suffer from the low spatial resolution in PET or low contrast in CT. In this work, we propose a novel method for the co-segmentation of the tumor in both PET and CT images, which makes use of advantages from each modality: the functionality information from PET and the anatomical structure information from CT. The approach formulates the segmentation problem as a minimization problem of a Markov random field model, which encodes the information from both modalities. The optimization is solved using a graph-cut based method. Two sub-graphs are constructed for the segmentation of the PET and the CT images, respectively. To achieve consistent results in two modalities, an adaptive context cost is enforced by adding context arcs between the two sub-graphs. An optimal solution can be obtained by solving a single maximum flow problem, which leads to simultaneous segmentation of the tumor volumes in both modalities. The proposed algorithm was validated in robust delineation of lung tumors on 23 PET-CT datasets and two head-and-neck cancer subjects. Both qualitative and quantitative results show significant improvement compared to the graph cut methods solely using PET or CT.},
keywords={cancer;computerised tomography;graph theory;image resolution;image segmentation;lung;Markov processes;medical image processing;minimisation;positron emission tomography;tumours;tumor optimal cosegmentation;PET-CT Image;context information;positron emission tomography-computed tomography image;radiotherapy treatment planning;single imaging modality;low spatial resolution;functionality information;anatomical structure information;minimization problem;Markov random field model;graph-cut based method;adaptive context cost;context arcs;subgraph;optimal solution;single maximum flow problem;tumor volume simultaneous segmentation;lung tumor;23 PET-CT dataset;head-and-neck cancer subject;Computed tomography;Positron emission tomography;Tumors;Image segmentation;Context;Lungs;Computational modeling;Context information;global optimization;graph cut;image segmentation;lung tumor;Positron emission tomography-computed tomography (PET-CT);Algorithms;Databases, Factual;Head and Neck Neoplasms;Humans;Image Processing, Computer-Assisted;Markov Chains;Positron-Emission Tomography;Tomography, X-Ray Computed},
doi={10.1109/TMI.2013.2263388},
ISSN={0278-0062},
month={Sept},}
@inproceedings{Zhong20173DAM,
	title={3D Alpha Matting Based Co-segmentation of Tumors on PET-CT Images},
	author={Zisha Zhong and Yusung Kim and John M. Buatti and Xiaodong Wu},
	booktitle={CMMI/RAMBO/SWITCH@MICCAI},
	year={2017}
}
@INPROCEEDINGS{8363561,
author={Z. Zhong and Y. Kim and L. Zhou and K. Plichta and B. Allen and J. Buatti and X. Wu},
booktitle={2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)},
title={3D fully convolutional networks for co-segmentation of tumors on PET-CT images},
year={2018},
volume={},
number={},
pages={228-231},
keywords={biomedical MRI;cancer;computerised tomography;image classification;image segmentation;learning (artificial intelligence);lung;medical image processing;positron emission tomography;tumours;graph cut;co-segmentation model;final tumor segmentation results;lung cancer patients;3D fully convolutional networks;PET-CT images;positron emission tomography;computed tomography;dual-modality imaging;critical diagnostic information;automated accurate tumor delineation;tumor reading;semantic segmentation framework;probability maps;PET-CT scans;cancer diagnosis;Tumors;Computed tomography;Image segmentation;Biomedical imaging;Three-dimensional displays;Lung;image segmentation;lung tumor segmentation;co-segmentation;fully convolutional networks;deep learning},
doi={10.1109/ISBI.2018.8363561},
ISSN={1945-8452},
month={April},}
@article{Han2011petct,
author = {Han, Dongfeng and Bayouth, John and Song, Qi and Taurani, Aakant and Sonka, Milan and Buatti, John and Wu, Xiaodong},
year = {2011},
month = {07},
pages = {245-56},
title = {Globally Optimal Tumor Segmentation in PET-CT Images: A Graph-Based Co-segmentation Method},
volume = {22},
journal = {Information processing in medical imaging : proceedings of the ... conference},
doi = {10.1007/978-3-642-22092-0_21}
}
@ARTICLE{2014arXiv1412.0767T,
       author = {{Tran}, Du and {Bourdev}, Lubomir and {Fergus}, Rob and {Torresani},
        Lorenzo and {Paluri}, Manohar},
        title = "{Learning Spatiotemporal Features with 3D Convolutional Networks}",
      journal = {ArXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2014,
        month = Dec,
          eid = {arXiv:1412.0767},
        pages = {arXiv:1412.0767},
archivePrefix = {arXiv},
       eprint = {1412.0767},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/#abs/2014arXiv1412.0767T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{7294713,
author={W. Ju and D. Xiang and B. Zhang and L. Wang and I. Kopriva and X. Chen},
journal={IEEE Transactions on Image Processing},
title={Random Walk and Graph Cut for Co-Segmentation of Lung Tumor on PET-CT Images},
year={2015},
volume={24},
number={12},
pages={5854-5867},
abstract={Accurate lung tumor delineation plays an important role in radiotherapy treatment planning. Since the lung tumor has poor boundary in positron emission tomography (PET) images and low contrast in computed tomography (CT) images, segmentation of tumor in the PET and CT images is a challenging task. In this paper, we effectively integrate the two modalities by making fully use of the superior contrast of PET images and superior spatial resolution of CT images. Random walk and graph cut method is integrated to solve the segmentation problem, in which random walk is utilized as an initialization tool to provide object seeds for graph cut segmentation on the PET and CT images. The co-segmentation problem is formulated as an energy minimization problem which is solved by max-flow/min-cut method. A graph, including two sub-graphs and a special link, is constructed, in which one sub-graph is for the PET and another is for CT, and the special link encodes a context term which penalizes the difference of the tumor segmentation on the two modalities. To fully utilize the characteristics of PET and CT images, a novel energy representation is devised. For the PET, a downhill cost and a 3D derivative cost are proposed. For the CT, a shape penalty cost is integrated into the energy function which helps to constrain the tumor region during the segmentation. We validate our algorithm on a data set which consists of 18 PET-CT images. The experimental results indicate that the proposed method is superior to the graph cut method solely using the PET or CT is more accurate compared with the random walk method, random walk co-segmentation method, and non-improved graph cut method.},
keywords={computerised tomography;image segmentation;lung;medical image processing;minimisation;positron emission tomography;tumours;lung tumor;PET-CT imaging;positron emission tomography;spatial resolution;graph cut segmentation method;energy minimization problem;max-flow-min-cut method;special link encodes;tumor segmentation;energy representation;3D derivative cost;shape penalty;energy function;random walk cosegmentation method;Positron emission tomography;Tumors;Computed tomography;Image segmentation;Lungs;Context;Three-dimensional displays;image segmentation;interactive segmentation;graph cut;random walk;prior information;lung tumor;Positron Emission Tomography (PET);Computed Tomography (CT);Image segmentation;interactive segmentation;graph cut;random walk;prior information;lung tumor;positron emission tomography (PET);computed tomography (CT);Algorithms;Carcinoma, Non-Small-Cell Lung;Databases, Factual;Humans;Image Processing, Computer-Assisted;Lung Neoplasms;Positron-Emission Tomography;Tomography, X-Ray Computed},
doi={10.1109/TIP.2015.2488902},
ISSN={1057-7149},
month={Dec},}

@article{minati2009current,
title={Current concepts in Alzheimer's disease: a multidisciplinary review.},
author={Minati, Ludovico and Edginton, Trudi and Bruzzone, Maria Grazia and Giaccone, Giorgio},
journal={American Journal of Alzheimers Disease and Other Dementias},
volume={24},
number={2},
pages={95--121},
year={2009}}
@ARTICLE{7426413,
author={S. {Pereira} and A. {Pinto} and V. {Alves} and C. A. {Silva}},
journal={IEEE Transactions on Medical Imaging},
title={Brain Tumor Segmentation Using Convolutional Neural Networks in MRI Images},
year={2016},
volume={35},
number={5},
pages={1240-1251},
keywords={biomedical MRI;brain;cancer;image segmentation;medical image processing;neurophysiology;tumours;brain tumor segmentation;convolutional neural networks;MRI images;gliomas;quality-of-life;oncological patients;magnetic resonance imaging;imaging technique;manual segmentation;precise quantitative measurements;clinical practice;automatic segmentation methods;reliable segmentation methods;spatial variability;structural variability;automatic segmentation;kernels;intensity normalization;preprocessing step;CNN-based segmentation methods;data augmentation;Dice similarity coefficient metrics;online evaluation platform;on-site BRATS 2015 Challenge;Tumors;Image segmentation;Magnetic resonance imaging;Kernel;Training;Brain modeling;Context;Brain tumor;brain tumor segmentation;convolutional neural networks;deep learning;glioma;magnetic resonance imaging;Brain Neoplasms;Glioma;Humans;Image Interpretation, Computer-Assisted;Machine Learning;Magnetic Resonance Imaging;Neural Networks (Computer)},
doi={10.1109/TMI.2016.2538465},
ISSN={},
month={May},}
@article{Litjens2016,
author = {Litjens, Geert and Sánchez, Clara and Timofeeva, Nadya and Hermsen, Meyke and Nagtegaal, Iris and Kovacs, Iringo and Hulsbergen-van de Kaa, Christina and Bult, Peter and Ginneken, Bram and van der Laak, Jeroen},
year = {2016},
month = {05},
pages = {26286},
title = {Deep learning as a tool for increased accuracy and efficiency of histopathological diagnosis},
volume = {6},
journal = {Scientific Reports},
doi = {10.1038/srep26286}
}
@article{10.1371/journal.pmed.1002711,
    author = {Hosny, Ahmed AND Parmar, Chintan AND Coroller, Thibaud P. AND Grossmann, Patrick AND Zeleznik, Roman AND Kumar, Avnish AND Bussink, Johan AND Gillies, Robert J. AND Mak, Raymond H. AND Aerts, Hugo J. W. L.},
    journal = {PLOS Medicine},
    publisher = {Public Library of Science},
    title = {Deep learning for lung cancer prognostication: A retrospective multi-cohort radiomics study},
    year = {2018},
    month = {11},
    volume = {15},
    url = {https://doi.org/10.1371/journal.pmed.1002711},
    pages = {1-25},
    abstract = {Hugo Aerts and colleagues evaluate the ability of deep learning networks to extract relevant features from computed tomography lung cancer images and stratify patients into low and high mortality risk groups.},
    number = {11},
    doi = {10.1371/journal.pmed.1002711}
}

@article{LITJENS201760,
title = "A survey on deep learning in medical image analysis",
journal = "Medical Image Analysis",
volume = "42",
pages = "60 - 88",
year = "2017",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2017.07.005",
url = "http://www.sciencedirect.com/science/article/pii/S1361841517301135",
author = "Geert Litjens and Thijs Kooi and Babak Ehteshami Bejnordi and Arnaud Arindra Adiyoso Setio and Francesco Ciompi and Mohsen Ghafoorian and Jeroen A.W.M. van der Laak and Bram van Ginneken and Clara I. Sánchez",
keywords = "Deep learning, Convolutional neural networks, Medical imaging, Survey",
abstract = "Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research."
}
@article{YI2019101552,
title = "Generative adversarial network in medical imaging: A review",
journal = "Medical Image Analysis",
volume = "58",
pages = "101552",
year = "2019",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2019.101552",
url = "http://www.sciencedirect.com/science/article/pii/S1361841518308430",
author = "Xin Yi and Ekta Walia and Paul Babyn",
keywords = "Deep learning, Generative adversarial network, Generative model, Medical imaging, Review",
abstract = "Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique."
}
@article{WANG2020101565,
title = "Semi-supervised mp-MRI data synthesis with StitchLayer and auxiliary distance maximization",
journal = "Medical Image Analysis",
volume = "59",
pages = "101565",
year = "2020",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2019.101565",
url = "http://www.sciencedirect.com/science/article/pii/S1361841519301057",
author = "Zhiwei Wang and Yi Lin and Kwang-Ting (Tim) Cheng and Xin Yang",
keywords = "Generative models, GAN, Multimodal image synthesis, Deep learning",
abstract = "The availability of a large amount of annotated data is critical for many medical image analysis applications, in particular for those relying on deep learning methods which are known to be data-hungry. However, annotated medical data, especially multimodal data, is often scarce and costly to obtain. In this paper, we address the problem of synthesizing multi-parameter magnetic resonance imaging data (i.e. mp-MRI), which typically consists of Apparent Diffusion Coefficient (ADC) and T2-weighted (T2w) images, containing clinically significant (CS) prostate cancer (PCa) via semi-supervised learning and adversarial learning. Specifically, our synthesizer generates mp-MRI data in a sequential manner: first utilizing a decoder to generate an ADC map from a 128-d latent vector, followed by translating the ADC to the T2w image via U-Net. The synthesizer is trained in a semi-supervised manner. In the supervised training process, a limited amount of paired ADC-T2w images and the corresponding ADC encodings are provided and the synthesizer learns the paired relationship by explicitly minimizing the reconstruction losses between synthetic and real images. To avoid overfitting limited ADC encodings, an unlimited amount of random latent vectors and unpaired ADC-T2w Images are utilized in the unsupervised training process for learning the marginal image distributions of real images. To improve the robustness for training the synthesizer, we decompose the difficult task of generating full-size images into several simpler tasks which generate sub-images only. A StitchLayer is then employed to seamlessly fuse sub-images together in an interlaced manner into a full-size image. In addition, to enforce the synthetic images to indeed contain distinguishable CS PCa lesions, we propose to also maximize an auxiliary distance of Jensen-Shannon divergence (JSD) between CS and nonCS images. Experimental results show that our method can effectively synthesize a large variety of mp-MRI images which contain meaningful CS PCa lesions, display a good visual quality and have the correct paired relationship between the two modalities of a pair. Compared to the state-of-the-art methods based on adversarial learning (Liu and Tuzel, 2016; Costa et al., 2017), our method achieves a significant improvement in terms of both visual quality and several popular quantitative evaluation metrics."
}
@article{SWIDERSKACHADAJ2019101547,
title = "Learning to detect lymphocytes in immunohistochemistry with deep learning",
journal = "Medical Image Analysis",
volume = "58",
pages = "101547",
year = "2019",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2019.101547",
url = "http://www.sciencedirect.com/science/article/pii/S1361841519300829",
author = "Zaneta Swiderska-Chadaj and Hans Pinckaers and Mart van Rijthoven and Maschenka Balkenhol and Margarita Melnikova and Oscar Geessink and Quirine Manson and Mark Sherman and Antonio Polonia and Jeremy Parry and Mustapha Abubakar and Geert Litjens and Jeroen van der Laak and Francesco Ciompi",
keywords = "Deep learning, Immune cell detection, Computational pathology, Immunohistochemistry",
abstract = "The immune system is of critical importance in the development of cancer. The evasion of destruction by the immune system is one of the emerging hallmarks of cancer. We have built a dataset of 171,166 manually annotated CD3+ and CD8+ cells, which we used to train deep learning algorithms for automatic detection of lymphocytes in histopathology images to better quantify immune response. Moreover, we investigate the effectiveness of four deep learning based methods when different subcompartments of the whole-slide image are considered: normal tissue areas, areas with immune cell clusters, and areas containing artifacts. We have compared the proposed methods in breast, colon and prostate cancer tissue slides collected from nine different medical centers. Finally, we report the results of an observer study on lymphocyte quantification, which involved four pathologists from different medical centers, and compare their performance with the automatic detection. The results give insights on the applicability of the proposed methods for clinical use. U-Net obtained the highest performance with an F1-score of 0.78 and the highest agreement with manual evaluation (κ=0.72), whereas the average pathologists agreement with reference standard was κ=0.64. The test set and the automatic evaluation procedure are publicly available at lyon19.grand-challenge.org."
}
@article{JAMALUDIN201763,
title = "SpineNet: Automated classification and evidence visualization in spinal MRIs",
journal = "Medical Image Analysis",
volume = "41",
pages = "63 - 73",
year = "2017",
note = "Special Issue on the 2016 Conference on Medical Image Computing and Computer Assisted Intervention (Analog to MICCAI 2015)",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2017.07.002",
url = "http://www.sciencedirect.com/science/article/pii/S136184151730110X",
author = "Amir Jamaludin and Timor Kadir and Andrew Zisserman",
keywords = "MRI analysis, Radiological classification, Spinal MRI",
abstract = "The objective of this work is to automatically produce radiological gradings of spinal lumbar MRIs and also localize the predicted pathologies. We show that this can be achieved via a Convolutional Neural Network (CNN) framework that takes intervertebral disc volumes as inputs and is trained only on disc-specific class labels. Our contributions are: (i) a CNN architecture that predicts multiple gradings at once, and we propose variants of the architecture including using 3D convolutions; (ii) showing that this architecture can be trained using a multi-task loss function without requiring segmentation level annotation; and (iii) a localization method that clearly shows pathological regions in the disc volumes. We compare three visualization methods for the localization. The network is applied to a large corpus of MRI T2 sagittal spinal MRIs (using a standard clinical scan protocol) acquired from multiple machines, and is used to automatically compute disk and vertebra gradings for each MRI. These are: Pfirrmann grading, disc narrowing, upper/lower endplate defects, upper/lower marrow changes, spondylolisthesis, and central canal stenosis. We report near human performances across the eight gradings, and also visualize the evidence for these gradings localized on the original scans."
}
@article{JAMALUDIN201763,
title = "SpineNet: Automated classification and evidence visualization in spinal MRIs",
journal = "Medical Image Analysis",
volume = "41",
pages = "63 - 73",
year = "2017",
note = "Special Issue on the 2016 Conference on Medical Image Computing and Computer Assisted Intervention (Analog to MICCAI 2015)",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2017.07.002",
url = "http://www.sciencedirect.com/science/article/pii/S136184151730110X",
author = "Amir Jamaludin and Timor Kadir and Andrew Zisserman",
keywords = "MRI analysis, Radiological classification, Spinal MRI",
abstract = "The objective of this work is to automatically produce radiological gradings of spinal lumbar MRIs and also localize the predicted pathologies. We show that this can be achieved via a Convolutional Neural Network (CNN) framework that takes intervertebral disc volumes as inputs and is trained only on disc-specific class labels. Our contributions are: (i) a CNN architecture that predicts multiple gradings at once, and we propose variants of the architecture including using 3D convolutions; (ii) showing that this architecture can be trained using a multi-task loss function without requiring segmentation level annotation; and (iii) a localization method that clearly shows pathological regions in the disc volumes. We compare three visualization methods for the localization. The network is applied to a large corpus of MRI T2 sagittal spinal MRIs (using a standard clinical scan protocol) acquired from multiple machines, and is used to automatically compute disk and vertebra gradings for each MRI. These are: Pfirrmann grading, disc narrowing, upper/lower endplate defects, upper/lower marrow changes, spondylolisthesis, and central canal stenosis. We report near human performances across the eight gradings, and also visualize the evidence for these gradings localized on the original scans."
}
@ARTICLE{8371286,
author={F. {Hohman} and M. {Kahng} and R. {Pienta} and D. H. {Chau}},
journal={IEEE Transactions on Visualization and Computer Graphics},
title={Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers},
year={2019},
volume={25},
number={8},
pages={2674-2693},
keywords={Machine learning;Conferences;Visual analytics;Data visualization;Neural networks;Computational modeling;Deep learning;visual analytics;information visualization;neural networks},
doi={10.1109/TVCG.2018.2843369},
ISSN={},
month={Aug},}
@article{LIU2019101555,
title = "Automated detection and classification of thyroid nodules in ultrasound images using clinical-knowledge-guided convolutional neural networks",
journal = "Medical Image Analysis",
volume = "58",
pages = "101555",
year = "2019",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2019.101555",
url = "http://www.sciencedirect.com/science/article/pii/S1361841519300970",
author = "Tianjiao Liu and Qianqian Guo and Chunfeng Lian and Xuhua Ren and Shujun Liang and Jing Yu and Lijuan Niu and Weidong Sun and Dinggang Shen",
keywords = "Ultrasound image, Thyroid nodule, Convolutional neural networks, Clinical knowledge",
abstract = "Accurate diagnosis of thyroid nodules using ultrasonography is a valuable but tough task even for experienced radiologists, considering both benign and malignant nodules have heterogeneous appearances. Computer-aided diagnosis (CAD) methods could potentially provide objective suggestions to assist radiologists. However, the performance of existing learning-based approaches is still limited, for direct application of general learning models often ignores critical domain knowledge related to the specific nodule diagnosis. In this study, we propose a novel deep-learning-based CAD system, guided by task-specific prior knowledge, for automated nodule detection and classification in ultrasound images. Our proposed CAD system consists of two stages. First, a multi-scale region-based detection network is designed to learn pyramidal features for detecting nodules at different feature scales. The region proposals are constrained by the prior knowledge about size and shape distributions of real nodules. Then, a multi-branch classification network is proposed to integrate multi-view diagnosis-oriented features, in which each network branch captures and enhances one specific group of characteristics that were generally used by radiologists. We evaluated and compared our method with the state-of-the-art CAD methods and experienced radiologists on two datasets, i.e. Dataset I and Dataset II. The detection and diagnostic accuracy on Dataset I were 97.5% and 97.1%, respectively. Besides, our CAD system also achieved better performance than experienced radiologists on Dataset II, with improvements of accuracy for 8%. The experimental results demonstrate that our proposed method is effective in the discrimination of thyroid nodules."
}
@article{JIMENEZCARRETERO2019144,
title = "A graph-cut approach for pulmonary artery-vein segmentation in noncontrast CT images",
journal = "Medical Image Analysis",
volume = "52",
pages = "144 - 159",
year = "2019",
issn = "1361-8415",
doi = "https://doi.org/10.1016/j.media.2018.11.011",
url = "http://www.sciencedirect.com/science/article/pii/S1361841518308740",
author = "Daniel Jimenez-Carretero and David Bermejo-Peláez and Pietro Nardelli and Patricia Fraga and Eduardo Fraile and Raúl San José Estépar and Maria J Ledesma-Carbayo",
keywords = "Artery-vein segmentation, Lung, Graph-cuts, Random forest, Arteries, Veins, Noncontrast CT, Phantoms",
abstract = "Lung vessel segmentation has been widely explored by the biomedical image processing community; however, the differentiation of arterial from venous irrigation is still a challenge. Pulmonary artery–vein (AV) segmentation using computed tomography (CT) is growing in importance owing to its undeniable utility in multiple cardiopulmonary pathological states, especially those implying vascular remodelling, allowing the study of both flow systems separately. We present a new framework to approach the separation of tree-like structures using local information and a specifically designed graph-cut methodology that ensures connectivity as well as the spatial and directional consistency of the derived subtrees. This framework has been applied to the pulmonary AV classification using a random forest (RF) pre-classifier to exploit the local anatomical differences of arteries and veins. The evaluation of the system was performed using 192 bronchopulmonary segment phantoms, 48 anthropomorphic pulmonary CT phantoms, and 26 lungs from noncontrast CT images with precise voxel-based reference standards obtained by manually labelling the vessel trees. The experiments reveal a relevant improvement in the accuracy ( ∼ 20%) of the vessel particle classification with the proposed framework with respect to using only the pre-classification based on local information applied to the whole area of the lung under study. The results demonstrated the accurate differentiation between arteries and veins in both clinical and synthetic cases, specifically when the image quality can guarantee a good airway segmentation, which opens a huge range of possibilities in the clinical study of cardiopulmonary diseases."
}